{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "TPOT gives the user a lot of options for customizing the search space, from hyperparameter ranges to model selection to pipeline configuration. TPOT is able to select models, optimize their hyperparameters, and build a complex pipeline structure. Each level of detail has multiple customization options. This tutorial will first explore how to set up a hyperparameter search space for a single method. Next, we will describe how to set up simultaneous model selection and hyperparameter tuning. Finally, we will cover how to utilize these steps to configure a search space for a fixed pipeline of multiple steps, as well as having TPOT optimize the pipeline structure itself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Search Spaces with ConfigSpace\n",
    "\n",
    "Hyperparameter search spaces are defined using the [ConfigSpace package found here](https://github.com/automl/ConfigSpace). More information on how to set up a hyperparameter space can be found in their [documentation here](https://automl.github.io/ConfigSpace/main/guide.html).\n",
    "\n",
    "TPOT uses `ConfigSpace.ConfigurationSpace` objects to define the hyperparameter search space for individual models. This object can be used to keep track of the desired hyperparameters as well as provide functions for random sampling from this space.\n",
    "\n",
    "In short, you can use the `Integer`, `Float`, and `Categorical` functions of `ConfigSpace` to define a range of values used for each param. Alternatively, a tuple with (min,max) ints or floats can be used to specify an int/float search space and a list can be used to specify a categorical search space. A fixed value can also be provided for parameters that are not tunned. The space parameter of `ConfigurationSpace` takes in a dictionary of param names to these ranges.\n",
    "\n",
    "Note: If you want reproducible results, you need to set a fixed random_state in the search space.\n",
    "\n",
    "Here is an example of a hyperparameter range for RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConfigSpace import ConfigurationSpace\n",
    "from ConfigSpace import ConfigurationSpace, Integer, Float, Categorical, Normal\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import tpot2\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "rf_configspace = ConfigurationSpace(\n",
    "    space = {\n",
    "            'n_estimators': 128, #as recommended by Oshiro et al. (2012\n",
    "            'max_features': Float(\"max_features\", bounds=(0.01,1), log=True), #log scale like autosklearn?\n",
    "            'criterion': Categorical(\"criterion\", ['gini', 'entropy']),\n",
    "            'min_samples_split': Integer(\"min_samples_split\", bounds=(2, 20)),\n",
    "            'min_samples_leaf': Integer(\"min_samples_leaf\", bounds=(1, 20)),\n",
    "            'bootstrap': Categorical(\"bootstrap\", [True, False]),\n",
    "            #random_state = 1, # If you want results to be reproducible, you can set a fixed random_state.\n",
    "        }\n",
    ")\n",
    "\n",
    "hyperparameters = dict(rf_configspace.sample_configuration())\n",
    "print(\"sampled hyperparameters\")\n",
    "print(hyperparameters)\n",
    "\n",
    "rf = RandomForestClassifier(**hyperparameters)\n",
    "rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_configspace = ConfigurationSpace(\n",
    "    space = {\n",
    "            'n_estimators': 128, #as recommended by Oshiro et al. (2012\n",
    "            'max_features':(0.01,1), #not log scaled\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'min_samples_split': (2, 20),\n",
    "            'min_samples_leaf': (1, 20),\n",
    "            'bootstrap': [True, False],\n",
    "            #random_state = 1, # If you want results to be reproducible, you can set a fixed random_state.\n",
    "        }\n",
    ")\n",
    "\n",
    "hyperparameters = dict(rf_configspace.sample_configuration())\n",
    "print(\"sampled hyperparameters\")\n",
    "print(hyperparameters)\n",
    "\n",
    "rf = RandomForestClassifier(**hyperparameters)\n",
    "rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPOT Search spaces\n",
    "\n",
    "TPOT allows you to create hyperparameter search spaces for individual methods and pipeline structure search spaces. For example, TPOT can create linear pipelines, trees, or graphs. \n",
    "\n",
    "TPOT search spaces are found in the `search_spaces` module. There are two primary kinds of search spaces, node and pipeline. Node search spaces specify a single sklearn `BaseEstimator` search space. Pipeline search spaces define the possible structures for a group of node search spaces. These take in node search spaces and produce a pipeline using nodes from that search space. Since sklearn Pipelines are also `BaseEstimator`, pipeline search spaces are also technically node search spaces. This means that pipeline search spaces can take in other pipeline search spaces in order to define more complex structures. The primary differentiating factor between node and pipeline search spaces is that pipeline search spaces must take in another search space as input to feed its individual nodes. Therefore, all search spaces eventually end in a node search space at the lowest level. Note that parameters for pipeline search spaces can differ, some take in only a single search space, some take in a list, or some take in multiple defined parameters.\n",
    "\n",
    "## node search spaces\n",
    "\n",
    "\n",
    "| Name      | Info       |\n",
    "| :---        |    :----:   |\n",
    "| EstimatorNode | Takes in a ConfigSpace along with the class of the method. This node will optimize the hyperparameters for a single method. |\n",
    "| GeneticFeatureSelectorNode | Uses evolution to optimize a set of features, exports a basic sklearn Selector that simply selects the features chosen by the node. |\n",
    "| FSSNode | FSS stands for FeatureSetSelector. This node takes in a list of user-defined subsets of features and selects a single predefined subset. Note that TPOT will not create new subsets nor will it select multiple subsets per node. If using a linear pipeline, this node should be set as the first step. In linear pipelines it is recommended that you only use a small number of feature sets. I recommend exploring using FSSNodes in pipelines that allow TPOT to select more than one FSSNode at a time. For example, DynamicUnionPipeline and GraphPipeline are both excellent combos for FSSNode. Use FFSNode inside a DynamicUnionPipeline at the start of linear pipeline to explore optimal combinations of subsets in linear pipelines. Set the leaf_search_space of GraphSearchPipeline TPOT can use multiple feature sets in different ways, for example, with different transformers for different sets. |\n",
    "\n",
    "\n",
    "\n",
    "## pipeline search spaces\n",
    "\n",
    "found in tpot2.search_spaces.pipelines\n",
    "\n",
    "WrapperPipeline -         This search space is for wrapping a sklearn estimator with a method that takes another estimator and hyperparameters as arguments.\n",
    "        For example, this can be used with sklearn.ensemble.BaggingClassifier or sklearn.ensemble.AdaBoostClassifier.\n",
    "\n",
    "\n",
    "| Name      | Info       |\n",
    "| :---        |    :----:   |\n",
    "| ChoicePipeline | Takes in a list of search spaces. Will select one node from the search space. |\n",
    "| SequentialPipeline | Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index. |\n",
    "| DynamicLinearPipeline | Takes in a single search space. Will produce a linear pipeline of variable length. Each step in the pipeline will be pulled from the search space provided. |\n",
    "| UnionPipeline | Takes in a list of search spaces. The returned pipeline will include one estimator per search space joined in an sklearn FeatureUnion. Useful for having many steps in one layer.  |\n",
    "| DynamicUnionPipeline | Takes in a single search space. It will pull anywhere from 1 to max_estimators number of estimators from the search space and concatenate them in a FeatureUnion. |\n",
    "| TreePipeline |Generates a pipeline of variable length. Pipeline will have a tree structure similar to TPOT1. |\n",
    "| GraphSearchPipeline | Generates a directed acyclic graph of variable size. Search spaces for root, leaf, and inner nodes can be defined separately if desired. |\n",
    "| WrapperPipeline   | This search space is for wrapping a sklearn estimator with a method that takes another estimator and hyperparameters as arguments. For example, this can be used with sklearn.ensemble.BaggingClassifier or sklearn.ensemble.AdaBoostClassifier.        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Search Space Examples\n",
    "\n",
    "Node search spaces represent the smallest unit of an sklearn pipeline. All node search spaces create and optimize a single node which exports a single estimator object. For example this could be a KNeighborsClassifier or a FeatureSetSelector.\n",
    "\n",
    "### EstimatorNode\n",
    "\n",
    "The EstimatorNode represents the hyperparameter search space for a scikit-learn estimator. \n",
    "\n",
    "Note that `ConfigSpace` doesn't support `None` in its search space, and does not support the booleans True or False as fixed parameters (though booleans seem to be allowed in Categorical search spaces). To get around this, use the macros defined in:\n",
    "\n",
    "`from tpot2.search_spaces.nodes.estimator_node import NONE_SPECIAL_STRING, TRUE_SPECIAL_STRING, FALSE_SPECIAL_STRING`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tpot2\n",
    "from ConfigSpace import ConfigurationSpace\n",
    "from ConfigSpace import ConfigurationSpace, Integer, Float, Categorical, Normal\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_configspace = ConfigurationSpace(\n",
    "    space = {\n",
    "\n",
    "        'n_neighbors': Integer(\"n_neighbors\", bounds=(1, 10)),\n",
    "        'weights': Categorical(\"weights\", ['uniform', 'distance']),\n",
    "        'p': Integer(\"p\", bounds=(1, 3)),\n",
    "        'metric': Categorical(\"metric\", ['euclidean', 'minkowski']),\n",
    "        'n_jobs': 1,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "knn_node = tpot2.search_spaces.nodes.EstimatorNode(\n",
    "    method = KNeighborsClassifier,\n",
    "    space = knn_configspace,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can sample generate an individual with the generate() function. This individual samples from the search space as well as provides mutation and crossover functions to modify the current sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_individual = knn_node.generate()\n",
    "knn_individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sampled hyperparameters\")\n",
    "print(knn_individual.hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Individual objects have mutation and crossover operators that TPOT uses to optimize the pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_individual.mutate() # mutate the individual\n",
    "print(\"mutated hyperparameters\")\n",
    "print(knn_individual.hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TPOT2, crossover only modifies the individual calling the crossover function, the second individual remains the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_individual1 = knn_node.generate()\n",
    "knn_individual2 = knn_node.generate()\n",
    "\n",
    "print(\"original hyperparameters for individual 1\")\n",
    "print(knn_individual1.hyperparameters)\n",
    "\n",
    "print(\"original hyperparameters for individual 2\")\n",
    "print(knn_individual2.hyperparameters)\n",
    "\n",
    "print()\n",
    "\n",
    "knn_individual1.crossover(knn_individual2) # crossover the individuals\n",
    "print(\"post crossover hyperparameters for individual 1\")\n",
    "print(knn_individual1.hyperparameters)\n",
    "print(\"post crossover hyperparameters for individual 2\")\n",
    "print(knn_individual2.hyperparameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All search spaces have an export_pipeline function that returns an sklearn `BaseEstimator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = knn_individual1.export_pipeline()\n",
    "est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a dictionary of parameters is passed instead of of a ConfigSpace object, then the hyperparameters will always be fixed and not learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tpot2\n",
    "from ConfigSpace import ConfigurationSpace\n",
    "from ConfigSpace import ConfigurationSpace, Integer, Float, Categorical, Normal\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "space = {\n",
    "\n",
    "    'n_neighbors':10,\n",
    "}\n",
    "\n",
    "knn_node = tpot2.search_spaces.nodes.EstimatorNode(\n",
    "    method = KNeighborsClassifier,\n",
    "    space = space,\n",
    ")\n",
    "\n",
    "knn_node.generate().export_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FSSNode and GeneticFeatureSelectorNode\n",
    "\n",
    "Both of these are given their own tutorials. See Tutorial 3 for FFSNode and Tutorial 5 for GeneticFeatureSelectorNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Search Space Examples\n",
    "\n",
    "Pipeline search spaces are used to define the structure and restrictions of the pipelines TPOT can search. Unlike Node search spaces, all pipeline search spaces take in other search spaces as inputs. Rather than sample hyperparameters, pipeline search spaces can select models from the input search spaces and organize them within a linear sklearn Pipeline or a TPOT GraphPipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChoicePipeline\n",
    "\n",
    "The simplest pipeline search space is the ChoicePipeline. This takes in a list of search spaces and simply selects and samples from one. In this example, we will construct a search space that takes in several options for a classifier. The resulting search space will then first select a model from KNeighborsClassifier, LogisticRegression or DecisionTreeClassifier, and then select the hyperparameters for the given model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tpot2\n",
    "from ConfigSpace import ConfigurationSpace\n",
    "from ConfigSpace import ConfigurationSpace, Integer, Float, Categorical, Normal\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "knn_configspace = ConfigurationSpace(\n",
    "    space = {\n",
    "\n",
    "        'n_neighbors': Integer(\"n_neighbors\", bounds=(1, 10)),\n",
    "        'weights': Categorical(\"weights\", ['uniform', 'distance']),\n",
    "        'p': Integer(\"p\", bounds=(1, 3)),\n",
    "        'metric': Categorical(\"metric\", ['euclidean', 'minkowski']),\n",
    "        'n_jobs': 1,\n",
    "    }\n",
    ")\n",
    "\n",
    "lr_configspace = ConfigurationSpace(\n",
    "        space = {\n",
    "            'solver': Categorical(\"solver\", ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']),\n",
    "            'penalty': Categorical(\"penalty\", ['l1', 'l2']),\n",
    "            'dual': Categorical(\"dual\", [True, False]),\n",
    "            'C': Float(\"C\", bounds=(1e-4, 1e4), log=True),\n",
    "            'class_weight': Categorical(\"class_weight\", ['balanced']),\n",
    "            'n_jobs': 1,\n",
    "            'max_iter': 1000,\n",
    "        }\n",
    "    )\n",
    "\n",
    "dt_configspace = ConfigurationSpace(\n",
    "        space = {\n",
    "            'criterion': Categorical(\"criterion\", ['gini', 'entropy']),\n",
    "            'max_depth': Integer(\"max_depth\", bounds=(1, 11)),\n",
    "            'min_samples_split': Integer(\"min_samples_split\", bounds=(2, 21)),\n",
    "            'min_samples_leaf': Integer(\"min_samples_leaf\", bounds=(1, 21)),\n",
    "            'max_features': Categorical(\"max_features\", ['sqrt', 'log2']),\n",
    "            'min_weight_fraction_leaf': 0.0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "knn_node = tpot2.search_spaces.nodes.EstimatorNode(\n",
    "    method = KNeighborsClassifier,\n",
    "    space = knn_configspace,\n",
    ")\n",
    "\n",
    "lr_node = tpot2.search_spaces.nodes.EstimatorNode(\n",
    "    method = LogisticRegression,\n",
    "    space = lr_configspace,\n",
    ")\n",
    "\n",
    "dt_node = tpot2.search_spaces.nodes.EstimatorNode(\n",
    "    method = DecisionTreeClassifier,\n",
    "    space = dt_configspace,\n",
    ")\n",
    "\n",
    "classifier_node = tpot2.search_spaces.pipelines.ChoicePipeline(\n",
    "    search_spaces=[\n",
    "        knn_node,\n",
    "        lr_node,\n",
    "        dt_node,\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "tpot2.search_spaces.pipelines.ChoicePipeline(\n",
    "    search_spaces = [\n",
    "        tpot2.search_spaces.nodes.EstimatorNode(\n",
    "            method = KNeighborsClassifier,\n",
    "            space = knn_configspace,\n",
    "            ),\n",
    "        tpot2.search_spaces.nodes.EstimatorNode(\n",
    "            method = LogisticRegression,\n",
    "            space = lr_configspace,\n",
    "        ),\n",
    "        tpot2.search_spaces.nodes.EstimatorNode(\n",
    "            method = DecisionTreeClassifier,\n",
    "            space = dt_configspace,\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search space objects provided by pipeline search spaces work the same as with node search spaces. Note that crossover only works when both individuals have sampled the same method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_individual = classifier_node.generate()\n",
    "\n",
    "print(\"sampled pipeline\")\n",
    "classifier_individual.export_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mutated pipeline\")\n",
    "classifier_individual.mutate()\n",
    "classifier_individual.export_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TPOT2 also comes with predefined search spaces. The current search spaces were adapted from a combination of the original TPOT package as well as the search spaces used in [AutoSklearn](https://github.com/automl/auto-sklearn/tree/development/autosklearn/pipeline/components). The helper function `tpot2.config.get_search_space` takes in a string or a list of strings, and returns either a EstimatorNode or a ChoicePipeline,respectively. \n",
    "\n",
    "strings can correspond to individual methods. There are also special strings that return predefined lists of methods. \n",
    "\n",
    "| Special String     | Included methods      |\n",
    "| :---        |    :----:   |\n",
    "| \"selectors\" | [\"SelectFwe\", \"SelectPercentile\", \"VarianceThreshold\",] |\n",
    "| \"selectors_classification\" | [\"SelectFwe\", \"SelectPercentile\", \"VarianceThreshold\", \"RFE_classification\", \"SelectFromModel_classification\"] |\n",
    "| \"selectors_regression\" | [\"SelectFwe\", \"SelectPercentile\", \"VarianceThreshold\", \"RFE_regression\", \"SelectFromModel_regression\"] |\n",
    "| \"classifiers\"  | [\"LGBMClassifier\", \"BaggingClassifier\", 'AdaBoostClassifier', 'BernoulliNB', 'DecisionTreeClassifier', 'ExtraTreesClassifier', 'GaussianNB', 'HistGradientBoostingClassifier', 'KNeighborsClassifier','LinearDiscriminantAnalysis', 'LogisticRegression', \"LinearSVC\", \"SVC\", 'MLPClassifier', 'MultinomialNB',  \"QuadraticDiscriminantAnalysis\", 'RandomForestClassifier', 'SGDClassifier', 'XGBClassifier'] |\n",
    "| \"regressors\" | [\"LGBMRegressor\", 'AdaBoostRegressor', \"ARDRegression\", 'DecisionTreeRegressor', 'ExtraTreesRegressor', 'HistGradientBoostingRegressor', 'KNeighborsRegressor',  'LinearSVR', \"MLPRegressor\", 'RandomForestRegressor', 'SGDRegressor', 'SVR', 'XGBRegressor'] |\n",
    "| \"transformers\" |  [\"PassKBinsDiscretizer\", \"Binarizer\", \"PCA\", \"ZeroCount\", \"ColumnOneHotEncoder\", \"FastICA\", \"FeatureAgglomeration\", \"Nystroem\", \"RBFSampler\", \"QuantileTransformer\", \"PowerTransformer\"] |\n",
    "| \"scalers\" | [\"MinMaxScaler\", \"RobustScaler\", \"StandardScaler\", \"MaxAbsScaler\", \"Normalizer\", ] |\n",
    "| \"all_transformers\" | [\"transformers\", \"scalers\"] |\n",
    "| \"arithmatic\" | [\"AddTransformer\", \"mul_neg_1_Transformer\", \"MulTransformer\", \"SafeReciprocalTransformer\", \"EQTransformer\", \"NETransformer\", \"GETransformer\", \"GTTransformer\", \"LETransformer\", \"LTTransformer\", \"MinTransformer\", \"MaxTransformer\"] |\n",
    "| \"imputers\" | [\"SimpleImputer\", \"IterativeImputer\", \"KNNImputer\"] |\n",
    "| \"skrebate\" | [\"ReliefF\", \"SURF\", \"SURFstar\", \"MultiSURF\"] |\n",
    "| \"genetic_encoders\" | [\"DominantEncoder\", \"RecessiveEncoder\", \"HeterosisEncoder\", \"UnderDominanceEncoder\", \"OverDominanceEncoder\"] |\n",
    "| \"classifiers_sklearnex\" | [\"RandomForestClassifier_sklearnex\", \"LogisticRegression_sklearnex\", \"KNeighborsClassifier_sklearnex\", \"SVC_sklearnex\",\"NuSVC_sklearnex\"] |\n",
    "| \"regressors_sklearnex\" | [\"LinearRegression_sklearnex\", \"Ridge_sklearnex\", \"Lasso_sklearnex\", \"ElasticNet_sklearnex\", \"SVR_sklearnex\", \"NuSVR_sklearnex\", \"RandomForestRegressor_sklearnex\", \"KNeighborsRegressor_sklearnex\"] |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same pipeline search space as before.\n",
    "classifier_choice = tpot2.config.get_search_space([\"KNeighborsClassifier\", \"LogisticRegression\", \"DecisionTreeClassifier\"])\n",
    "\n",
    "print(\"sampled pipeline 1\")\n",
    "classifier_choice.generate().export_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sampled pipeline 2\")\n",
    "classifier_choice.generate().export_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search space for all classifiers\n",
    "classifier_choice = tpot2.config.get_search_space(\"classifiers\")\n",
    "\n",
    "print(\"sampled pipeline 1\")\n",
    "classifier_choice.generate().export_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sampled pipeline 2\")\n",
    "classifier_choice.generate().export_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SequentialPipeline\n",
    "\n",
    "SequentialPipelines are of fixed length and sample from a predefined distribution for each step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_choicepipeline = tpot2.config.get_search_space(\"VarianceThreshold\")\n",
    "transformer_choicepipeline =  tpot2.config.get_search_space(\"PCA\")\n",
    "classifier_choicepipeline = tpot2.config.get_search_space(\"LogisticRegression\")\n",
    "\n",
    "stc_pipeline = tpot2.search_spaces.pipelines.SequentialPipeline([\n",
    "    selector_choicepipeline,\n",
    "    transformer_choicepipeline,\n",
    "    classifier_choicepipeline,\n",
    "])\n",
    "\n",
    "print(\"sampled pipeline\")\n",
    "stc_pipeline.generate().export_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of the form Selector-Transformer-Classifier.\n",
    "\n",
    "Note that each step in the sequence is a ChoicePipeline this time. Here, the SequentialPipeline can sample from search provided search space in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_choicepipeline = tpot2.config.get_search_space(\"selectors\")\n",
    "transformer_choicepipeline =  tpot2.config.get_search_space(\"transformers\")\n",
    "classifier_choicepipeline = tpot2.config.get_search_space(\"classifiers\")\n",
    "\n",
    "stc_pipeline = tpot2.search_spaces.pipelines.SequentialPipeline([\n",
    "    selector_choicepipeline,\n",
    "    transformer_choicepipeline,\n",
    "    classifier_choicepipeline,\n",
    "])\n",
    "\n",
    "print(\"sampled pipeline\")\n",
    "stc_pipeline.generate().export_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sampled pipeline\")\n",
    "stc_pipeline.generate().export_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DynamicLinearPipeline\n",
    "\n",
    "DynamicLinearPipeline takes in a single search space and randomly samples and places estimators in a list without a predefined sequence. DynamicLinearPipeline are most often used when paired with LinearPipeline. A common strategy is to use DynamicLinearPipeline to optimize a series of preprocessing or feature engineering steps, followed by a final classifier or regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tpot2.config\n",
    "\n",
    "\n",
    "linear_feature_engineering = tpot2.search_spaces.pipelines.DynamicLinearPipeline(search_space = tpot2.config.get_search_space([\"all_transformers\",\"selectors_classification\"]), max_length=10)\n",
    "print(\"sampled pipeline\")\n",
    "linear_feature_engineering.generate().export_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sampled pipeline\")\n",
    "linear_feature_engineering.generate().export_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_search_space = tpot2.search_spaces.pipelines.SequentialPipeline([\n",
    "    linear_feature_engineering,\n",
    "    tpot2.config.get_search_space(\"classifiers\"),\n",
    "])\n",
    "\n",
    "print(\"sampled pipeline\")\n",
    "full_search_space.generate().export_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sampled pipeline\")\n",
    "full_search_space.generate().export_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UnionPipeline\n",
    "\n",
    "Union pipelines can be useful when you want to either do multiple transformations in a single layer. Another common strategy is to do a union with a transformer and a passthrough for when you want to keep the original data in addition to the transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_and_passthrough = tpot2.search_spaces.pipelines.UnionPipeline([\n",
    "    tpot2.config.get_search_space(\"transformers\"),\n",
    "    tpot2.config.get_search_space(\"Passthrough\"),\n",
    "])\n",
    "\n",
    "transform_and_passthrough.generate().export_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UnionPipelines are an excellent tool to expand the capabilities of the linear search spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stc_pipeline2 = tpot2.search_spaces.pipelines.SequentialPipeline([\n",
    "    tpot2.config.get_search_space(\"selectors\"),\n",
    "    transform_and_passthrough,\n",
    "    tpot2.config.get_search_space(\"classifiers\"),\n",
    "])\n",
    "\n",
    "stc_pipeline2.generate().export_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Union pipelines can also be used to create \"branches\" if you are trying to create a tree-like search space. This can be particularly useful when paired with the FeatureSetSelector node (FSSNode) as each branch can learn different feature engineering for different subsets of the features, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_pipeline = tpot2.search_spaces.pipelines.SequentialPipeline([\n",
    "    tpot2.config.get_search_space(\"selectors\"),\n",
    "    tpot2.config.get_search_space(\"transformers\"),\n",
    "])\n",
    "\n",
    "branched_pipeline = tpot2.search_spaces.pipelines.SequentialPipeline([\n",
    "    tpot2.search_spaces.pipelines.UnionPipeline([\n",
    "        st_pipeline,\n",
    "        st_pipeline,\n",
    "    ]),\n",
    "    tpot2.config.get_search_space(\"classifiers\"),\n",
    "])\n",
    "\n",
    "branched_pipeline.generate().export_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DynamicUnionPipeline\n",
    "\n",
    "DynamicUnionPipeline works similarly as UnionPipeline. Whereas UnionPipeline is fixed length, with each index corresponding to the search space provided as a list, DynamicUnionPipeline takes in a single search space and will sample 1 or more estimators/pipelines and concatenate them with a FeatureUnion. \n",
    "\n",
    "Note that DynamicUnionPipeline will check for pipeline uniqueness, so it will never concatenate two completely identical pipelines. In other words, all steps within the feature union will be unique.\n",
    "\n",
    "This can be useful when you want multiple transformers (or in some cases, pipelines), but are not sure how many or which ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_transformers = tpot2.search_spaces.pipelines.DynamicUnionPipeline(tpot2.config.get_search_space(\"transformers\"), max_estimators=4)\n",
    "dynamic_transformers.generate().export_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One good strategy could be to pair this with Passthrough in a feature union so that you output all the transformations along with the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_transformers_with_passthrough = tpot2.search_spaces.pipelines.UnionPipeline([\n",
    "    dynamic_transformers,\n",
    "    tpot2.config.get_search_space(\"Passthrough\")],\n",
    "    )\n",
    "\n",
    "dynamic_transformers_with_passthrough.generate().export_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stc_pipeline3 = tpot2.search_spaces.pipelines.SequentialPipeline([\n",
    "    tpot2.config.get_search_space(\"selectors\"),\n",
    "    dynamic_transformers_with_passthrough,\n",
    "    tpot2.config.get_search_space(\"classifiers\"),\n",
    "])\n",
    "\n",
    "stc_pipeline3.generate().export_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WrapperPipeline\n",
    "\n",
    "Some sklearn estimators take in other sklearn estimators as a parameter. The wrapper pipeline is used to tune both the original estimators hyperparameters simultaneously with the inner estimators hyperparameters. In fact, the inner estimator in WrapperPipeline can be any search space defined with any of the methods described in this Tutorial.\n",
    "\n",
    "The `get_search_space` will automatically create an inner search space for sklearn estimators that do use require an inner estimator. For example \"SelectFromModel_classification\" will return the following search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SelectFromModel_configspace_part = ConfigurationSpace(\n",
    "                                        space = {\n",
    "                                            'threshold': Float('threshold', bounds=(1e-4, 1.0), log=True),\n",
    "                                        }\n",
    "                                    )\n",
    "\n",
    "extratrees_estimator_node = tpot2.config.get_search_space(\"ExtraTreesClassifier\") #this exports an ExtraTreesClassifier node\n",
    "extratrees_estimator_node.generate().export_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "select_from_model_wrapper_searchspace = tpot2.search_spaces.pipelines.WrapperPipeline(\n",
    "            method=SelectFromModel,\n",
    "            space = SelectFromModel_configspace_part,\n",
    "            estimator_search_space= extratrees_estimator_node,\n",
    "        )\n",
    "\n",
    "select_from_model_wrapper_searchspace.generate().export_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WrapperPipeline strategy for ensembles/inner classifiers/regressors EstimatorTransformer- \n",
    "\n",
    "Sklearn Pipelines only allow classifiers/regressors as the final step. All other steps are expected to implement a transform function. We can get around this by wrapping it in another transformer class that returns the output of predict or predict_proba inside the transform() function.\n",
    "\n",
    "To wrap classifiers as transfomers, you can use the following class: `tpot2.builtin_modules.EstimatorTransformer`. You can specify whether to pass the outputs of predict, predict_proba, or decision function with the `method` parameter. \n",
    "\n",
    "An additional consideration is whether or not to use `cross_val_predict_cv`. Stacking classifiers can sometimes perform better when the predictions it is trained on come from out of sample predictions which is accomplished with CV. This is because the following pipelines can potentially better identify the accuracy of preceding models. Otherwise, some models may overfit the data and their predictions, making them seem like good predictors, which would then be weighted too highly by subsequent models. By default, TPOT `cross_val_predict_cv` is not enabled due to its computational cost.\n",
    "\n",
    "Note: This is not necessary for `GraphSearchPipeline` as the exported GraphPipeline estimator does have builtin support for inner/regressors. Instead of using a wrapper, you can set the `cross_val_predict_cv` param when initializing the `GraphSearchPipeline` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = tpot2.config.get_search_space(\"classifiers\")\n",
    "wrapped_estimators = tpot2.search_spaces.pipelines.WrapperPipeline(tpot2.builtin_modules.EstimatorTransformer, {}, classifiers)\n",
    "\n",
    "est = wrapped_estimators.generate().export_pipeline() #returns an estimator with a transform function\n",
    "est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X, y = np.random.rand(100, 10), np.random.randint(0, 2, 100)\n",
    "\n",
    "est.fit_transform(X, y)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can manually set the settings for an estimator the same way you would do it for an EstimatorNode. Here's another example with cross_val_predict and method being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = tpot2.config.get_search_space(\"classifiers\")\n",
    "wrapped_estimators_cv = tpot2.search_spaces.pipelines.WrapperPipeline(tpot2.builtin_modules.EstimatorTransformer, {'cross_val_predict_cv':10, 'method':'predict'}, classifiers)\n",
    "est = wrapped_estimators_cv.generate().export_pipeline() #returns an estimator with a transform function\n",
    "est.fit_transform(X, y)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These can now be used inside a linear pipeline. This is fairly similar to the default linear pipeline search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_wrapped_classifiers_with_passthrough = tpot2.search_spaces.pipelines.UnionPipeline([\n",
    "    tpot2.search_spaces.pipelines.DynamicUnionPipeline(wrapped_estimators_cv, max_estimators=4),\n",
    "    tpot2.config.get_search_space(\"Passthrough\")\n",
    "    ])\n",
    "\n",
    "stc_pipeline4 = tpot2.search_spaces.pipelines.SequentialPipeline([\n",
    "    tpot2.config.get_search_space(\"scalers\"),\n",
    "    dynamic_transformers_with_passthrough,\n",
    "    dynamic_wrapped_classifiers_with_passthrough,\n",
    "    tpot2.config.get_search_space(\"classifiers\"),\n",
    "])\n",
    "\n",
    "stc_pipeline4.generate().export_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphSearchPipeline\n",
    "\n",
    "The GraphSearchPipeline is a flexible search space without a prior restriction of pipeline structure. With GraphSearchPipeline, TPOT will create a pipeline in the shape of a directed acyclic graph. Throughout the optimization process, TPOT may add/remove nodes, add/remove edges, and performs model selection and hyperparameter tuning for each node.\n",
    "\n",
    "The primary parameters for the graph_search_space are the root_search_space, inner_search_space, and leaf_search_space.\n",
    "\n",
    "| Parameter              | Type                                | Description                                                                                               |\n",
    "|------------------------|-------------------------------------|-----------------------------------------------------------------------------------------------------------|\n",
    "| root_search_space      | SklearnIndividualGenerator          | The search space for the root node of the graph. This node will be the final estimator in the pipeline.    |\n",
    "| inner_search_space     | SklearnIndividualGenerator, optional| The search space for the inner nodes of the graph. If not defined, there will be no inner nodes.           |\n",
    "| leaf_search_space      | SklearnIndividualGenerator, optional| The search space for the leaf nodes of the graph. If not defined, the leaf nodes will be drawn from the inner_search_space. |\n",
    "| crossover_same_depth   | bool, optional                      | If True, crossover will only occur between nodes at the same depth in the graph. If False, crossover will occur between nodes at any depth. |\n",
    "| cross_val_predict_cv   | int, cross-validation generator or an iterable, optional | Determines the cross-validation splitting strategy used in inner classifiers or regressors.               |\n",
    "| method                 | str, optional                       | The prediction method to use for the inner classifiers or regressors. If 'auto', it will try to use predict_proba, decision_function, or predict in that order. |\n",
    "\n",
    "This search space exports a `tpot2.GraphPipeline`. This is similar to a scikit-learn Pipeline, but for directed acyclic graph pipelines. You can learn more about using this module in Tutorial 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_search_space = tpot2.search_spaces.pipelines.GraphSearchPipeline(\n",
    "    root_search_space= tpot2.config.get_search_space([\"KNeighborsClassifier\", \"LogisticRegression\", \"DecisionTreeClassifier\"]),\n",
    "    leaf_search_space = tpot2.config.get_search_space(\"selectors\"), \n",
    "    inner_search_space = tpot2.config.get_search_space([\"transformers\"]),\n",
    "    max_size = 10,\n",
    ")\n",
    "\n",
    "ind = graph_search_space.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est1 = ind.export_pipeline()\n",
    "est1.plot() #GraphPipelines have a helpful plotting function to visualize the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets add a few more mutations and plot the final pipeline to get a sense of the diversity of pipelines that can be generated with this search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,50):\n",
    "    ind.mutate()\n",
    "    if i%5==0:\n",
    "        est = ind.export_pipeline()\n",
    "        est.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TreePipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TreePipelines work the same way as GraphPipelines, but they are limited to a tree structure. This is similar to the search space in the original TPOT.\n",
    "\n",
    "(This search space is still experimental and currently built off GraphSearchPipeline. It may be rewritten with its own code in the future.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_search_space = tpot2.search_spaces.pipelines.TreePipeline(\n",
    "    root_search_space= tpot2.config.get_search_space([\"KNeighborsClassifier\", \"LogisticRegression\", \"DecisionTreeClassifier\"]),\n",
    "    leaf_search_space = tpot2.config.get_search_space(\"selectors\"), \n",
    "    inner_search_space = tpot2.config.get_search_space([\"transformers\"]),\n",
    "    max_size = 10,\n",
    ")\n",
    "\n",
    "ind = graph_search_space.generate()\n",
    "exp = ind.export_pipeline()\n",
    "exp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips and Tricks\n",
    "\n",
    "* Two very helpful transformers to use with search spaces are tpot2.buildin_models.Passthrough and tpot2.builtin_models.SkipTransformer. \n",
    "  Passthrough will simply pass through the exact inputs it receives into the next step. This is particularly useful inside UnionSearchSpace as it allows for both the transformed data as well as the original data to be passed into the next step.\n",
    "  SkipTransformer will always return nothing. This is helpful when inside a union with Passthrough and an optional second method. For example, if you are unsure of whether or not you will need a transformer, you can have SkipTransformer be one option that will skip the transformation step if selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the FeatureUnion layer will always have at least one transformer selected and will always have one passthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot2.search_spaces.pipelines import *\n",
    "from tpot2.config import get_search_space\n",
    "\n",
    "#This FeatureUnion layer will always have at least one transformer selected and will always have one passthrough\n",
    "transformers_with_passthrough = UnionPipeline([\n",
    "                        DynamicUnionPipeline(get_search_space([\"transformers\"])),\n",
    "                        get_search_space(\"Passthrough\")\n",
    "                        ]\n",
    "                    )\n",
    "\n",
    "transformers_with_passthrough.generate().export_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the FeatureUnion layer will always one passthrough. In addition, it may select one or more transformer, but it may skip transformers altogether and only include a Passthrough. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_transformers_layer =UnionPipeline([\n",
    "                        ChoicePipeline([\n",
    "                            DynamicUnionPipeline(get_search_space([\"transformers\"])),\n",
    "                            get_search_space(\"SkipTransformer\"),\n",
    "                        ]),\n",
    "                        get_search_space(\"Passthrough\")\n",
    "                        ]\n",
    "                    )\n",
    "\n",
    "final_transformers_layer.generate().export_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_estimators_layer = UnionPipeline([\n",
    "                            ChoicePipeline([\n",
    "                                DynamicUnionPipeline(wrapped_estimators, max_estimators=4),\n",
    "                                get_search_space(\"SkipTransformer\"),\n",
    "                            ]),\n",
    "                            get_search_space(\"Passthrough\")]\n",
    "                        )\n",
    "\n",
    "inner_estimators_layer.generate().export_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_linear_pipeline = SequentialPipeline([\n",
    "                            get_search_space(\"scalers\"),\n",
    "                            final_transformers_layer,\n",
    "                            inner_estimators_layer,\n",
    "                            get_search_space(\"classifiers\"),\n",
    "                        ])\n",
    "\n",
    "final_linear_pipeline.generate().export_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize Search Space with TPOTEstimator\n",
    "\n",
    "Once you have constructed a search space, you can use TPOTEstimator to optimize a pipeline within that space. Simply pass that search space into the `search_space` parameter. Here is a cell where you can select different search spaces that we created in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_search_spaces ={\n",
    "    \"classifiers_only\" : classifier_choice,\n",
    "    \"stc_pipeline\" : stc_pipeline,\n",
    "    \"stc_pipeline2\": stc_pipeline2,\n",
    "    \"stc_pipeline3\": stc_pipeline3,\n",
    "    \"stc_pipeline4\": stc_pipeline4,\n",
    "    \"final_linear_pipeline\": final_linear_pipeline,\n",
    "    \"graph_pipeline\": graph_search_space,\n",
    "}\n",
    "\n",
    "X, y = sklearn.datasets.load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_search_space = all_search_spaces[\"stc_pipeline\"] #change this to select a different search space\n",
    "\n",
    "\n",
    "est = tpot2.TPOTEstimator(\n",
    "    scorers=[\"roc_auc_ovr\", tpot2.objectives.complexity_scorer],\n",
    "    scorers_weights=[1.0, -1.0],\n",
    "    classification = True,\n",
    "    cv = 5,\n",
    "    search_space = selected_search_space,\n",
    "    generations = 5,\n",
    "    max_eval_time_mins = 10,\n",
    "    early_stop = 2,\n",
    "    verbose = 2,\n",
    "    n_jobs=4,\n",
    ")\n",
    "\n",
    "est.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the model\n",
    "auroc_scorer = sklearn.metrics.get_scorer(\"roc_auc\")\n",
    "auroc_score = auroc_scorer(est, X_test, y_test)\n",
    "\n",
    "print(\"auroc score\", auroc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the best pipeline\n",
    "if isinstance(est.fitted_pipeline_, tpot2.GraphPipeline):\n",
    "    est.fitted_pipeline_.plot()\n",
    "    \n",
    "est.fitted_pipeline_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template Search Spaces\n",
    "\n",
    "As mentioned in Tutorial 1, TPOT has several buildin search spaces. Here is the same table:\n",
    "\n",
    "| String     | Description      |\n",
    "| :---        |    :----:   |\n",
    "| linear  | A linear pipeline with the structure of \"Selector->(transformers+Passthrough)->(classifiers/regressors+Passthrough)->final classifier/regressor.\" For both the transformer and inner estimator layers, TPOT may choose one or more transformers/classifiers, or it may choose none. The inner classifier/regressor layer is optional. |\n",
    "| linear-light | Same search space as linear, but without the inner classifier/regressor layer and with a reduced set of faster running estimators. |\n",
    "| graph | TPOT will optimize a pipeline in the shape of a directed acyclic graph. The nodes of the graph can include selectors, scalers, transformers, or classifiers/regressors (inner classifiers/regressors can optionally be not included). This will return a custom GraphPipeline rather than an sklearn Pipeline. More details in Tutorial 6. |\n",
    "| graph-light | Same as graph search space, but without the inner classifier/regressors and with a reduced set of faster running estimators. |\n",
    "| mdr |TPOT will search over a series of feature selectors and Multifactor Dimensionality Reduction models to find a series of operators that maximize prediction accuracy. The TPOT MDR configuration is specialized for genome-wide association studies (GWAS), and is described in detail online here. |\n",
    "\n",
    "Rather than create your own search space, you can simply pass the string into the `search_space` param. Alternatively, you can access tpot2.config.template_search_spaces.get_template_search_spaces directly which offers a few more customizable options for each template including `cross_val_predict_cv` and whether or not stacked classifiers/regressors are allowed. Or you can copy the code and customize it manually!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer-only pipelines - imputation optimization example\n",
    "\n",
    "Pipelines don't necessarily need to end in a classifier or regressor. Transformer only pipelines are possible as long as you have a custom objective function to match. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import numpy as np\n",
    "import tpot2\n",
    "\n",
    "#in practice, cross validation is likely better, but this simple example is fine for demonstration purposes\n",
    "def rmse_obective(est, X, missing_add=.2, rng=1, fitted=False):\n",
    "    rng = np.random.default_rng(rng)\n",
    "    X_missing = X.copy()\n",
    "    missing_idx = rng.random(X.shape) < missing_add\n",
    "    X_missing[missing_idx] = np.nan\n",
    "    \n",
    "    if not fitted:\n",
    "        est.fit(X_missing)\n",
    "    \n",
    "    X_filled = est.transform(X_missing)\n",
    "    return np.sqrt(np.mean((X_filled[missing_idx] - X[missing_idx])**2))\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "X, y = sklearn.datasets.load_diabetes(return_X_y=True)\n",
    "\n",
    "imp = SimpleImputer(strategy=\"mean\")\n",
    "\n",
    "rmse_obective(imp, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tpot2.search_spaces\n",
    "from ConfigSpace import ConfigurationSpace, Integer, Float, Categorical, Normal\n",
    "\n",
    "#set up an imputation search space that includes simple imputer, knn imputer, and iterative imputer (with an optimized ExtraTreesRegressor)\n",
    "\n",
    "simple_imputer = tpot2.config.get_search_space(\"SimpleImputer\")\n",
    "knn_imputer = tpot2.config.get_search_space(\"KNNImputer\")\n",
    "\n",
    "space = ConfigurationSpace({ 'initial_strategy' : Categorical('initial_strategy', \n",
    "                                            ['mean', 'median', \n",
    "                                            'most_frequent', 'constant']),\n",
    "            'n_nearest_features' : Integer('n_nearest_features', \n",
    "                                        bounds=(1, X.shape[1])),\n",
    "            'imputation_order' : Categorical('imputation_order', \n",
    "                                            ['ascending', 'descending', \n",
    "                                            'roman', 'arabic', 'random']),\n",
    "})\n",
    "\n",
    "# This optimizes both the iterative imputer parameters and the ExtraTreesRegressor parameters\n",
    "iterative_imputer_sp = tpot2.search_spaces.pipelines.WrapperPipeline(\n",
    "    method = sklearn.impute.IterativeImputer,\n",
    "    space = space,\n",
    "    estimator_search_space = tpot2.config.get_search_space(\"ExtraTreesRegressor\"),\n",
    ")\n",
    "#this is equivalent to\n",
    "# iterative_imputer_sp = tpot2.config.get_search_space(\"IterativeImputer_learned_estimators\")\n",
    "\n",
    "imputation_search_space = tpot2.search_spaces.pipelines.ChoicePipeline(\n",
    "    search_spaces = [simple_imputer, knn_imputer, iterative_imputer_sp],\n",
    ")\n",
    "imputation_search_space.generate().export_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "final_objective = partial(rmse_obective, X=X, missing_add=.2)\n",
    "\n",
    "est = tpot2.TPOTEstimator(\n",
    "    scorers = [],\n",
    "    scorers_weights = [],\n",
    "    other_objective_functions = [final_objective],\n",
    "    other_objective_functions_weights = [-1],\n",
    "    objective_function_names = [\"rmse\"],\n",
    "    classification = True,\n",
    "    search_space = imputation_search_space,\n",
    "    generations = 5,\n",
    "    max_eval_time_mins = 60*5,\n",
    "    verbose = 3,\n",
    "    n_jobs=20,\n",
    ")\n",
    "\n",
    "est.fit(X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the model\n",
    "rmse_score = final_objective(est, fitted=True)\n",
    "print(\"final rmse score\", rmse_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est.fitted_pipeline_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Search Space Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot2.search_spaces.pipelines import *\n",
    "from tpot2.config import get_search_space\n",
    "\n",
    "selectors = get_search_space([\"selectors_classification\", \"Passthrough\"])\n",
    "estimators = get_search_space([\"classifiers\"])\n",
    "\n",
    "\n",
    "# this allows us to wrap the classifiers in the EstimatorTransformer\n",
    "# this is necessary so that classifiers can be used inside of sklearn pipelines\n",
    "wrapped_estimators = WrapperPipeline(tpot2.builtin_modules.EstimatorTransformer, {}, estimators)\n",
    "\n",
    "scalers = get_search_space([\"scalers\",\"Passthrough\"])\n",
    "\n",
    "transformers_layer =UnionPipeline([\n",
    "                        ChoicePipeline([\n",
    "                            DynamicUnionPipeline(get_search_space([\"transformers\"])),\n",
    "                            get_search_space(\"SkipTransformer\"),\n",
    "                        ]),\n",
    "                        get_search_space(\"Passthrough\")\n",
    "                        ]\n",
    "                    )\n",
    "\n",
    "inner_estimators_layer = UnionPipeline([\n",
    "                            ChoicePipeline([\n",
    "                                DynamicUnionPipeline(wrapped_estimators),\n",
    "                                get_search_space(\"SkipTransformer\"),\n",
    "                            ]),\n",
    "                            get_search_space(\"Passthrough\")]\n",
    "                        )\n",
    "\n",
    "\n",
    "search_space = SequentialPipeline(search_spaces=[\n",
    "                                        scalers,\n",
    "                                        selectors, \n",
    "                                        transformers_layer,\n",
    "                                        inner_estimators_layer,\n",
    "                                        estimators,\n",
    "                                        ])\n",
    "\n",
    "est = tpot2.TPOTEstimator(\n",
    "    scorers = [\"roc_auc\"],\n",
    "    scorers_weights = [1],\n",
    "    classification = True,\n",
    "    cv = 5,\n",
    "    search_space = search_space,\n",
    "    generations = 5,\n",
    "    max_eval_time_mins = 60*5,\n",
    "    verbose = 2,\n",
    "    n_jobs=20,\n",
    ")\n",
    "\n",
    "est.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est.fitted_pipeline_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tpot2env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
