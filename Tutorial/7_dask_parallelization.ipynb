{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelization\n",
    "\n",
    "TPOT2 uses the Dask package for parallelization either locally (dask.destributed.LocalCluster) or multi-node via a job schedule (dask-jobqueue). \n",
    "\n",
    "## Local Machine Parallelization\n",
    "\n",
    "TPOT2 can be easily parallelized on a local computer by setting the n_jobs and memory_limit parameters.\n",
    "\n",
    "`n_jobs` dictates how many dask workers to launch. In TPOT2 this corresponds to the number of pipelines to evaluate in parallel.\n",
    "\n",
    "`memory_limit` is the amount of RAM to use per worker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tpot2\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import numpy as np\n",
    "scorer = sklearn.metrics.get_scorer('roc_auc_ovr')\n",
    "X, y = sklearn.datasets.load_digits(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)\n",
    "\n",
    "\n",
    "est = tpot2.TPOTClassifier(population_size= 8, generations=5, n_jobs=4, memory_limit=\"4GB\", verbose=1)\n",
    "est.fit(X_train, y_train)\n",
    "print(scorer(est, X_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Dask Clients and Dashboard\n",
    "\n",
    "You can also manually initialize a dask client. This can be useful to gain additional control over the parallelization, debugging, as well as viewing a dashboard of the live performance of TPOT2.\n",
    "\n",
    "You can find more details in the official [documentation here.](https://docs.dask.org/en/stable/)\n",
    "\n",
    "\n",
    "[Dask Python Tutorial](https://docs.dask.org/en/stable/deploying-python.html)\n",
    "[Dask Dashboard](https://docs.dask.org/en/stable/dashboard.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing a basic dask local cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "n_jobs = 4\n",
    "memory_limit = \"4GB\"\n",
    "\n",
    "cluster = LocalCluster(n_workers=n_jobs, #if no client is passed in and no global client exists, create our own\n",
    "                        threads_per_worker=1,\n",
    "                        memory_limit=memory_limit)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the link to view the dask Dashboard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " client.dashboard_link"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass into TPOT to Train.\n",
    "Note that the if a client is passed in manually, TPOT will ignore n_jobs and memory_limit.\n",
    "If there is no client passed in, TPOT will ignore any global/existing client and create its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = tpot2.TPOTClassifier(population_size= 8, generations=5, client=client verbose=1)\n",
    "# this is equivalent to: \n",
    "# est = tpot2.TPOTClassifier(population_size= 8, generations=5, n_jobs=4, memory_limit=\"4GB\", verbose=1)\n",
    "est.fit(X_train, y_train)\n",
    "print(scorer(est, X_test, y_test))\n",
    "\n",
    "#It is good to close the client and cluster when you are done with them\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2\n",
    "\n",
    "You can initialize the cluster and client with a context manager that will automatically close them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "import tpot2\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import numpy as np\n",
    "\n",
    "scorer = sklearn.metrics.get_scorer('roc_auc_ovr')\n",
    "X, y = sklearn.datasets.load_digits(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)\n",
    "\n",
    "\n",
    "n_jobs = 4\n",
    "memory_limit = \"4GB\"\n",
    "\n",
    "with LocalCluster(  \n",
    "    n_workers=n_jobs,\n",
    "    threads_per_worker=1,\n",
    "    memory_limit='4GB',\n",
    ") as cluster, Client(cluster) as client:\n",
    "    est = tpot2.TPOTClassifier(population_size= 8, generations=5, client=client, verbose=1)\n",
    "    est.fit(X_train, y_train)\n",
    "    print(scorer(est, X_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask multi node parallelization\n",
    "\n",
    "Dask can parallelize across multiple nodes via job queueing systems. This is done using the dask-jobqueue package. More information can be found in the official [documentation here.]( https://jobqueue.dask.org/en/latest/)\n",
    "\n",
    "To parallelize TPOT2 with dask-jobqueue, simply pass in a client based on a jobqueue cluster with desired settings into the client parameter. Each job will evaluate a single pipeline.\n",
    "\n",
    "Note that TPOT will ignore n_jobs and memory_limit as these should be set inside the dask cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import tpot2\n",
    "\n",
    "from dask_jobqueue import SGECluster # or SLURMCluster, PBSCluster, etc. Replace SGE with your scheduler.\n",
    "cluster = SGECluster(\n",
    "    queue='all.q',\n",
    "    cores=2,\n",
    "    memory=\"50 GB\"\n",
    "\n",
    ")\n",
    "\n",
    "cluster.adapt(minimum_jobs=10, maximum_jobs=100)  # auto-scale between 10 and 100 jobs\n",
    "\n",
    "client = Client(cluster)\n",
    "\n",
    "scorer = sklearn.metrics.get_scorer('roc_auc_ovr')\n",
    "X, y = sklearn.datasets.load_digits(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)\n",
    "\n",
    "est = tpot2.TPOTClassifier(population_size= 100, generations=5, client=client, verbose=1)\n",
    "# this is equivalent to: \n",
    "# est = tpot2.TPOTClassifier(population_size= 8, generations=5, n_jobs=4, memory_limit=\"4GB\", verbose=1)\n",
    "est.fit(X_train, y_train)\n",
    "print(scorer(est, X_test, y_test))\n",
    "\n",
    "#It is good to close the client and cluster when you are done with them\n",
    "client.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tpot_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7fe1fe9ef32cd5efd76326a08046147513534f0dd2318301a1a96ae9071c1c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
