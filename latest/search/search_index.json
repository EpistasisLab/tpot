{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"documentation/tpot/_version/","title":"version","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/graphsklearn/","title":"Graphsklearn","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/graphsklearn/#tpot.graphsklearn.GraphPipeline","title":"<code>GraphPipeline</code>","text":"<p>               Bases: <code>_BaseComposition</code></p> Source code in <code>tpot/graphsklearn.py</code> <pre><code>class GraphPipeline(_BaseComposition):\n    def __init__(\n                self,\n                graph,\n                cross_val_predict_cv=0, #signature function(estimator, X, y=none)\n                method='auto',\n                memory=None,\n                use_label_encoder=False,\n                **kwargs,\n                ):\n        super().__init__(**kwargs)\n        '''\n        An sklearn baseestimator that uses genetic programming to optimize a pipeline.\n\n        Parameters\n        ----------\n\n        graph: networkx.DiGraph\n            A directed graph where the nodes are sklearn estimators and the edges are the inputs to those estimators.\n\n        cross_val_predict_cv: int, cross-validation generator or an iterable, optional\n            Determines the cross-validation splitting strategy used in inner classifiers or regressors\n\n        method: str, optional\n            The prediction method to use for the inner classifiers or regressors. If 'auto', it will try to use predict_proba, decision_function, or predict in that order.\n\n        memory: str or object with the joblib.Memory interface, optional\n            Used to cache the input and outputs of nodes to prevent refitting or computationally heavy transformations. By default, no caching is performed. If a string is given, it is the path to the caching directory.\n\n        use_label_encoder: bool, optional\n            If True, the label encoder is used to encode the labels to be 0 to N. If False, the label encoder is not used.\n            Mainly useful for classifiers (XGBoost) that require labels to be ints from 0 to N.\n\n            Can also be a sklearn.preprocessing.LabelEncoder object. If so, that label encoder is used.\n\n        '''\n\n        self.graph = graph\n        self.cross_val_predict_cv = cross_val_predict_cv\n        self.method = method\n        self.memory = memory\n        self.use_label_encoder = use_label_encoder\n\n        setup_ordered_successors(graph)\n\n        self.topo_sorted_nodes = list(nx.topological_sort(self.graph))\n        self.topo_sorted_nodes.reverse()\n\n        self.root = self.topo_sorted_nodes[-1]\n\n        if self.use_label_encoder:\n            if type(self.use_label_encoder) == LabelEncoder:\n                self.label_encoder = self.use_label_encoder\n            else:\n                self.label_encoder = LabelEncoder()\n\n\n        #TODO clean this up\n        try:\n            nx.find_cycle(self.G)\n            raise BaseException \n        except: \n            pass\n\n    def __str__(self):\n        if len(self.graph.edges) &gt; 0:\n            return str(self.graph.edges)\n        else:\n            return str(self.graph.nodes)\n\n    def fit(self, X, y):\n\n\n        if self.use_label_encoder:\n            if type(self.use_label_encoder) == LabelEncoder:\n                y = self.label_encoder.transform(y)\n            else:\n                y = self.label_encoder.fit_transform(y)\n\n\n\n        fit_sklearn_digraph(   graph=self.graph,\n                                X=X,\n                                y=y,\n                                method=self.method,\n                                cross_val_predict_cv = self.cross_val_predict_cv,\n                                memory = self.memory,\n                                topo_sort = self.topo_sorted_nodes,\n                                )\n\n        return self\n\n    def plot(self, ):\n        plot(graph = self.graph)\n\n    def __sklearn_is_fitted__(self):\n        '''Indicate whether pipeline has been fit.'''\n        try:\n            # check if the last step of the pipeline is fitted\n            # we only check the last step since if the last step is fit, it\n            # means the previous steps should also be fit. This is faster than\n            # checking if every step of the pipeline is fit.\n            sklearn.utils.validation.check_is_fitted(self.graph.nodes[self.root][\"instance\"])\n            return True\n        except sklearn.exceptions.NotFittedError:\n            return False\n\n    @available_if(_estimator_has('predict'))\n    def predict(self, X, **predict_params):\n\n\n        this_X = get_inputs_to_node(self.graph,\n                    X, \n                    self.root,\n                    method = self.method,\n                    topo_sort = self.topo_sorted_nodes,\n                    )\n\n        preds = self.graph.nodes[self.root][\"instance\"].predict(this_X, **predict_params)\n\n        if self.use_label_encoder:\n            preds = self.label_encoder.inverse_transform(preds)\n\n        return preds\n\n    @available_if(_estimator_has('predict_proba'))\n    def predict_proba(self, X, **predict_params):\n\n\n        this_X = get_inputs_to_node(self.graph,\n                    X, \n                    self.root,\n                    method = self.method,\n                    topo_sort = self.topo_sorted_nodes,\n                    )\n        return self.graph.nodes[self.root][\"instance\"].predict_proba(this_X, **predict_params)\n\n    @available_if(_estimator_has('decision_function'))\n    def decision_function(self, X, **predict_params):\n\n        this_X = get_inputs_to_node(self.graph,\n                    X, \n                    self.root,\n                    method = self.method,\n                    topo_sort = self.topo_sorted_nodes,\n                    )\n        return self.graph.nodes[self.root][\"instance\"].decision_function(this_X, **predict_params)\n\n    @available_if(_estimator_has('transform'))\n    def transform(self, X, **predict_params):\n\n        this_X = get_inputs_to_node(self.graph,\n                    X, \n                    self.root,\n                    method = self.method,\n                    topo_sort = self.topo_sorted_nodes,\n                    )\n        return self.graph.nodes[self.root][\"instance\"].transform(this_X, **predict_params)\n\n    @property\n    def classes_(self):\n        \"\"\"The classes labels. Only exist if the last step is a classifier.\"\"\"\n\n        if self.use_label_encoder:\n            return self.label_encoder.classes_\n        else:\n            return self.graph.nodes[self.root][\"instance\"].classes_\n\n    @property\n    def _estimator_type(self):\n        return self.graph.nodes[self.root][\"instance\"]._estimator_type\n</code></pre>"},{"location":"documentation/tpot/graphsklearn/#tpot.graphsklearn.GraphPipeline.classes_","title":"<code>classes_</code>  <code>property</code>","text":"<p>The classes labels. Only exist if the last step is a classifier.</p>"},{"location":"documentation/tpot/graphsklearn/#tpot.graphsklearn.GraphPipeline.__sklearn_is_fitted__","title":"<code>__sklearn_is_fitted__()</code>","text":"<p>Indicate whether pipeline has been fit.</p> Source code in <code>tpot/graphsklearn.py</code> <pre><code>def __sklearn_is_fitted__(self):\n    '''Indicate whether pipeline has been fit.'''\n    try:\n        # check if the last step of the pipeline is fitted\n        # we only check the last step since if the last step is fit, it\n        # means the previous steps should also be fit. This is faster than\n        # checking if every step of the pipeline is fit.\n        sklearn.utils.validation.check_is_fitted(self.graph.nodes[self.root][\"instance\"])\n        return True\n    except sklearn.exceptions.NotFittedError:\n        return False\n</code></pre>"},{"location":"documentation/tpot/individual/","title":"Individual","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/logbook/","title":"Logbook","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/population/","title":"Population","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/population/#tpot.population.Population","title":"<code>Population</code>","text":"<p>Primary usage is to keep track of evaluated individuals</p> <p>Parameters:</p> Name Type Description Default <code>initial_population</code> <code>list of BaseIndividuals</code> <p>Initial population to start with. If None, start with an empty population.</p> <code>list of BaseIndividuals</code> <code>use_unique_id</code> <code>Bool</code> <p>If True, individuals are treated as unique if they have the same unique_id(). If False, all new individuals are treated as unique.</p> <code>Bool</code> <code>callback</code> <code>function</code> <p>NOT YET IMPLEMENTED A function to call after each generation. The function should take a Population object as its only argument.</p> <code>function</code> <p>Attributes:</p> Name Type Description <code>population</code> <code>{list of BaseIndividuals}</code> <p>The current population of individuals. Contains the live instances of BaseIndividuals.</p> <code>evaluated_individuals</code> <code>{dict}</code> <p>A dictionary of dictionaries. The keys are the unique_id() or self of each BaseIndividual. Can be thought of as a table with the unique_id() as the row index and the inner dictionary keys as the columns.</p> Source code in <code>tpot/population.py</code> <pre><code>class Population():\n    '''\n    Primary usage is to keep track of evaluated individuals\n\n    Parameters\n    ----------\n    initial_population : {list of BaseIndividuals}, default=None\n        Initial population to start with. If None, start with an empty population.\n    use_unique_id : {Bool}, default=True\n        If True, individuals are treated as unique if they have the same unique_id().\n        If False, all new individuals are treated as unique.\n    callback : {function}, default=None\n        NOT YET IMPLEMENTED\n        A function to call after each generation. The function should take a Population object as its only argument.\n\n    Attributes\n    ----------\n    population : {list of BaseIndividuals}\n        The current population of individuals. Contains the live instances of BaseIndividuals.\n    evaluated_individuals : {dict}\n        A dictionary of dictionaries. The keys are the unique_id() or self of each BaseIndividual.\n        Can be thought of as a table with the unique_id() as the row index and the inner dictionary keys as the columns.\n    '''\n    def __init__(   self,\n                    column_names: typing.List[str] = None,\n                    n_jobs: int = 1,\n                    callback=None,\n                    ) -&gt; None:\n\n        if column_names is not None:\n\n            column_names = column_names+[\"Parents\", \"Variation_Function\"]\n        else:\n            column_names = [\"Parents\", \"Variation_Function\"]\n        self.evaluated_individuals = pd.DataFrame(columns=column_names)\n        self.evaluated_individuals[\"Parents\"] = self.evaluated_individuals[\"Parents\"].astype('object')\n\n        self.use_unique_id = True #Todo clean this up. perhaps pull unique_id() out of baseestimator and have it be supplied as a function\n        self.n_jobs = n_jobs\n        self.callback=callback\n        self.population = []\n\n    def survival_select(self, selector, weights, columns_names, n_survivors, rng, inplace=True):\n        rng = np.random.default_rng(rng)\n        weighted_scores = self.get_column(self.population, column_names=columns_names) * weights\n        new_population_index = np.ravel(selector(weighted_scores, k=n_survivors, rng=rng)) #TODO make it clear that we are concatenating scores...\n        new_population = np.array(self.population)[new_population_index]\n        if inplace:\n            self.set_population(new_population, rng=rng)\n        return new_population\n\n    def parent_select(self, selector, weights, columns_names, k, n_parents, rng):\n        rng = np.random.default_rng(rng)\n        weighted_scores = self.get_column(self.population, column_names=columns_names) * weights\n        parents_index = selector(weighted_scores, k=k, n_parents=n_parents, rng=rng)\n        parents = np.array(self.population)[parents_index]\n        return parents\n\n\n    #remove individuals that either do not have a column_name value or a nan in that value\n    #TODO take into account when the value is not a list/tuple?\n    #TODO make invalid a global variable?\n    def remove_invalid_from_population(self, column_names, invalid_value = \"INVALID\"):\n        '''\n        Remove individuals from the live population if either do not have a value in the column_name column or if the value contains np.nan.\n\n        Parameters\n        ----------\n        column_name : {str}\n            The name of the column to check for np.nan values.\n\n        Returns\n        -------\n        None\n        '''\n        if isinstance(column_names, str): #TODO check this\n            column_names = [column_names]\n        is_valid = lambda ind: ind.unique_id() not in self.evaluated_individuals.index or invalid_value not in self.evaluated_individuals.loc[ind.unique_id(),column_names].to_list()\n        self.population = [ind for ind in self.population if is_valid(ind)]\n\n\n\n    # takes the list of individuals and adds it to the live population list.\n    # if keep_repeats is False, repeated individuals are not added to the population\n    # returns a list of individuals added to the live population\n    #TODO make keep repeats allow for previously evaluated individuals,\n    #but make sure that the live population only includes one of each, no repeats\n    def add_to_population(self, individuals: typing.List[BaseIndividual], rng, keep_repeats=False, mutate_until_unique=True):\n        '''\n        Add individuals to the live population. Add individuals to the evaluated_individuals if they are not already there.\n\n        Parameters:\n        -----------\n        individuals : {list of BaseIndividuals}\n            The individuals to add to the live population.\n        keep_repeats : {bool}, default=False\n            If True, allow the population to have repeated individuals.\n            If False, only add individuals that have not yet been added to geneology.\n        '''\n\n        rng = np.random.default_rng(rng)\n\n        if not isinstance(individuals, collections.abc.Iterable):\n            individuals = [individuals]\n\n        new_individuals = []\n        #TODO check for proper inputs\n        for individual in individuals:\n            key = individual.unique_id()\n\n            if key not in self.evaluated_individuals.index: #If its new, we always add it\n                self.evaluated_individuals.loc[key] = np.nan\n                self.evaluated_individuals.loc[key,\"Individual\"] = copy.deepcopy(individual)\n                self.population.append(individual)\n                new_individuals.append(individual)\n\n            else:#If its old\n                if keep_repeats: #If we want to keep repeats, we add it\n                    self.population.append(individual)\n                    new_individuals.append(individual)\n                elif mutate_until_unique: #If its old and we don't want repeats, we can optionally mutate it until it is unique\n                    for _ in range(20):\n                        individual = copy.deepcopy(individual)\n                        individual.mutate(rng=rng)\n                        key = individual.unique_id()\n                        if key not in self.evaluated_individuals.index:\n                            self.evaluated_individuals.loc[key] = np.nan\n                            self.evaluated_individuals.loc[key,\"Individual\"] = copy.deepcopy(individual)\n                            self.population.append(individual)\n                            new_individuals.append(individual)\n                            break\n\n        return new_individuals\n\n\n    def update_column(self, individual, column_names, data):\n        '''\n        Update the column_name column in the evaluated_individuals with the data.\n        If the data is a list, it must be the same length as the evaluated_individuals.\n        If the data is a single value, it will be applied to all individuals in the evaluated_individuals.\n        '''\n        if isinstance(individual, collections.abc.Iterable):\n            if self.use_unique_id:\n                key = [ind.unique_id() for ind in individual]\n            else:\n                key = individual\n        else:\n            if self.use_unique_id:\n                key = individual.unique_id()\n            else:\n                key = individual\n\n        self.evaluated_individuals.loc[key,column_names] = data\n\n\n    def get_column(self, individual, column_names=None, to_numpy=True):\n        '''\n        Update the column_name column in the evaluated_individuals with the data.\n        If the data is a list, it must be the same length as the evaluated_individuals.\n        If the data is a single value, it will be applied to all individuals in the evaluated_individuals.\n        '''\n        if isinstance(individual, collections.abc.Iterable):\n            if self.use_unique_id:\n                key = [ind.unique_id() for ind in individual]\n            else:\n                key = individual\n        else:\n            if self.use_unique_id:\n                key = individual.unique_id()\n            else:\n                key = individual\n\n        if column_names is not None:\n            slice = self.evaluated_individuals.loc[key,column_names]\n        else:\n            slice = self.evaluated_individuals.loc[key]\n        if to_numpy:\n            slice.reset_index(drop=True, inplace=True)\n            return slice.to_numpy()\n        else:\n            return slice\n\n\n    #returns the individuals without a 'column' as a key in geneology\n    #TODO make sure not to get repeats in this list even if repeats are in the \"live\" population\n    def get_unevaluated_individuals(self, column_names, individual_list=None):\n        if individual_list is None:\n            individual_list = self.population\n\n        if self.use_unique_id:\n            unevaluated_filter = lambda individual: individual.unique_id() not in self.evaluated_individuals.index or any(self.evaluated_individuals.loc[individual.unique_id(), column_names].isna())\n        else:\n            unevaluated_filter = lambda individual: individual not in self.evaluated_individuals.index or any(self.evaluated_individuals.loc[individual.unique_id(), column_names].isna())\n\n        return [individual for individual in individual_list if unevaluated_filter(individual)]\n\n    # def get_valid_evaluated_individuals_df(self, column_names_to_check, invalid_values=[\"TIMEOUT\",\"INVALID\"]):\n    #     '''\n    #     Returns a dataframe of the evaluated individuals that do no have invalid_values in column_names_to_check.\n    #     '''\n    #     return self.evaluated_individuals[~self.evaluated_individuals[column_names_to_check].isin(invalid_values).any(axis=1)]\n\n    #the live population empied and is set to new_population\n    def set_population(self,  new_population, rng, keep_repeats=True):\n        '''\n        sets population to new population\n        for selection?\n        '''\n        rng = np.random.default_rng(rng)\n        self.population = []\n        self.add_to_population(new_population, rng=rng, keep_repeats=keep_repeats)\n\n    #TODO should we just generate one offspring per crossover?\n    def create_offspring(self, parents_list, var_op_list, rng, add_to_population=True, keep_repeats=False, mutate_until_unique=True, n_jobs=1):\n        '''\n        parents_list: a list of lists of parents.\n        var_op_list: a list of var_ops to apply to each list of parents. Should be the same length as parents_list.\n\n        for example:\n        parents_list = [[parent1, parent2], [parent3]]\n        var_op_list = [\"crossover\", \"mutate\"]\n\n        This will apply crossover to parent1 and parent2 and mutate to parent3.\n\n        Creates offspring from parents using the var_op_list.\n        If string, will use a built in method\n            - \"crossover\" : crossover\n            - \"mutate\" : mutate\n            - \"mutate_and_crossover\" : mutate_and_crossover\n            - \"cross_and_mutate\" : cross_and_mutate\n        '''\n        rng = np.random.default_rng(rng)\n        new_offspring = []\n        all_offspring = parallel_create_offspring(parents_list, var_op_list, rng=rng, n_jobs=n_jobs)\n\n        for parents, offspring, var_op in zip(parents_list, all_offspring, var_op_list):\n\n            # if var_op in built_in_var_ops_dict:\n            #     var_op = built_in_var_ops_dict[var_op]\n\n            # offspring = copy.deepcopy(parents)\n            # offspring = var_op(offspring)\n            # if isinstance(offspring, collections.abc.Iterable):\n            #     offspring = offspring[0]\n\n            if add_to_population:\n                added = self.add_to_population(offspring, rng=rng, keep_repeats=keep_repeats, mutate_until_unique=mutate_until_unique)\n                if len(added) &gt; 0:\n                    for new_child in added:\n                        parent_keys = [parent.unique_id() for parent in parents]\n                        if not pd.api.types.is_object_dtype(self.evaluated_individuals[\"Parents\"]): #TODO Is there a cleaner way of doing this? Not required for some python environments?\n                            self.evaluated_individuals[\"Parents\"] = self.evaluated_individuals[\"Parents\"].astype('object')\n                        if not pd.api.types.is_object_dtype(self.evaluated_individuals[\"Variation_Function\"]):#TODO Is there a cleaner way of doing this? Not required for some python environments?\n                            self.evaluated_individuals[\"Variation_Function\"] = self.evaluated_individuals[\"Variation_Function\"].astype('object')\n                        self.evaluated_individuals.at[new_child.unique_id(),\"Parents\"] = tuple(parent_keys)\n\n                        #if var_op is a function\n                        if hasattr(var_op, '__call__'):\n                            self.evaluated_individuals.at[new_child.unique_id(),\"Variation_Function\"] = var_op.__name__\n                        else:\n                            self.evaluated_individuals.at[new_child.unique_id(),\"Variation_Function\"] = str(var_op)\n\n\n                        new_offspring.append(new_child)\n\n            else:\n                new_offspring.append(offspring)\n\n\n        return new_offspring\n\n\n    #TODO should we just generate one offspring per crossover?\n    def create_offspring2(self, parents_list, var_op_list, mutation_functions,mutation_function_weights, crossover_functions,crossover_function_weights, rng, add_to_population=True, keep_repeats=False, mutate_until_unique=True):\n\n        rng = np.random.default_rng(rng)\n        new_offspring = []\n\n        all_offspring = []\n        chosen_ops = []\n\n        for parents, var_op in zip(parents_list,var_op_list):\n            #TODO put this loop in population class\n            if var_op == \"mutate\":\n                mutation_op = rng.choice(mutation_functions, p=mutation_function_weights)\n                all_offspring.append(copy_and_mutate(parents[0], mutation_op, rng=rng))\n                chosen_ops.append(mutation_op.__name__)\n\n\n            elif var_op == \"crossover\":\n                crossover_op = rng.choice(crossover_functions, p=crossover_function_weights)\n                all_offspring.append(copy_and_crossover(parents, crossover_op, rng=rng))\n                chosen_ops.append(crossover_op.__name__)\n            elif var_op == \"mutate_then_crossover\":\n\n                mutation_op1 = rng.choice(mutation_functions, p=mutation_function_weights)\n                mutation_op2 = rng.choice(mutation_functions, p=mutation_function_weights)\n                crossover_op = rng.choice(crossover_functions, p=crossover_function_weights)\n                p1 = copy_and_mutate(parents[0], mutation_op1, rng=rng)\n                p2 = copy_and_mutate(parents[1], mutation_op2, rng=rng)\n                crossover_op(p1,p2,rng=rng)\n                all_offspring.append(p1)\n                chosen_ops.append(f\"{mutation_op1.__name__} , {mutation_op2.__name__} , {crossover_op.__name__}\")\n            elif var_op == \"crossover_then_mutate\":\n                crossover_op = rng.choice(crossover_functions, p=crossover_function_weights)\n                child = copy_and_crossover(parents, crossover_op, rng=rng)\n                mutation_op = rng.choice(mutation_functions, p=mutation_function_weights)\n                mutation_op(child, rng=rng)\n                all_offspring.append(child)\n                chosen_ops.append(f\"{crossover_op.__name__} , {mutation_op.__name__}\")\n\n\n        for parents, offspring, var_op in zip(parents_list, all_offspring, chosen_ops):\n\n            # if var_op in built_in_var_ops_dict:\n            #     var_op = built_in_var_ops_dict[var_op]\n\n            # offspring = copy.deepcopy(parents)\n            # offspring = var_op(offspring)\n            # if isinstance(offspring, collections.abc.Iterable):\n            #     offspring = offspring[0]\n\n            if add_to_population:\n                added = self.add_to_population(offspring, rng=rng, keep_repeats=keep_repeats, mutate_until_unique=mutate_until_unique)\n                if len(added) &gt; 0:\n                    for new_child in added:\n                        parent_keys = [parent.unique_id() for parent in parents]\n                        if not pd.api.types.is_object_dtype(self.evaluated_individuals[\"Parents\"]): #TODO Is there a cleaner way of doing this? Not required for some python environments?\n                            self.evaluated_individuals[\"Parents\"] = self.evaluated_individuals[\"Parents\"].astype('object')\n                        self.evaluated_individuals.at[new_child.unique_id(),\"Parents\"] = tuple(parent_keys)\n\n                        #check if Variation_Function variable is an object type\n                        if not pd.api.types.is_object_dtype(self.evaluated_individuals[\"Variation_Function\"]): #TODO Is there a cleaner way of doing this? Not required for some python environments?\n                            self.evaluated_individuals[\"Variation_Function\"] = self.evaluated_individuals[\"Variation_Function\"].astype('object')\n\n                        #if var_op is a function\n                        if hasattr(var_op, '__call__'):\n                            self.evaluated_individuals.at[new_child.unique_id(),\"Variation_Function\"] = var_op.__name__\n                        else:\n                            self.evaluated_individuals.at[new_child.unique_id(),\"Variation_Function\"] = str(var_op)\n\n\n                        new_offspring.append(new_child)\n\n            else:\n                new_offspring.append(offspring)\n\n\n        return new_offspring\n</code></pre>"},{"location":"documentation/tpot/population/#tpot.population.Population.add_to_population","title":"<code>add_to_population(individuals, rng, keep_repeats=False, mutate_until_unique=True)</code>","text":"<p>Add individuals to the live population. Add individuals to the evaluated_individuals if they are not already there.</p> Parameters: <p>individuals : {list of BaseIndividuals}     The individuals to add to the live population. keep_repeats : {bool}, default=False     If True, allow the population to have repeated individuals.     If False, only add individuals that have not yet been added to geneology.</p> Source code in <code>tpot/population.py</code> <pre><code>def add_to_population(self, individuals: typing.List[BaseIndividual], rng, keep_repeats=False, mutate_until_unique=True):\n    '''\n    Add individuals to the live population. Add individuals to the evaluated_individuals if they are not already there.\n\n    Parameters:\n    -----------\n    individuals : {list of BaseIndividuals}\n        The individuals to add to the live population.\n    keep_repeats : {bool}, default=False\n        If True, allow the population to have repeated individuals.\n        If False, only add individuals that have not yet been added to geneology.\n    '''\n\n    rng = np.random.default_rng(rng)\n\n    if not isinstance(individuals, collections.abc.Iterable):\n        individuals = [individuals]\n\n    new_individuals = []\n    #TODO check for proper inputs\n    for individual in individuals:\n        key = individual.unique_id()\n\n        if key not in self.evaluated_individuals.index: #If its new, we always add it\n            self.evaluated_individuals.loc[key] = np.nan\n            self.evaluated_individuals.loc[key,\"Individual\"] = copy.deepcopy(individual)\n            self.population.append(individual)\n            new_individuals.append(individual)\n\n        else:#If its old\n            if keep_repeats: #If we want to keep repeats, we add it\n                self.population.append(individual)\n                new_individuals.append(individual)\n            elif mutate_until_unique: #If its old and we don't want repeats, we can optionally mutate it until it is unique\n                for _ in range(20):\n                    individual = copy.deepcopy(individual)\n                    individual.mutate(rng=rng)\n                    key = individual.unique_id()\n                    if key not in self.evaluated_individuals.index:\n                        self.evaluated_individuals.loc[key] = np.nan\n                        self.evaluated_individuals.loc[key,\"Individual\"] = copy.deepcopy(individual)\n                        self.population.append(individual)\n                        new_individuals.append(individual)\n                        break\n\n    return new_individuals\n</code></pre>"},{"location":"documentation/tpot/population/#tpot.population.Population.create_offspring","title":"<code>create_offspring(parents_list, var_op_list, rng, add_to_population=True, keep_repeats=False, mutate_until_unique=True, n_jobs=1)</code>","text":"<p>parents_list: a list of lists of parents. var_op_list: a list of var_ops to apply to each list of parents. Should be the same length as parents_list.</p> <p>for example: parents_list = [[parent1, parent2], [parent3]] var_op_list = [\"crossover\", \"mutate\"]</p> <p>This will apply crossover to parent1 and parent2 and mutate to parent3.</p> <p>Creates offspring from parents using the var_op_list. If string, will use a built in method     - \"crossover\" : crossover     - \"mutate\" : mutate     - \"mutate_and_crossover\" : mutate_and_crossover     - \"cross_and_mutate\" : cross_and_mutate</p> Source code in <code>tpot/population.py</code> <pre><code>def create_offspring(self, parents_list, var_op_list, rng, add_to_population=True, keep_repeats=False, mutate_until_unique=True, n_jobs=1):\n    '''\n    parents_list: a list of lists of parents.\n    var_op_list: a list of var_ops to apply to each list of parents. Should be the same length as parents_list.\n\n    for example:\n    parents_list = [[parent1, parent2], [parent3]]\n    var_op_list = [\"crossover\", \"mutate\"]\n\n    This will apply crossover to parent1 and parent2 and mutate to parent3.\n\n    Creates offspring from parents using the var_op_list.\n    If string, will use a built in method\n        - \"crossover\" : crossover\n        - \"mutate\" : mutate\n        - \"mutate_and_crossover\" : mutate_and_crossover\n        - \"cross_and_mutate\" : cross_and_mutate\n    '''\n    rng = np.random.default_rng(rng)\n    new_offspring = []\n    all_offspring = parallel_create_offspring(parents_list, var_op_list, rng=rng, n_jobs=n_jobs)\n\n    for parents, offspring, var_op in zip(parents_list, all_offspring, var_op_list):\n\n        # if var_op in built_in_var_ops_dict:\n        #     var_op = built_in_var_ops_dict[var_op]\n\n        # offspring = copy.deepcopy(parents)\n        # offspring = var_op(offspring)\n        # if isinstance(offspring, collections.abc.Iterable):\n        #     offspring = offspring[0]\n\n        if add_to_population:\n            added = self.add_to_population(offspring, rng=rng, keep_repeats=keep_repeats, mutate_until_unique=mutate_until_unique)\n            if len(added) &gt; 0:\n                for new_child in added:\n                    parent_keys = [parent.unique_id() for parent in parents]\n                    if not pd.api.types.is_object_dtype(self.evaluated_individuals[\"Parents\"]): #TODO Is there a cleaner way of doing this? Not required for some python environments?\n                        self.evaluated_individuals[\"Parents\"] = self.evaluated_individuals[\"Parents\"].astype('object')\n                    if not pd.api.types.is_object_dtype(self.evaluated_individuals[\"Variation_Function\"]):#TODO Is there a cleaner way of doing this? Not required for some python environments?\n                        self.evaluated_individuals[\"Variation_Function\"] = self.evaluated_individuals[\"Variation_Function\"].astype('object')\n                    self.evaluated_individuals.at[new_child.unique_id(),\"Parents\"] = tuple(parent_keys)\n\n                    #if var_op is a function\n                    if hasattr(var_op, '__call__'):\n                        self.evaluated_individuals.at[new_child.unique_id(),\"Variation_Function\"] = var_op.__name__\n                    else:\n                        self.evaluated_individuals.at[new_child.unique_id(),\"Variation_Function\"] = str(var_op)\n\n\n                    new_offspring.append(new_child)\n\n        else:\n            new_offspring.append(offspring)\n\n\n    return new_offspring\n</code></pre>"},{"location":"documentation/tpot/population/#tpot.population.Population.get_column","title":"<code>get_column(individual, column_names=None, to_numpy=True)</code>","text":"<p>Update the column_name column in the evaluated_individuals with the data. If the data is a list, it must be the same length as the evaluated_individuals. If the data is a single value, it will be applied to all individuals in the evaluated_individuals.</p> Source code in <code>tpot/population.py</code> <pre><code>def get_column(self, individual, column_names=None, to_numpy=True):\n    '''\n    Update the column_name column in the evaluated_individuals with the data.\n    If the data is a list, it must be the same length as the evaluated_individuals.\n    If the data is a single value, it will be applied to all individuals in the evaluated_individuals.\n    '''\n    if isinstance(individual, collections.abc.Iterable):\n        if self.use_unique_id:\n            key = [ind.unique_id() for ind in individual]\n        else:\n            key = individual\n    else:\n        if self.use_unique_id:\n            key = individual.unique_id()\n        else:\n            key = individual\n\n    if column_names is not None:\n        slice = self.evaluated_individuals.loc[key,column_names]\n    else:\n        slice = self.evaluated_individuals.loc[key]\n    if to_numpy:\n        slice.reset_index(drop=True, inplace=True)\n        return slice.to_numpy()\n    else:\n        return slice\n</code></pre>"},{"location":"documentation/tpot/population/#tpot.population.Population.remove_invalid_from_population","title":"<code>remove_invalid_from_population(column_names, invalid_value='INVALID')</code>","text":"<p>Remove individuals from the live population if either do not have a value in the column_name column or if the value contains np.nan.</p> <p>Parameters:</p> Name Type Description Default <code>column_name</code> <code>str</code> <p>The name of the column to check for np.nan values.</p> <code>str</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>tpot/population.py</code> <pre><code>def remove_invalid_from_population(self, column_names, invalid_value = \"INVALID\"):\n    '''\n    Remove individuals from the live population if either do not have a value in the column_name column or if the value contains np.nan.\n\n    Parameters\n    ----------\n    column_name : {str}\n        The name of the column to check for np.nan values.\n\n    Returns\n    -------\n    None\n    '''\n    if isinstance(column_names, str): #TODO check this\n        column_names = [column_names]\n    is_valid = lambda ind: ind.unique_id() not in self.evaluated_individuals.index or invalid_value not in self.evaluated_individuals.loc[ind.unique_id(),column_names].to_list()\n    self.population = [ind for ind in self.population if is_valid(ind)]\n</code></pre>"},{"location":"documentation/tpot/population/#tpot.population.Population.set_population","title":"<code>set_population(new_population, rng, keep_repeats=True)</code>","text":"<p>sets population to new population for selection?</p> Source code in <code>tpot/population.py</code> <pre><code>def set_population(self,  new_population, rng, keep_repeats=True):\n    '''\n    sets population to new population\n    for selection?\n    '''\n    rng = np.random.default_rng(rng)\n    self.population = []\n    self.add_to_population(new_population, rng=rng, keep_repeats=keep_repeats)\n</code></pre>"},{"location":"documentation/tpot/population/#tpot.population.Population.update_column","title":"<code>update_column(individual, column_names, data)</code>","text":"<p>Update the column_name column in the evaluated_individuals with the data. If the data is a list, it must be the same length as the evaluated_individuals. If the data is a single value, it will be applied to all individuals in the evaluated_individuals.</p> Source code in <code>tpot/population.py</code> <pre><code>def update_column(self, individual, column_names, data):\n    '''\n    Update the column_name column in the evaluated_individuals with the data.\n    If the data is a list, it must be the same length as the evaluated_individuals.\n    If the data is a single value, it will be applied to all individuals in the evaluated_individuals.\n    '''\n    if isinstance(individual, collections.abc.Iterable):\n        if self.use_unique_id:\n            key = [ind.unique_id() for ind in individual]\n        else:\n            key = individual\n    else:\n        if self.use_unique_id:\n            key = individual.unique_id()\n        else:\n            key = individual\n\n    self.evaluated_individuals.loc[key,column_names] = data\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/","title":"Arithmetictransformer","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.AddTransformer","title":"<code>AddTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>class AddTransformer(BaseEstimator,TransformerMixin):\n    def __init__(self):\n          \"\"\"\n          A transformer that adds all elements along axis 1.\n          \"\"\"\n          pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        transformed_X = np.array(self.transform_helper(np.array(X)))\n        if transformed_X.dtype != float:\n            transformed_X = transformed_X.astype(float)\n\n        return transformed_X\n\n    def transform_helper(self, X):\n        X = np.array(X)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X,0)\n        return np.expand_dims(np.sum(X,1),1)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.AddTransformer.__init__","title":"<code>__init__()</code>","text":"<p>A transformer that adds all elements along axis 1.</p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>def __init__(self):\n      \"\"\"\n      A transformer that adds all elements along axis 1.\n      \"\"\"\n      pass\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.ArithmeticTransformer","title":"<code>ArithmeticTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>class ArithmeticTransformer(BaseEstimator,TransformerMixin):\n\n    #functions = [\"add\", \"mul_neg_1\", \"mul\", \"safe_reciprocal\", \"eq\",\"ne\",\"ge\",\"gt\",\"le\",\"lt\", \"min\",\"max\",\"0\",\"1\"]\n    def __init__(self, function,):\n        \"\"\"\n        A transformer that applies a function to the input array along axis 1.\n        Parameters\n        ----------\n\n        function : str\n            The function to apply to the input array. The following functions are supported:\n            - 'add' : Add all elements along axis 1\n            - 'mul_neg_1' : Multiply all elements along axis 1 by -1\n            - 'mul' : Multiply all elements along axis 1\n            - 'safe_reciprocal' : Take the reciprocal of all elements along axis 1, with a safe division by zero\n            - 'eq' : Check if all elements along axis 1 are equal\n            - 'ne' : Check if all elements along axis 1 are not equal\n            - 'ge' : Check if all elements along axis 1 are greater than or equal to 0\n            - 'gt' : Check if all elements along axis 1 are greater than 0\n            - 'le' : Check if all elements along axis 1 are less than or equal to 0\n            - 'lt' : Check if all elements along axis 1 are less than 0\n            - 'min' : Take the minimum of all elements along axis 1\n            - 'max' : Take the maximum of all elements along axis 1\n            - '0' : Return an array of zeros\n            - '1' : Return an array of ones\n        \"\"\"\n        self.function = function\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        transformed_X = np.array(self.transform_helper(np.array(X)))\n        if transformed_X.dtype != float:\n            transformed_X = transformed_X.astype(float)\n\n        return transformed_X\n\n    def transform_helper(self, X):\n        X = np.array(X)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X,0)\n        if self.function == \"add\":\n                return np.expand_dims(np.sum(X,1),1)\n        elif self.function == \"mul_neg_1\":\n                return X*-1\n        elif self.function == \"mul\":\n                return np.expand_dims(np.prod(X,1),1)\n\n        elif self.function == \"safe_reciprocal\":\n                results = np.divide(1.0, X.astype(float), out=np.zeros_like(X).astype(float), where=X!=0) #TODO remove astypefloat?\n                return results\n\n        elif self.function == \"eq\":\n                return np.expand_dims(np.all(X == X[0,:], axis = 1),1).astype(float)\n\n        elif self.function == \"ne\":\n                return 1- np.expand_dims(np.all(X == X[0,:], axis = 1),1).astype(float)\n\n        #TODO these could be \"sorted order\"\n        elif self.function == \"ge\":\n                result = X &gt;= 0\n                return  result.astype(float)\n\n        elif self.function == \"gt\":\n                result = X &gt; 0\n                return  result.astype(float)\n        elif self.function ==  \"le\":\n                result = X &lt;= 0\n                return  result.astype(float)\n        elif self.function ==  \"lt\":\n                result = X &lt; 0\n                return  result.astype(float)\n\n\n        elif self.function ==   \"min\":\n                return np.expand_dims(np.amin(X,1),1)\n        elif self.function ==  \"max\":\n                return np.expand_dims(np.amax(X,1),1)\n\n        elif self.function ==  \"0\":\n                return np.zeros((X.shape[0],1))\n        elif self.function ==  \"1\":\n                return np.ones((X.shape[0],1))\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.ArithmeticTransformer.__init__","title":"<code>__init__(function)</code>","text":"<p>A transformer that applies a function to the input array along axis 1.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>str</code> <p>The function to apply to the input array. The following functions are supported: - 'add' : Add all elements along axis 1 - 'mul_neg_1' : Multiply all elements along axis 1 by -1 - 'mul' : Multiply all elements along axis 1 - 'safe_reciprocal' : Take the reciprocal of all elements along axis 1, with a safe division by zero - 'eq' : Check if all elements along axis 1 are equal - 'ne' : Check if all elements along axis 1 are not equal - 'ge' : Check if all elements along axis 1 are greater than or equal to 0 - 'gt' : Check if all elements along axis 1 are greater than 0 - 'le' : Check if all elements along axis 1 are less than or equal to 0 - 'lt' : Check if all elements along axis 1 are less than 0 - 'min' : Take the minimum of all elements along axis 1 - 'max' : Take the maximum of all elements along axis 1 - '0' : Return an array of zeros - '1' : Return an array of ones</p> required Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>def __init__(self, function,):\n    \"\"\"\n    A transformer that applies a function to the input array along axis 1.\n    Parameters\n    ----------\n\n    function : str\n        The function to apply to the input array. The following functions are supported:\n        - 'add' : Add all elements along axis 1\n        - 'mul_neg_1' : Multiply all elements along axis 1 by -1\n        - 'mul' : Multiply all elements along axis 1\n        - 'safe_reciprocal' : Take the reciprocal of all elements along axis 1, with a safe division by zero\n        - 'eq' : Check if all elements along axis 1 are equal\n        - 'ne' : Check if all elements along axis 1 are not equal\n        - 'ge' : Check if all elements along axis 1 are greater than or equal to 0\n        - 'gt' : Check if all elements along axis 1 are greater than 0\n        - 'le' : Check if all elements along axis 1 are less than or equal to 0\n        - 'lt' : Check if all elements along axis 1 are less than 0\n        - 'min' : Take the minimum of all elements along axis 1\n        - 'max' : Take the maximum of all elements along axis 1\n        - '0' : Return an array of zeros\n        - '1' : Return an array of ones\n    \"\"\"\n    self.function = function\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.EQTransformer","title":"<code>EQTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>class EQTransformer(BaseEstimator,TransformerMixin):\n\n    def __init__(self):\n        \"\"\"\n        A transformer that takes checks if all elements in a row are equal.\n        \"\"\"\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        transformed_X = np.array(self.transform_helper(np.array(X)))\n        if transformed_X.dtype != float:\n            transformed_X = transformed_X.astype(float)\n\n        return transformed_X\n\n    def transform_helper(self, X):\n        X = np.array(X)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X,0)\n        return np.expand_dims(np.all(X == X[0,:], axis = 1),1).astype(float)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.EQTransformer.__init__","title":"<code>__init__()</code>","text":"<p>A transformer that takes checks if all elements in a row are equal.</p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    A transformer that takes checks if all elements in a row are equal.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.GETransformer","title":"<code>GETransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>class GETransformer(BaseEstimator,TransformerMixin):\n\n    def __init__(self):\n        \"\"\"\n        A transformer that takes checks if all elements in a row are greater than or equal to 0.\n        \"\"\"\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        transformed_X = np.array(self.transform_helper(np.array(X)))\n        if transformed_X.dtype != float:\n            transformed_X = transformed_X.astype(float)\n\n        return transformed_X\n\n    def transform_helper(self, X):\n        X = np.array(X)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X,0)\n        result = X &gt;= 0\n        return  result.astype(float)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.GETransformer.__init__","title":"<code>__init__()</code>","text":"<p>A transformer that takes checks if all elements in a row are greater than or equal to 0.</p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    A transformer that takes checks if all elements in a row are greater than or equal to 0.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.GTTransformer","title":"<code>GTTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>class GTTransformer(BaseEstimator,TransformerMixin):\n    def __init__(self):\n          \"\"\"\n          A transformer that takes checks if all elements in a row are greater than 0.\n          \"\"\"\n          pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        transformed_X = np.array(self.transform_helper(np.array(X)))\n        if transformed_X.dtype != float:\n            transformed_X = transformed_X.astype(float)\n\n        return transformed_X\n\n    def transform_helper(self, X):\n        X = np.array(X)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X,0)\n        result = X &gt; 0\n        return  result.astype(float)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.GTTransformer.__init__","title":"<code>__init__()</code>","text":"<p>A transformer that takes checks if all elements in a row are greater than 0.</p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>def __init__(self):\n      \"\"\"\n      A transformer that takes checks if all elements in a row are greater than 0.\n      \"\"\"\n      pass\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.LETransformer","title":"<code>LETransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>class LETransformer(BaseEstimator,TransformerMixin):\n    def __init__(self):\n        \"\"\"\n        A transformer that takes checks if all elements in a row are less than or equal to 0.\n        \"\"\"\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        transformed_X = np.array(self.transform_helper(np.array(X)))\n        if transformed_X.dtype != float:\n            transformed_X = transformed_X.astype(float)\n\n        return transformed_X\n\n    def transform_helper(self, X):\n        X = np.array(X)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X,0)\n        result = X &lt;= 0\n        return  result.astype(float)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.LETransformer.__init__","title":"<code>__init__()</code>","text":"<p>A transformer that takes checks if all elements in a row are less than or equal to 0.</p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    A transformer that takes checks if all elements in a row are less than or equal to 0.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.LTTransformer","title":"<code>LTTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>class LTTransformer(BaseEstimator,TransformerMixin):\n    def __init__(self):\n        \"\"\"\n        A transformer that takes checks if all elements in a row are less than 0.\n        \"\"\"\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        transformed_X = np.array(self.transform_helper(np.array(X)))\n        if transformed_X.dtype != float:\n            transformed_X = transformed_X.astype(float)\n\n        return transformed_X\n\n    def transform_helper(self, X):\n        X = np.array(X)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X,0)\n        result = X &lt; 0\n        return  result.astype(float)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.LTTransformer.__init__","title":"<code>__init__()</code>","text":"<p>A transformer that takes checks if all elements in a row are less than 0.</p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    A transformer that takes checks if all elements in a row are less than 0.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.MaxTransformer","title":"<code>MaxTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>class MaxTransformer(BaseEstimator,TransformerMixin):\n\n    def __init__(self):\n          \"\"\"\n          A transformer that takes the maximum of all elements in a row.\n          \"\"\"\n          pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        transformed_X = np.array(self.transform_helper(np.array(X)))\n        if transformed_X.dtype != float:\n            transformed_X = transformed_X.astype(float)\n\n        return transformed_X\n\n    def transform_helper(self, X):\n        X = np.array(X)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X,0)\n        return np.expand_dims(np.amax(X,1),1)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.MaxTransformer.__init__","title":"<code>__init__()</code>","text":"<p>A transformer that takes the maximum of all elements in a row.</p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>def __init__(self):\n      \"\"\"\n      A transformer that takes the maximum of all elements in a row.\n      \"\"\"\n      pass\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.MinTransformer","title":"<code>MinTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>class MinTransformer(BaseEstimator,TransformerMixin):\n    def __init__(self):\n        \"\"\"\n        A transformer that takes the minimum of all elements in a row.\n        \"\"\"\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        transformed_X = np.array(self.transform_helper(np.array(X)))\n        if transformed_X.dtype != float:\n            transformed_X = transformed_X.astype(float)\n\n        return transformed_X\n\n    def transform_helper(self, X):\n        X = np.array(X)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X,0)\n        return np.expand_dims(np.amin(X,1),1)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.MinTransformer.__init__","title":"<code>__init__()</code>","text":"<p>A transformer that takes the minimum of all elements in a row.</p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    A transformer that takes the minimum of all elements in a row.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.MulTransformer","title":"<code>MulTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>class MulTransformer(BaseEstimator,TransformerMixin):\n\n    def __init__(self):\n        \"\"\"\n        A transformer that multiplies all elements along axis 1.\n        \"\"\"\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        transformed_X = np.array(self.transform_helper(np.array(X)))\n        if transformed_X.dtype != float:\n            transformed_X = transformed_X.astype(float)\n\n        return transformed_X\n\n    def transform_helper(self, X):\n        X = np.array(X)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X,0)\n        return np.expand_dims(np.prod(X,1),1)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.MulTransformer.__init__","title":"<code>__init__()</code>","text":"<p>A transformer that multiplies all elements along axis 1.</p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    A transformer that multiplies all elements along axis 1.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.NETransformer","title":"<code>NETransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>class NETransformer(BaseEstimator,TransformerMixin):\n\n    def __init__(self):\n        \"\"\"\n        A transformer that takes checks if all elements in a row are not equal.\n        \"\"\"  \n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        transformed_X = np.array(self.transform_helper(np.array(X)))\n        if transformed_X.dtype != float:\n            transformed_X = transformed_X.astype(float)\n\n        return transformed_X\n\n    def transform_helper(self, X):\n        X = np.array(X)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X,0)\n        return 1- np.expand_dims(np.all(X == X[0,:], axis = 1),1).astype(float)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.NETransformer.__init__","title":"<code>__init__()</code>","text":"<p>A transformer that takes checks if all elements in a row are not equal.</p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    A transformer that takes checks if all elements in a row are not equal.\n    \"\"\"  \n    pass\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.NTransformer","title":"<code>NTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>class NTransformer(BaseEstimator,TransformerMixin):\n\n    def __init__(self, n):\n        \"\"\"\n        A transformer that returns an array of n.\n        \"\"\"\n        self.n = n\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        transformed_X = np.array(self.transform_helper(np.array(X)))\n        if transformed_X.dtype != float:\n            transformed_X = transformed_X.astype(float)\n\n        return transformed_X\n\n    def transform_helper(self, X):\n        X = np.array(X)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X,0)\n        return np.ones((X.shape[0],1))*self.n\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.NTransformer.__init__","title":"<code>__init__(n)</code>","text":"<p>A transformer that returns an array of n.</p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>def __init__(self, n):\n    \"\"\"\n    A transformer that returns an array of n.\n    \"\"\"\n    self.n = n\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.OneTransformer","title":"<code>OneTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>class OneTransformer(BaseEstimator,TransformerMixin):\n    def __init__(self):\n          \"\"\"\n          A transformer that returns an array of ones.\n          \"\"\"\n          pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        transformed_X = np.array(self.transform_helper(np.array(X)))\n        if transformed_X.dtype != float:\n            transformed_X = transformed_X.astype(float)\n\n        return transformed_X\n\n    def transform_helper(self, X):\n        X = np.array(X)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X,0)\n        return np.ones((X.shape[0],1))\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.OneTransformer.__init__","title":"<code>__init__()</code>","text":"<p>A transformer that returns an array of ones.</p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>def __init__(self):\n      \"\"\"\n      A transformer that returns an array of ones.\n      \"\"\"\n      pass\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.SafeReciprocalTransformer","title":"<code>SafeReciprocalTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>class SafeReciprocalTransformer(BaseEstimator,TransformerMixin):\n\n    def __init__(self):\n        \"\"\"\n        A transformer that takes the reciprocal of all elements, with a safe division by zero.\n        \"\"\"\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        transformed_X = np.array(self.transform_helper(np.array(X)))\n        if transformed_X.dtype != float:\n            transformed_X = transformed_X.astype(float)\n\n        return transformed_X\n\n    def transform_helper(self, X):\n        X = np.array(X)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X,0)\n        return np.divide(1.0, X.astype(float), out=np.zeros_like(X).astype(float), where=X!=0) #TODO remove astypefloat?\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.SafeReciprocalTransformer.__init__","title":"<code>__init__()</code>","text":"<p>A transformer that takes the reciprocal of all elements, with a safe division by zero.</p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    A transformer that takes the reciprocal of all elements, with a safe division by zero.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.ZeroTransformer","title":"<code>ZeroTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>class ZeroTransformer(BaseEstimator,TransformerMixin):\n\n    def __init__(self):\n          \"\"\"\n        A transformer that returns an array of zeros.\n          \"\"\"\n          pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        transformed_X = np.array(self.transform_helper(np.array(X)))\n        if transformed_X.dtype != float:\n            transformed_X = transformed_X.astype(float)\n\n        return transformed_X\n\n    def transform_helper(self, X):\n        X = np.array(X)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X,0)\n        return np.zeros((X.shape[0],1))\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.ZeroTransformer.__init__","title":"<code>__init__()</code>","text":"<p>A transformer that returns an array of zeros.</p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>def __init__(self):\n      \"\"\"\n    A transformer that returns an array of zeros.\n      \"\"\"\n      pass\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.mul_neg_1_Transformer","title":"<code>mul_neg_1_Transformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>class mul_neg_1_Transformer(BaseEstimator,TransformerMixin):\n    def __init__(self):\n        \"\"\"\n        A transformer that multiplies all elements by -1.\n        \"\"\"\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        transformed_X = np.array(self.transform_helper(np.array(X)))\n        if transformed_X.dtype != float:\n            transformed_X = transformed_X.astype(float)\n\n        return transformed_X\n\n    def transform_helper(self, X):\n        X = np.array(X)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X,0)\n        return X*-1\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.mul_neg_1_Transformer.__init__","title":"<code>__init__()</code>","text":"<p>A transformer that multiplies all elements by -1.</p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    A transformer that multiplies all elements by -1.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/column_one_hot_encoder/","title":"Column one hot encoder","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/builtin_modules/column_one_hot_encoder/#tpot.builtin_modules.column_one_hot_encoder.ColumnOneHotEncoder","title":"<code>ColumnOneHotEncoder</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/column_one_hot_encoder.py</code> <pre><code>class ColumnOneHotEncoder(BaseEstimator, TransformerMixin):\n\n\n    def __init__(self, columns='auto', drop=None, handle_unknown='infrequent_if_exist', sparse_output=False, min_frequency=None,max_categories=None):\n        '''\n        A wrapper for OneHotEncoder that allows for onehot encoding of specific columns in a DataFrame or np array.\n\n        Parameters\n        ----------\n\n        columns : str, list, default='auto'\n            Determines which columns to onehot encode with sklearn.preprocessing.OneHotEncoder.\n            - 'auto' : Automatically select categorical features based on columns with less than 10 unique values\n            - 'categorical' : Automatically select categorical features\n            - 'numeric' : Automatically select numeric features\n            - 'all' : Select all features\n            - list : A list of columns to select\n\n        drop, handle_unknown, sparse_output, min_frequency, max_categories : see sklearn.preprocessing.OneHotEncoder\n\n        '''\n\n        self.columns = columns\n        self.drop = drop\n        self.handle_unknown = handle_unknown\n        self.sparse_output = sparse_output\n        self.min_frequency = min_frequency\n        self.max_categories = max_categories\n\n\n\n    def fit(self, X, y=None):\n        \"\"\"Fit OneHotEncoder to X, then transform X.\n\n        Equivalent to self.fit(X).transform(X), but more convenient and more\n        efficient. See fit for the parameters, transform for the return value.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape=(n_samples, n_features)\n            Dense array or sparse matrix.\n        y: array-like {n_samples,} (Optional, ignored)\n            Feature labels\n        \"\"\"\n\n        if (self.columns == \"categorical\" or self.columns == \"numeric\") and not isinstance(X, pd.DataFrame):\n            raise ValueError(f\"Invalid value for columns: {self.columns}. \"\n                             \"Only 'all' or &lt;list&gt; is supported for np arrays\")\n\n        if self.columns == \"categorical\":\n            self.columns_ = list(X.select_dtypes(exclude='number').columns)\n        elif self.columns == \"numeric\":\n            self.columns_ =  [col for col in X.columns if is_numeric_dtype(X[col])]\n        elif self.columns == \"auto\":\n            self.columns_ = auto_select_categorical_features(X)\n        elif self.columns == \"all\":\n            if isinstance(X, pd.DataFrame):\n                self.columns_ = X.columns\n            else:\n                self.columns_ = list(range(X.shape[1]))\n        elif isinstance(self.columns, list):\n            self.columns_ = self.columns\n        else:\n            raise ValueError(f\"Invalid value for columns: {self.columns}\")\n\n\n\n        if len(self.columns_) == 0:\n            return self\n\n        self.enc = sklearn.preprocessing.OneHotEncoder( categories='auto',   \n                                                        drop = self.drop,\n                                                        handle_unknown = self.handle_unknown,\n                                                        sparse_output = self.sparse_output,\n                                                        min_frequency = self.min_frequency,\n                                                        max_categories = self.max_categories)\n\n        #TODO make this more consistent with sklearn baseimputer/baseencoder\n        if isinstance(X, pd.DataFrame):\n            self.enc.set_output(transform=\"pandas\")\n            for col in X.columns:\n                # check if the column name is not a string\n                if not isinstance(col, str):\n                    # if it's not a string, rename the column with \"X\" prefix\n                    X.rename(columns={col: f\"X{col}\"}, inplace=True)\n\n\n        if len(self.columns_) == X.shape[1]:\n            X_sel = self.enc.fit(X)\n        else:\n            X_sel, X_not_sel = _X_selected(X, self.columns_)\n            X_sel = self.enc.fit(X_sel)\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Transform X using one-hot encoding.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape=(n_samples, n_features)\n            Dense array or sparse matrix.\n\n        Returns\n        -------\n        X_out : sparse matrix if sparse=True else a 2-d array, dtype=int\n            Transformed input.\n        \"\"\"\n\n\n        if len(self.columns_) == 0:\n            return X\n\n        #TODO make this more consistent with sklearn baseimputer/baseencoder\n        if isinstance(X, pd.DataFrame):\n            for col in X.columns:\n                # check if the column name is not a string\n                if not isinstance(col, str):\n                    # if it's not a string, rename the column with \"X\" prefix\n                    X.rename(columns={col: f\"X{col}\"}, inplace=True)\n\n        if len(self.columns_) == X.shape[1]:\n            return self.enc.transform(X)\n        else:\n\n            X_sel, X_not_sel= _X_selected(X, self.columns_)\n            X_sel = self.enc.transform(X_sel)\n\n            #If X is dataframe\n            if isinstance(X, pd.DataFrame):\n\n                X_sel = pd.DataFrame(X_sel, columns=self.enc.get_feature_names_out())\n                return pd.concat([X_not_sel.reset_index(drop=True), X_sel.reset_index(drop=True)], axis=1)\n            else:\n                return np.hstack((X_not_sel, X_sel))\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/column_one_hot_encoder/#tpot.builtin_modules.column_one_hot_encoder.ColumnOneHotEncoder.__init__","title":"<code>__init__(columns='auto', drop=None, handle_unknown='infrequent_if_exist', sparse_output=False, min_frequency=None, max_categories=None)</code>","text":"<p>A wrapper for OneHotEncoder that allows for onehot encoding of specific columns in a DataFrame or np array.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>(str, list)</code> <p>Determines which columns to onehot encode with sklearn.preprocessing.OneHotEncoder. - 'auto' : Automatically select categorical features based on columns with less than 10 unique values - 'categorical' : Automatically select categorical features - 'numeric' : Automatically select numeric features - 'all' : Select all features - list : A list of columns to select</p> <code>'auto'</code> <code>drop</code> <code>see sklearn.preprocessing.OneHotEncoder</code> <code>None</code> <code>handle_unknown</code> <code>see sklearn.preprocessing.OneHotEncoder</code> <code>None</code> <code>sparse_output</code> <code>see sklearn.preprocessing.OneHotEncoder</code> <code>None</code> <code>min_frequency</code> <code>see sklearn.preprocessing.OneHotEncoder</code> <code>None</code> <code>max_categories</code> <code>see sklearn.preprocessing.OneHotEncoder</code> <code>None</code> Source code in <code>tpot/builtin_modules/column_one_hot_encoder.py</code> <pre><code>def __init__(self, columns='auto', drop=None, handle_unknown='infrequent_if_exist', sparse_output=False, min_frequency=None,max_categories=None):\n    '''\n    A wrapper for OneHotEncoder that allows for onehot encoding of specific columns in a DataFrame or np array.\n\n    Parameters\n    ----------\n\n    columns : str, list, default='auto'\n        Determines which columns to onehot encode with sklearn.preprocessing.OneHotEncoder.\n        - 'auto' : Automatically select categorical features based on columns with less than 10 unique values\n        - 'categorical' : Automatically select categorical features\n        - 'numeric' : Automatically select numeric features\n        - 'all' : Select all features\n        - list : A list of columns to select\n\n    drop, handle_unknown, sparse_output, min_frequency, max_categories : see sklearn.preprocessing.OneHotEncoder\n\n    '''\n\n    self.columns = columns\n    self.drop = drop\n    self.handle_unknown = handle_unknown\n    self.sparse_output = sparse_output\n    self.min_frequency = min_frequency\n    self.max_categories = max_categories\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/column_one_hot_encoder/#tpot.builtin_modules.column_one_hot_encoder.ColumnOneHotEncoder.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit OneHotEncoder to X, then transform X.</p> <p>Equivalent to self.fit(X).transform(X), but more convenient and more efficient. See fit for the parameters, transform for the return value.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like or sparse matrix, shape=(n_samples, n_features)</code> <p>Dense array or sparse matrix.</p> required <code>y</code> <p>Feature labels</p> <code>None</code> Source code in <code>tpot/builtin_modules/column_one_hot_encoder.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Fit OneHotEncoder to X, then transform X.\n\n    Equivalent to self.fit(X).transform(X), but more convenient and more\n    efficient. See fit for the parameters, transform for the return value.\n\n    Parameters\n    ----------\n    X : array-like or sparse matrix, shape=(n_samples, n_features)\n        Dense array or sparse matrix.\n    y: array-like {n_samples,} (Optional, ignored)\n        Feature labels\n    \"\"\"\n\n    if (self.columns == \"categorical\" or self.columns == \"numeric\") and not isinstance(X, pd.DataFrame):\n        raise ValueError(f\"Invalid value for columns: {self.columns}. \"\n                         \"Only 'all' or &lt;list&gt; is supported for np arrays\")\n\n    if self.columns == \"categorical\":\n        self.columns_ = list(X.select_dtypes(exclude='number').columns)\n    elif self.columns == \"numeric\":\n        self.columns_ =  [col for col in X.columns if is_numeric_dtype(X[col])]\n    elif self.columns == \"auto\":\n        self.columns_ = auto_select_categorical_features(X)\n    elif self.columns == \"all\":\n        if isinstance(X, pd.DataFrame):\n            self.columns_ = X.columns\n        else:\n            self.columns_ = list(range(X.shape[1]))\n    elif isinstance(self.columns, list):\n        self.columns_ = self.columns\n    else:\n        raise ValueError(f\"Invalid value for columns: {self.columns}\")\n\n\n\n    if len(self.columns_) == 0:\n        return self\n\n    self.enc = sklearn.preprocessing.OneHotEncoder( categories='auto',   \n                                                    drop = self.drop,\n                                                    handle_unknown = self.handle_unknown,\n                                                    sparse_output = self.sparse_output,\n                                                    min_frequency = self.min_frequency,\n                                                    max_categories = self.max_categories)\n\n    #TODO make this more consistent with sklearn baseimputer/baseencoder\n    if isinstance(X, pd.DataFrame):\n        self.enc.set_output(transform=\"pandas\")\n        for col in X.columns:\n            # check if the column name is not a string\n            if not isinstance(col, str):\n                # if it's not a string, rename the column with \"X\" prefix\n                X.rename(columns={col: f\"X{col}\"}, inplace=True)\n\n\n    if len(self.columns_) == X.shape[1]:\n        X_sel = self.enc.fit(X)\n    else:\n        X_sel, X_not_sel = _X_selected(X, self.columns_)\n        X_sel = self.enc.fit(X_sel)\n\n    return self\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/column_one_hot_encoder/#tpot.builtin_modules.column_one_hot_encoder.ColumnOneHotEncoder.transform","title":"<code>transform(X)</code>","text":"<p>Transform X using one-hot encoding.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like or sparse matrix, shape=(n_samples, n_features)</code> <p>Dense array or sparse matrix.</p> required <p>Returns:</p> Name Type Description <code>X_out</code> <code>sparse matrix if sparse=True else a 2-d array, dtype=int</code> <p>Transformed input.</p> Source code in <code>tpot/builtin_modules/column_one_hot_encoder.py</code> <pre><code>def transform(self, X):\n    \"\"\"Transform X using one-hot encoding.\n\n    Parameters\n    ----------\n    X : array-like or sparse matrix, shape=(n_samples, n_features)\n        Dense array or sparse matrix.\n\n    Returns\n    -------\n    X_out : sparse matrix if sparse=True else a 2-d array, dtype=int\n        Transformed input.\n    \"\"\"\n\n\n    if len(self.columns_) == 0:\n        return X\n\n    #TODO make this more consistent with sklearn baseimputer/baseencoder\n    if isinstance(X, pd.DataFrame):\n        for col in X.columns:\n            # check if the column name is not a string\n            if not isinstance(col, str):\n                # if it's not a string, rename the column with \"X\" prefix\n                X.rename(columns={col: f\"X{col}\"}, inplace=True)\n\n    if len(self.columns_) == X.shape[1]:\n        return self.enc.transform(X)\n    else:\n\n        X_sel, X_not_sel= _X_selected(X, self.columns_)\n        X_sel = self.enc.transform(X_sel)\n\n        #If X is dataframe\n        if isinstance(X, pd.DataFrame):\n\n            X_sel = pd.DataFrame(X_sel, columns=self.enc.get_feature_names_out())\n            return pd.concat([X_not_sel.reset_index(drop=True), X_sel.reset_index(drop=True)], axis=1)\n        else:\n            return np.hstack((X_not_sel, X_sel))\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/column_one_hot_encoder/#tpot.builtin_modules.column_one_hot_encoder.ColumnOrdinalEncoder","title":"<code>ColumnOrdinalEncoder</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/column_one_hot_encoder.py</code> <pre><code>class ColumnOrdinalEncoder(BaseEstimator, TransformerMixin):\n\n\n    def __init__(self, columns='auto', handle_unknown='error', unknown_value = -1, encoded_missing_value = np.nan, min_frequency=None,max_categories=None):\n        '''\n\n        Parameters\n        ----------\n\n        columns : str, list, default='auto'\n            Determines which columns to onehot encode with sklearn.preprocessing.OneHotEncoder.\n            - 'auto' : Automatically select categorical features based on columns with less than 10 unique values\n            - 'categorical' : Automatically select categorical features\n            - 'numeric' : Automatically select numeric features\n            - 'all' : Select all features\n            - list : A list of columns to select\n\n        drop, handle_unknown, sparse_output, min_frequency, max_categories : see sklearn.preprocessing.OneHotEncoder\n\n        '''\n\n        self.columns = columns\n        self.handle_unknown = handle_unknown\n        self.unknown_value = unknown_value\n        self.encoded_missing_value = encoded_missing_value\n        self.min_frequency = min_frequency\n        self.max_categories = max_categories\n\n\n\n    def fit(self, X, y=None):\n        \"\"\"Fit OneHotEncoder to X, then transform X.\n\n        Equivalent to self.fit(X).transform(X), but more convenient and more\n        efficient. See fit for the parameters, transform for the return value.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape=(n_samples, n_features)\n            Dense array or sparse matrix.\n        y: array-like {n_samples,} (Optional, ignored)\n            Feature labels\n        \"\"\"\n\n        if (self.columns == \"categorical\" or self.columns == \"numeric\") and not isinstance(X, pd.DataFrame):\n            raise ValueError(f\"Invalid value for columns: {self.columns}. \"\n                             \"Only 'all' or &lt;list&gt; is supported for np arrays\")\n\n        if self.columns == \"categorical\":\n            self.columns_ = list(X.select_dtypes(exclude='number').columns)\n        elif self.columns == \"numeric\":\n            self.columns_ =  [col for col in X.columns if is_numeric_dtype(X[col])]\n        elif self.columns == \"auto\":\n            self.columns_ = auto_select_categorical_features(X)\n        elif self.columns == \"all\":\n            if isinstance(X, pd.DataFrame):\n                self.columns_ = X.columns\n            else:\n                self.columns_ = list(range(X.shape[1]))\n        elif isinstance(self.columns, list):\n            self.columns_ = self.columns\n        else:\n            raise ValueError(f\"Invalid value for columns: {self.columns}\")\n\n        if len(self.columns_) == 0:\n            return self\n\n        self.enc = sklearn.preprocessing.OrdinalEncoder(categories='auto',   \n                                                        handle_unknown = self.handle_unknown,\n                                                        unknown_value = self.unknown_value, \n                                                        encoded_missing_value = self.encoded_missing_value,\n                                                        min_frequency = self.min_frequency,\n                                                        max_categories = self.max_categories)\n        #TODO make this more consistent with sklearn baseimputer/baseencoder\n        '''\n        if isinstance(X, pd.DataFrame):\n            self.enc.set_output(transform=\"pandas\")\n            for col in X.columns:\n                # check if the column name is not a string\n                if not isinstance(col, str):\n                    # if it's not a string, rename the column with \"X\" prefix\n                    X.rename(columns={col: f\"X{col}\"}, inplace=True)\n        '''\n\n        if len(self.columns_) == X.shape[1]:\n            X_sel = self.enc.fit(X)\n        else:\n            X_sel, X_not_sel = _X_selected(X, self.columns_)\n            X_sel = self.enc.fit(X_sel)\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Transform X using one-hot encoding.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape=(n_samples, n_features)\n            Dense array or sparse matrix.\n\n        Returns\n        -------\n        X_out : sparse matrix if sparse=True else a 2-d array, dtype=int\n            Transformed input.\n        \"\"\"\n\n\n        if len(self.columns_) == 0:\n            return X\n\n        #TODO make this more consistent with sklearn baseimputer/baseencoder\n        '''\n        if isinstance(X, pd.DataFrame):\n            for col in X.columns:\n                # check if the column name is not a string\n                if not isinstance(col, str):\n                    # if it's not a string, rename the column with \"X\" prefix\n                    X.rename(columns={col: f\"X{col}\"}, inplace=True)\n        '''\n\n        if len(self.columns_) == X.shape[1]:\n            return self.enc.transform(X)\n        else:\n\n            X_sel, X_not_sel= _X_selected(X, self.columns_)\n            X_sel = self.enc.transform(X_sel)\n\n            #If X is dataframe\n            if isinstance(X, pd.DataFrame):\n\n                X_sel = pd.DataFrame(X_sel, columns=self.enc.get_feature_names_out())\n                return pd.concat([X_not_sel.reset_index(drop=True), X_sel.reset_index(drop=True)], axis=1)\n            else:\n                return np.hstack((X_not_sel, X_sel))\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/column_one_hot_encoder/#tpot.builtin_modules.column_one_hot_encoder.ColumnOrdinalEncoder.__init__","title":"<code>__init__(columns='auto', handle_unknown='error', unknown_value=-1, encoded_missing_value=np.nan, min_frequency=None, max_categories=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>columns</code> <code>(str, list)</code> <p>Determines which columns to onehot encode with sklearn.preprocessing.OneHotEncoder. - 'auto' : Automatically select categorical features based on columns with less than 10 unique values - 'categorical' : Automatically select categorical features - 'numeric' : Automatically select numeric features - 'all' : Select all features - list : A list of columns to select</p> <code>'auto'</code> <code>drop</code> <code>see sklearn.preprocessing.OneHotEncoder</code> <code>'error'</code> <code>handle_unknown</code> <code>see sklearn.preprocessing.OneHotEncoder</code> <code>'error'</code> <code>sparse_output</code> <code>see sklearn.preprocessing.OneHotEncoder</code> <code>'error'</code> <code>min_frequency</code> <code>see sklearn.preprocessing.OneHotEncoder</code> <code>'error'</code> <code>max_categories</code> <code>see sklearn.preprocessing.OneHotEncoder</code> <code>'error'</code> Source code in <code>tpot/builtin_modules/column_one_hot_encoder.py</code> <pre><code>def __init__(self, columns='auto', handle_unknown='error', unknown_value = -1, encoded_missing_value = np.nan, min_frequency=None,max_categories=None):\n    '''\n\n    Parameters\n    ----------\n\n    columns : str, list, default='auto'\n        Determines which columns to onehot encode with sklearn.preprocessing.OneHotEncoder.\n        - 'auto' : Automatically select categorical features based on columns with less than 10 unique values\n        - 'categorical' : Automatically select categorical features\n        - 'numeric' : Automatically select numeric features\n        - 'all' : Select all features\n        - list : A list of columns to select\n\n    drop, handle_unknown, sparse_output, min_frequency, max_categories : see sklearn.preprocessing.OneHotEncoder\n\n    '''\n\n    self.columns = columns\n    self.handle_unknown = handle_unknown\n    self.unknown_value = unknown_value\n    self.encoded_missing_value = encoded_missing_value\n    self.min_frequency = min_frequency\n    self.max_categories = max_categories\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/column_one_hot_encoder/#tpot.builtin_modules.column_one_hot_encoder.ColumnOrdinalEncoder.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit OneHotEncoder to X, then transform X.</p> <p>Equivalent to self.fit(X).transform(X), but more convenient and more efficient. See fit for the parameters, transform for the return value.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like or sparse matrix, shape=(n_samples, n_features)</code> <p>Dense array or sparse matrix.</p> required <code>y</code> <p>Feature labels</p> <code>None</code> Source code in <code>tpot/builtin_modules/column_one_hot_encoder.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Fit OneHotEncoder to X, then transform X.\n\n    Equivalent to self.fit(X).transform(X), but more convenient and more\n    efficient. See fit for the parameters, transform for the return value.\n\n    Parameters\n    ----------\n    X : array-like or sparse matrix, shape=(n_samples, n_features)\n        Dense array or sparse matrix.\n    y: array-like {n_samples,} (Optional, ignored)\n        Feature labels\n    \"\"\"\n\n    if (self.columns == \"categorical\" or self.columns == \"numeric\") and not isinstance(X, pd.DataFrame):\n        raise ValueError(f\"Invalid value for columns: {self.columns}. \"\n                         \"Only 'all' or &lt;list&gt; is supported for np arrays\")\n\n    if self.columns == \"categorical\":\n        self.columns_ = list(X.select_dtypes(exclude='number').columns)\n    elif self.columns == \"numeric\":\n        self.columns_ =  [col for col in X.columns if is_numeric_dtype(X[col])]\n    elif self.columns == \"auto\":\n        self.columns_ = auto_select_categorical_features(X)\n    elif self.columns == \"all\":\n        if isinstance(X, pd.DataFrame):\n            self.columns_ = X.columns\n        else:\n            self.columns_ = list(range(X.shape[1]))\n    elif isinstance(self.columns, list):\n        self.columns_ = self.columns\n    else:\n        raise ValueError(f\"Invalid value for columns: {self.columns}\")\n\n    if len(self.columns_) == 0:\n        return self\n\n    self.enc = sklearn.preprocessing.OrdinalEncoder(categories='auto',   \n                                                    handle_unknown = self.handle_unknown,\n                                                    unknown_value = self.unknown_value, \n                                                    encoded_missing_value = self.encoded_missing_value,\n                                                    min_frequency = self.min_frequency,\n                                                    max_categories = self.max_categories)\n    #TODO make this more consistent with sklearn baseimputer/baseencoder\n    '''\n    if isinstance(X, pd.DataFrame):\n        self.enc.set_output(transform=\"pandas\")\n        for col in X.columns:\n            # check if the column name is not a string\n            if not isinstance(col, str):\n                # if it's not a string, rename the column with \"X\" prefix\n                X.rename(columns={col: f\"X{col}\"}, inplace=True)\n    '''\n\n    if len(self.columns_) == X.shape[1]:\n        X_sel = self.enc.fit(X)\n    else:\n        X_sel, X_not_sel = _X_selected(X, self.columns_)\n        X_sel = self.enc.fit(X_sel)\n\n    return self\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/column_one_hot_encoder/#tpot.builtin_modules.column_one_hot_encoder.ColumnOrdinalEncoder.transform","title":"<code>transform(X)</code>","text":"<p>Transform X using one-hot encoding.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like or sparse matrix, shape=(n_samples, n_features)</code> <p>Dense array or sparse matrix.</p> required <p>Returns:</p> Name Type Description <code>X_out</code> <code>sparse matrix if sparse=True else a 2-d array, dtype=int</code> <p>Transformed input.</p> Source code in <code>tpot/builtin_modules/column_one_hot_encoder.py</code> <pre><code>def transform(self, X):\n    \"\"\"Transform X using one-hot encoding.\n\n    Parameters\n    ----------\n    X : array-like or sparse matrix, shape=(n_samples, n_features)\n        Dense array or sparse matrix.\n\n    Returns\n    -------\n    X_out : sparse matrix if sparse=True else a 2-d array, dtype=int\n        Transformed input.\n    \"\"\"\n\n\n    if len(self.columns_) == 0:\n        return X\n\n    #TODO make this more consistent with sklearn baseimputer/baseencoder\n    '''\n    if isinstance(X, pd.DataFrame):\n        for col in X.columns:\n            # check if the column name is not a string\n            if not isinstance(col, str):\n                # if it's not a string, rename the column with \"X\" prefix\n                X.rename(columns={col: f\"X{col}\"}, inplace=True)\n    '''\n\n    if len(self.columns_) == X.shape[1]:\n        return self.enc.transform(X)\n    else:\n\n        X_sel, X_not_sel= _X_selected(X, self.columns_)\n        X_sel = self.enc.transform(X_sel)\n\n        #If X is dataframe\n        if isinstance(X, pd.DataFrame):\n\n            X_sel = pd.DataFrame(X_sel, columns=self.enc.get_feature_names_out())\n            return pd.concat([X_not_sel.reset_index(drop=True), X_sel.reset_index(drop=True)], axis=1)\n        else:\n            return np.hstack((X_not_sel, X_sel))\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/estimatortransformer/","title":"Estimatortransformer","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/builtin_modules/estimatortransformer/#tpot.builtin_modules.estimatortransformer.EstimatorTransformer","title":"<code>EstimatorTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/estimatortransformer.py</code> <pre><code>class EstimatorTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, estimator, method='auto', passthrough=False, cross_val_predict_cv=None):\n        \"\"\"\n        A class for using a sklearn estimator as a transformer. When calling fit_transform, this class returns the out put of cross_val_predict\n        and trains the estimator on the full dataset. When calling transform, this class uses the estimator fit on the full dataset to transform the data.\n\n        Parameters\n        ----------\n        estimator : sklear.base. BaseEstimator\n            The estimator to use as a transformer.\n        method : str, default='auto'\n            The method to use for the transformation. If 'auto', will try to use predict_proba, decision_function, or predict in that order.\n            - predict_proba: use the predict_proba method of the estimator.\n            - decision_function: use the decision_function method of the estimator.\n            - predict: use the predict method of the estimator.\n        passthrough : bool, default=False\n            Whether to pass the original input through.\n        cross_val_predict_cv : int, default=0\n            Number of folds to use for the cross_val_predict function for inner classifiers and regressors. Estimators will still be fit on the full dataset, but the following node will get the outputs from cross_val_predict.\n\n            - 0-1 : When set to 0 or 1, the cross_val_predict function will not be used. The next layer will get the outputs from fitting and transforming the full dataset.\n            - &gt;=2 : When fitting pipelines with inner classifiers or regressors, they will still be fit on the full dataset.\n                    However, the output to the next node will come from cross_val_predict with the specified number of folds.\n\n        \"\"\"\n        self.estimator = estimator\n        self.method = method\n        self.passthrough = passthrough\n        self.cross_val_predict_cv = cross_val_predict_cv\n\n    def fit(self, X, y=None):\n        self.estimator.fit(X, y)\n        return self\n\n    def transform(self, X, y=None):\n        #Does not do cross val predict, just uses the estimator to transform the data. This is used for the actual transformation in practice, so the real transformation without fitting is needed\n        if self.method == 'auto':\n            if hasattr(self.estimator, 'predict_proba'):\n                method = 'predict_proba'\n            elif hasattr(self.estimator, 'decision_function'):\n                method = 'decision_function'\n            elif hasattr(self.estimator, 'predict'):\n                method = 'predict'\n            else:\n                raise ValueError('Estimator has no valid method')\n        else:\n            method = self.method\n\n        output = getattr(self.estimator, method)(X)\n        output=np.array(output)\n\n        if len(output.shape) == 1:\n            output = output.reshape(-1,1)\n\n        if self.passthrough:\n            return np.hstack((output, X))\n        else:\n            return output\n\n\n\n    def fit_transform(self, X, y=None):\n        #Does use cross_val_predict if cross_val_predict_cv is greater than 0. this function is only used in training the model. \n        self.estimator.fit(X,y)\n\n        if self.method == 'auto':\n            if hasattr(self.estimator, 'predict_proba'):\n                method = 'predict_proba'\n            elif hasattr(self.estimator, 'decision_function'):\n                method = 'decision_function'\n            elif hasattr(self.estimator, 'predict'):\n                method = 'predict'\n            else:\n                raise ValueError('Estimator has no valid method')\n        else:\n            method = self.method\n\n        if self.cross_val_predict_cv is not None:\n            output = cross_val_predict(self.estimator, X, y=y, cv=self.cross_val_predict_cv)\n        else:\n            output = getattr(self.estimator, method)(X)\n            #reshape if needed\n\n        if len(output.shape) == 1:\n            output = output.reshape(-1,1)\n\n        output=np.array(output)\n        if self.passthrough:\n            return np.hstack((output, X))\n        else:\n            return output\n\n    def _estimator_has(attr):\n        '''Check if we can delegate a method to the underlying estimator.\n        First, we check the first fitted final estimator if available, otherwise we\n        check the unfitted final estimator.\n        '''\n        return  lambda self: (self.estimator is not None and\n            hasattr(self.estimator, attr)\n        )\n\n    @available_if(_estimator_has('predict'))\n    def predict(self, X, **predict_params):\n        check_is_fitted(self.estimator)\n        #X = check_array(X)\n\n        preds = self.estimator.predict(X,**predict_params)\n        return preds\n\n    @available_if(_estimator_has('predict_proba'))\n    def predict_proba(self, X, **predict_params):\n        check_is_fitted(self.estimator)\n        #X = check_array(X)\n        return self.estimator.predict_proba(X,**predict_params)\n\n    @available_if(_estimator_has('decision_function'))\n    def decision_function(self, X, **predict_params):\n        check_is_fitted(self.estimator)\n        #X = check_array(X)\n        return self.estimator.decision_function(X,**predict_params)\n\n    def __sklearn_is_fitted__(self):\n        \"\"\"\n        Check fitted status and return a Boolean value.\n        \"\"\"\n        return check_is_fitted(self.estimator)\n\n\n    # @property\n    # def _estimator_type(self):\n    #     return self.estimator._estimator_type\n\n\n\n    @property\n    def classes_(self):\n        \"\"\"The classes labels. Only exist if the last step is a classifier.\"\"\"\n        return self.estimator._classes\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/estimatortransformer/#tpot.builtin_modules.estimatortransformer.EstimatorTransformer.classes_","title":"<code>classes_</code>  <code>property</code>","text":"<p>The classes labels. Only exist if the last step is a classifier.</p>"},{"location":"documentation/tpot/builtin_modules/estimatortransformer/#tpot.builtin_modules.estimatortransformer.EstimatorTransformer.__init__","title":"<code>__init__(estimator, method='auto', passthrough=False, cross_val_predict_cv=None)</code>","text":"<p>A class for using a sklearn estimator as a transformer. When calling fit_transform, this class returns the out put of cross_val_predict and trains the estimator on the full dataset. When calling transform, this class uses the estimator fit on the full dataset to transform the data.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>BaseEstimator</code> <p>The estimator to use as a transformer.</p> required <code>method</code> <code>str</code> <p>The method to use for the transformation. If 'auto', will try to use predict_proba, decision_function, or predict in that order. - predict_proba: use the predict_proba method of the estimator. - decision_function: use the decision_function method of the estimator. - predict: use the predict method of the estimator.</p> <code>'auto'</code> <code>passthrough</code> <code>bool</code> <p>Whether to pass the original input through.</p> <code>False</code> <code>cross_val_predict_cv</code> <code>int</code> <p>Number of folds to use for the cross_val_predict function for inner classifiers and regressors. Estimators will still be fit on the full dataset, but the following node will get the outputs from cross_val_predict.</p> <ul> <li>0-1 : When set to 0 or 1, the cross_val_predict function will not be used. The next layer will get the outputs from fitting and transforming the full dataset.</li> <li> <p>=2 : When fitting pipelines with inner classifiers or regressors, they will still be fit on the full dataset.         However, the output to the next node will come from cross_val_predict with the specified number of folds.</p> </li> </ul> <code>0</code> Source code in <code>tpot/builtin_modules/estimatortransformer.py</code> <pre><code>def __init__(self, estimator, method='auto', passthrough=False, cross_val_predict_cv=None):\n    \"\"\"\n    A class for using a sklearn estimator as a transformer. When calling fit_transform, this class returns the out put of cross_val_predict\n    and trains the estimator on the full dataset. When calling transform, this class uses the estimator fit on the full dataset to transform the data.\n\n    Parameters\n    ----------\n    estimator : sklear.base. BaseEstimator\n        The estimator to use as a transformer.\n    method : str, default='auto'\n        The method to use for the transformation. If 'auto', will try to use predict_proba, decision_function, or predict in that order.\n        - predict_proba: use the predict_proba method of the estimator.\n        - decision_function: use the decision_function method of the estimator.\n        - predict: use the predict method of the estimator.\n    passthrough : bool, default=False\n        Whether to pass the original input through.\n    cross_val_predict_cv : int, default=0\n        Number of folds to use for the cross_val_predict function for inner classifiers and regressors. Estimators will still be fit on the full dataset, but the following node will get the outputs from cross_val_predict.\n\n        - 0-1 : When set to 0 or 1, the cross_val_predict function will not be used. The next layer will get the outputs from fitting and transforming the full dataset.\n        - &gt;=2 : When fitting pipelines with inner classifiers or regressors, they will still be fit on the full dataset.\n                However, the output to the next node will come from cross_val_predict with the specified number of folds.\n\n    \"\"\"\n    self.estimator = estimator\n    self.method = method\n    self.passthrough = passthrough\n    self.cross_val_predict_cv = cross_val_predict_cv\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/estimatortransformer/#tpot.builtin_modules.estimatortransformer.EstimatorTransformer.__sklearn_is_fitted__","title":"<code>__sklearn_is_fitted__()</code>","text":"<p>Check fitted status and return a Boolean value.</p> Source code in <code>tpot/builtin_modules/estimatortransformer.py</code> <pre><code>def __sklearn_is_fitted__(self):\n    \"\"\"\n    Check fitted status and return a Boolean value.\n    \"\"\"\n    return check_is_fitted(self.estimator)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/feature_encoding_frequency_selector/","title":"Feature encoding frequency selector","text":"<p>From https://github.com/EpistasisLab/autoqtl</p>"},{"location":"documentation/tpot/builtin_modules/feature_encoding_frequency_selector/#tpot.builtin_modules.feature_encoding_frequency_selector.FeatureEncodingFrequencySelector","title":"<code>FeatureEncodingFrequencySelector</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>SelectorMixin</code></p> <p>Feature selector based on Encoding Frequency. Encoding frequency is the frequency of each unique element(0/1/2/3) present in a feature set.  Features are selected on the basis of a threshold assigned for encoding frequency. If frequency of any unique element is less than or equal to threshold, the feature is removed.</p> Source code in <code>tpot/builtin_modules/feature_encoding_frequency_selector.py</code> <pre><code>class FeatureEncodingFrequencySelector(BaseEstimator, SelectorMixin):\n    \"\"\"Feature selector based on Encoding Frequency. Encoding frequency is the frequency of each unique element(0/1/2/3) present in a feature set. \n     Features are selected on the basis of a threshold assigned for encoding frequency. If frequency of any unique element is less than or equal to threshold, the feature is removed.  \"\"\"\n\n    @property\n    def __name__(self):\n        \"\"\"Instance name is the same as the class name. \"\"\"\n        return self.__class__.__name__\n\n    def __init__(self, threshold):\n        \"\"\"Create a FeatureEncodingFrequencySelector object.\n\n        Parameters\n        ----------\n        threshold : float, required\n            Threshold value for allele frequency. If frequency of A or frequency of a is less than the threshold value then the feature is dropped.\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n        self.threshold = threshold\n\n    \"\"\"def fit(self, X, y=None):\n        Fit FeatureAlleleFrequencySelector for feature selection\n\n        Parameters\n        ----------\n        X : numpy ndarray, {n_samples, n_features}\n            The training input samples.\n        y : numpy array {n_samples,}\n            The training target values.\n\n        Returns\n        -------\n        self : object\n            Returns a copy of the estimator\n\n        self.selected_feature_indexes = []\n        self.no_of_features = X.shape[1]\n\n        # Finding the no of alleles in each feature column\n        for i in range(0, X.shape[1]):\n            no_of_AA_featurewise = np.count_nonzero(X[:,i]==0)\n            no_of_Aa_featurewise = np.count_nonzero(X[:,i]==1)\n            no_of_aa_featurewise = np.count_nonzero(X[:,i]==2)\n\n\n            frequency_A_featurewise = (2*no_of_AA_featurewise + no_of_Aa_featurewise) / (2*no_of_AA_featurewise + \n            2*no_of_Aa_featurewise + 2*no_of_aa_featurewise)\n\n            frequency_a_featurewise = 1 - frequency_A_featurewise\n\n            if(not(frequency_A_featurewise &lt;= self.threshold) and not(frequency_a_featurewise &lt;= self.threshold)):\n                self.selected_feature_indexes.append(i)\n        return self\"\"\"\n\n    \"\"\"def transform(self, X):\n        Make subset after fit\n\n        Parameters\n        ----------\n        X : numpy ndarray, {n_samples, n_features}\n            New data, where n_samples is the number of samples and n_features is the number of features.\n\n        Returns\n        -------\n        X_transformed : numpy ndarray, {n_samples, n_features}\n            The transformed feature set.\n\n\n        X_transformed = X[:, self.selected_feature_indexes]\n\n        return X_transformed\"\"\"\n\n    def fit(self, X, y=None) :\n        \"\"\"Fit FeatureEncodingFrequencySelector for feature selection. This function gets the appropriate features. \"\"\"\n\n        self.selected_feature_indexes = []\n        self.no_of_original_features = X.shape[1]\n\n        # Finding the frequency of all the unique elements present featurewise in the input variable X\n        for i in range(0, X.shape[1]):\n            unique, counts = np.unique(X[:,i], return_counts=True)\n            element_count_dict_featurewise = dict(zip(unique, counts))\n            element_frequency_dict_featurewise = {}\n            feature_column_selected = True\n\n            for x in unique:\n                x_frequency_featurewise = element_count_dict_featurewise[x] / sum(counts)\n                element_frequency_dict_featurewise[x] = x_frequency_featurewise\n\n            for frequency in element_frequency_dict_featurewise.values():\n                if frequency &lt;= self.threshold :\n                    feature_column_selected = False\n                    break\n\n            if feature_column_selected == True :\n                self.selected_feature_indexes.append(i)\n\n        if not len(self.selected_feature_indexes):\n            \"\"\"msg = \"No feature in X meets the encoding frequency threshold {0:.5f}\"\n            raise ValueError(msg.format(self.threshold))\"\"\"\n            for i in range(0, X.shape[1]):\n                self.selected_feature_indexes.append(i)\n\n        return self\n\n    def transform(self, X):\n        \"\"\" Make subset after fit. This function returns a transformed version of X.  \"\"\"\n        X_transformed = X[:, self.selected_feature_indexes]\n\n        return X_transformed\n\n\n    def _get_support_mask(self):\n        \"\"\"\n        Get the boolean mask indicating which features are selected\n        It is the abstractmethod\n\n        Returns\n        -------\n        support : boolean array of shape [# input features]\n            An element is True iff its corresponding feature is selected for retention.\n            \"\"\"\n        n_features = self.no_of_original_features\n        mask = np.zeros(n_features, dtype=bool)\n        mask[np.asarray(self.selected_feature_indexes)] = True\n\n        return mask\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/feature_encoding_frequency_selector/#tpot.builtin_modules.feature_encoding_frequency_selector.FeatureEncodingFrequencySelector.__name__","title":"<code>__name__</code>  <code>property</code>","text":"<p>Instance name is the same as the class name.</p>"},{"location":"documentation/tpot/builtin_modules/feature_encoding_frequency_selector/#tpot.builtin_modules.feature_encoding_frequency_selector.FeatureEncodingFrequencySelector.__init__","title":"<code>__init__(threshold)</code>","text":"<p>Create a FeatureEncodingFrequencySelector object.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>(float, required)</code> <p>Threshold value for allele frequency. If frequency of A or frequency of a is less than the threshold value then the feature is dropped.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>tpot/builtin_modules/feature_encoding_frequency_selector.py</code> <pre><code>def __init__(self, threshold):\n    \"\"\"Create a FeatureEncodingFrequencySelector object.\n\n    Parameters\n    ----------\n    threshold : float, required\n        Threshold value for allele frequency. If frequency of A or frequency of a is less than the threshold value then the feature is dropped.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    self.threshold = threshold\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/feature_encoding_frequency_selector/#tpot.builtin_modules.feature_encoding_frequency_selector.FeatureEncodingFrequencySelector.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit FeatureEncodingFrequencySelector for feature selection. This function gets the appropriate features.</p> Source code in <code>tpot/builtin_modules/feature_encoding_frequency_selector.py</code> <pre><code>def fit(self, X, y=None) :\n    \"\"\"Fit FeatureEncodingFrequencySelector for feature selection. This function gets the appropriate features. \"\"\"\n\n    self.selected_feature_indexes = []\n    self.no_of_original_features = X.shape[1]\n\n    # Finding the frequency of all the unique elements present featurewise in the input variable X\n    for i in range(0, X.shape[1]):\n        unique, counts = np.unique(X[:,i], return_counts=True)\n        element_count_dict_featurewise = dict(zip(unique, counts))\n        element_frequency_dict_featurewise = {}\n        feature_column_selected = True\n\n        for x in unique:\n            x_frequency_featurewise = element_count_dict_featurewise[x] / sum(counts)\n            element_frequency_dict_featurewise[x] = x_frequency_featurewise\n\n        for frequency in element_frequency_dict_featurewise.values():\n            if frequency &lt;= self.threshold :\n                feature_column_selected = False\n                break\n\n        if feature_column_selected == True :\n            self.selected_feature_indexes.append(i)\n\n    if not len(self.selected_feature_indexes):\n        \"\"\"msg = \"No feature in X meets the encoding frequency threshold {0:.5f}\"\n        raise ValueError(msg.format(self.threshold))\"\"\"\n        for i in range(0, X.shape[1]):\n            self.selected_feature_indexes.append(i)\n\n    return self\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/feature_encoding_frequency_selector/#tpot.builtin_modules.feature_encoding_frequency_selector.FeatureEncodingFrequencySelector.transform","title":"<code>transform(X)</code>","text":"<p>Make subset after fit. This function returns a transformed version of X.</p> Source code in <code>tpot/builtin_modules/feature_encoding_frequency_selector.py</code> <pre><code>def transform(self, X):\n    \"\"\" Make subset after fit. This function returns a transformed version of X.  \"\"\"\n    X_transformed = X[:, self.selected_feature_indexes]\n\n    return X_transformed\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/feature_set_selector/","title":"Feature set selector","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/builtin_modules/feature_set_selector/#tpot.builtin_modules.feature_set_selector.FeatureSetSelector","title":"<code>FeatureSetSelector</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>SelectorMixin</code></p> <p>Select predefined feature subsets.</p> Source code in <code>tpot/builtin_modules/feature_set_selector.py</code> <pre><code>class FeatureSetSelector(BaseEstimator, SelectorMixin):\n    \"\"\"\n    Select predefined feature subsets.\n\n\n    \"\"\"\n\n    def __init__(self, sel_subset=None, name=None):\n        \"\"\"Create a FeatureSetSelector object.\n\n        Parameters\n        ----------\n        sel_subset: list or int\n            If X is a dataframe, items in sel_subset list must correspond to column names\n            If X is a numpy array, items in sel_subset list must correspond to column indexes\n            int: index of a single column\n        Returns\n        -------\n        None\n\n        \"\"\"\n        self.name = name\n        self.sel_subset = sel_subset\n\n\n    def fit(self, X, y=None):\n        \"\"\"Fit FeatureSetSelector for feature selection\n\n        Parameters\n        ----------\n        X: array-like of shape (n_samples, n_features)\n            The training input samples.\n        y: array-like, shape (n_samples,)\n            The target values (integers that correspond to classes in classification, real numbers in regression).\n\n        Returns\n        -------\n        self: object\n            Returns a copy of the estimator\n        \"\"\"\n        if isinstance(self.sel_subset, int) or isinstance(self.sel_subset, str):\n            self.sel_subset = [self.sel_subset]\n\n        #generate  self.feat_list_idx\n        if isinstance(X, pd.DataFrame):\n            self.feature_names_in_ = X.columns.tolist()\n            self.feat_list_idx = sorted([self.feature_names_in_.index(feat) for feat in self.sel_subset])\n\n\n        elif isinstance(X, np.ndarray):\n            self.feature_names_in_ = None#list(range(X.shape[1]))\n\n            self.feat_list_idx = sorted(self.sel_subset)\n\n        n_features = X.shape[1]\n        self.mask = np.zeros(n_features, dtype=bool)\n        self.mask[np.asarray(self.feat_list_idx)] = True\n\n        return self\n\n    #TODO keep returned as dataframe if input is dataframe? may not be consistent with sklearn\n\n    # def transform(self, X):\n\n    def _get_tags(self):\n        tags = {\"allow_nan\": True, \"requires_y\": False}\n        return tags\n\n    def _get_support_mask(self):\n        \"\"\"\n        Get the boolean mask indicating which features are selected\n        Returns\n        -------\n        support : boolean array of shape [# input features]\n            An element is True iff its corresponding feature is selected for\n            retention.\n        \"\"\"\n        return self.mask\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/feature_set_selector/#tpot.builtin_modules.feature_set_selector.FeatureSetSelector.__init__","title":"<code>__init__(sel_subset=None, name=None)</code>","text":"<p>Create a FeatureSetSelector object.</p> <p>Parameters:</p> Name Type Description Default <code>sel_subset</code> <p>If X is a dataframe, items in sel_subset list must correspond to column names If X is a numpy array, items in sel_subset list must correspond to column indexes int: index of a single column</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>tpot/builtin_modules/feature_set_selector.py</code> <pre><code>def __init__(self, sel_subset=None, name=None):\n    \"\"\"Create a FeatureSetSelector object.\n\n    Parameters\n    ----------\n    sel_subset: list or int\n        If X is a dataframe, items in sel_subset list must correspond to column names\n        If X is a numpy array, items in sel_subset list must correspond to column indexes\n        int: index of a single column\n    Returns\n    -------\n    None\n\n    \"\"\"\n    self.name = name\n    self.sel_subset = sel_subset\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/feature_set_selector/#tpot.builtin_modules.feature_set_selector.FeatureSetSelector.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit FeatureSetSelector for feature selection</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>The training input samples.</p> required <code>y</code> <p>The target values (integers that correspond to classes in classification, real numbers in regression).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Returns a copy of the estimator</p> Source code in <code>tpot/builtin_modules/feature_set_selector.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Fit FeatureSetSelector for feature selection\n\n    Parameters\n    ----------\n    X: array-like of shape (n_samples, n_features)\n        The training input samples.\n    y: array-like, shape (n_samples,)\n        The target values (integers that correspond to classes in classification, real numbers in regression).\n\n    Returns\n    -------\n    self: object\n        Returns a copy of the estimator\n    \"\"\"\n    if isinstance(self.sel_subset, int) or isinstance(self.sel_subset, str):\n        self.sel_subset = [self.sel_subset]\n\n    #generate  self.feat_list_idx\n    if isinstance(X, pd.DataFrame):\n        self.feature_names_in_ = X.columns.tolist()\n        self.feat_list_idx = sorted([self.feature_names_in_.index(feat) for feat in self.sel_subset])\n\n\n    elif isinstance(X, np.ndarray):\n        self.feature_names_in_ = None#list(range(X.shape[1]))\n\n        self.feat_list_idx = sorted(self.sel_subset)\n\n    n_features = X.shape[1]\n    self.mask = np.zeros(n_features, dtype=bool)\n    self.mask[np.asarray(self.feat_list_idx)] = True\n\n    return self\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/feature_transformers/","title":"Feature transformers","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/builtin_modules/feature_transformers/#tpot.builtin_modules.feature_transformers.CategoricalSelector","title":"<code>CategoricalSelector</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Meta-transformer for selecting categorical features and transform them using OneHotEncoder.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>int</code> <p>Maximum number of unique values per feature to consider the feature to be categorical.</p> <code>10</code> <code>minimum_fraction</code> <p>Minimum fraction of unique values in a feature to consider the feature to be categorical.</p> <code>None</code> Source code in <code>tpot/builtin_modules/feature_transformers.py</code> <pre><code>class CategoricalSelector(BaseEstimator, TransformerMixin):\n    \"\"\"Meta-transformer for selecting categorical features and transform them using OneHotEncoder.\n\n    Parameters\n    ----------\n\n    threshold : int, default=10\n        Maximum number of unique values per feature to consider the feature\n        to be categorical.\n\n    minimum_fraction: float, default=None\n        Minimum fraction of unique values in a feature to consider the feature\n        to be categorical.\n    \"\"\"\n\n    def __init__(self, threshold=10, minimum_fraction=None):\n        \"\"\"Create a CategoricalSelector object.\"\"\"\n        self.threshold = threshold\n        self.minimum_fraction = minimum_fraction\n\n\n    def fit(self, X, y=None):\n        \"\"\"Do nothing and return the estimator unchanged\n        This method is just there to implement the usual API and hence\n        work in pipelines.\n        Parameters\n        ----------\n        X : array-like\n        \"\"\"\n        X = check_array(X, accept_sparse='csr')\n        return self\n\n\n    def transform(self, X):\n        \"\"\"Select categorical features and transform them using OneHotEncoder.\n\n        Parameters\n        ----------\n        X: numpy ndarray, {n_samples, n_components}\n            New data, where n_samples is the number of samples and n_components is the number of components.\n\n        Returns\n        -------\n        array-like, {n_samples, n_components}\n        \"\"\"\n        selected = auto_select_categorical_features(X, threshold=self.threshold)\n        X_sel, _, n_selected, _ = _X_selected(X, selected)\n\n        if n_selected == 0:\n            # No features selected.\n            raise ValueError('No categorical feature was found!')\n        else:\n            ohe = OneHotEncoder(categorical_features='all', sparse=False, minimum_fraction=self.minimum_fraction)\n            return ohe.fit_transform(X_sel)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/feature_transformers/#tpot.builtin_modules.feature_transformers.CategoricalSelector.__init__","title":"<code>__init__(threshold=10, minimum_fraction=None)</code>","text":"<p>Create a CategoricalSelector object.</p> Source code in <code>tpot/builtin_modules/feature_transformers.py</code> <pre><code>def __init__(self, threshold=10, minimum_fraction=None):\n    \"\"\"Create a CategoricalSelector object.\"\"\"\n    self.threshold = threshold\n    self.minimum_fraction = minimum_fraction\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/feature_transformers/#tpot.builtin_modules.feature_transformers.CategoricalSelector.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Do nothing and return the estimator unchanged This method is just there to implement the usual API and hence work in pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> required Source code in <code>tpot/builtin_modules/feature_transformers.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Do nothing and return the estimator unchanged\n    This method is just there to implement the usual API and hence\n    work in pipelines.\n    Parameters\n    ----------\n    X : array-like\n    \"\"\"\n    X = check_array(X, accept_sparse='csr')\n    return self\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/feature_transformers/#tpot.builtin_modules.feature_transformers.CategoricalSelector.transform","title":"<code>transform(X)</code>","text":"<p>Select categorical features and transform them using OneHotEncoder.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>New data, where n_samples is the number of samples and n_components is the number of components.</p> required <p>Returns:</p> Type Description <code>(array - like, {n_samples, n_components})</code> Source code in <code>tpot/builtin_modules/feature_transformers.py</code> <pre><code>def transform(self, X):\n    \"\"\"Select categorical features and transform them using OneHotEncoder.\n\n    Parameters\n    ----------\n    X: numpy ndarray, {n_samples, n_components}\n        New data, where n_samples is the number of samples and n_components is the number of components.\n\n    Returns\n    -------\n    array-like, {n_samples, n_components}\n    \"\"\"\n    selected = auto_select_categorical_features(X, threshold=self.threshold)\n    X_sel, _, n_selected, _ = _X_selected(X, selected)\n\n    if n_selected == 0:\n        # No features selected.\n        raise ValueError('No categorical feature was found!')\n    else:\n        ohe = OneHotEncoder(categorical_features='all', sparse=False, minimum_fraction=self.minimum_fraction)\n        return ohe.fit_transform(X_sel)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/feature_transformers/#tpot.builtin_modules.feature_transformers.ContinuousSelector","title":"<code>ContinuousSelector</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Meta-transformer for selecting continuous features and transform them using PCA.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>int</code> <p>Maximum number of unique values per feature to consider the feature to be categorical.</p> <code>10</code> <code>svd_solver</code> <code>string {'auto', 'full', 'arpack', 'randomized'}</code> <p>auto :     the solver is selected by a default policy based on <code>X.shape</code> and     <code>n_components</code>: if the input data is larger than 500x500 and the     number of components to extract is lower than 80% of the smallest     dimension of the data, then the more efficient 'randomized'     method is enabled. Otherwise the exact full SVD is computed and     optionally truncated afterwards. full :     run exact full SVD calling the standard LAPACK solver via     <code>scipy.linalg.svd</code> and select the components by postprocessing arpack :     run SVD truncated to n_components calling ARPACK solver via     <code>scipy.sparse.linalg.svds</code>. It requires strictly     0 &lt; n_components &lt; X.shape[1] randomized :     run randomized SVD by the method of Halko et al.</p> <code>'randomized'</code> <code>iterated_power</code> <code>int &gt;= 0, or 'auto', (default 'auto')</code> <p>Number of iterations for the power method computed by svd_solver == 'randomized'.</p> <code>'auto'</code> Source code in <code>tpot/builtin_modules/feature_transformers.py</code> <pre><code>class ContinuousSelector(BaseEstimator, TransformerMixin):\n    \"\"\"Meta-transformer for selecting continuous features and transform them using PCA.\n\n    Parameters\n    ----------\n\n    threshold : int, default=10\n        Maximum number of unique values per feature to consider the feature\n        to be categorical.\n\n    svd_solver : string {'auto', 'full', 'arpack', 'randomized'}\n        auto :\n            the solver is selected by a default policy based on `X.shape` and\n            `n_components`: if the input data is larger than 500x500 and the\n            number of components to extract is lower than 80% of the smallest\n            dimension of the data, then the more efficient 'randomized'\n            method is enabled. Otherwise the exact full SVD is computed and\n            optionally truncated afterwards.\n        full :\n            run exact full SVD calling the standard LAPACK solver via\n            `scipy.linalg.svd` and select the components by postprocessing\n        arpack :\n            run SVD truncated to n_components calling ARPACK solver via\n            `scipy.sparse.linalg.svds`. It requires strictly\n            0 &lt; n_components &lt; X.shape[1]\n        randomized :\n            run randomized SVD by the method of Halko et al.\n\n    iterated_power : int &gt;= 0, or 'auto', (default 'auto')\n        Number of iterations for the power method computed by\n        svd_solver == 'randomized'.\n\n    \"\"\"\n\n    def __init__(self, threshold=10, svd_solver='randomized' ,iterated_power='auto', random_state=42):\n        \"\"\"Create a ContinuousSelector object.\"\"\"\n        self.threshold = threshold\n        self.svd_solver = svd_solver\n        self.iterated_power = iterated_power\n        self.random_state = random_state\n\n\n    def fit(self, X, y=None):\n        \"\"\"Do nothing and return the estimator unchanged\n        This method is just there to implement the usual API and hence\n        work in pipelines.\n        Parameters\n        ----------\n        X : array-like\n        \"\"\"\n        X = check_array(X)\n        return self\n\n\n    def transform(self, X):\n        \"\"\"Select continuous features and transform them using PCA.\n\n        Parameters\n        ----------\n        X: numpy ndarray, {n_samples, n_components}\n            New data, where n_samples is the number of samples and n_components is the number of components.\n\n        Returns\n        -------\n        array-like, {n_samples, n_components}\n        \"\"\"\n        selected = auto_select_categorical_features(X, threshold=self.threshold)\n        _, X_sel, n_selected, _ = _X_selected(X, selected)\n\n        if n_selected == 0:\n            # No features selected.\n            raise ValueError('No continuous feature was found!')\n        else:\n            pca = PCA(svd_solver=self.svd_solver, iterated_power=self.iterated_power, random_state=self.random_state)\n            return pca.fit_transform(X_sel)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/feature_transformers/#tpot.builtin_modules.feature_transformers.ContinuousSelector.__init__","title":"<code>__init__(threshold=10, svd_solver='randomized', iterated_power='auto', random_state=42)</code>","text":"<p>Create a ContinuousSelector object.</p> Source code in <code>tpot/builtin_modules/feature_transformers.py</code> <pre><code>def __init__(self, threshold=10, svd_solver='randomized' ,iterated_power='auto', random_state=42):\n    \"\"\"Create a ContinuousSelector object.\"\"\"\n    self.threshold = threshold\n    self.svd_solver = svd_solver\n    self.iterated_power = iterated_power\n    self.random_state = random_state\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/feature_transformers/#tpot.builtin_modules.feature_transformers.ContinuousSelector.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Do nothing and return the estimator unchanged This method is just there to implement the usual API and hence work in pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> required Source code in <code>tpot/builtin_modules/feature_transformers.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Do nothing and return the estimator unchanged\n    This method is just there to implement the usual API and hence\n    work in pipelines.\n    Parameters\n    ----------\n    X : array-like\n    \"\"\"\n    X = check_array(X)\n    return self\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/feature_transformers/#tpot.builtin_modules.feature_transformers.ContinuousSelector.transform","title":"<code>transform(X)</code>","text":"<p>Select continuous features and transform them using PCA.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>New data, where n_samples is the number of samples and n_components is the number of components.</p> required <p>Returns:</p> Type Description <code>(array - like, {n_samples, n_components})</code> Source code in <code>tpot/builtin_modules/feature_transformers.py</code> <pre><code>def transform(self, X):\n    \"\"\"Select continuous features and transform them using PCA.\n\n    Parameters\n    ----------\n    X: numpy ndarray, {n_samples, n_components}\n        New data, where n_samples is the number of samples and n_components is the number of components.\n\n    Returns\n    -------\n    array-like, {n_samples, n_components}\n    \"\"\"\n    selected = auto_select_categorical_features(X, threshold=self.threshold)\n    _, X_sel, n_selected, _ = _X_selected(X, selected)\n\n    if n_selected == 0:\n        # No features selected.\n        raise ValueError('No continuous feature was found!')\n    else:\n        pca = PCA(svd_solver=self.svd_solver, iterated_power=self.iterated_power, random_state=self.random_state)\n        return pca.fit_transform(X_sel)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/genetic_encoders/","title":"Genetic encoders","text":"<p>Code from https://github.com/EpistasisLab/autoqtl This file contains the class definition for all the genetic encoders. All the genetic encoder classes inherit the Scikit learn BaseEstimator and TransformerMixin classes to follow the Scikit-learn paradigm.</p>"},{"location":"documentation/tpot/builtin_modules/genetic_encoders/#tpot.builtin_modules.genetic_encoders.DominantEncoder","title":"<code>DominantEncoder</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>This class contains the function definition for encoding the input features as a Dominant genetic model. The encoding used is AA(0)-&gt;1, Aa(1)-&gt;1, aa(2)-&gt;0.</p> Source code in <code>tpot/builtin_modules/genetic_encoders.py</code> <pre><code>class DominantEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"This class contains the function definition for encoding the input features as a Dominant genetic model.\n    The encoding used is AA(0)-&gt;1, Aa(1)-&gt;1, aa(2)-&gt;0. \"\"\"\n\n    def fit(self, X, y=None):\n        \"\"\"Do nothing and return the estimator unchanged.\n        Dummy function to fit in with the sklearn API and hence work in pipelines.\n\n        Parameters\n        ----------\n        X : array-like\n        \"\"\"\n        return self\n\n    def transform(self, X, y=None):\n        \"\"\"Transform the data by applying the Dominant encoding.\n\n        Parameters\n        ----------\n        X : numpy ndarray, {n_samples, n_components}\n            New data, where n_samples is the number of samples (number of individuals)\n            and n_components is the number of components (number of features).\n        y : None\n            Unused\n\n        Returns\n        -------\n        X_transformed: numpy ndarray, {n_samples, n_components}\n            The encoded feature set\n        \"\"\"\n        X = check_array(X)\n        map = {0: 1, 1: 1, 2: 0}\n        mapping_function = np.vectorize(lambda i: map[i] if i in map else i)\n\n        X_transformed = mapping_function(X)\n\n        return X_transformed\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/genetic_encoders/#tpot.builtin_modules.genetic_encoders.DominantEncoder.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Do nothing and return the estimator unchanged. Dummy function to fit in with the sklearn API and hence work in pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> required Source code in <code>tpot/builtin_modules/genetic_encoders.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Do nothing and return the estimator unchanged.\n    Dummy function to fit in with the sklearn API and hence work in pipelines.\n\n    Parameters\n    ----------\n    X : array-like\n    \"\"\"\n    return self\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/genetic_encoders/#tpot.builtin_modules.genetic_encoders.DominantEncoder.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Transform the data by applying the Dominant encoding.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy ndarray, {n_samples, n_components}</code> <p>New data, where n_samples is the number of samples (number of individuals) and n_components is the number of components (number of features).</p> required <code>y</code> <code>None</code> <p>Unused</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_transformed</code> <code>numpy ndarray, {n_samples, n_components}</code> <p>The encoded feature set</p> Source code in <code>tpot/builtin_modules/genetic_encoders.py</code> <pre><code>def transform(self, X, y=None):\n    \"\"\"Transform the data by applying the Dominant encoding.\n\n    Parameters\n    ----------\n    X : numpy ndarray, {n_samples, n_components}\n        New data, where n_samples is the number of samples (number of individuals)\n        and n_components is the number of components (number of features).\n    y : None\n        Unused\n\n    Returns\n    -------\n    X_transformed: numpy ndarray, {n_samples, n_components}\n        The encoded feature set\n    \"\"\"\n    X = check_array(X)\n    map = {0: 1, 1: 1, 2: 0}\n    mapping_function = np.vectorize(lambda i: map[i] if i in map else i)\n\n    X_transformed = mapping_function(X)\n\n    return X_transformed\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/genetic_encoders/#tpot.builtin_modules.genetic_encoders.HeterosisEncoder","title":"<code>HeterosisEncoder</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>This class contains the function definition for encoding the input features as a Heterozygote Advantage genetic model. The encoding used is AA(0)-&gt;0, Aa(1)-&gt;1, aa(2)-&gt;0.</p> Source code in <code>tpot/builtin_modules/genetic_encoders.py</code> <pre><code>class HeterosisEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"This class contains the function definition for encoding the input features as a Heterozygote Advantage genetic model.\n    The encoding used is AA(0)-&gt;0, Aa(1)-&gt;1, aa(2)-&gt;0. \"\"\"\n\n    def fit(self, X, y=None):\n        \"\"\"Do nothing and return the estimator unchanged.\n        Dummy function to fit in with the sklearn API and hence work in pipelines.\n\n        Parameters\n        ----------\n        X : array-like\n        \"\"\"\n        return self\n\n    def transform(self, X, y=None):\n        \"\"\"Transform the data by applying the Heterosis encoding.\n\n        Parameters\n        ----------\n        X : numpy ndarray, {n_samples, n_components}\n            New data, where n_samples is the number of samples (number of individuals)\n            and n_components is the number of components (number of features).\n        y : None\n            Unused\n\n        Returns\n        -------\n        X_transformed: numpy ndarray, {n_samples, n_components}\n            The encoded feature set\n        \"\"\"\n        X = check_array(X)\n        map = {0: 0, 1: 1, 2: 0}\n        mapping_function = np.vectorize(lambda i: map[i] if i in map else i)\n\n        X_transformed = mapping_function(X)\n\n        return X_transformed\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/genetic_encoders/#tpot.builtin_modules.genetic_encoders.HeterosisEncoder.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Do nothing and return the estimator unchanged. Dummy function to fit in with the sklearn API and hence work in pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> required Source code in <code>tpot/builtin_modules/genetic_encoders.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Do nothing and return the estimator unchanged.\n    Dummy function to fit in with the sklearn API and hence work in pipelines.\n\n    Parameters\n    ----------\n    X : array-like\n    \"\"\"\n    return self\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/genetic_encoders/#tpot.builtin_modules.genetic_encoders.HeterosisEncoder.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Transform the data by applying the Heterosis encoding.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy ndarray, {n_samples, n_components}</code> <p>New data, where n_samples is the number of samples (number of individuals) and n_components is the number of components (number of features).</p> required <code>y</code> <code>None</code> <p>Unused</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_transformed</code> <code>numpy ndarray, {n_samples, n_components}</code> <p>The encoded feature set</p> Source code in <code>tpot/builtin_modules/genetic_encoders.py</code> <pre><code>def transform(self, X, y=None):\n    \"\"\"Transform the data by applying the Heterosis encoding.\n\n    Parameters\n    ----------\n    X : numpy ndarray, {n_samples, n_components}\n        New data, where n_samples is the number of samples (number of individuals)\n        and n_components is the number of components (number of features).\n    y : None\n        Unused\n\n    Returns\n    -------\n    X_transformed: numpy ndarray, {n_samples, n_components}\n        The encoded feature set\n    \"\"\"\n    X = check_array(X)\n    map = {0: 0, 1: 1, 2: 0}\n    mapping_function = np.vectorize(lambda i: map[i] if i in map else i)\n\n    X_transformed = mapping_function(X)\n\n    return X_transformed\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/genetic_encoders/#tpot.builtin_modules.genetic_encoders.OverDominanceEncoder","title":"<code>OverDominanceEncoder</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>This class contains the function definition for encoding the input features as a Over Dominance genetic model. The encoding used is AA(0)-&gt;1, Aa(1)-&gt;2, aa(2)-&gt;0.</p> Source code in <code>tpot/builtin_modules/genetic_encoders.py</code> <pre><code>class OverDominanceEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"This class contains the function definition for encoding the input features as a Over Dominance genetic model.\n    The encoding used is AA(0)-&gt;1, Aa(1)-&gt;2, aa(2)-&gt;0. \"\"\"\n\n    def fit(self, X, y=None):\n        \"\"\"Do nothing and return the estimator unchanged.\n        Dummy function to fit in with the sklearn API and hence work in pipelines.\n\n        Parameters\n        ----------\n        X : array-like\n        \"\"\"\n        return self\n\n    def transform(self, X, y=None):\n        \"\"\"Transform the data by applying the Heterosis encoding.\n\n        Parameters\n        ----------\n        X : numpy ndarray, {n_samples, n_components}\n            New data, where n_samples is the number of samples (number of individuals)\n            and n_components is the number of components (number of features).\n        y : None\n            Unused\n\n        Returns\n        -------\n        X_transformed: numpy ndarray, {n_samples, n_components}\n            The encoded feature set\n        \"\"\"\n        X = check_array(X)\n        map = {0: 1, 1: 2, 2: 0}\n        mapping_function = np.vectorize(lambda i: map[i] if i in map else i)\n\n        X_transformed = mapping_function(X)\n\n        return X_transformed\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/genetic_encoders/#tpot.builtin_modules.genetic_encoders.OverDominanceEncoder.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Do nothing and return the estimator unchanged. Dummy function to fit in with the sklearn API and hence work in pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> required Source code in <code>tpot/builtin_modules/genetic_encoders.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Do nothing and return the estimator unchanged.\n    Dummy function to fit in with the sklearn API and hence work in pipelines.\n\n    Parameters\n    ----------\n    X : array-like\n    \"\"\"\n    return self\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/genetic_encoders/#tpot.builtin_modules.genetic_encoders.OverDominanceEncoder.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Transform the data by applying the Heterosis encoding.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy ndarray, {n_samples, n_components}</code> <p>New data, where n_samples is the number of samples (number of individuals) and n_components is the number of components (number of features).</p> required <code>y</code> <code>None</code> <p>Unused</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_transformed</code> <code>numpy ndarray, {n_samples, n_components}</code> <p>The encoded feature set</p> Source code in <code>tpot/builtin_modules/genetic_encoders.py</code> <pre><code>def transform(self, X, y=None):\n    \"\"\"Transform the data by applying the Heterosis encoding.\n\n    Parameters\n    ----------\n    X : numpy ndarray, {n_samples, n_components}\n        New data, where n_samples is the number of samples (number of individuals)\n        and n_components is the number of components (number of features).\n    y : None\n        Unused\n\n    Returns\n    -------\n    X_transformed: numpy ndarray, {n_samples, n_components}\n        The encoded feature set\n    \"\"\"\n    X = check_array(X)\n    map = {0: 1, 1: 2, 2: 0}\n    mapping_function = np.vectorize(lambda i: map[i] if i in map else i)\n\n    X_transformed = mapping_function(X)\n\n    return X_transformed\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/genetic_encoders/#tpot.builtin_modules.genetic_encoders.RecessiveEncoder","title":"<code>RecessiveEncoder</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>This class contains the function definition for encoding the input features as a Recessive genetic model. The encoding used is AA(0)-&gt;0, Aa(1)-&gt;1, aa(2)-&gt;1.</p> Source code in <code>tpot/builtin_modules/genetic_encoders.py</code> <pre><code>class RecessiveEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"This class contains the function definition for encoding the input features as a Recessive genetic model.\n    The encoding used is AA(0)-&gt;0, Aa(1)-&gt;1, aa(2)-&gt;1. \"\"\"\n\n    def fit(self, X, y=None):\n        \"\"\"Do nothing and return the estimator unchanged.\n        Dummy function to fit in with the sklearn API and hence work in pipelines.\n\n        Parameters\n        ----------\n        X : array-like\n        \"\"\"\n        return self\n\n    def transform(self, X, y=None):\n        \"\"\"Transform the data by applying the Recessive encoding.\n\n        Parameters\n        ----------\n        X : numpy ndarray, {n_samples, n_components}\n            New data, where n_samples is the number of samples (number of individuals)\n            and n_components is the number of components (number of features).\n        y : None\n            Unused\n\n        Returns\n        -------\n        X_transformed: numpy ndarray, {n_samples, n_components}\n            The encoded feature set\n        \"\"\"\n        X = check_array(X)\n        map = {0: 0, 1: 1, 2: 1}\n        mapping_function = np.vectorize(lambda i: map[i] if i in map else i)\n\n        X_transformed = mapping_function(X)\n\n        return X_transformed\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/genetic_encoders/#tpot.builtin_modules.genetic_encoders.RecessiveEncoder.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Do nothing and return the estimator unchanged. Dummy function to fit in with the sklearn API and hence work in pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> required Source code in <code>tpot/builtin_modules/genetic_encoders.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Do nothing and return the estimator unchanged.\n    Dummy function to fit in with the sklearn API and hence work in pipelines.\n\n    Parameters\n    ----------\n    X : array-like\n    \"\"\"\n    return self\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/genetic_encoders/#tpot.builtin_modules.genetic_encoders.RecessiveEncoder.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Transform the data by applying the Recessive encoding.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy ndarray, {n_samples, n_components}</code> <p>New data, where n_samples is the number of samples (number of individuals) and n_components is the number of components (number of features).</p> required <code>y</code> <code>None</code> <p>Unused</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_transformed</code> <code>numpy ndarray, {n_samples, n_components}</code> <p>The encoded feature set</p> Source code in <code>tpot/builtin_modules/genetic_encoders.py</code> <pre><code>def transform(self, X, y=None):\n    \"\"\"Transform the data by applying the Recessive encoding.\n\n    Parameters\n    ----------\n    X : numpy ndarray, {n_samples, n_components}\n        New data, where n_samples is the number of samples (number of individuals)\n        and n_components is the number of components (number of features).\n    y : None\n        Unused\n\n    Returns\n    -------\n    X_transformed: numpy ndarray, {n_samples, n_components}\n        The encoded feature set\n    \"\"\"\n    X = check_array(X)\n    map = {0: 0, 1: 1, 2: 1}\n    mapping_function = np.vectorize(lambda i: map[i] if i in map else i)\n\n    X_transformed = mapping_function(X)\n\n    return X_transformed\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/genetic_encoders/#tpot.builtin_modules.genetic_encoders.UnderDominanceEncoder","title":"<code>UnderDominanceEncoder</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>This class contains the function definition for encoding the input features as a Under Dominance genetic model. The encoding used is AA(0)-&gt;2, Aa(1)-&gt;0, aa(2)-&gt;1.</p> Source code in <code>tpot/builtin_modules/genetic_encoders.py</code> <pre><code>class UnderDominanceEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"This class contains the function definition for encoding the input features as a Under Dominance genetic model.\n    The encoding used is AA(0)-&gt;2, Aa(1)-&gt;0, aa(2)-&gt;1. \"\"\"\n\n    def fit(self, X, y=None):\n        \"\"\"Do nothing and return the estimator unchanged.\n        Dummy function to fit in with the sklearn API and hence work in pipelines.\n\n        Parameters\n        ----------\n        X : array-like\n        \"\"\"\n        return self\n\n    def transform(self, X, y=None):\n        \"\"\"Transform the data by applying the Heterosis encoding.\n\n        Parameters\n        ----------\n        X : numpy ndarray, {n_samples, n_components}\n            New data, where n_samples is the number of samples (number of individuals)\n            and n_components is the number of components (number of features).\n        y : None\n            Unused\n\n        Returns\n        -------\n        X_transformed: numpy ndarray, {n_samples, n_components}\n            The encoded feature set\n        \"\"\"\n        X = check_array(X)\n        map = {0: 2, 1: 0, 2: 1}\n        mapping_function = np.vectorize(lambda i: map[i] if i in map else i)\n\n        X_transformed = mapping_function(X)\n\n        return X_transformed\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/genetic_encoders/#tpot.builtin_modules.genetic_encoders.UnderDominanceEncoder.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Do nothing and return the estimator unchanged. Dummy function to fit in with the sklearn API and hence work in pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> required Source code in <code>tpot/builtin_modules/genetic_encoders.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Do nothing and return the estimator unchanged.\n    Dummy function to fit in with the sklearn API and hence work in pipelines.\n\n    Parameters\n    ----------\n    X : array-like\n    \"\"\"\n    return self\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/genetic_encoders/#tpot.builtin_modules.genetic_encoders.UnderDominanceEncoder.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Transform the data by applying the Heterosis encoding.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy ndarray, {n_samples, n_components}</code> <p>New data, where n_samples is the number of samples (number of individuals) and n_components is the number of components (number of features).</p> required <code>y</code> <code>None</code> <p>Unused</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_transformed</code> <code>numpy ndarray, {n_samples, n_components}</code> <p>The encoded feature set</p> Source code in <code>tpot/builtin_modules/genetic_encoders.py</code> <pre><code>def transform(self, X, y=None):\n    \"\"\"Transform the data by applying the Heterosis encoding.\n\n    Parameters\n    ----------\n    X : numpy ndarray, {n_samples, n_components}\n        New data, where n_samples is the number of samples (number of individuals)\n        and n_components is the number of components (number of features).\n    y : None\n        Unused\n\n    Returns\n    -------\n    X_transformed: numpy ndarray, {n_samples, n_components}\n        The encoded feature set\n    \"\"\"\n    X = check_array(X)\n    map = {0: 2, 1: 0, 2: 1}\n    mapping_function = np.vectorize(lambda i: map[i] if i in map else i)\n\n    X_transformed = mapping_function(X)\n\n    return X_transformed\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/imputer/","title":"Imputer","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/builtin_modules/imputer/#tpot.builtin_modules.imputer.ColumnSimpleImputer","title":"<code>ColumnSimpleImputer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/imputer.py</code> <pre><code>class ColumnSimpleImputer(BaseEstimator, TransformerMixin):\n    def __init__(self,  columns=\"all\",         \n                        missing_values=np.nan,\n                        strategy=\"mean\",\n                        fill_value=None,\n                        copy=True,\n                        add_indicator=False,\n                        keep_empty_features=False,):\n        \"\"\"\"\n        A wrapper for SimpleImputer that allows for imputation of specific columns in a DataFrame or np array.\n        Passes through columns that are not imputed.\n\n        Parameters\n        ----------\n        columns : str, list, default='all'\n            Determines which columns to impute with sklearn.impute.SimpleImputer.\n            - 'categorical' : Automatically select categorical features\n            - 'numeric' : Automatically select numeric features\n            - 'all' : Select all features\n            - list : A list of columns to select\n\n        # See documentation from sklearn.impute.SimpleImputer for the following parameters\n        missing_values, strategy, fill_value, copy, add_indicator, keep_empty_features\n\n        \"\"\"\n\n        self.columns = columns\n        self.missing_values = missing_values\n        self.strategy = strategy\n        self.fill_value = fill_value\n        self.copy = copy\n        self.add_indicator = add_indicator\n        self.keep_empty_features = keep_empty_features\n\n\n    def fit(self, X, y=None):\n        if (self.columns == \"categorical\" or self.columns == \"numeric\") and not isinstance(X, pd.DataFrame):\n            raise ValueError(f\"Invalid value for columns: {self.columns}. \"\n                             \"Only 'all' or &lt;list&gt; is supported for np arrays\")\n\n        if self.columns == \"categorical\":\n            self.columns_ = list(X.select_dtypes(exclude='number').columns)\n        elif self.columns == \"numeric\":\n            self.columns_ =  [col for col in X.columns if is_numeric_dtype(X[col])]\n        elif self.columns == \"all\":\n            if isinstance(X, pd.DataFrame):\n                self.columns_ = X.columns\n            else:\n                self.columns_ = list(range(X.shape[1]))\n        elif isinstance(self.columns, list):\n            self.columns_ = self.columns\n        else:\n            raise ValueError(f\"Invalid value for columns: {self.columns}\")\n\n        if len(self.columns_) == 0:\n            return self\n\n        self.imputer = sklearn.impute.SimpleImputer(missing_values=self.missing_values,\n                                                    strategy=self.strategy,\n                                                    fill_value=self.fill_value,\n                                                    copy=self.copy,\n                                                    add_indicator=self.add_indicator,\n                                                    keep_empty_features=self.keep_empty_features)\n\n        if isinstance(X, pd.DataFrame):\n            self.imputer.set_output(transform=\"pandas\")\n\n        if isinstance(X, pd.DataFrame):\n            self.imputer.fit(X[self.columns_], y)\n        else:\n            self.imputer.fit(X[:, self.columns_], y)\n\n        return self\n\n    def transform(self, X):\n        if len(self.columns_) == 0:\n            return X\n\n        if isinstance(X, pd.DataFrame):\n            X = X.copy()\n            X[self.columns_] = self.imputer.transform(X[self.columns_])\n            return X\n        else:\n            X = np.copy(X)\n            X[:, self.columns_] = self.imputer.transform(X[:, self.columns_])\n            return X\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/imputer/#tpot.builtin_modules.imputer.ColumnSimpleImputer.__init__","title":"<code>__init__(columns='all', missing_values=np.nan, strategy='mean', fill_value=None, copy=True, add_indicator=False, keep_empty_features=False)</code>","text":"<p>\" A wrapper for SimpleImputer that allows for imputation of specific columns in a DataFrame or np array. Passes through columns that are not imputed.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>(str, list)</code> <p>Determines which columns to impute with sklearn.impute.SimpleImputer. - 'categorical' : Automatically select categorical features - 'numeric' : Automatically select numeric features - 'all' : Select all features - list : A list of columns to select</p> <code>'all'</code> <code>missing_values</code> <code>nan</code> <code>strategy</code> <code>nan</code> <code>fill_value</code> <code>nan</code> <code>copy</code> <code>nan</code> <code>add_indicator</code> <code>nan</code> <code>keep_empty_features</code> <code>nan</code> Source code in <code>tpot/builtin_modules/imputer.py</code> <pre><code>def __init__(self,  columns=\"all\",         \n                    missing_values=np.nan,\n                    strategy=\"mean\",\n                    fill_value=None,\n                    copy=True,\n                    add_indicator=False,\n                    keep_empty_features=False,):\n    \"\"\"\"\n    A wrapper for SimpleImputer that allows for imputation of specific columns in a DataFrame or np array.\n    Passes through columns that are not imputed.\n\n    Parameters\n    ----------\n    columns : str, list, default='all'\n        Determines which columns to impute with sklearn.impute.SimpleImputer.\n        - 'categorical' : Automatically select categorical features\n        - 'numeric' : Automatically select numeric features\n        - 'all' : Select all features\n        - list : A list of columns to select\n\n    # See documentation from sklearn.impute.SimpleImputer for the following parameters\n    missing_values, strategy, fill_value, copy, add_indicator, keep_empty_features\n\n    \"\"\"\n\n    self.columns = columns\n    self.missing_values = missing_values\n    self.strategy = strategy\n    self.fill_value = fill_value\n    self.copy = copy\n    self.add_indicator = add_indicator\n    self.keep_empty_features = keep_empty_features\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/nn/","title":"Nn","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/builtin_modules/nn/#tpot.builtin_modules.nn.PytorchClassifier","title":"<code>PytorchClassifier</code>","text":"<p>               Bases: <code>PytorchEstimator</code>, <code>ClassifierMixin</code></p> Source code in <code>tpot/builtin_modules/nn.py</code> <pre><code>class PytorchClassifier(PytorchEstimator, ClassifierMixin):\n    @abstractmethod\n    def _init_model(self, X, y): # pragma: no cover\n        pass\n\n    def fit(self, X, y):\n        \"\"\"Generalizable method for fitting a PyTorch estimator to a training\n        set.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vector, where n_samples is the number of samples and\n            n_features is the number of features.\n        y : array-like of shape (n_samples,)\n            Target vector relative to X.\n\n        Returns\n        -------\n        self\n            Fitted estimator.\n        \"\"\"\n        # pylint: disable=no-member\n\n        self._init_model(X, y)\n\n        assert _pytorch_model_is_fully_initialized(self)\n\n        for epoch in range(self.num_epochs):\n            for i, (samples, labels) in enumerate(self.data_loader):\n                samples = samples.to(self.device)\n                labels = labels.to(self.device)\n\n                self.optimizer.zero_grad()\n                outputs = self.network(samples)\n\n                loss = self.loss_function(outputs, labels)\n                loss.backward()\n                self.optimizer.step()\n\n                if self.verbose and ((i + 1) % 100 == 0):\n                    print(\n                        \"Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f\"\n                        % (\n                            epoch + 1,\n                            self.num_epochs,\n                            i + 1,\n                            self.train_dset_len // self.batch_size,\n                            loss.item(),\n                        )\n                    )\n\n        # pylint: disable=attribute-defined-outside-init\n        self.is_fitted_ = True\n        return self\n\n    def validate_inputs(self, X, y):\n        # Things we don't want to allow until we've tested them:\n        # - Sparse inputs\n        # - Multiclass outputs (e.g., more than 2 classes in `y`)\n        # - Non-finite inputs\n        # - Complex inputs\n\n        X, y = check_X_y(X, y, accept_sparse=False, allow_nd=False)\n\n        # Throw a ValueError if X or y contains NaN or infinity.\n        assert_all_finite(X)\n        assert_all_finite(y)\n\n        if type_of_target(y) != 'binary':\n            raise ValueError(\"Non-binary targets not supported\")\n\n        if np.any(np.iscomplex(X)) or np.any(np.iscomplex(y)):\n            raise ValueError(\"Complex data not supported\")\n        if np.issubdtype(X.dtype, np.object_) or np.issubdtype(y.dtype, np.object_):\n            try:\n                X = X.astype(float)\n                y = y.astype(int)\n            except (TypeError, ValueError):\n                raise ValueError(\"argument must be a string.* number\")\n\n        return (X, y)\n\n    def predict(self, X):\n        # pylint: disable=no-member\n\n        X = check_array(X, accept_sparse=True)\n        check_is_fitted(self, 'is_fitted_')\n\n        X = torch.tensor(X, dtype=torch.float32).to(self.device)\n        predictions = np.empty(len(X), dtype=int)\n        for i, rows in enumerate(X):\n            rows = Variable(rows.view(-1, self.input_size))\n            outputs = self.network(rows)\n\n            _, predicted = torch.max(outputs.data, 1)\n            predictions[i] = int(predicted)\n        return predictions.reshape(-1, 1)\n\n    def transform(self, X):\n        return self.predict(X)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/nn/#tpot.builtin_modules.nn.PytorchClassifier.fit","title":"<code>fit(X, y)</code>","text":"<p>Generalizable method for fitting a PyTorch estimator to a training set.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>Training vector, where n_samples is the number of samples and n_features is the number of features.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>Target vector relative to X.</p> required <p>Returns:</p> Type Description <code>self</code> <p>Fitted estimator.</p> Source code in <code>tpot/builtin_modules/nn.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Generalizable method for fitting a PyTorch estimator to a training\n    set.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n    y : array-like of shape (n_samples,)\n        Target vector relative to X.\n\n    Returns\n    -------\n    self\n        Fitted estimator.\n    \"\"\"\n    # pylint: disable=no-member\n\n    self._init_model(X, y)\n\n    assert _pytorch_model_is_fully_initialized(self)\n\n    for epoch in range(self.num_epochs):\n        for i, (samples, labels) in enumerate(self.data_loader):\n            samples = samples.to(self.device)\n            labels = labels.to(self.device)\n\n            self.optimizer.zero_grad()\n            outputs = self.network(samples)\n\n            loss = self.loss_function(outputs, labels)\n            loss.backward()\n            self.optimizer.step()\n\n            if self.verbose and ((i + 1) % 100 == 0):\n                print(\n                    \"Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f\"\n                    % (\n                        epoch + 1,\n                        self.num_epochs,\n                        i + 1,\n                        self.train_dset_len // self.batch_size,\n                        loss.item(),\n                    )\n                )\n\n    # pylint: disable=attribute-defined-outside-init\n    self.is_fitted_ = True\n    return self\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/nn/#tpot.builtin_modules.nn.PytorchEstimator","title":"<code>PytorchEstimator</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Base class for Pytorch-based estimators (currently only classifiers) for use in TPOT.</p> <p>In the future, these will be merged into TPOT's main code base.</p> Source code in <code>tpot/builtin_modules/nn.py</code> <pre><code>class PytorchEstimator(BaseEstimator):\n    \"\"\"Base class for Pytorch-based estimators (currently only classifiers) for\n    use in TPOT.\n\n    In the future, these will be merged into TPOT's main code base.\n    \"\"\"\n\n    @abstractmethod\n    def fit(self, X, y): # pragma: no cover\n        pass\n\n    @abstractmethod\n    def transform(self, X): # pragma: no cover\n        pass\n\n    def predict(self, X):\n        return self.transform(X)\n\n    def fit_transform(self, X, y):\n        self.fit(X, y)\n        return self.transform(X)\n\n    def set_params(self, **parameters):\n        for parameter, value in parameters.items():\n            setattr(self, parameter, value)\n        return self\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/nn/#tpot.builtin_modules.nn.PytorchLRClassifier","title":"<code>PytorchLRClassifier</code>","text":"<p>               Bases: <code>PytorchClassifier</code></p> <p>Logistic Regression classifier, implemented in PyTorch, for use with TPOT.</p> <p>For examples on standalone use (i.e., non-TPOT) refer to: https://github.com/trang1618/tpot-nn/blob/master/tpot_nn/estimator_sandbox.py</p> Source code in <code>tpot/builtin_modules/nn.py</code> <pre><code>class PytorchLRClassifier(PytorchClassifier):\n    \"\"\"Logistic Regression classifier, implemented in PyTorch, for use with\n    TPOT.\n\n    For examples on standalone use (i.e., non-TPOT) refer to:\n    https://github.com/trang1618/tpot-nn/blob/master/tpot_nn/estimator_sandbox.py\n    \"\"\"\n\n    def __init__(\n        self,\n        num_epochs=10,\n        batch_size=16,\n        learning_rate=0.02,\n        weight_decay=1e-4,\n        verbose=False\n    ):\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.learning_rate = learning_rate\n        self.weight_decay = weight_decay\n        self.verbose = verbose\n\n        self.input_size = None\n        self.num_classes = None\n        self.network = None\n        self.loss_function = None\n        self.optimizer = None\n        self.data_loader = None\n        self.train_dset_len = None\n        self.device = None\n\n    def _init_model(self, X, y):\n        device = _get_cuda_device_if_available()\n\n        X, y = self.validate_inputs(X, y)\n\n        self.input_size = X.shape[-1]\n        self.num_classes = len(set(y))\n\n        X = torch.tensor(X, dtype=torch.float32)\n        y = torch.tensor(y, dtype=torch.long)\n\n        train_dset = TensorDataset(X, y)\n\n        # Set parameters of the network\n        self.network = _LR(self.input_size, self.num_classes).to(device)\n        self.loss_function = nn.CrossEntropyLoss()\n        self.optimizer = Adam(self.network.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n        self.data_loader = DataLoader(\n            train_dset, batch_size=self.batch_size, shuffle=True, num_workers=2\n        )\n        self.train_dset_len = len(train_dset)\n        self.device = device\n\n    def _more_tags(self):\n        return {'non_deterministic': True, 'binary_only': True}\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/nn/#tpot.builtin_modules.nn.PytorchMLPClassifier","title":"<code>PytorchMLPClassifier</code>","text":"<p>               Bases: <code>PytorchClassifier</code></p> <p>Multilayer Perceptron, implemented in PyTorch, for use with TPOT.</p> Source code in <code>tpot/builtin_modules/nn.py</code> <pre><code>class PytorchMLPClassifier(PytorchClassifier):\n    \"\"\"Multilayer Perceptron, implemented in PyTorch, for use with TPOT.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_epochs=10,\n        batch_size=8,\n        learning_rate=0.01,\n        weight_decay=0,\n        verbose=False\n    ):\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.learning_rate = learning_rate\n        self.weight_decay = weight_decay\n        self.verbose = verbose\n\n        self.input_size = None\n        self.num_classes = None\n        self.network = None\n        self.loss_function = None\n        self.optimizer = None\n        self.data_loader = None\n        self.train_dset_len = None\n        self.device = None\n\n    def _init_model(self, X, y):\n        device = _get_cuda_device_if_available()\n\n        X, y = self.validate_inputs(X, y)\n\n        self.input_size = X.shape[-1]\n        self.num_classes = len(set(y))\n\n        X = torch.tensor(X, dtype=torch.float32)\n        y = torch.tensor(y, dtype=torch.long)\n\n        train_dset = TensorDataset(X, y)\n\n        # Set parameters of the network\n        self.network = _MLP(self.input_size, self.num_classes).to(device)\n        self.loss_function = nn.CrossEntropyLoss()\n        self.optimizer = Adam(self.network.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n        self.data_loader = DataLoader(\n            train_dset, batch_size=self.batch_size, shuffle=True, num_workers=2\n        )\n        self.train_dset_len = len(train_dset)\n        self.device = device\n\n    def _more_tags(self):\n        return {'non_deterministic': True, 'binary_only': True}\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/passkbinsdiscretizer/","title":"Passkbinsdiscretizer","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/builtin_modules/passkbinsdiscretizer/#tpot.builtin_modules.passkbinsdiscretizer.PassKBinsDiscretizer","title":"<code>PassKBinsDiscretizer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/passkbinsdiscretizer.py</code> <pre><code>class PassKBinsDiscretizer(BaseEstimator, TransformerMixin):\n    def __init__(self, n_bins=5,  encode='onehot-dense', strategy='quantile', subsample=None, random_state=None):\n        self.n_bins = n_bins\n        self.encode = encode\n        self.strategy = strategy\n        self.subsample = subsample\n        self.random_state = random_state\n        \"\"\"\n        Same as sklearn.preprocessing.KBinsDiscretizer, but passes through columns that are not discretized due to having fewer than n_bins unique values instead of ignoring them.\n        See sklearn.preprocessing.KBinsDiscretizer for more information.\n        \"\"\"\n\n    def fit(self, X, y=None):\n        # Identify columns with more than n unique values\n        # Create a ColumnTransformer to select and discretize the chosen columns\n        self.selected_columns_ = select_features(X, min_unique=10)\n        if isinstance(X, pd.DataFrame):\n            self.not_selected_columns_ = [col for col in X.columns if col not in self.selected_columns_]\n        else:\n            self.not_selected_columns_ = [i for i in range(X.shape[1]) if i not in self.selected_columns_]\n\n        enc = KBinsDiscretizer(n_bins=self.n_bins, encode=self.encode, strategy=self.strategy, subsample=self.subsample, random_state=self.random_state)\n        self.transformer = ColumnTransformer([\n            ('discretizer', enc, self.selected_columns_),\n            ('passthrough', 'passthrough', self.not_selected_columns_)\n        ])\n        self.transformer.fit(X)\n        return self\n\n    def transform(self, X):\n        return self.transformer.transform(X)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/passkbinsdiscretizer/#tpot.builtin_modules.passkbinsdiscretizer.PassKBinsDiscretizer.random_state","title":"<code>random_state = random_state</code>  <code>instance-attribute</code>","text":"<p>Same as sklearn.preprocessing.KBinsDiscretizer, but passes through columns that are not discretized due to having fewer than n_bins unique values instead of ignoring them. See sklearn.preprocessing.KBinsDiscretizer for more information.</p>"},{"location":"documentation/tpot/builtin_modules/passkbinsdiscretizer/#tpot.builtin_modules.passkbinsdiscretizer.select_features","title":"<code>select_features(X, min_unique=10)</code>","text":"<p>Given a DataFrame or numpy array, return a list of column indices that have more than min_unique unique values.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>Data to select features from</p> required <code>min_unique</code> <p>Minimum number of unique values a column must have to be selected</p> <code>10</code> <p>Returns:</p> Type Description <code>list</code> <p>List of column indices that have more than min_unique unique values</p> Source code in <code>tpot/builtin_modules/passkbinsdiscretizer.py</code> <pre><code>def select_features(X, min_unique=10,):\n    \"\"\"\n    Given a DataFrame or numpy array, return a list of column indices that have more than min_unique unique values.\n\n    Parameters\n    ----------\n    X: DataFrame or numpy array\n        Data to select features from\n    min_unique: int, default=10\n        Minimum number of unique values a column must have to be selected\n\n    Returns\n    -------\n    list\n        List of column indices that have more than min_unique unique values\n\n    \"\"\"\n\n    if isinstance(X, pd.DataFrame):\n        return [col for col in X.columns if len(X[col].unique()) &gt; min_unique]\n    else:\n        return [i for i in range(X.shape[1]) if len(np.unique(X[:, i])) &gt; min_unique]\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/passthrough/","title":"Passthrough","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/builtin_modules/passthrough/#tpot.builtin_modules.passthrough.Passthrough","title":"<code>Passthrough</code>","text":"<p>               Bases: <code>TransformerMixin</code>, <code>BaseEstimator</code></p> <p>A transformer that does nothing. It just passes the input array as is.</p> Source code in <code>tpot/builtin_modules/passthrough.py</code> <pre><code>class Passthrough(TransformerMixin,BaseEstimator):\n    \"\"\"\n    A transformer that does nothing. It just passes the input array as is.\n    \"\"\"\n\n    def fit(self, X=None, y=None):\n        \"\"\"\n        Nothing to fit, just returns self.\n        \"\"\"\n        return self\n\n    def transform(self, X):\n        \"\"\"\n        returns the input array as is.\n        \"\"\"\n        return X\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/passthrough/#tpot.builtin_modules.passthrough.Passthrough.fit","title":"<code>fit(X=None, y=None)</code>","text":"<p>Nothing to fit, just returns self.</p> Source code in <code>tpot/builtin_modules/passthrough.py</code> <pre><code>def fit(self, X=None, y=None):\n    \"\"\"\n    Nothing to fit, just returns self.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/passthrough/#tpot.builtin_modules.passthrough.Passthrough.transform","title":"<code>transform(X)</code>","text":"<p>returns the input array as is.</p> Source code in <code>tpot/builtin_modules/passthrough.py</code> <pre><code>def transform(self, X):\n    \"\"\"\n    returns the input array as is.\n    \"\"\"\n    return X\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/passthrough/#tpot.builtin_modules.passthrough.SkipTransformer","title":"<code>SkipTransformer</code>","text":"<p>               Bases: <code>TransformerMixin</code>, <code>BaseEstimator</code></p> <p>A transformer returns an empty array. When combined with FeatureUnion, it can be used to skip a branch.</p> Source code in <code>tpot/builtin_modules/passthrough.py</code> <pre><code>class SkipTransformer(TransformerMixin,BaseEstimator):\n    \"\"\"\n    A transformer returns an empty array. When combined with FeatureUnion, it can be used to skip a branch.\n    \"\"\"\n    def fit(self, X=None, y=None):\n        \"\"\"\n        Nothing to fit, just returns self.\n        \"\"\"\n        return self\n\n    def transform(self, X):\n        \"\"\"\n        returns an empty array.\n        \"\"\"\n        return np.array([]).reshape(X.shape[0],0)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/passthrough/#tpot.builtin_modules.passthrough.SkipTransformer.fit","title":"<code>fit(X=None, y=None)</code>","text":"<p>Nothing to fit, just returns self.</p> Source code in <code>tpot/builtin_modules/passthrough.py</code> <pre><code>def fit(self, X=None, y=None):\n    \"\"\"\n    Nothing to fit, just returns self.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/passthrough/#tpot.builtin_modules.passthrough.SkipTransformer.transform","title":"<code>transform(X)</code>","text":"<p>returns an empty array.</p> Source code in <code>tpot/builtin_modules/passthrough.py</code> <pre><code>def transform(self, X):\n    \"\"\"\n    returns an empty array.\n    \"\"\"\n    return np.array([]).reshape(X.shape[0],0)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/zero_count/","title":"Zero count","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/builtin_modules/zero_count/#tpot.builtin_modules.zero_count.ZeroCount","title":"<code>ZeroCount</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Adds the count of zeros and count of non-zeros per sample as features.</p> Source code in <code>tpot/builtin_modules/zero_count.py</code> <pre><code>class ZeroCount(BaseEstimator, TransformerMixin):\n    \"\"\"Adds the count of zeros and count of non-zeros per sample as features.\"\"\"\n\n    def fit(self, X, y=None):\n        \"\"\"Dummy function to fit in with the sklearn API.\"\"\"\n        return self\n\n    def transform(self, X, y=None):\n        \"\"\"Transform data by adding two virtual features.\n\n        Parameters\n        ----------\n        X: numpy ndarray, {n_samples, n_components}\n            New data, where n_samples is the number of samples and n_components\n            is the number of components.\n        y: None\n            Unused\n\n        Returns\n        -------\n        X_transformed: array-like, shape (n_samples, n_features)\n            The transformed feature set\n        \"\"\"\n        X = check_array(X)\n        n_features = X.shape[1]\n\n        X_transformed = np.copy(X)\n\n        non_zero_vector = np.count_nonzero(X_transformed, axis=1)\n        non_zero = np.reshape(non_zero_vector, (-1, 1))\n        zero_col = np.reshape(n_features - non_zero_vector, (-1, 1))\n\n        X_transformed = np.hstack((non_zero, X_transformed))\n        X_transformed = np.hstack((zero_col, X_transformed))\n\n        return X_transformed\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/zero_count/#tpot.builtin_modules.zero_count.ZeroCount.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Dummy function to fit in with the sklearn API.</p> Source code in <code>tpot/builtin_modules/zero_count.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Dummy function to fit in with the sklearn API.\"\"\"\n    return self\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/zero_count/#tpot.builtin_modules.zero_count.ZeroCount.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Transform data by adding two virtual features.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>New data, where n_samples is the number of samples and n_components is the number of components.</p> required <code>y</code> <p>Unused</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_transformed</code> <code>(array - like, shape(n_samples, n_features))</code> <p>The transformed feature set</p> Source code in <code>tpot/builtin_modules/zero_count.py</code> <pre><code>def transform(self, X, y=None):\n    \"\"\"Transform data by adding two virtual features.\n\n    Parameters\n    ----------\n    X: numpy ndarray, {n_samples, n_components}\n        New data, where n_samples is the number of samples and n_components\n        is the number of components.\n    y: None\n        Unused\n\n    Returns\n    -------\n    X_transformed: array-like, shape (n_samples, n_features)\n        The transformed feature set\n    \"\"\"\n    X = check_array(X)\n    n_features = X.shape[1]\n\n    X_transformed = np.copy(X)\n\n    non_zero_vector = np.count_nonzero(X_transformed, axis=1)\n    non_zero = np.reshape(non_zero_vector, (-1, 1))\n    zero_col = np.reshape(n_features - non_zero_vector, (-1, 1))\n\n    X_transformed = np.hstack((non_zero, X_transformed))\n    X_transformed = np.hstack((zero_col, X_transformed))\n\n    return X_transformed\n</code></pre>"},{"location":"documentation/tpot/config/autoqtl_builtins/","title":"Autoqtl builtins","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/config/classifiers/","title":"Classifiers","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/config/classifiers_sklearnex/","title":"Classifiers sklearnex","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/config/get_configspace/","title":"Get configspace","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/config/get_configspace/#tpot.config.get_configspace.get_configspace","title":"<code>get_configspace(name, n_classes=3, n_samples=1000, n_features=100, random_state=None, n_jobs=1)</code>","text":"<p>This function returns the ConfigSpace.ConfigurationSpace with the hyperparameter ranges for the given scikit-learn method. It also uses the n_classes, n_samples, n_features, and random_state to set the hyperparameters that depend on these values.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The str name of the scikit-learn method for which to create the ConfigurationSpace. (e.g. 'RandomForestClassifier' for sklearn.ensemble.RandomForestClassifier)</p> required <code>n_classes</code> <code>int</code> <p>The number of classes in the target variable. Default is 3.</p> <code>3</code> <code>n_samples</code> <code>int</code> <p>The number of samples in the dataset. Default is 1000.</p> <code>1000</code> <code>n_features</code> <code>int</code> <p>The number of features in the dataset. Default is 100.</p> <code>100</code> <code>random_state</code> <code>int</code> <p>The random_state to use in the ConfigurationSpace. Default is None. If None, the random_state hyperparameter is not included in the ConfigurationSpace. Use this to set the random state for the individual methods if you want to ensure reproducibility.</p> <code>None</code> <code>n_jobs</code> <code>int(default=1)</code> <p>Sets the n_jobs parameter for estimators that have it. Default is 1.</p> <code>1</code> Source code in <code>tpot/config/get_configspace.py</code> <pre><code>def get_configspace(name, n_classes=3, n_samples=1000, n_features=100, random_state=None, n_jobs=1):\n    \"\"\"\n    This function returns the ConfigSpace.ConfigurationSpace with the hyperparameter ranges for the given\n    scikit-learn method. It also uses the n_classes, n_samples, n_features, and random_state to set the\n    hyperparameters that depend on these values.\n\n    Parameters\n    ----------\n    name : str\n        The str name of the scikit-learn method for which to create the ConfigurationSpace. (e.g. 'RandomForestClassifier' for sklearn.ensemble.RandomForestClassifier)\n    n_classes : int\n        The number of classes in the target variable. Default is 3.\n    n_samples : int\n        The number of samples in the dataset. Default is 1000.\n    n_features : int\n        The number of features in the dataset. Default is 100.\n    random_state : int\n        The random_state to use in the ConfigurationSpace. Default is None.\n        If None, the random_state hyperparameter is not included in the ConfigurationSpace.\n        Use this to set the random state for the individual methods if you want to ensure reproducibility.\n    n_jobs : int (default=1)\n        Sets the n_jobs parameter for estimators that have it. Default is 1.\n\n    \"\"\"\n    match name:\n\n        #autoqtl_builtins.py\n        case \"FeatureEncodingFrequencySelector\":\n            return autoqtl_builtins.FeatureEncodingFrequencySelector_ConfigurationSpace\n        case \"DominantEncoder\":\n            return {}\n        case \"RecessiveEncoder\":\n            return {}\n        case \"HeterosisEncoder\":\n            return {}\n        case \"UnderDominanceEncoder\":\n            return {}\n        case \"OverDominanceEncoder\":\n            return {}\n\n        case \"Passthrough\":\n            return {}\n        case \"SkipTransformer\":\n            return {}\n\n        #classifiers.py\n        case \"LinearDiscriminantAnalysis\":\n            return classifiers.get_LinearDiscriminantAnalysis_ConfigurationSpace()\n        case \"AdaBoostClassifier\":\n            return classifiers.get_AdaBoostClassifier_ConfigurationSpace(random_state=random_state)\n        case \"LogisticRegression\":\n            return classifiers.get_LogisticRegression_ConfigurationSpace(random_state=random_state, n_jobs=n_jobs)\n        case \"KNeighborsClassifier\":\n            return classifiers.get_KNeighborsClassifier_ConfigurationSpace(n_samples=n_samples, n_jobs=n_jobs)\n        case \"DecisionTreeClassifier\":\n            return classifiers.get_DecisionTreeClassifier_ConfigurationSpace(n_featues=n_features, random_state=random_state)\n        case \"SVC\":\n            return classifiers.get_SVC_ConfigurationSpace(random_state=random_state)\n        case \"LinearSVC\":\n            return classifiers.get_LinearSVC_ConfigurationSpace(random_state=random_state)\n        case \"RandomForestClassifier\":\n            return classifiers.get_RandomForestClassifier_ConfigurationSpace(random_state=random_state, n_jobs=n_jobs)\n        case \"GradientBoostingClassifier\":\n            return classifiers.get_GradientBoostingClassifier_ConfigurationSpace(n_classes=n_classes, random_state=random_state)\n        case \"HistGradientBoostingClassifier\":\n            return classifiers.get_HistGradientBoostingClassifier_ConfigurationSpace(random_state=random_state)\n        case \"XGBClassifier\":\n            return classifiers.get_XGBClassifier_ConfigurationSpace(random_state=random_state, n_jobs=n_jobs)\n        case \"LGBMClassifier\":\n            return classifiers.get_LGBMClassifier_ConfigurationSpace(random_state=random_state, n_jobs=n_jobs)\n        case \"ExtraTreesClassifier\":\n            return classifiers.get_ExtraTreesClassifier_ConfigurationSpace(random_state=random_state, n_jobs=n_jobs)\n        case \"SGDClassifier\":\n            return classifiers.get_SGDClassifier_ConfigurationSpace(random_state=random_state, n_jobs=n_jobs)\n        case \"MLPClassifier\":\n            return classifiers.get_MLPClassifier_ConfigurationSpace(random_state=random_state)\n        case \"BernoulliNB\":\n            return classifiers.get_BernoulliNB_ConfigurationSpace()\n        case \"MultinomialNB\":\n            return classifiers.get_MultinomialNB_ConfigurationSpace()\n        case \"GaussianNB\":\n            return {}\n        case \"LassoLarsCV\":\n            return {}\n        case \"ElasticNetCV\":\n            return regressors.ElasticNetCV_configspace\n        case \"RidgeCV\":\n            return {}\n        case \"PassiveAggressiveClassifier\":\n            return classifiers.get_PassiveAggressiveClassifier_ConfigurationSpace(random_state=random_state)\n        case \"QuadraticDiscriminantAnalysis\":\n            return classifiers.get_QuadraticDiscriminantAnalysis_ConfigurationSpace()\n        case \"GaussianProcessClassifier\":\n            return classifiers.get_GaussianProcessClassifier_ConfigurationSpace(n_features=n_features, random_state=random_state)\n        case \"BaggingClassifier\":\n            return classifiers.get_BaggingClassifier_ConfigurationSpace(random_state=random_state, n_jobs=n_jobs)\n\n        #regressors.py\n        case \"RandomForestRegressor\":\n            return regressors.get_RandomForestRegressor_ConfigurationSpace(random_state=random_state, n_jobs=n_jobs)\n        case \"SGDRegressor\":\n            return regressors.get_SGDRegressor_ConfigurationSpace(random_state=random_state)\n        case \"Ridge\":\n            return regressors.get_Ridge_ConfigurationSpace(random_state=random_state)\n        case \"Lasso\":\n            return regressors.get_Lasso_ConfigurationSpace(random_state=random_state)\n        case \"ElasticNet\":\n            return regressors.get_ElasticNet_ConfigurationSpace(random_state=random_state)\n        case \"Lars\":\n            return regressors.get_Lars_ConfigurationSpace(random_state=random_state)\n        case \"OthogonalMatchingPursuit\":\n            return regressors.get_OthogonalMatchingPursuit_ConfigurationSpace()\n        case \"BayesianRidge\":\n            return regressors.get_BayesianRidge_ConfigurationSpace()\n        case \"LassoLars\":\n            return regressors.get_LassoLars_ConfigurationSpace(random_state=random_state)\n        case \"BaggingRegressor\":\n            return regressors.get_BaggingRegressor_ConfigurationSpace(random_state=random_state)\n        case \"ARDRegression\":\n            return regressors.get_ARDRegression_ConfigurationSpace()\n        case \"TheilSenRegressor\":\n            return regressors.get_TheilSenRegressor_ConfigurationSpace(random_state=random_state)\n        case \"Perceptron\":\n            return regressors.get_Perceptron_ConfigurationSpace(random_state=random_state)\n        case \"DecisionTreeRegressor\":\n            return regressors.get_DecisionTreeRegressor_ConfigurationSpace(random_state=random_state)\n        case \"LinearSVR\":\n            return regressors.get_LinearSVR_ConfigurationSpace(random_state=random_state)\n        case \"SVR\":\n            return regressors.get_SVR_ConfigurationSpace()\n        case \"XGBRegressor\":\n            return regressors.get_XGBRegressor_ConfigurationSpace(random_state=random_state, n_jobs=n_jobs)\n        case \"AdaBoostRegressor\":\n            return regressors.get_AdaBoostRegressor_ConfigurationSpace(random_state=random_state)\n        case \"ExtraTreesRegressor\":\n            return regressors.get_ExtraTreesRegressor_ConfigurationSpace(random_state=random_state, n_jobs=n_jobs)\n        case \"GradientBoostingRegressor\":\n            return regressors.get_GradientBoostingRegressor_ConfigurationSpace(random_state=random_state)\n        case \"HistGradientBoostingRegressor\":\n            return regressors.get_HistGradientBoostingRegressor_ConfigurationSpace(random_state=random_state)\n        case \"MLPRegressor\":\n            return regressors.get_MLPRegressor_ConfigurationSpace(random_state=random_state)\n        case \"KNeighborsRegressor\":\n            return regressors.get_KNeighborsRegressor_ConfigurationSpace(n_samples=n_samples, n_jobs=n_jobs)\n        case \"GaussianProcessRegressor\":\n            return regressors.get_GaussianProcessRegressor_ConfigurationSpace(n_features=n_features, random_state=random_state)\n        case \"LGBMRegressor\":\n            return regressors.get_LGBMRegressor_ConfigurationSpace(random_state=random_state, n_jobs=n_jobs)\n        case \"BaggingRegressor\":\n            return regressors.get_BaggingRegressor_ConfigurationSpace(random_state=random_state, n_jobs=n_jobs)\n\n        #transformers.py\n        case \"Binarizer\":\n            return transformers.Binarizer_configspace\n        case \"Normalizer\":\n            return transformers.Normalizer_configspace\n        case \"PCA\":\n            return transformers.PCA_configspace\n        case \"ZeroCount\":\n            return transformers.ZeroCount_configspace\n        case \"FastICA\":\n            return transformers.get_FastICA_configspace(n_features=n_features, random_state=random_state)\n        case \"FeatureAgglomeration\":\n            return transformers.get_FeatureAgglomeration_configspace(n_features=n_features)\n        case \"Nystroem\":\n            return transformers.get_Nystroem_configspace(n_features=n_features, random_state=random_state)\n        case \"RBFSampler\":\n            return transformers.get_RBFSampler_configspace(n_features=n_features, random_state=random_state)\n        case \"MinMaxScaler\":\n            return {}\n        case \"PowerTransformer\":\n            return {}\n        case \"QuantileTransformer\":\n            return transformers.get_QuantileTransformer_configspace(n_samples=n_samples, random_state=random_state)\n        case \"RobustScaler\":\n            return transformers.RobustScaler_configspace\n        case \"MaxAbsScaler\":\n            return {}\n        case \"PolynomialFeatures\":\n            return transformers.PolynomialFeatures_configspace\n        case \"StandardScaler\":\n            return {}\n        case \"PassKBinsDiscretizer\":\n            return transformers.get_passkbinsdiscretizer_configspace(random_state=random_state)\n        case \"KBinsDiscretizer\":\n            return transformers.get_passkbinsdiscretizer_configspace(random_state=random_state)\n        case \"ColumnOneHotEncoder\":\n            return {}\n        case \"ColumnOrdinalEncoder\":\n            return {}\n\n        #selectors.py\n        case \"SelectFwe\":\n            return selectors.SelectFwe_configspace \n        case \"SelectPercentile\":\n            return selectors.SelectPercentile_configspace\n        case \"VarianceThreshold\":\n            return selectors.VarianceThreshold_configspace\n        case \"RFE\":\n            return selectors.RFE_configspace_part\n        case \"SelectFromModel\":\n            return selectors.SelectFromModel_configspace_part\n\n\n        #special_configs.py\n        case \"AddTransformer\":\n            return {}\n        case \"mul_neg_1_Transformer\":\n            return {}\n        case \"MulTransformer\":\n            return {}\n        case \"SafeReciprocalTransformer\":\n            return {}\n        case \"EQTransformer\":\n            return {}\n        case \"NETransformer\":\n            return {}\n        case \"GETransformer\":\n            return {}\n        case \"GTTransformer\":\n            return {}\n        case \"LETransformer\":\n            return {}\n        case \"LTTransformer\":\n            return {}        \n        case \"MinTransformer\":\n            return {}\n        case \"MaxTransformer\":\n            return {}\n        case \"ZeroTransformer\":\n            return {}\n        case \"OneTransformer\":\n            return {}\n        case \"NTransformer\":\n            return ConfigurationSpace(\n\n                space = {\n\n                    'n': Float(\"n\", bounds=(-1e2, 1e2)),\n                }\n            ) \n\n        #imputers.py\n        case \"SimpleImputer\":\n            return imputers.simple_imputer_cs\n        case \"IterativeImputer\":\n            return imputers.get_IterativeImputer_config_space(n_features=n_features, random_state=random_state)\n        case \"IterativeImputer_no_estimator\":\n            return imputers.get_IterativeImputer_config_space_no_estimator(n_features=n_features, random_state=random_state)\n\n        case \"KNNImputer\":\n            return imputers.get_KNNImputer_config_space(n_samples=n_samples)\n\n        #mdr_configs.py\n        case \"MDR\":\n            return mdr_configs.MDR_configspace\n        case \"ContinuousMDR\":\n            return mdr_configs.MDR_configspace\n        case \"ReliefF\":\n            return mdr_configs.get_skrebate_ReliefF_config_space(n_features=n_features)\n        case \"SURF\":\n            return mdr_configs.get_skrebate_SURF_config_space(n_features=n_features)\n        case \"SURFstar\":\n            return mdr_configs.get_skrebate_SURFstar_config_space(n_features=n_features)\n        case \"MultiSURF\":\n            return mdr_configs.get_skrebate_MultiSURF_config_space(n_features=n_features)\n\n        #classifiers_sklearnex.py\n        case \"RandomForestClassifier_sklearnex\":\n            return classifiers_sklearnex.get_RandomForestClassifier_ConfigurationSpace(random_state=random_state, n_jobs=n_jobs)\n        case \"LogisticRegression_sklearnex\":\n            return classifiers_sklearnex.get_LogisticRegression_ConfigurationSpace(random_state=random_state)\n        case \"KNeighborsClassifier_sklearnex\":\n            return classifiers_sklearnex.get_KNeighborsClassifier_ConfigurationSpace(n_samples=n_samples)\n        case \"SVC_sklearnex\":\n            return classifiers_sklearnex.get_SVC_ConfigurationSpace(random_state=random_state)\n        case \"NuSVC_sklearnex\":\n            return classifiers_sklearnex.get_NuSVC_ConfigurationSpace(random_state=random_state)\n\n        #regressors_sklearnex.py\n        case \"LinearRegression_sklearnex\":\n            return {}\n        case \"Ridge_sklearnex\":\n            return regressors_sklearnex.get_Ridge_ConfigurationSpace(random_state=random_state)\n        case \"Lasso_sklearnex\":\n            return regressors_sklearnex.get_Lasso_ConfigurationSpace(random_state=random_state)\n        case \"ElasticNet_sklearnex\":\n            return regressors_sklearnex.get_ElasticNet_ConfigurationSpace(random_state=random_state)\n        case \"SVR_sklearnex\":\n            return regressors_sklearnex.get_SVR_ConfigurationSpace(random_state=random_state)\n        case \"NuSVR_sklearnex\":\n            return regressors_sklearnex.get_NuSVR_ConfigurationSpace(random_state=random_state)\n        case \"RandomForestRegressor_sklearnex\":\n            return regressors_sklearnex.get_RandomForestRegressor_ConfigurationSpace(random_state=random_state)\n        case \"KNeighborsRegressor_sklearnex\":\n            return regressors_sklearnex.get_KNeighborsRegressor_ConfigurationSpace(n_samples=n_samples)\n\n    #raise error\n    raise ValueError(f\"Could not find configspace for {name}\")\n</code></pre>"},{"location":"documentation/tpot/config/get_configspace/#tpot.config.get_configspace.get_node","title":"<code>get_node(name, n_classes=3, n_samples=100, n_features=100, random_state=None, base_node=EstimatorNode, n_jobs=1)</code>","text":"<p>Helper function for get_search_space. Returns a single EstimatorNode for the given scikit-learn method. Also includes special cases for nodes that require custom parsing of the hyperparameters or methods that wrap other methods.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str or list</code> <p>The name of the scikit-learn method or group of methods for which to create the search space. - str: The name of the scikit-learn method. (e.g. 'RandomForestClassifier' for sklearn.ensemble.RandomForestClassifier) Alternatively, the name of a group of methods. (e.g. 'classifiers' for all classifiers). - list: A list of scikit-learn method names. (e.g. ['RandomForestClassifier', 'ExtraTreesClassifier'])</p> required <code>n_classes</code> <code>int(default=3)</code> <p>The number of classes in the target variable.</p> <code>3</code> <code>n_samples</code> <code>int(default=1000)</code> <p>The number of samples in the dataset.</p> <code>100</code> <code>n_features</code> <code>int(default=100)</code> <p>The number of features in the dataset.</p> <code>100</code> <code>random_state</code> <code>int(default=None)</code> <p>A fixed random_state to pass through to all methods that have a random_state hyperparameter.</p> <code>None</code> <code>return_choice_pipeline</code> <code>bool(default=True)</code> <p>If False, returns a list of TPOT.search_spaces.nodes.EstimatorNode objects. If True, returns a single TPOT.search_spaces.pipelines.ChoicePipeline that includes and samples from all EstimatorNodes.</p> required <code>base_node</code> <p>The SearchSpace to pass the configuration space to. If you want to experiment with custom mutation/crossover operators, you can pass a custom SearchSpace node here.</p> <code>EstimatorNode</code> <code>n_jobs</code> <code>int(default=1)</code> <p>Sets the n_jobs parameter for estimators that have it. Default is 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>    Returns an SearchSpace object that can be optimized by TPOT.</code> <ul> <li>TPOT.search_spaces.nodes.EstimatorNode (or base_node).</li> <li>TPOT.search_spaces.pipelines.WrapperPipeline object if the method requires a wrapped estimator.</li> </ul> Source code in <code>tpot/config/get_configspace.py</code> <pre><code>def get_node(name, n_classes=3, n_samples=100, n_features=100, random_state=None, base_node=EstimatorNode, n_jobs=1):\n    \"\"\"\n    Helper function for get_search_space. Returns a single EstimatorNode for the given scikit-learn method. Also includes special cases for nodes that require custom parsing of the hyperparameters or methods that wrap other methods.\n\n    Parameters\n    ----------\n\n    name : str or list\n        The name of the scikit-learn method or group of methods for which to create the search space.\n        - str: The name of the scikit-learn method. (e.g. 'RandomForestClassifier' for sklearn.ensemble.RandomForestClassifier)\n        Alternatively, the name of a group of methods. (e.g. 'classifiers' for all classifiers).\n        - list: A list of scikit-learn method names. (e.g. ['RandomForestClassifier', 'ExtraTreesClassifier'])\n    n_classes : int (default=3)\n        The number of classes in the target variable.\n    n_samples : int (default=1000)\n        The number of samples in the dataset.\n    n_features : int (default=100)\n        The number of features in the dataset.\n    random_state : int (default=None)\n        A fixed random_state to pass through to all methods that have a random_state hyperparameter. \n    return_choice_pipeline : bool (default=True)\n        If False, returns a list of TPOT.search_spaces.nodes.EstimatorNode objects.\n        If True, returns a single TPOT.search_spaces.pipelines.ChoicePipeline that includes and samples from all EstimatorNodes.\n    base_node: TPOT.search_spaces.base.SearchSpace (default=TPOT.search_spaces.nodes.EstimatorNode)\n        The SearchSpace to pass the configuration space to. If you want to experiment with custom mutation/crossover operators, you can pass a custom SearchSpace node here.\n    n_jobs : int (default=1)\n        Sets the n_jobs parameter for estimators that have it. Default is 1.\n\n    Returns\n    -------\n        Returns an SearchSpace object that can be optimized by TPOT.\n        - TPOT.search_spaces.nodes.EstimatorNode (or base_node).\n        - TPOT.search_spaces.pipelines.WrapperPipeline object if the method requires a wrapped estimator.\n\n\n    \"\"\"\n\n    if name == \"LinearSVC_wrapped\":\n        ext = get_node(\"LinearSVC\", n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return WrapperPipeline(estimator_search_space=ext, method=sklearn.calibration.CalibratedClassifierCV, space={})\n    if name == \"RFE_classification\":\n        rfe_sp = get_configspace(name=\"RFE\", n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        ext = get_node(\"ExtraTreesClassifier\", n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return WrapperPipeline(estimator_search_space=ext, method=RFE, space=rfe_sp)\n    if name == \"RFE_regression\":\n        rfe_sp = get_configspace(name=\"RFE\", n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        ext = get_node(\"ExtraTreesRegressor\", n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return WrapperPipeline(estimator_search_space=ext, method=RFE, space=rfe_sp)\n    if name == \"SelectFromModel_classification\":\n        sfm_sp = get_configspace(name=\"SelectFromModel\", n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        ext = get_node(\"ExtraTreesClassifier\", n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return WrapperPipeline(estimator_search_space=ext, method=SelectFromModel, space=sfm_sp)\n    if name == \"SelectFromModel_regression\":\n        sfm_sp = get_configspace(name=\"SelectFromModel\", n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        ext = get_node(\"ExtraTreesRegressor\", n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return WrapperPipeline(estimator_search_space=ext, method=SelectFromModel, space=sfm_sp)\n    # TODO Add IterativeImputer with more estimator methods\n    if name == \"IterativeImputer_learned_estimators\":\n        iteative_sp = get_configspace(name=\"IterativeImputer_no_estimator\", n_features=n_features, random_state=random_state, n_jobs=n_jobs)\n        regressor_searchspace = get_node(\"ExtraTreesRegressor\", n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return WrapperPipeline(estimator_search_space=regressor_searchspace, method=IterativeImputer, space=iteative_sp)\n\n    #these are nodes that have special search spaces which require custom parsing of the hyperparameters\n    if name == \"IterativeImputer\":\n        configspace = get_configspace(name, n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return EstimatorNode(STRING_TO_CLASS[name], configspace, hyperparameter_parser=imputers.IterativeImputer_hyperparameter_parser)\n    if name == \"RobustScaler\":\n        configspace = get_configspace(name, n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return base_node(STRING_TO_CLASS[name], configspace, hyperparameter_parser=transformers.robust_scaler_hyperparameter_parser)\n    if name == \"GradientBoostingClassifier\":\n        configspace = get_configspace(name, n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return base_node(STRING_TO_CLASS[name], configspace, hyperparameter_parser=classifiers.GradientBoostingClassifier_hyperparameter_parser)\n    if name == \"HistGradientBoostingClassifier\":\n        configspace = get_configspace(name, n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return base_node(STRING_TO_CLASS[name], configspace, hyperparameter_parser=classifiers.HistGradientBoostingClassifier_hyperparameter_parser)\n    if name == \"GradientBoostingRegressor\":\n        configspace = get_configspace(name, n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return base_node(STRING_TO_CLASS[name], configspace, hyperparameter_parser=regressors.GradientBoostingRegressor_hyperparameter_parser)\n    if  name == \"HistGradientBoostingRegressor\":\n        configspace = get_configspace(name, n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return base_node(STRING_TO_CLASS[name], configspace, hyperparameter_parser=regressors.HistGradientBoostingRegressor_hyperparameter_parser)\n    if name == \"MLPClassifier\":\n        configspace = get_configspace(name, n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return base_node(STRING_TO_CLASS[name], configspace, hyperparameter_parser=classifiers.MLPClassifier_hyperparameter_parser)\n    if name == \"MLPRegressor\":\n        configspace = get_configspace(name, n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return base_node(STRING_TO_CLASS[name], configspace, hyperparameter_parser=regressors.MLPRegressor_hyperparameter_parser)\n    if name == \"GaussianProcessRegressor\":\n        configspace = get_configspace(name, n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return base_node(STRING_TO_CLASS[name], configspace, hyperparameter_parser=regressors.GaussianProcessRegressor_hyperparameter_parser)\n    if name == \"GaussianProcessClassifier\":\n        configspace = get_configspace(name, n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return base_node(STRING_TO_CLASS[name], configspace, hyperparameter_parser=classifiers.GaussianProcessClassifier_hyperparameter_parser)\n    if name == \"FeatureAgglomeration\":\n        configspace = get_configspace(name, n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return base_node(STRING_TO_CLASS[name], configspace, hyperparameter_parser=transformers.FeatureAgglomeration_hyperparameter_parser)\n\n    configspace = get_configspace(name, n_classes=n_classes, n_samples=n_samples, n_features=n_features, random_state=random_state, n_jobs=n_jobs)\n    if configspace is None:\n        #raise warning\n        warnings.warn(f\"Could not find configspace for {name}\")\n        return None\n\n    return base_node(STRING_TO_CLASS[name], configspace)\n</code></pre>"},{"location":"documentation/tpot/config/get_configspace/#tpot.config.get_configspace.get_search_space","title":"<code>get_search_space(name, n_classes=3, n_samples=1000, n_features=100, random_state=None, return_choice_pipeline=True, base_node=EstimatorNode, n_jobs=1)</code>","text":"<p>Returns a TPOT search space for a given scikit-learn method or group of methods.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str or list</code> <p>The name of the scikit-learn method or group of methods for which to create the search space. - str: The name of the scikit-learn method. (e.g. 'RandomForestClassifier' for sklearn.ensemble.RandomForestClassifier) Alternatively, the name of a group of methods. (e.g. 'classifiers' for all classifiers). - list: A list of scikit-learn method names. (e.g. ['RandomForestClassifier', 'ExtraTreesClassifier'])</p> required <code>n_classes</code> <code>int(default=3)</code> <p>The number of classes in the target variable.</p> <code>3</code> <code>n_samples</code> <code>int(default=1000)</code> <p>The number of samples in the dataset.</p> <code>1000</code> <code>n_features</code> <code>int(default=100)</code> <p>The number of features in the dataset.</p> <code>100</code> <code>random_state</code> <code>int(default=None)</code> <p>A fixed random_state to pass through to all methods that have a random_state hyperparameter.</p> <code>None</code> <code>return_choice_pipeline</code> <code>bool(default=True)</code> <p>If False, returns a list of TPOT.search_spaces.nodes.EstimatorNode objects. If True, returns a single TPOT.search_spaces.pipelines.ChoicePipeline that includes and samples from all EstimatorNodes.</p> <code>True</code> <code>base_node</code> <p>The SearchSpace to pass the configuration space to. If you want to experiment with custom mutation/crossover operators, you can pass a custom SearchSpace node here.</p> <code>EstimatorNode</code> <code>n_jobs</code> <code>int(default=1)</code> <p>Sets the n_jobs parameter for estimators that have it. Default is 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>    Returns an SearchSpace object that can be optimized by TPOT.</code> <ul> <li>TPOT.search_spaces.nodes.EstimatorNode (or base_node) if there is only one search space.</li> <li>List of TPOT.search_spaces.nodes.EstimatorNode (or base_node) objects if there are multiple search spaces.</li> <li>TPOT.search_spaces.pipelines.ChoicePipeline object if return_choice_pipeline is True. Note: for some special cases with methods using wrapped estimators, the returned search space is a TPOT.search_spaces.pipelines.WrapperPipeline object.</li> </ul> Source code in <code>tpot/config/get_configspace.py</code> <pre><code>def get_search_space(name, n_classes=3, n_samples=1000, n_features=100, random_state=None, return_choice_pipeline=True, base_node=EstimatorNode, n_jobs=1):\n    \"\"\"\n    Returns a TPOT search space for a given scikit-learn method or group of methods.\n\n    Parameters\n    ----------\n    name : str or list\n        The name of the scikit-learn method or group of methods for which to create the search space.\n        - str: The name of the scikit-learn method. (e.g. 'RandomForestClassifier' for sklearn.ensemble.RandomForestClassifier)\n        Alternatively, the name of a group of methods. (e.g. 'classifiers' for all classifiers).\n        - list: A list of scikit-learn method names. (e.g. ['RandomForestClassifier', 'ExtraTreesClassifier'])\n    n_classes : int (default=3)\n        The number of classes in the target variable.\n    n_samples : int (default=1000)\n        The number of samples in the dataset.\n    n_features : int (default=100)\n        The number of features in the dataset.\n    random_state : int (default=None)\n        A fixed random_state to pass through to all methods that have a random_state hyperparameter. \n    return_choice_pipeline : bool (default=True)\n        If False, returns a list of TPOT.search_spaces.nodes.EstimatorNode objects.\n        If True, returns a single TPOT.search_spaces.pipelines.ChoicePipeline that includes and samples from all EstimatorNodes.\n    base_node: TPOT.search_spaces.base.SearchSpace (default=TPOT.search_spaces.nodes.EstimatorNode)\n        The SearchSpace to pass the configuration space to. If you want to experiment with custom mutation/crossover operators, you can pass a custom SearchSpace node here.\n    n_jobs : int (default=1)\n        Sets the n_jobs parameter for estimators that have it. Default is 1.\n\n    Returns\n    -------\n        Returns an SearchSpace object that can be optimized by TPOT.\n        - TPOT.search_spaces.nodes.EstimatorNode (or base_node) if there is only one search space.\n        - List of TPOT.search_spaces.nodes.EstimatorNode (or base_node) objects if there are multiple search spaces.\n        - TPOT.search_spaces.pipelines.ChoicePipeline object if return_choice_pipeline is True.\n        Note: for some special cases with methods using wrapped estimators, the returned search space is a TPOT.search_spaces.pipelines.WrapperPipeline object.\n\n    \"\"\"\n    name = flatten_group_names(name)\n\n    #if list of names, return a list of EstimatorNodes\n    if isinstance(name, list) or isinstance(name, np.ndarray):\n        search_spaces = [get_search_space(n, n_classes=n_classes, n_samples=n_samples, n_features=n_features, random_state=random_state, return_choice_pipeline=False, base_node=base_node, n_jobs=n_jobs) for n in name]\n        #remove Nones\n        search_spaces = [s for s in search_spaces if s is not None]\n\n        if return_choice_pipeline:\n            return ChoicePipeline(search_spaces=np.hstack(search_spaces))\n        else:\n            return np.hstack(search_spaces)\n\n    # if name in GROUPNAMES:\n    #     name_list = GROUPNAMES[name]\n    #     return get_search_space(name_list, n_classes=n_classes, n_samples=n_samples, n_features=n_features, random_state=random_state, return_choice_pipeline=return_choice_pipeline, base_node=base_node)\n\n    return get_node(name, n_classes=n_classes, n_samples=n_samples, n_features=n_features, random_state=random_state, base_node=base_node, n_jobs=n_jobs)\n</code></pre>"},{"location":"documentation/tpot/config/imputers/","title":"Imputers","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/config/mdr_configs/","title":"Mdr configs","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/config/regressors/","title":"Regressors","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/config/regressors_sklearnex/","title":"Regressors sklearnex","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/config/selectors/","title":"Selectors","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/config/special_configs/","title":"Special configs","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/config/template_search_spaces/","title":"Template search spaces","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.DynamicUnionPipeline","title":"<code>DynamicUnionPipeline</code>","text":"<p>               Bases: <code>SearchSpace</code></p> Source code in <code>tpot/search_spaces/pipelines/dynamicunion.py</code> <pre><code>class DynamicUnionPipeline(SearchSpace):\n    def __init__(self, search_space : SearchSpace, max_estimators=None, allow_repeats=False ) -&gt; None:\n        \"\"\"\n        Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.\n        \"\"\"\n\n        self.search_space = search_space\n        self.max_estimators = max_estimators\n        self.allow_repeats = allow_repeats\n\n    def generate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        return DynamicUnionPipelineIndividual(self.search_space, max_estimators=self.max_estimators, allow_repeats=self.allow_repeats, rng=rng)\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.DynamicUnionPipeline.__init__","title":"<code>__init__(search_space, max_estimators=None, allow_repeats=False)</code>","text":"<p>Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.</p> Source code in <code>tpot/search_spaces/pipelines/dynamicunion.py</code> <pre><code>def __init__(self, search_space : SearchSpace, max_estimators=None, allow_repeats=False ) -&gt; None:\n    \"\"\"\n    Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.\n    \"\"\"\n\n    self.search_space = search_space\n    self.max_estimators = max_estimators\n    self.allow_repeats = allow_repeats\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.DynamicUnionPipelineIndividual","title":"<code>DynamicUnionPipelineIndividual</code>","text":"<p>               Bases: <code>SklearnIndividual</code></p> <p>Takes in one search space. Will produce a FeatureUnion of up to max_estimators number of steps. The output of the FeatureUnion will the all of the steps concatenated together.</p> Source code in <code>tpot/search_spaces/pipelines/dynamicunion.py</code> <pre><code>class DynamicUnionPipelineIndividual(SklearnIndividual):\n    \"\"\"\n    Takes in one search space.\n    Will produce a FeatureUnion of up to max_estimators number of steps.\n    The output of the FeatureUnion will the all of the steps concatenated together.\n\n    \"\"\"\n\n    def __init__(self, search_space : SearchSpace, max_estimators=None, allow_repeats=False, rng=None) -&gt; None:\n        super().__init__()\n        self.search_space = search_space\n\n        if max_estimators is None:\n            self.max_estimators = np.inf\n        else:\n            self.max_estimators = max_estimators\n\n        self.allow_repeats = allow_repeats\n\n        self.union_dict = {}\n\n        if self.max_estimators == np.inf:\n            init_max = 3\n        else:\n            init_max = self.max_estimators\n\n        rng = np.random.default_rng(rng)\n\n        for _ in range(rng.integers(1, init_max)):\n            self._mutate_add_step(rng)\n\n\n    def mutate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        mutation_funcs = [self._mutate_add_step, self._mutate_remove_step, self._mutate_replace_step, self._mutate_note]\n        rng.shuffle(mutation_funcs)\n        for mutation_func in mutation_funcs:\n            if mutation_func(rng):\n                return True\n\n    def _mutate_add_step(self, rng):\n        rng = np.random.default_rng(rng)\n        max_attempts = 10\n        if len(self.union_dict) &lt; self.max_estimators:\n            for _ in range(max_attempts):\n                new_step = self.search_space.generate(rng)\n                if new_step.unique_id() not in self.union_dict:\n                    self.union_dict[new_step.unique_id()] = new_step\n                    return True\n        return False\n\n    def _mutate_remove_step(self, rng):\n        rng = np.random.default_rng(rng)\n        if len(self.union_dict) &gt; 1:\n            self.union_dict.pop( rng.choice(list(self.union_dict.keys())))  \n            return True\n        return False\n\n    def _mutate_replace_step(self, rng):\n        rng = np.random.default_rng(rng)        \n        changed = self._mutate_remove_step(rng) or self._mutate_add_step(rng)\n        return changed\n\n    #TODO mutate one step or multiple?\n    def _mutate_note(self, rng):\n        rng = np.random.default_rng(rng)\n        changed = False\n        values = list(self.union_dict.values())\n        for step in values:\n            if rng.random() &lt; 0.5:\n                changed = step.mutate(rng) or changed\n\n        self.union_dict = {step.unique_id(): step for step in values}\n\n        return changed\n\n\n    def crossover(self, other, rng=None):\n        rng = np.random.default_rng(rng)\n\n        cx_funcs = [self._crossover_swap_multiple_nodes, self._crossover_node]\n        rng.shuffle(cx_funcs)\n        for cx_func in cx_funcs:\n            if cx_func(other, rng):\n                return True\n\n        return False\n\n\n    def _crossover_swap_multiple_nodes(self, other, rng):\n        rng = np.random.default_rng(rng)\n        self_values = list(self.union_dict.values())\n        other_values = list(other.union_dict.values())\n\n        rng.shuffle(self_values)\n        rng.shuffle(other_values)\n\n        self_idx = rng.integers(0,len(self_values))\n        other_idx = rng.integers(0,len(other_values))\n\n        #Note that this is not one-point-crossover since the sequence doesn't matter. this is just a quick way to swap multiple random items\n        self_values[:self_idx], other_values[:other_idx] = other_values[:other_idx], self_values[:self_idx]\n\n        self.union_dict = {step.unique_id(): step for step in self_values}\n        other.union_dict = {step.unique_id(): step for step in other_values}\n\n        return True\n\n\n    def _crossover_node(self, other, rng):\n        rng = np.random.default_rng(rng)\n\n        changed = False\n        self_values = list(self.union_dict.values())\n        other_values = list(other.union_dict.values())\n\n        rng.shuffle(self_values)\n        rng.shuffle(other_values)\n\n        for self_step, other_step in zip(self_values, other_values):\n            if rng.random() &lt; 0.5:\n                changed = self_step.crossover(other_step, rng) or changed\n\n        self.union_dict = {step.unique_id(): step for step in self_values}\n        other.union_dict = {step.unique_id(): step for step in other_values}\n\n        return changed\n\n    def export_pipeline(self, **kwargs):\n        values = list(self.union_dict.values())\n        return sklearn.pipeline.make_union(*[step.export_pipeline(**kwargs) for step in values])\n\n    def unique_id(self):\n        values = list(self.union_dict.values())\n        l = [step.unique_id() for step in values]\n        # if all items are strings, then sort them\n        if all([isinstance(x, str) for x in l]):\n            l.sort()\n        l = [\"FeatureUnion\"] + l\n        return TupleIndex(frozenset(l))\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.EstimatorNodeIndividual","title":"<code>EstimatorNodeIndividual</code>","text":"<p>               Bases: <code>SklearnIndividual</code></p> <p>Note that ConfigurationSpace does not support None as a parameter. Instead, use the special string \"\". TPOT will automatically replace instances of this string with the Python None.  <p>Parameters:</p> Name Type Description Default <code>method</code> <code>type</code> <p>The class of the estimator to be used</p> required <code>space</code> <code>ConfigurationSpace | dict</code> <p>The hyperparameter space to be used. If a dict is passed, hyperparameters are fixed and not learned.</p> required Source code in <code>tpot/search_spaces/nodes/estimator_node.py</code> <pre><code>class EstimatorNodeIndividual(SklearnIndividual):\n    \"\"\"\n    Note that ConfigurationSpace does not support None as a parameter. Instead, use the special string \"&lt;NONE&gt;\". TPOT will automatically replace instances of this string with the Python None. \n\n    Parameters\n    ----------\n    method : type\n        The class of the estimator to be used\n\n    space : ConfigurationSpace|dict\n        The hyperparameter space to be used. If a dict is passed, hyperparameters are fixed and not learned.\n\n    \"\"\"\n    def __init__(self, method: type, \n                        space: ConfigurationSpace|dict, #TODO If a dict is passed, hyperparameters are fixed and not learned. Is this confusing? Should we make a second node type?\n                        hyperparameter_parser: callable = None,\n                        rng=None) -&gt; None:\n        super().__init__()\n        self.method = method\n        self.space = space\n\n        if hyperparameter_parser is None:\n            self.hyperparameter_parser = default_hyperparameter_parser\n        else:\n            self.hyperparameter_parser = hyperparameter_parser\n\n        if isinstance(space, dict):\n            self.hyperparameters = space\n        else:\n            rng = np.random.default_rng(rng)\n            self.space.seed(rng.integers(0, 2**32))\n            self.hyperparameters = dict(self.space.sample_configuration())\n\n    def mutate(self, rng=None):\n        if isinstance(self.space, dict): \n            return False\n\n        rng = np.random.default_rng(rng)\n        self.space.seed(rng.integers(0, 2**32))\n        self.hyperparameters = dict(self.space.sample_configuration())\n        return True\n\n    def crossover(self, other, rng=None):\n        if isinstance(self.space, dict):\n            return False\n\n        rng = np.random.default_rng(rng)\n        if self.method != other.method:\n            return False\n\n        #loop through hyperparameters, randomly swap items in self.hyperparameters with items in other.hyperparameters\n        for hyperparameter in self.space:\n            if rng.choice([True, False]):\n                if hyperparameter in other.hyperparameters:\n                    self.hyperparameters[hyperparameter] = other.hyperparameters[hyperparameter]\n\n        return True\n\n\n\n    @final #this method should not be overridden, instead override hyperparameter_parser\n    def export_pipeline(self, **kwargs):\n        return self.method(**self.hyperparameter_parser(self.hyperparameters))\n\n    def unique_id(self):\n        #return a dictionary of the method and the hyperparameters\n        method_str = self.method.__name__\n        params = list(self.hyperparameters.keys())\n        params = sorted(params)\n\n        id_str = f\"{method_str}({', '.join([f'{param}={self.hyperparameters[param]}' for param in params])})\"\n\n        return id_str\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.FSSIndividual","title":"<code>FSSIndividual</code>","text":"<p>               Bases: <code>SklearnIndividual</code></p> Source code in <code>tpot/search_spaces/nodes/fss_node.py</code> <pre><code>class FSSIndividual(SklearnIndividual):\n    def __init__(   self,\n                    subsets,\n                    rng=None,\n                ):\n\n        \"\"\"\n        An individual for representing a specific FeatureSetSelector. \n        The FeatureSetSelector selects a feature list of list of predefined feature subsets.\n\n        This instance will select one set initially. Mutation and crossover can swap the selected subset with another.\n\n        Parameters\n        ----------\n        subsets : str or list, default=None\n            Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries. \n            Features are defined by column names if using a Pandas data frame, or ints corresponding to indexes if using numpy arrays.\n            - str : If a string, it is assumed to be a path to a csv file with the subsets. \n                The first column is assumed to be the name of the subset and the remaining columns are the features in the subset.\n            - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets (i.e a list of lists).\n            - dict : A dictionary where keys are the names of the subsets and the values are the list of features.\n            - int : If an int, it is assumed to be the number of subsets to generate. Each subset will contain one feature.\n            - None : If None, each column will be treated as a subset. One column will be selected per subset.\n        rng : int, np.random.Generator, optional\n            The random number generator. The default is None.\n            Only used to select the first subset.\n\n        Returns\n        -------\n        None    \n        \"\"\"\n\n        subsets = subsets\n        rng = np.random.default_rng(rng)\n\n        if isinstance(subsets, str):\n            df = pd.read_csv(subsets,header=None,index_col=0)\n            df['features'] = df.apply(lambda x: list([x[c] for c in df.columns]),axis=1)\n            self.subset_dict = {}\n            for row in df.index:\n                self.subset_dict[row] = df.loc[row]['features']\n        elif isinstance(subsets, dict):\n            self.subset_dict = subsets\n        elif isinstance(subsets, list) or isinstance(subsets, np.ndarray):\n            self.subset_dict = {str(i):subsets[i] for i in range(len(subsets))}\n        elif isinstance(subsets, int):\n            self.subset_dict = {\"{0}\".format(i):i for i in range(subsets)}\n        else:\n            raise ValueError(\"Subsets must be a string, dictionary, list, int, or numpy array\")\n\n        self.names_list = list(self.subset_dict.keys())\n\n\n        self.selected_subset_name = rng.choice(self.names_list)\n        self.sel_subset = self.subset_dict[self.selected_subset_name]\n\n\n    def mutate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        #get list of names not including the current one\n        names = [name for name in self.names_list if name != self.selected_subset_name]\n        self.selected_subset_name = rng.choice(names)\n        self.sel_subset = self.subset_dict[self.selected_subset_name]\n\n\n    def crossover(self, other, rng=None):\n        self.selected_subset_name = other.selected_subset_name\n        self.sel_subset = other.sel_subset\n\n    def export_pipeline(self, **kwargs):\n        return FeatureSetSelector(sel_subset=self.sel_subset, name=self.selected_subset_name)\n\n\n    def unique_id(self):\n        id_str = \"FeatureSetSelector({0})\".format(self.selected_subset_name)\n        return id_str\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.FSSIndividual.__init__","title":"<code>__init__(subsets, rng=None)</code>","text":"<p>An individual for representing a specific FeatureSetSelector.  The FeatureSetSelector selects a feature list of list of predefined feature subsets.</p> <p>This instance will select one set initially. Mutation and crossover can swap the selected subset with another.</p> <p>Parameters:</p> Name Type Description Default <code>subsets</code> <code>str or list</code> <p>Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries.  Features are defined by column names if using a Pandas data frame, or ints corresponding to indexes if using numpy arrays. - str : If a string, it is assumed to be a path to a csv file with the subsets.      The first column is assumed to be the name of the subset and the remaining columns are the features in the subset. - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets (i.e a list of lists). - dict : A dictionary where keys are the names of the subsets and the values are the list of features. - int : If an int, it is assumed to be the number of subsets to generate. Each subset will contain one feature. - None : If None, each column will be treated as a subset. One column will be selected per subset.</p> <code>None</code> <code>rng</code> <code>(int, Generator)</code> <p>The random number generator. The default is None. Only used to select the first subset.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>tpot/search_spaces/nodes/fss_node.py</code> <pre><code>def __init__(   self,\n                subsets,\n                rng=None,\n            ):\n\n    \"\"\"\n    An individual for representing a specific FeatureSetSelector. \n    The FeatureSetSelector selects a feature list of list of predefined feature subsets.\n\n    This instance will select one set initially. Mutation and crossover can swap the selected subset with another.\n\n    Parameters\n    ----------\n    subsets : str or list, default=None\n        Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries. \n        Features are defined by column names if using a Pandas data frame, or ints corresponding to indexes if using numpy arrays.\n        - str : If a string, it is assumed to be a path to a csv file with the subsets. \n            The first column is assumed to be the name of the subset and the remaining columns are the features in the subset.\n        - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets (i.e a list of lists).\n        - dict : A dictionary where keys are the names of the subsets and the values are the list of features.\n        - int : If an int, it is assumed to be the number of subsets to generate. Each subset will contain one feature.\n        - None : If None, each column will be treated as a subset. One column will be selected per subset.\n    rng : int, np.random.Generator, optional\n        The random number generator. The default is None.\n        Only used to select the first subset.\n\n    Returns\n    -------\n    None    \n    \"\"\"\n\n    subsets = subsets\n    rng = np.random.default_rng(rng)\n\n    if isinstance(subsets, str):\n        df = pd.read_csv(subsets,header=None,index_col=0)\n        df['features'] = df.apply(lambda x: list([x[c] for c in df.columns]),axis=1)\n        self.subset_dict = {}\n        for row in df.index:\n            self.subset_dict[row] = df.loc[row]['features']\n    elif isinstance(subsets, dict):\n        self.subset_dict = subsets\n    elif isinstance(subsets, list) or isinstance(subsets, np.ndarray):\n        self.subset_dict = {str(i):subsets[i] for i in range(len(subsets))}\n    elif isinstance(subsets, int):\n        self.subset_dict = {\"{0}\".format(i):i for i in range(subsets)}\n    else:\n        raise ValueError(\"Subsets must be a string, dictionary, list, int, or numpy array\")\n\n    self.names_list = list(self.subset_dict.keys())\n\n\n    self.selected_subset_name = rng.choice(self.names_list)\n    self.sel_subset = self.subset_dict[self.selected_subset_name]\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.FSSNode","title":"<code>FSSNode</code>","text":"<p>               Bases: <code>SearchSpace</code></p> Source code in <code>tpot/search_spaces/nodes/fss_node.py</code> <pre><code>class FSSNode(SearchSpace):\n    def __init__(self,                     \n                    subsets,\n                ):\n        \"\"\"\n        A search space for a FeatureSetSelector. \n        The FeatureSetSelector selects a feature list of list of predefined feature subsets.\n\n        Parameters\n        ----------\n        subsets : str or list, default=None\n            Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries. \n            Features are defined by column names if using a Pandas data frame, or ints corresponding to indexes if using numpy arrays.\n            - str : If a string, it is assumed to be a path to a csv file with the subsets. \n                The first column is assumed to be the name of the subset and the remaining columns are the features in the subset.\n            - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets (i.e a list of lists).\n            - dict : A dictionary where keys are the names of the subsets and the values are the list of features.\n            - int : If an int, it is assumed to be the number of subsets to generate. Each subset will contain one feature.\n            - None : If None, each column will be treated as a subset. One column will be selected per subset.\n\n        Returns\n        -------\n        None    \n\n        \"\"\"\n\n        self.subsets = subsets\n\n    def generate(self, rng=None) -&gt; SklearnIndividual:\n        return FSSIndividual(   \n            subsets=self.subsets,\n            rng=rng,\n            )\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.FSSNode.__init__","title":"<code>__init__(subsets)</code>","text":"<p>A search space for a FeatureSetSelector.  The FeatureSetSelector selects a feature list of list of predefined feature subsets.</p> <p>Parameters:</p> Name Type Description Default <code>subsets</code> <code>str or list</code> <p>Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries.  Features are defined by column names if using a Pandas data frame, or ints corresponding to indexes if using numpy arrays. - str : If a string, it is assumed to be a path to a csv file with the subsets.      The first column is assumed to be the name of the subset and the remaining columns are the features in the subset. - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets (i.e a list of lists). - dict : A dictionary where keys are the names of the subsets and the values are the list of features. - int : If an int, it is assumed to be the number of subsets to generate. Each subset will contain one feature. - None : If None, each column will be treated as a subset. One column will be selected per subset.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>tpot/search_spaces/nodes/fss_node.py</code> <pre><code>def __init__(self,                     \n                subsets,\n            ):\n    \"\"\"\n    A search space for a FeatureSetSelector. \n    The FeatureSetSelector selects a feature list of list of predefined feature subsets.\n\n    Parameters\n    ----------\n    subsets : str or list, default=None\n        Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries. \n        Features are defined by column names if using a Pandas data frame, or ints corresponding to indexes if using numpy arrays.\n        - str : If a string, it is assumed to be a path to a csv file with the subsets. \n            The first column is assumed to be the name of the subset and the remaining columns are the features in the subset.\n        - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets (i.e a list of lists).\n        - dict : A dictionary where keys are the names of the subsets and the values are the list of features.\n        - int : If an int, it is assumed to be the number of subsets to generate. Each subset will contain one feature.\n        - None : If None, each column will be treated as a subset. One column will be selected per subset.\n\n    Returns\n    -------\n    None    \n\n    \"\"\"\n\n    self.subsets = subsets\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.FeatureSetSelector","title":"<code>FeatureSetSelector</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>SelectorMixin</code></p> <p>Select predefined feature subsets.</p> Source code in <code>tpot/builtin_modules/feature_set_selector.py</code> <pre><code>class FeatureSetSelector(BaseEstimator, SelectorMixin):\n    \"\"\"\n    Select predefined feature subsets.\n\n\n    \"\"\"\n\n    def __init__(self, sel_subset=None, name=None):\n        \"\"\"Create a FeatureSetSelector object.\n\n        Parameters\n        ----------\n        sel_subset: list or int\n            If X is a dataframe, items in sel_subset list must correspond to column names\n            If X is a numpy array, items in sel_subset list must correspond to column indexes\n            int: index of a single column\n        Returns\n        -------\n        None\n\n        \"\"\"\n        self.name = name\n        self.sel_subset = sel_subset\n\n\n    def fit(self, X, y=None):\n        \"\"\"Fit FeatureSetSelector for feature selection\n\n        Parameters\n        ----------\n        X: array-like of shape (n_samples, n_features)\n            The training input samples.\n        y: array-like, shape (n_samples,)\n            The target values (integers that correspond to classes in classification, real numbers in regression).\n\n        Returns\n        -------\n        self: object\n            Returns a copy of the estimator\n        \"\"\"\n        if isinstance(self.sel_subset, int) or isinstance(self.sel_subset, str):\n            self.sel_subset = [self.sel_subset]\n\n        #generate  self.feat_list_idx\n        if isinstance(X, pd.DataFrame):\n            self.feature_names_in_ = X.columns.tolist()\n            self.feat_list_idx = sorted([self.feature_names_in_.index(feat) for feat in self.sel_subset])\n\n\n        elif isinstance(X, np.ndarray):\n            self.feature_names_in_ = None#list(range(X.shape[1]))\n\n            self.feat_list_idx = sorted(self.sel_subset)\n\n        n_features = X.shape[1]\n        self.mask = np.zeros(n_features, dtype=bool)\n        self.mask[np.asarray(self.feat_list_idx)] = True\n\n        return self\n\n    #TODO keep returned as dataframe if input is dataframe? may not be consistent with sklearn\n\n    # def transform(self, X):\n\n    def _get_tags(self):\n        tags = {\"allow_nan\": True, \"requires_y\": False}\n        return tags\n\n    def _get_support_mask(self):\n        \"\"\"\n        Get the boolean mask indicating which features are selected\n        Returns\n        -------\n        support : boolean array of shape [# input features]\n            An element is True iff its corresponding feature is selected for\n            retention.\n        \"\"\"\n        return self.mask\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.FeatureSetSelector.__init__","title":"<code>__init__(sel_subset=None, name=None)</code>","text":"<p>Create a FeatureSetSelector object.</p> <p>Parameters:</p> Name Type Description Default <code>sel_subset</code> <p>If X is a dataframe, items in sel_subset list must correspond to column names If X is a numpy array, items in sel_subset list must correspond to column indexes int: index of a single column</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>tpot/builtin_modules/feature_set_selector.py</code> <pre><code>def __init__(self, sel_subset=None, name=None):\n    \"\"\"Create a FeatureSetSelector object.\n\n    Parameters\n    ----------\n    sel_subset: list or int\n        If X is a dataframe, items in sel_subset list must correspond to column names\n        If X is a numpy array, items in sel_subset list must correspond to column indexes\n        int: index of a single column\n    Returns\n    -------\n    None\n\n    \"\"\"\n    self.name = name\n    self.sel_subset = sel_subset\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.FeatureSetSelector.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit FeatureSetSelector for feature selection</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>The training input samples.</p> required <code>y</code> <p>The target values (integers that correspond to classes in classification, real numbers in regression).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Returns a copy of the estimator</p> Source code in <code>tpot/builtin_modules/feature_set_selector.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Fit FeatureSetSelector for feature selection\n\n    Parameters\n    ----------\n    X: array-like of shape (n_samples, n_features)\n        The training input samples.\n    y: array-like, shape (n_samples,)\n        The target values (integers that correspond to classes in classification, real numbers in regression).\n\n    Returns\n    -------\n    self: object\n        Returns a copy of the estimator\n    \"\"\"\n    if isinstance(self.sel_subset, int) or isinstance(self.sel_subset, str):\n        self.sel_subset = [self.sel_subset]\n\n    #generate  self.feat_list_idx\n    if isinstance(X, pd.DataFrame):\n        self.feature_names_in_ = X.columns.tolist()\n        self.feat_list_idx = sorted([self.feature_names_in_.index(feat) for feat in self.sel_subset])\n\n\n    elif isinstance(X, np.ndarray):\n        self.feature_names_in_ = None#list(range(X.shape[1]))\n\n        self.feat_list_idx = sorted(self.sel_subset)\n\n    n_features = X.shape[1]\n    self.mask = np.zeros(n_features, dtype=bool)\n    self.mask[np.asarray(self.feat_list_idx)] = True\n\n    return self\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.GeneticFeatureSelectorNode","title":"<code>GeneticFeatureSelectorNode</code>","text":"<p>               Bases: <code>SearchSpace</code></p> Source code in <code>tpot/search_spaces/nodes/genetic_feature_selection.py</code> <pre><code>class GeneticFeatureSelectorNode(SearchSpace):\n    def __init__(self,                     \n                    n_features,\n                    start_p=0.2,\n                    mutation_rate = 0.1,\n                    crossover_rate = 0.1,\n                    mutation_rate_rate = 0, # These are still experimental but seem to help. Theory is that it takes slower steps as it gets closer to the optimal solution.\n                    crossover_rate_rate = 0,# Otherwise is mutation_rate is too small, it takes forever, and if its too large, it never converges.\n                    ):\n        \"\"\"\n        A node that generates a GeneticFeatureSelectorIndividual. Uses genetic algorithm to select novel subsets of features.\n\n        Parameters\n        ----------\n        n_features : int\n            Number of features in the dataset.\n        start_p : float\n            Probability of selecting a given feature for the initial subset of features.\n        mutation_rate : float\n            Probability of adding/removing a feature from the subset of features.\n        crossover_rate : float\n            Probability of swapping a feature between two subsets of features.\n        mutation_rate_rate : float\n            Probability of changing the mutation rate. (experimental)\n        crossover_rate_rate : float\n            Probability of changing the crossover rate. (experimental)\n\n        \"\"\"\n\n        self.n_features = n_features\n        self.start_p = start_p\n        self.mutation_rate = mutation_rate\n        self.crossover_rate = crossover_rate\n        self.mutation_rate_rate = mutation_rate_rate\n        self.crossover_rate_rate = crossover_rate_rate\n\n\n    def generate(self, rng=None) -&gt; SklearnIndividual:\n        return GeneticFeatureSelectorIndividual(   mask=self.n_features,\n                                                    start_p=self.start_p,\n                                                    mutation_rate=self.mutation_rate,\n                                                    crossover_rate=self.crossover_rate,\n                                                    mutation_rate_rate=self.mutation_rate_rate,\n                                                    crossover_rate_rate=self.crossover_rate_rate,\n                                                    rng=rng\n                                                )\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.GeneticFeatureSelectorNode.__init__","title":"<code>__init__(n_features, start_p=0.2, mutation_rate=0.1, crossover_rate=0.1, mutation_rate_rate=0, crossover_rate_rate=0)</code>","text":"<p>A node that generates a GeneticFeatureSelectorIndividual. Uses genetic algorithm to select novel subsets of features.</p> <p>Parameters:</p> Name Type Description Default <code>n_features</code> <code>int</code> <p>Number of features in the dataset.</p> required <code>start_p</code> <code>float</code> <p>Probability of selecting a given feature for the initial subset of features.</p> <code>0.2</code> <code>mutation_rate</code> <code>float</code> <p>Probability of adding/removing a feature from the subset of features.</p> <code>0.1</code> <code>crossover_rate</code> <code>float</code> <p>Probability of swapping a feature between two subsets of features.</p> <code>0.1</code> <code>mutation_rate_rate</code> <code>float</code> <p>Probability of changing the mutation rate. (experimental)</p> <code>0</code> <code>crossover_rate_rate</code> <code>float</code> <p>Probability of changing the crossover rate. (experimental)</p> <code>0</code> Source code in <code>tpot/search_spaces/nodes/genetic_feature_selection.py</code> <pre><code>def __init__(self,                     \n                n_features,\n                start_p=0.2,\n                mutation_rate = 0.1,\n                crossover_rate = 0.1,\n                mutation_rate_rate = 0, # These are still experimental but seem to help. Theory is that it takes slower steps as it gets closer to the optimal solution.\n                crossover_rate_rate = 0,# Otherwise is mutation_rate is too small, it takes forever, and if its too large, it never converges.\n                ):\n    \"\"\"\n    A node that generates a GeneticFeatureSelectorIndividual. Uses genetic algorithm to select novel subsets of features.\n\n    Parameters\n    ----------\n    n_features : int\n        Number of features in the dataset.\n    start_p : float\n        Probability of selecting a given feature for the initial subset of features.\n    mutation_rate : float\n        Probability of adding/removing a feature from the subset of features.\n    crossover_rate : float\n        Probability of swapping a feature between two subsets of features.\n    mutation_rate_rate : float\n        Probability of changing the mutation rate. (experimental)\n    crossover_rate_rate : float\n        Probability of changing the crossover rate. (experimental)\n\n    \"\"\"\n\n    self.n_features = n_features\n    self.start_p = start_p\n    self.mutation_rate = mutation_rate\n    self.crossover_rate = crossover_rate\n    self.mutation_rate_rate = mutation_rate_rate\n    self.crossover_rate_rate = crossover_rate_rate\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.GraphKey","title":"<code>GraphKey</code>","text":"<p>A class that can be used as a key for a graph.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>Graph</code> <p>The graph to use as a key. Node Attributes are used for the hash.</p> required <code>matched_label</code> <code>str</code> <p>The node attribute to consider for the hash.</p> <code>'label'</code> Source code in <code>tpot/search_spaces/pipelines/graph.py</code> <pre><code>class GraphKey():\n    '''\n    A class that can be used as a key for a graph.\n\n    Parameters\n    ----------\n    graph : (nx.Graph)\n        The graph to use as a key. Node Attributes are used for the hash.\n    matched_label : (str)\n        The node attribute to consider for the hash.\n    '''\n\n    def __init__(self, graph, matched_label='label') -&gt; None:#['hyperparameters', 'method_class']) -&gt; None:\n\n\n        self.graph = graph\n        self.matched_label = matched_label\n        self.node_match = partial(node_match, matched_labels=[matched_label])\n        self.key = int(nx.weisfeiler_lehman_graph_hash(self.graph, node_attr=self.matched_label),16) #hash(tuple(sorted([val for (node, val) in self.graph.degree()])))\n\n\n    #If hash is different, node is definitely different\n    # https://arxiv.org/pdf/2002.06653.pdf\n    def __hash__(self) -&gt; int:\n\n        return self.key\n\n    #If hash is same, use __eq__ to know if they are actually different\n    def __eq__(self, other):\n        return nx.is_isomorphic(self.graph, other.graph, node_match=self.node_match)\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.GraphPipelineIndividual","title":"<code>GraphPipelineIndividual</code>","text":"<p>               Bases: <code>SklearnIndividual</code></p> <p>Defines a search space of pipelines in the shape of a Directed Acyclic Graphs. The search spaces for root, leaf, and inner nodes can be defined separately if desired. Each graph will have a single root serving as the final estimator which is drawn from the <code>root_search_space</code>. If the <code>leaf_search_space</code> is defined, all leaves  in the pipeline will be drawn from that search space. If the <code>leaf_search_space</code> is not defined, all leaves will be drawn from the <code>inner_search_space</code>. Nodes that are not leaves or roots will be drawn from the <code>inner_search_space</code>. If the <code>inner_search_space</code> is not defined, there will be no inner nodes.</p> <p><code>cross_val_predict_cv</code>, <code>method</code>, <code>memory</code>, and <code>use_label_encoder</code> are passed to the GraphPipeline object when the pipeline is exported and not directly used in the search space.</p> <p>Exports to a GraphPipeline object.</p> <p>Parameters:</p> Name Type Description Default <code>root_search_space</code> <code>SearchSpace</code> <p>The search space for the root node of the graph. This node will be the final estimator in the pipeline.</p> required <code>inner_search_space</code> <code>SearchSpace</code> <p>The search space for the inner nodes of the graph. If not defined, there will be no inner nodes.</p> <code>None</code> <code>leaf_search_space</code> <code>SearchSpace</code> <p>The search space for the leaf nodes of the graph. If not defined, the leaf nodes will be drawn from the inner_search_space.</p> <code>None</code> <code>crossover_same_depth</code> <code>bool</code> <p>If True, crossover will only occur between nodes at the same depth in the graph. If False, crossover will occur between nodes at any depth.</p> <code>False</code> <code>cross_val_predict_cv</code> <code>Union[int, Callable]</code> <p>Determines the cross-validation splitting strategy used in inner classifiers or regressors</p> <code>0</code> <code>method</code> <code>str</code> <p>The prediction method to use for the inner classifiers or regressors. If 'auto', it will try to use predict_proba, decision_function, or predict in that order.</p> <code>'auto'</code> <code>memory</code> <p>Used to cache the input and outputs of nodes to prevent refitting or computationally heavy transformations. By default, no caching is performed. If a string is given, it is the path to the caching directory.</p> required <code>use_label_encoder</code> <code>bool</code> <p>If True, the label encoder is used to encode the labels to be 0 to N. If False, the label encoder is not used. Mainly useful for classifiers (XGBoost) that require labels to be ints from 0 to N. Can also be a sklearn.preprocessing.LabelEncoder object. If so, that label encoder is used.</p> <code>False</code> <code>rng</code> <p>Seed for sampling the first graph instance.</p> <code>None</code> Source code in <code>tpot/search_spaces/pipelines/graph.py</code> <pre><code>class GraphPipelineIndividual(SklearnIndividual):\n    \"\"\"\n        Defines a search space of pipelines in the shape of a Directed Acyclic Graphs. The search spaces for root, leaf, and inner nodes can be defined separately if desired.\n        Each graph will have a single root serving as the final estimator which is drawn from the `root_search_space`. If the `leaf_search_space` is defined, all leaves \n        in the pipeline will be drawn from that search space. If the `leaf_search_space` is not defined, all leaves will be drawn from the `inner_search_space`.\n        Nodes that are not leaves or roots will be drawn from the `inner_search_space`. If the `inner_search_space` is not defined, there will be no inner nodes.\n\n        `cross_val_predict_cv`, `method`, `memory`, and `use_label_encoder` are passed to the GraphPipeline object when the pipeline is exported and not directly used in the search space.\n\n        Exports to a GraphPipeline object.\n\n        Parameters\n        ----------\n\n        root_search_space: SearchSpace\n            The search space for the root node of the graph. This node will be the final estimator in the pipeline.\n\n        inner_search_space: SearchSpace, optional\n            The search space for the inner nodes of the graph. If not defined, there will be no inner nodes.\n\n        leaf_search_space: SearchSpace, optional\n            The search space for the leaf nodes of the graph. If not defined, the leaf nodes will be drawn from the inner_search_space.\n\n        crossover_same_depth: bool, optional\n            If True, crossover will only occur between nodes at the same depth in the graph. If False, crossover will occur between nodes at any depth.\n\n        cross_val_predict_cv: int, cross-validation generator or an iterable, optional\n            Determines the cross-validation splitting strategy used in inner classifiers or regressors\n\n        method: str, optional\n            The prediction method to use for the inner classifiers or regressors. If 'auto', it will try to use predict_proba, decision_function, or predict in that order.\n\n        memory: str or object with the joblib.Memory interface, optional\n            Used to cache the input and outputs of nodes to prevent refitting or computationally heavy transformations. By default, no caching is performed. If a string is given, it is the path to the caching directory.\n\n        use_label_encoder: bool, optional\n            If True, the label encoder is used to encode the labels to be 0 to N. If False, the label encoder is not used.\n            Mainly useful for classifiers (XGBoost) that require labels to be ints from 0 to N.\n            Can also be a sklearn.preprocessing.LabelEncoder object. If so, that label encoder is used.\n\n        rng: int, RandomState instance or None, optional\n            Seed for sampling the first graph instance. \n\n        \"\"\"\n\n    def __init__(\n            self,  \n            root_search_space: SearchSpace, \n            leaf_search_space: SearchSpace = None, \n            inner_search_space: SearchSpace = None, \n            max_size: int = np.inf,\n            crossover_same_depth: bool = False,\n            cross_val_predict_cv: Union[int, Callable] = 0, #signature function(estimator, X, y=none)\n            method: str = 'auto',\n            use_label_encoder: bool = False,\n            rng=None):\n\n        super().__init__()\n\n        self.__debug = False\n\n        rng = np.random.default_rng(rng)\n\n        self.root_search_space = root_search_space\n        self.leaf_search_space = leaf_search_space\n        self.inner_search_space = inner_search_space\n        self.max_size = max_size\n        self.crossover_same_depth = crossover_same_depth\n\n        self.cross_val_predict_cv = cross_val_predict_cv\n        self.method = method\n        self.use_label_encoder = use_label_encoder\n\n        self.root = self.root_search_space.generate(rng)\n        self.graph = nx.DiGraph()\n        self.graph.add_node(self.root)\n\n        if self.leaf_search_space is not None:\n            self.leaf = self.leaf_search_space.generate(rng)\n            self.graph.add_node(self.leaf)\n            self.graph.add_edge(self.root, self.leaf)\n\n        if self.inner_search_space is None and self.leaf_search_space is None:\n            self.mutate_methods_list = [self._mutate_node]\n            self.crossover_methods_list = [self._crossover_swap_branch,]#[self._crossover_swap_branch, self._crossover_swap_node, self._crossover_take_branch]  #TODO self._crossover_nodes, \n\n        else:\n            self.mutate_methods_list = [self._mutate_insert_leaf, self._mutate_insert_inner_node, self._mutate_remove_node, self._mutate_node, self._mutate_insert_bypass_node]\n            self.crossover_methods_list = [self._crossover_swap_branch, self._crossover_nodes, self._crossover_take_branch ]#[self._crossover_swap_branch, self._crossover_swap_node, self._crossover_take_branch]  #TODO self._crossover_nodes, \n\n        self.merge_duplicated_nodes_toggle = True\n\n        self.graphkey = None\n\n\n    def mutate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        rng.shuffle(self.mutate_methods_list)\n        for mutate_method in self.mutate_methods_list:\n            if mutate_method(rng=rng):\n\n                if self.merge_duplicated_nodes_toggle:\n                    self._merge_duplicated_nodes()\n\n                if self.__debug:\n                    print(mutate_method)\n\n                    if self.root not in self.graph.nodes:\n                        print('lost root something went wrong with ', mutate_method)\n\n                    if len(self.graph.predecessors(self.root)) &gt; 0:\n                        print('root has parents ', mutate_method)\n\n                    if any([n in nx.ancestors(self.graph,n) for n in self.graph.nodes]):\n                        print('a node is connecting to itself...')\n\n                    if self.__debug:\n                        try:\n                            nx.find_cycle(self.graph)\n                            print('something went wrong with ', mutate_method)\n                        except:\n                            pass\n\n                self.graphkey = None\n\n        return False\n\n\n\n\n    def _mutate_insert_leaf(self, rng=None):\n        rng = np.random.default_rng(rng)\n        if self.max_size &gt; self.graph.number_of_nodes():\n            sorted_nodes_list = list(self.graph.nodes)\n            rng.shuffle(sorted_nodes_list) #TODO: sort by number of children and/or parents? bias model one way or another\n            for node in sorted_nodes_list:\n                #if leafs are protected, check if node is a leaf\n                #if node is a leaf, skip because we don't want to add node on top of node\n                if (self.leaf_search_space is not None #if leafs are protected\n                    and   len(list(self.graph.successors(node))) == 0 #if node is leaf\n                    and  len(list(self.graph.predecessors(node))) &gt; 0 #except if node is root, in which case we want to add a leaf even if it happens to be a leaf too\n                    ):\n\n                    continue\n\n                #If node *is* the root or is not a leaf, add leaf node. (dont want to add leaf on top of leaf)\n                if self.leaf_search_space is not None:\n                    new_node = self.leaf_search_space.generate(rng)\n                else:\n                    new_node = self.inner_search_space.generate(rng)\n\n                self.graph.add_node(new_node)\n                self.graph.add_edge(node, new_node)\n                return True\n\n        return False\n\n    def _mutate_insert_inner_node(self, rng=None):\n        \"\"\"\n        Finds an edge in the graph and inserts a new node between the two nodes. Removes the edge between the two nodes.\n        \"\"\"\n        rng = np.random.default_rng(rng)\n        if self.max_size &gt; self.graph.number_of_nodes():\n            sorted_nodes_list = list(self.graph.nodes)\n            sorted_nodes_list2 = list(self.graph.nodes)\n            rng.shuffle(sorted_nodes_list) #TODO: sort by number of children and/or parents? bias model one way or another\n            rng.shuffle(sorted_nodes_list2)\n            for node in sorted_nodes_list:\n                #loop through children of node\n                for child_node in list(self.graph.successors(node)):\n\n                    if child_node is not node and child_node not in nx.ancestors(self.graph, node):\n                        if self.leaf_search_space is not None:\n                            #If if we are protecting leafs, dont add connection into a leaf\n                            if len(list(nx.descendants(self.graph,node))) ==0 :\n                                continue\n\n                        new_node = self.inner_search_space.generate(rng)\n\n                        self.graph.add_node(new_node)\n                        self.graph.add_edges_from([(node, new_node), (new_node, child_node)])\n                        self.graph.remove_edge(node, child_node)\n                        return True\n\n        return False\n\n\n    def _mutate_remove_node(self, rng=None):\n        '''\n        Removes a randomly chosen node and connects its parents to its children.\n        If the node is the only leaf for an inner node and 'leaf_search_space' is not none, we do not remove it.\n        '''\n        rng = np.random.default_rng(rng)\n        nodes_list = list(self.graph.nodes)\n        nodes_list.remove(self.root)\n        leaves = get_leaves(self.graph)\n\n        while len(nodes_list) &gt; 0:\n            node = rng.choice(nodes_list)\n            nodes_list.remove(node)\n\n            if self.leaf_search_space is not None and len(list(nx.descendants(self.graph,node))) == 0 : #if the node is a leaf\n                if len(leaves) &lt;= 1:\n                    continue #dont remove the last leaf\n                leaf_parents = self.graph.predecessors(node)\n\n                # if any of the parents of the node has one one child, continue\n                if any([len(list(self.graph.successors(lp))) &lt; 2 for lp in leaf_parents]): #dont remove a leaf if it is the only input into another node.\n                    continue\n\n                remove_and_stitch(self.graph, node)\n                remove_nodes_disconnected_from_node(self.graph, self.root)\n                return True\n\n            else:\n                remove_and_stitch(self.graph, node)\n                remove_nodes_disconnected_from_node(self.graph, self.root)\n                return True\n\n        return False\n\n\n\n    def _mutate_node(self, rng=None):\n        '''\n        Mutates the hyperparameters for a randomly chosen node in the graph.\n        '''\n        rng = np.random.default_rng(rng)\n        sorted_nodes_list = list(self.graph.nodes)\n        rng.shuffle(sorted_nodes_list)\n        completed_one = False\n        for node in sorted_nodes_list:\n            if node.mutate(rng):\n                return True\n        return False\n\n    def _mutate_remove_edge(self, rng=None):\n        '''\n        Deletes an edge as long as deleting that edge does not make the graph disconnected.\n        '''\n        rng = np.random.default_rng(rng)\n        sorted_nodes_list = list(self.graph.nodes)\n        rng.shuffle(sorted_nodes_list)\n        for child_node in sorted_nodes_list:\n            parents = list(self.graph.predecessors(child_node))\n            if len(parents) &gt; 1: # if it has more than one parent, you can remove an edge (if this is the only child of a node, it will become a leaf)\n\n                for parent_node in parents:\n                    # if removing the egde will make the parent_node a leaf node, skip\n                    if self.leaf_search_space is not None and len(list(self.graph.successors(parent_node))) &lt; 2:\n                        continue\n\n                    self.graph.remove_edge(parent_node, child_node)\n                    return True\n        return False   \n\n    def _mutate_add_edge(self, rng=None):\n        '''\n        Randomly add an edge from a node to another node that is not an ancestor of the first node.\n        '''\n        rng = np.random.default_rng(rng)\n        sorted_nodes_list = list(self.graph.nodes)\n        rng.shuffle(sorted_nodes_list)\n        for child_node in sorted_nodes_list:\n            for parent_node in sorted_nodes_list:\n                if self.leaf_search_space is not None:\n                    if len(list(self.graph.successors(parent_node))) == 0:\n                        continue\n\n                # skip if\n                # - parent and child are the same node\n                # - edge already exists\n                # - child is an ancestor of parent\n                if  (child_node is not parent_node) and not self.graph.has_edge(parent_node,child_node) and (child_node not in nx.ancestors(self.graph, parent_node)):\n                    self.graph.add_edge(parent_node,child_node)\n                    return True\n\n        return False\n\n    def _mutate_insert_bypass_node(self, rng=None):\n        \"\"\"\n        Pick two nodes (doesn't necessarily need to be connected). Create a new node. connect one node to the new node and the new node to the other node.\n        Does not remove any edges.\n        \"\"\"\n        rng = np.random.default_rng(rng)\n        if self.max_size &gt; self.graph.number_of_nodes():\n            sorted_nodes_list = list(self.graph.nodes)\n            sorted_nodes_list2 = list(self.graph.nodes)\n            rng.shuffle(sorted_nodes_list) #TODO: sort by number of children and/or parents? bias model one way or another\n            rng.shuffle(sorted_nodes_list2)\n            for node in sorted_nodes_list:\n                for child_node in sorted_nodes_list2:\n                    if child_node is not node and child_node not in nx.ancestors(self.graph, node):\n                        if self.leaf_search_space is not None:\n                            #If if we are protecting leafs, dont add connection into a leaf\n                            if len(list(nx.descendants(self.graph,node))) ==0 :\n                                continue\n\n                        new_node = self.inner_search_space.generate(rng)\n\n                        self.graph.add_node(new_node)\n                        self.graph.add_edges_from([(node, new_node), (new_node, child_node)])\n                        return True\n\n        return False\n\n\n    def crossover(self, ind2, rng=None):\n        '''\n        self is the first individual, ind2 is the second individual\n        If crossover_same_depth, it will select graphindividuals at the same recursive depth.\n        Otherwise, it will select graphindividuals randomly from the entire graph and its subgraphs.\n\n        This does not impact graphs without subgraphs. And it does not impacts nodes that are not graphindividuals. Cros\n        '''\n\n        rng = np.random.default_rng(rng)\n\n        rng.shuffle(self.crossover_methods_list)\n\n        finished = False\n\n        for crossover_method in self.crossover_methods_list:\n            if crossover_method(ind2, rng=rng):\n                self._merge_duplicated_nodes()\n                finished = True\n                break\n\n        if self.__debug:\n            try:\n                nx.find_cycle(self.graph)\n                print('something went wrong with ', crossover_method)\n            except:\n                pass\n\n        if finished:\n            self.graphkey = None\n\n        return finished\n\n\n    def _crossover_swap_branch(self, G2, rng=None):\n        '''\n        swaps a branch from parent1 with a branch from parent2. does not modify parent2\n        '''\n        rng = np.random.default_rng(rng)\n\n        if self.crossover_same_depth:\n            pair_gen = select_nodes_same_depth(self.graph, self.root, G2.graph, G2.root, rng=rng)\n        else:\n            pair_gen = select_nodes_randomly(self.graph, G2.graph, rng=rng)\n\n        for node1, node2 in pair_gen:\n            #TODO: if root is in inner_search_space, then do use it?\n            if node1 is self.root or node2 is G2.root: #dont want to add root as inner node\n                continue\n\n            #check if node1 is a leaf and leafs are protected, don't add an input to the leave\n            if self.leaf_search_space is not None: #if we are protecting leaves,\n                node1_is_leaf = len(list(self.graph.successors(node1))) == 0\n                node2_is_leaf = len(list(G2.graph.successors(node2))) == 0\n                #if not ((node1_is_leaf and node1_is_leaf) or (not node1_is_leaf and not node2_is_leaf)): #if node1 is a leaf\n                #if (node1_is_leaf and (not node2_is_leaf)) or ( (not node1_is_leaf) and node2_is_leaf):\n                if not node1_is_leaf:\n                    #only continue if node1 and node2 are both leaves or both not leaves\n                    continue\n\n            temp_graph_1 = self.graph.copy()\n            temp_graph_1.remove_node(node1)\n            remove_nodes_disconnected_from_node(temp_graph_1, self.root)\n\n            #isolating the branch\n            branch2 = G2.graph.copy()\n            n2_descendants = nx.descendants(branch2,node2)\n            for n in list(branch2.nodes):\n                if n not in n2_descendants and n is not node2: #removes all nodes not in the branch\n                    branch2.remove_node(n)\n\n            branch2 = copy.deepcopy(branch2)\n            branch2_root = get_roots(branch2)[0]\n            temp_graph_1.add_edges_from(branch2.edges)\n            for p in list(self.graph.predecessors(node1)):\n                temp_graph_1.add_edge(p,branch2_root)\n\n            if temp_graph_1.number_of_nodes() &gt; self.max_size:\n                continue\n\n            self.graph = temp_graph_1\n\n            return True\n        return False\n\n\n    def _crossover_take_branch(self, G2, rng=None):\n        '''\n        Takes a subgraph from Parent2 and add it to a randomly chosen node in Parent1.\n        '''\n        rng = np.random.default_rng(rng)\n\n        if self.crossover_same_depth:\n            pair_gen = select_nodes_same_depth(self.graph, self.root, G2.graph, G2.root, rng=rng)\n        else:\n            pair_gen = select_nodes_randomly(self.graph, G2.graph, rng=rng)\n\n        for node1, node2 in pair_gen:\n            #TODO: if root is in inner_search_space, then do use it?\n            if node2 is G2.root: #dont want to add root as inner node\n                continue\n\n\n            #check if node1 is a leaf and leafs are protected, don't add an input to the leave\n            if self.leaf_search_space is not None and len(list(self.graph.successors(node1))) == 0:\n                continue\n\n            #icheck if node2 is graph individual\n            # if isinstance(node2,GraphIndividual):\n            #     if not ((isinstance(node2,GraphIndividual) and (\"Recursive\" in self.inner_search_space or \"Recursive\" in self.leaf_search_space))):\n            #         continue\n\n            #isolating the branch\n            branch2 = G2.graph.copy()\n            n2_descendants = nx.descendants(branch2,node2)\n            for n in list(branch2.nodes):\n                if n not in n2_descendants and n is not node2: #removes all nodes not in the branch\n                    branch2.remove_node(n)\n\n            #if node1 plus node2 branch has more than max_children, skip\n            if branch2.number_of_nodes() + self.graph.number_of_nodes() &gt; self.max_size:\n                continue\n\n            branch2 = copy.deepcopy(branch2)\n            branch2_root = get_roots(branch2)[0]\n            self.graph.add_edges_from(branch2.edges)\n            self.graph.add_edge(node1,branch2_root)\n\n            return True\n        return False\n\n\n\n    def _crossover_nodes(self, G2, rng=None):\n        '''\n        Swaps the hyperparamters of one randomly chosen node in Parent1 with the hyperparameters of randomly chosen node in Parent2.\n        '''\n        rng = np.random.default_rng(rng)\n\n        if self.crossover_same_depth:\n            pair_gen = select_nodes_same_depth(self.graph, self.root, G2.graph, G2.root, rng=rng)\n        else:\n            pair_gen = select_nodes_randomly(self.graph, G2.graph, rng=rng)\n\n        for node1, node2 in pair_gen:\n\n            #if both nodes are leaves\n            if len(list(self.graph.successors(node1)))==0 and len(list(G2.graph.successors(node2)))==0:\n                if node1.crossover(node2):\n                    return True\n\n\n            #if both nodes are inner nodes\n            if len(list(self.graph.successors(node1)))&gt;0 and len(list(G2.graph.successors(node2)))&gt;0:\n                if len(list(self.graph.predecessors(node1)))&gt;0 and len(list(G2.graph.predecessors(node2)))&gt;0:\n                    if node1.crossover(node2):\n                        return True\n\n            #if both nodes are root nodes\n            if node1 is self.root and node2 is G2.root:\n                if node1.crossover(node2):\n                    return True\n\n\n        return False\n\n    #not including the nodes, just their children\n    #Finds leaves attached to nodes and swaps them\n    def _crossover_swap_leaf_at_node(self, G2, rng=None):\n        rng = np.random.default_rng(rng)\n\n        if self.crossover_same_depth:\n            pair_gen = select_nodes_same_depth(self.graph, self.root, G2.graph, G2.root, rng=rng)\n        else:\n            pair_gen = select_nodes_randomly(self.graph, G2.graph, rng=rng)\n\n        success = False\n        for node1, node2 in pair_gen:\n            # if leaves are protected node1 and node2 must both be leaves or both be inner nodes\n            if self.leaf_search_space is not None and not (len(list(self.graph.successors(node1)))==0 ^ len(list(G2.graph.successors(node2)))==0):\n                continue\n            #self_leafs = [c for c in nx.descendants(self.graph,node1) if len(list(self.graph.successors(c)))==0 and c is not node1]\n            node_leafs = [c for c in nx.descendants(G2.graph,node2) if len(list(G2.graph.successors(c)))==0 and c is not node2]\n\n            # if len(self_leafs) &gt;0:\n            #     for c in self_leafs:\n            #         if random.choice([True,False]):\n            #             self.graph.remove_node(c)\n            #             G2.graph.add_edge(node2, c)\n            #             success = True\n\n            if len(node_leafs) &gt;0:\n                for c in node_leafs:\n                    if rng.choice([True,False]):\n                        G2.graph.remove_node(c)\n                        self.graph.add_edge(node1, c)\n                        success = True\n\n        return success\n\n\n\n    #TODO edit so that G2 is not modified\n    def _crossover_swap_node(self, G2, rng=None):\n        '''\n        Swaps randomly chosen node from Parent1 with a randomly chosen node from Parent2.\n        '''\n        rng = np.random.default_rng(rng)\n\n        if self.crossover_same_depth:\n            pair_gen = select_nodes_same_depth(self.graph, self.root, G2.graph, G2.root, rng=rng)\n        else:\n            pair_gen = select_nodes_randomly(self.graph, G2.graph, rng=rng)\n\n        for node1, node2 in pair_gen:\n            if node1 is self.root or node2 is G2.root: #TODO: allow root\n                continue\n\n            #if leaves are protected\n            if self.leaf_search_space is not None:\n                #if one node is a leaf, the other must be a leaf\n                if not((len(list(self.graph.successors(node1)))==0) ^ (len(list(G2.graph.successors(node2)))==0)):\n                    continue #only continue if both are leaves, or both are not leaves\n\n\n            n1_s = self.graph.successors(node1)\n            n1_p = self.graph.predecessors(node1)\n\n            n2_s = G2.graph.successors(node2)\n            n2_p = G2.graph.predecessors(node2)\n\n            self.graph.remove_node(node1)\n            G2.graph.remove_node(node2)\n\n            self.graph.add_node(node2)\n\n            self.graph.add_edges_from([ (node2, n) for n in n1_s])\n            G2.graph.add_edges_from([ (node1, n) for n in n2_s])\n\n            self.graph.add_edges_from([ (n, node2) for n in n1_p])\n            G2.graph.add_edges_from([ (n, node1) for n in n2_p])\n\n            return True\n\n        return False\n\n\n    def _merge_duplicated_nodes(self):\n\n        graph_changed = False\n        merged = False\n        while(not merged):\n            node_list = list(self.graph.nodes)\n            merged = True\n            for node, other_node in itertools.product(node_list, node_list):\n                if node is other_node:\n                    continue\n\n                #If nodes are same class/hyperparameters\n                if node.unique_id() == other_node.unique_id():\n                    node_children = set(self.graph.successors(node))\n                    other_node_children = set(self.graph.successors(other_node))\n                    #if nodes have identical children, they can be merged\n                    if node_children == other_node_children:\n                        for other_node_parent in list(self.graph.predecessors(other_node)):\n                            if other_node_parent not in self.graph.predecessors(node):\n                                self.graph.add_edge(other_node_parent,node)\n\n                        self.graph.remove_node(other_node)\n                        merged=False\n                        graph_changed = True\n                        break\n\n        return graph_changed\n\n\n    def export_pipeline(self, memory=None, **kwargs):\n        estimator_graph = self.graph.copy()\n\n        #mapping = {node:node.method_class(**node.hyperparameters) for node in estimator_graph}\n        label_remapping = {}\n        label_to_instance = {}\n\n        for node in estimator_graph:\n            this_pipeline_node = node.export_pipeline(memory=memory, **kwargs)\n            found_unique_label = False\n            i=1\n            while not found_unique_label:\n                label = \"{0}_{1}\".format(this_pipeline_node.__class__.__name__, i)\n                if label not in label_to_instance:\n                    found_unique_label = True\n                else:\n                    i+=1\n\n            label_remapping[node] = label\n            label_to_instance[label] = this_pipeline_node\n\n        estimator_graph = nx.relabel_nodes(estimator_graph, label_remapping)\n\n        for label, instance in label_to_instance.items():\n            estimator_graph.nodes[label][\"instance\"] = instance\n\n        return tpot.GraphPipeline(graph=estimator_graph, memory=memory, use_label_encoder=self.use_label_encoder, method=self.method, cross_val_predict_cv=self.cross_val_predict_cv)\n\n\n    def plot(self):\n        G = self.graph.reverse()\n        #TODO clean this up\n        try:\n            pos = nx.planar_layout(G)  # positions for all nodes\n        except:\n            pos = nx.shell_layout(G)\n        # nodes\n        options = {'edgecolors': 'tab:gray', 'node_size': 800, 'alpha': 0.9}\n        nodelist = list(G.nodes)\n        node_color = [plt.cm.Set1(G.nodes[n]['recursive depth']) for n in G]\n\n        fig, ax = plt.subplots()\n\n        nx.draw(G, pos, nodelist=nodelist, node_color=node_color, ax=ax,  **options)\n\n\n        '''edgelist = []\n        for n in n1.node_set:\n            for child in n.children:\n                edgelist.append((n,child))'''\n\n        # edges\n        #nx.draw_networkx_edges(G, pos, width=3.0, arrows=True)\n        '''nx.draw_networkx_edges(\n            G,\n            pos,\n            edgelist=[edgelist],\n            width=8,\n            alpha=0.5,\n            edge_color='tab:red',\n        )'''\n\n\n\n        # some math labels\n        labels = {}\n        for i, n in enumerate(G.nodes):\n            labels[n] = n.method_class.__name__ + \"\\n\" + str(n.hyperparameters)\n\n\n        nx.draw_networkx_labels(G, pos, labels,ax=ax, font_size=7, font_color='black')\n\n        plt.tight_layout()\n        plt.axis('off')\n        plt.show()\n\n\n    def unique_id(self):\n        if self.graphkey is None:\n            #copy self.graph\n            new_graph = self.graph.copy()\n            for n in new_graph.nodes:\n                new_graph.nodes[n]['label'] = n.unique_id()\n\n            new_graph = nx.convert_node_labels_to_integers(new_graph)\n            self.graphkey = GraphKey(new_graph)\n\n        return self.graphkey\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.GraphPipelineIndividual.crossover","title":"<code>crossover(ind2, rng=None)</code>","text":"<p>self is the first individual, ind2 is the second individual If crossover_same_depth, it will select graphindividuals at the same recursive depth. Otherwise, it will select graphindividuals randomly from the entire graph and its subgraphs.</p> <p>This does not impact graphs without subgraphs. And it does not impacts nodes that are not graphindividuals. Cros</p> Source code in <code>tpot/search_spaces/pipelines/graph.py</code> <pre><code>def crossover(self, ind2, rng=None):\n    '''\n    self is the first individual, ind2 is the second individual\n    If crossover_same_depth, it will select graphindividuals at the same recursive depth.\n    Otherwise, it will select graphindividuals randomly from the entire graph and its subgraphs.\n\n    This does not impact graphs without subgraphs. And it does not impacts nodes that are not graphindividuals. Cros\n    '''\n\n    rng = np.random.default_rng(rng)\n\n    rng.shuffle(self.crossover_methods_list)\n\n    finished = False\n\n    for crossover_method in self.crossover_methods_list:\n        if crossover_method(ind2, rng=rng):\n            self._merge_duplicated_nodes()\n            finished = True\n            break\n\n    if self.__debug:\n        try:\n            nx.find_cycle(self.graph)\n            print('something went wrong with ', crossover_method)\n        except:\n            pass\n\n    if finished:\n        self.graphkey = None\n\n    return finished\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.GraphSearchPipeline","title":"<code>GraphSearchPipeline</code>","text":"<p>               Bases: <code>SearchSpace</code></p> Source code in <code>tpot/search_spaces/pipelines/graph.py</code> <pre><code>class GraphSearchPipeline(SearchSpace):\n    def __init__(self, \n        root_search_space: SearchSpace, \n        leaf_search_space: SearchSpace = None, \n        inner_search_space: SearchSpace = None, \n        max_size: int = np.inf,\n        crossover_same_depth: bool = False,\n        cross_val_predict_cv: Union[int, Callable] = 0, #signature function(estimator, X, y=none)\n        method: str = 'auto',\n        use_label_encoder: bool = False):\n\n        \"\"\"\n        Defines a search space of pipelines in the shape of a Directed Acyclic Graphs. The search spaces for root, leaf, and inner nodes can be defined separately if desired.\n        Each graph will have a single root serving as the final estimator which is drawn from the `root_search_space`. If the `leaf_search_space` is defined, all leaves \n        in the pipeline will be drawn from that search space. If the `leaf_search_space` is not defined, all leaves will be drawn from the `inner_search_space`.\n        Nodes that are not leaves or roots will be drawn from the `inner_search_space`. If the `inner_search_space` is not defined, there will be no inner nodes.\n\n        `cross_val_predict_cv`, `method`, `memory`, and `use_label_encoder` are passed to the GraphPipeline object when the pipeline is exported and not directly used in the search space.\n\n        Exports to a GraphPipeline object.\n\n        Parameters\n        ----------\n\n        root_search_space: SearchSpace\n            The search space for the root node of the graph. This node will be the final estimator in the pipeline.\n\n        inner_search_space: SearchSpace, optional\n            The search space for the inner nodes of the graph. If not defined, there will be no inner nodes.\n\n        leaf_search_space: SearchSpace, optional\n            The search space for the leaf nodes of the graph. If not defined, the leaf nodes will be drawn from the inner_search_space.\n\n        crossover_same_depth: bool, optional\n            If True, crossover will only occur between nodes at the same depth in the graph. If False, crossover will occur between nodes at any depth.\n\n        cross_val_predict_cv : int, default=0\n            Number of folds to use for the cross_val_predict function for inner classifiers and regressors. Estimators will still be fit on the full dataset, but the following node will get the outputs from cross_val_predict.\n\n            - 0-1 : When set to 0 or 1, the cross_val_predict function will not be used. The next layer will get the outputs from fitting and transforming the full dataset.\n            - &gt;=2 : When fitting pipelines with inner classifiers or regressors, they will still be fit on the full dataset.\n                    However, the output to the next node will come from cross_val_predict with the specified number of folds.\n\n        method: str, optional\n            The prediction method to use for the inner classifiers or regressors. If 'auto', it will try to use predict_proba, decision_function, or predict in that order.\n\n        memory: str or object with the joblib.Memory interface, optional\n            Used to cache the input and outputs of nodes to prevent refitting or computationally heavy transformations. By default, no caching is performed. If a string is given, it is the path to the caching directory.\n\n        use_label_encoder: bool, optional\n            If True, the label encoder is used to encode the labels to be 0 to N. If False, the label encoder is not used.\n            Mainly useful for classifiers (XGBoost) that require labels to be ints from 0 to N.\n            Can also be a sklearn.preprocessing.LabelEncoder object. If so, that label encoder is used.\n\n        \"\"\"\n\n\n        self.root_search_space = root_search_space\n        self.leaf_search_space = leaf_search_space\n        self.inner_search_space = inner_search_space\n        self.max_size = max_size\n        self.crossover_same_depth = crossover_same_depth\n\n        self.cross_val_predict_cv = cross_val_predict_cv\n        self.method = method\n        self.use_label_encoder = use_label_encoder\n\n    def generate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        ind =  GraphPipelineIndividual(self.root_search_space, self.leaf_search_space, self.inner_search_space, self.max_size, self.crossover_same_depth, \n                                       self.cross_val_predict_cv, self.method, self.use_label_encoder, rng=rng)  \n            # if user specified limit, grab a random number between that limit\n\n        if self.max_size is None or self.max_size == np.inf:\n            n_nodes = rng.integers(1, 5)\n        else:\n            n_nodes = min(rng.integers(1, self.max_size), 5)\n\n        starting_ops = []\n        if self.inner_search_space is not None:\n            starting_ops.append(ind._mutate_insert_inner_node)\n        if self.leaf_search_space is not None or self.inner_search_space is not None:\n            starting_ops.append(ind._mutate_insert_leaf)\n            n_nodes -= 1\n\n        if len(starting_ops) &gt; 0:\n            for _ in range(n_nodes-1):\n                func = rng.choice(starting_ops)\n                func(rng=rng)\n\n        ind._merge_duplicated_nodes()\n\n        return ind\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.GraphSearchPipeline.__init__","title":"<code>__init__(root_search_space, leaf_search_space=None, inner_search_space=None, max_size=np.inf, crossover_same_depth=False, cross_val_predict_cv=0, method='auto', use_label_encoder=False)</code>","text":"<p>Defines a search space of pipelines in the shape of a Directed Acyclic Graphs. The search spaces for root, leaf, and inner nodes can be defined separately if desired. Each graph will have a single root serving as the final estimator which is drawn from the <code>root_search_space</code>. If the <code>leaf_search_space</code> is defined, all leaves  in the pipeline will be drawn from that search space. If the <code>leaf_search_space</code> is not defined, all leaves will be drawn from the <code>inner_search_space</code>. Nodes that are not leaves or roots will be drawn from the <code>inner_search_space</code>. If the <code>inner_search_space</code> is not defined, there will be no inner nodes.</p> <p><code>cross_val_predict_cv</code>, <code>method</code>, <code>memory</code>, and <code>use_label_encoder</code> are passed to the GraphPipeline object when the pipeline is exported and not directly used in the search space.</p> <p>Exports to a GraphPipeline object.</p> <p>Parameters:</p> Name Type Description Default <code>root_search_space</code> <code>SearchSpace</code> <p>The search space for the root node of the graph. This node will be the final estimator in the pipeline.</p> required <code>inner_search_space</code> <code>SearchSpace</code> <p>The search space for the inner nodes of the graph. If not defined, there will be no inner nodes.</p> <code>None</code> <code>leaf_search_space</code> <code>SearchSpace</code> <p>The search space for the leaf nodes of the graph. If not defined, the leaf nodes will be drawn from the inner_search_space.</p> <code>None</code> <code>crossover_same_depth</code> <code>bool</code> <p>If True, crossover will only occur between nodes at the same depth in the graph. If False, crossover will occur between nodes at any depth.</p> <code>False</code> <code>cross_val_predict_cv</code> <code>int</code> <p>Number of folds to use for the cross_val_predict function for inner classifiers and regressors. Estimators will still be fit on the full dataset, but the following node will get the outputs from cross_val_predict.</p> <ul> <li>0-1 : When set to 0 or 1, the cross_val_predict function will not be used. The next layer will get the outputs from fitting and transforming the full dataset.</li> <li> <p>=2 : When fitting pipelines with inner classifiers or regressors, they will still be fit on the full dataset.         However, the output to the next node will come from cross_val_predict with the specified number of folds.</p> </li> </ul> <code>0</code> <code>method</code> <code>str</code> <p>The prediction method to use for the inner classifiers or regressors. If 'auto', it will try to use predict_proba, decision_function, or predict in that order.</p> <code>'auto'</code> <code>memory</code> <p>Used to cache the input and outputs of nodes to prevent refitting or computationally heavy transformations. By default, no caching is performed. If a string is given, it is the path to the caching directory.</p> required <code>use_label_encoder</code> <code>bool</code> <p>If True, the label encoder is used to encode the labels to be 0 to N. If False, the label encoder is not used. Mainly useful for classifiers (XGBoost) that require labels to be ints from 0 to N. Can also be a sklearn.preprocessing.LabelEncoder object. If so, that label encoder is used.</p> <code>False</code> Source code in <code>tpot/search_spaces/pipelines/graph.py</code> <pre><code>def __init__(self, \n    root_search_space: SearchSpace, \n    leaf_search_space: SearchSpace = None, \n    inner_search_space: SearchSpace = None, \n    max_size: int = np.inf,\n    crossover_same_depth: bool = False,\n    cross_val_predict_cv: Union[int, Callable] = 0, #signature function(estimator, X, y=none)\n    method: str = 'auto',\n    use_label_encoder: bool = False):\n\n    \"\"\"\n    Defines a search space of pipelines in the shape of a Directed Acyclic Graphs. The search spaces for root, leaf, and inner nodes can be defined separately if desired.\n    Each graph will have a single root serving as the final estimator which is drawn from the `root_search_space`. If the `leaf_search_space` is defined, all leaves \n    in the pipeline will be drawn from that search space. If the `leaf_search_space` is not defined, all leaves will be drawn from the `inner_search_space`.\n    Nodes that are not leaves or roots will be drawn from the `inner_search_space`. If the `inner_search_space` is not defined, there will be no inner nodes.\n\n    `cross_val_predict_cv`, `method`, `memory`, and `use_label_encoder` are passed to the GraphPipeline object when the pipeline is exported and not directly used in the search space.\n\n    Exports to a GraphPipeline object.\n\n    Parameters\n    ----------\n\n    root_search_space: SearchSpace\n        The search space for the root node of the graph. This node will be the final estimator in the pipeline.\n\n    inner_search_space: SearchSpace, optional\n        The search space for the inner nodes of the graph. If not defined, there will be no inner nodes.\n\n    leaf_search_space: SearchSpace, optional\n        The search space for the leaf nodes of the graph. If not defined, the leaf nodes will be drawn from the inner_search_space.\n\n    crossover_same_depth: bool, optional\n        If True, crossover will only occur between nodes at the same depth in the graph. If False, crossover will occur between nodes at any depth.\n\n    cross_val_predict_cv : int, default=0\n        Number of folds to use for the cross_val_predict function for inner classifiers and regressors. Estimators will still be fit on the full dataset, but the following node will get the outputs from cross_val_predict.\n\n        - 0-1 : When set to 0 or 1, the cross_val_predict function will not be used. The next layer will get the outputs from fitting and transforming the full dataset.\n        - &gt;=2 : When fitting pipelines with inner classifiers or regressors, they will still be fit on the full dataset.\n                However, the output to the next node will come from cross_val_predict with the specified number of folds.\n\n    method: str, optional\n        The prediction method to use for the inner classifiers or regressors. If 'auto', it will try to use predict_proba, decision_function, or predict in that order.\n\n    memory: str or object with the joblib.Memory interface, optional\n        Used to cache the input and outputs of nodes to prevent refitting or computationally heavy transformations. By default, no caching is performed. If a string is given, it is the path to the caching directory.\n\n    use_label_encoder: bool, optional\n        If True, the label encoder is used to encode the labels to be 0 to N. If False, the label encoder is not used.\n        Mainly useful for classifiers (XGBoost) that require labels to be ints from 0 to N.\n        Can also be a sklearn.preprocessing.LabelEncoder object. If so, that label encoder is used.\n\n    \"\"\"\n\n\n    self.root_search_space = root_search_space\n    self.leaf_search_space = leaf_search_space\n    self.inner_search_space = inner_search_space\n    self.max_size = max_size\n    self.crossover_same_depth = crossover_same_depth\n\n    self.cross_val_predict_cv = cross_val_predict_cv\n    self.method = method\n    self.use_label_encoder = use_label_encoder\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.MaskSelector","title":"<code>MaskSelector</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>SelectorMixin</code></p> <p>Select predefined feature subsets.</p> Source code in <code>tpot/search_spaces/nodes/genetic_feature_selection.py</code> <pre><code>class MaskSelector(BaseEstimator, SelectorMixin):\n    \"\"\"Select predefined feature subsets.\"\"\"\n\n    def __init__(self, mask, set_output_transform=None):\n        self.mask = mask\n        self.set_output_transform = set_output_transform\n        if set_output_transform is not None:\n            self.set_output(transform=set_output_transform)\n\n    def fit(self, X, y=None):\n        self.n_features_in_ = X.shape[1]\n        if isinstance(X, pd.DataFrame):\n            self.feature_names_in_ = X.columns\n        #     self.set_output(transform=\"pandas\")\n        self.is_fitted_ = True #so sklearn knows it's fitted\n        return self\n\n    def _get_tags(self):\n        tags = {\"allow_nan\": True, \"requires_y\": False}\n        return tags\n\n    def _get_support_mask(self):\n        return np.array(self.mask)\n\n    def get_feature_names_out(self, input_features=None):\n        return self.feature_names_in_[self.get_support()]\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.SequentialPipeline","title":"<code>SequentialPipeline</code>","text":"<p>               Bases: <code>SearchSpace</code></p> Source code in <code>tpot/search_spaces/pipelines/sequential.py</code> <pre><code>class SequentialPipeline(SearchSpace):\n    def __init__(self, search_spaces : List[SearchSpace] ) -&gt; None:\n        \"\"\"\n        Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.\n        \"\"\"\n\n        self.search_spaces = search_spaces\n\n    def generate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        return SequentialPipelineIndividual(self.search_spaces, rng=rng)\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.SequentialPipeline.__init__","title":"<code>__init__(search_spaces)</code>","text":"<p>Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.</p> Source code in <code>tpot/search_spaces/pipelines/sequential.py</code> <pre><code>def __init__(self, search_spaces : List[SearchSpace] ) -&gt; None:\n    \"\"\"\n    Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.\n    \"\"\"\n\n    self.search_spaces = search_spaces\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.SklearnIndividual","title":"<code>SklearnIndividual</code>","text":"<p>               Bases: <code>BaseIndividual</code></p> Source code in <code>tpot/search_spaces/base.py</code> <pre><code>class SklearnIndividual(tpot.BaseIndividual):\n\n    def __init_subclass__(cls):\n        cls.crossover = cls.validate_same_type(cls.crossover)\n\n\n    def __init__(self,) -&gt; None:\n        super().__init__()\n\n    def mutate(self, rng=None):\n        return\n\n    def crossover(self, other, rng=None, **kwargs):\n        return \n\n    @final\n    def validate_same_type(func):\n\n        def wrapper(self, other, rng=None, **kwargs):\n            if not isinstance(other, type(self)):\n                return False\n            return func(self, other, rng=rng, **kwargs)\n\n        return wrapper\n\n    def export_pipeline(self, **kwargs) -&gt; BaseEstimator:\n        return\n\n    def unique_id(self):\n        \"\"\"\n        Returns a unique identifier for the individual. Used for preventing duplicate individuals from being evaluated.\n        \"\"\"\n        return self\n\n    #TODO currently TPOT population class manually uses the unique_id to generate the index for the population data frame.\n    #alternatively, the index could be the individual itself, with the __eq__ and __hash__ methods implemented.\n\n    # Though this breaks the graphpipeline. When a mutation is called, it changes the __eq__ and __hash__ outputs.\n    # Since networkx uses the hash and eq to determine if a node is already in the graph, this causes the graph thing that \n    # This is a new node not in the graph. But this could be changed if when the graphpipeline mutates nodes, \n    # it \"replaces\" the existing node with the mutated node. This would require a change in the graphpipeline class.\n\n    # def __eq__(self, other):\n    #     return self.unique_id() == other.unique_id()\n\n    # def __hash__(self):\n    #     return hash(self.unique_id())\n\n    #number of components in the pipeline\n    def get_size(self):\n        return 1\n\n    @final\n    def export_flattened_graphpipeline(self, **graphpipeline_kwargs) -&gt; tpot.GraphPipeline:\n        return flatten_to_graphpipeline(self.export_pipeline(), **graphpipeline_kwargs)\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.SklearnIndividual.unique_id","title":"<code>unique_id()</code>","text":"<p>Returns a unique identifier for the individual. Used for preventing duplicate individuals from being evaluated.</p> Source code in <code>tpot/search_spaces/base.py</code> <pre><code>def unique_id(self):\n    \"\"\"\n    Returns a unique identifier for the individual. Used for preventing duplicate individuals from being evaluated.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.TreePipeline","title":"<code>TreePipeline</code>","text":"<p>               Bases: <code>SearchSpace</code></p> Source code in <code>tpot/search_spaces/pipelines/tree.py</code> <pre><code>class TreePipeline(SearchSpace):\n    def __init__(self, root_search_space : SearchSpace, \n                        leaf_search_space : SearchSpace = None, \n                        inner_search_space : SearchSpace =None, \n                        min_size: int = 2, \n                        max_size: int = 10,\n                        crossover_same_depth=False) -&gt; None:\n\n        \"\"\"\n        Generates a pipeline of variable length. Pipeline will have a tree structure similar to TPOT1.\n\n        \"\"\"\n\n        self.search_space = root_search_space\n        self.leaf_search_space = leaf_search_space\n        self.inner_search_space = inner_search_space\n        self.min_size = min_size\n        self.max_size = max_size\n        self.crossover_same_depth = crossover_same_depth\n\n    def generate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        return TreePipelineIndividual(self.search_space, self.leaf_search_space, self.inner_search_space, self.min_size, self.max_size, self.crossover_same_depth, rng=rng) \n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.TreePipeline.__init__","title":"<code>__init__(root_search_space, leaf_search_space=None, inner_search_space=None, min_size=2, max_size=10, crossover_same_depth=False)</code>","text":"<p>Generates a pipeline of variable length. Pipeline will have a tree structure similar to TPOT1.</p> Source code in <code>tpot/search_spaces/pipelines/tree.py</code> <pre><code>def __init__(self, root_search_space : SearchSpace, \n                    leaf_search_space : SearchSpace = None, \n                    inner_search_space : SearchSpace =None, \n                    min_size: int = 2, \n                    max_size: int = 10,\n                    crossover_same_depth=False) -&gt; None:\n\n    \"\"\"\n    Generates a pipeline of variable length. Pipeline will have a tree structure similar to TPOT1.\n\n    \"\"\"\n\n    self.search_space = root_search_space\n    self.leaf_search_space = leaf_search_space\n    self.inner_search_space = inner_search_space\n    self.min_size = min_size\n    self.max_size = max_size\n    self.crossover_same_depth = crossover_same_depth\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.TupleIndex","title":"<code>TupleIndex</code>","text":"<p>TPOT uses tuples to create a unique id for some pipeline search spaces. However, tuples sometimes don't interact correctly with pandas indexes. This class is a wrapper around a tuple that allows it to be used as a key in a dictionary, without it being an itereable.</p> <p>An alternative could be to make unique id return a string, but this would not work with graphpipelines, which require a special object. This class allows linear pipelines to contain graph pipelines while still being able to be used as a key in a dictionary.</p> Source code in <code>tpot/search_spaces/tuple_index.py</code> <pre><code>class TupleIndex():\n    \"\"\"\n    TPOT uses tuples to create a unique id for some pipeline search spaces. However, tuples sometimes don't interact correctly with pandas indexes.\n    This class is a wrapper around a tuple that allows it to be used as a key in a dictionary, without it being an itereable.\n\n    An alternative could be to make unique id return a string, but this would not work with graphpipelines, which require a special object.\n    This class allows linear pipelines to contain graph pipelines while still being able to be used as a key in a dictionary.\n\n    \"\"\"\n    def __init__(self, tup):\n        self.tup = tup\n\n    def __eq__(self,other) -&gt; bool:\n        return self.tup == other\n\n    def __hash__(self) -&gt; int:\n        return self.tup.__hash__()\n\n    def __str__(self) -&gt; str:\n        return self.tup.__str__()\n\n    def __repr__(self) -&gt; str:\n        return self.tup.__repr__()\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.UnionPipeline","title":"<code>UnionPipeline</code>","text":"<p>               Bases: <code>SearchSpace</code></p> Source code in <code>tpot/search_spaces/pipelines/union.py</code> <pre><code>class UnionPipeline(SearchSpace):\n    def __init__(self, search_spaces : List[SearchSpace] ) -&gt; None:\n        \"\"\"\n        Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.\n        \"\"\"\n\n        self.search_spaces = search_spaces\n\n    def generate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        return UnionPipelineIndividual(self.search_spaces, rng=rng)\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.UnionPipeline.__init__","title":"<code>__init__(search_spaces)</code>","text":"<p>Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.</p> Source code in <code>tpot/search_spaces/pipelines/union.py</code> <pre><code>def __init__(self, search_spaces : List[SearchSpace] ) -&gt; None:\n    \"\"\"\n    Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.\n    \"\"\"\n\n    self.search_spaces = search_spaces\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.UnionPipelineIndividual","title":"<code>UnionPipelineIndividual</code>","text":"<p>               Bases: <code>SklearnIndividual</code></p> <p>Takes in a list of search spaces. each space is a list of SearchSpaces. Will produce a FeatureUnion pipeline. Each step in the pipeline will correspond to the the search space provided in the same index. The resulting pipeline will be a FeatureUnion of the steps in the pipeline.</p> Source code in <code>tpot/search_spaces/pipelines/union.py</code> <pre><code>class UnionPipelineIndividual(SklearnIndividual):\n    \"\"\"\n    Takes in a list of search spaces. each space is a list of SearchSpaces.\n    Will produce a FeatureUnion pipeline. Each step in the pipeline will correspond to the the search space provided in the same index.\n    The resulting pipeline will be a FeatureUnion of the steps in the pipeline.\n\n    \"\"\"\n\n    def __init__(self, search_spaces : List[SearchSpace], rng=None) -&gt; None:\n        super().__init__()\n        self.search_spaces = search_spaces\n\n        self.pipeline = []\n        for space in self.search_spaces:\n            self.pipeline.append(space.generate(rng))\n\n    def mutate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        step = rng.choice(self.pipeline)\n        return step.mutate(rng)\n\n\n    def crossover(self, other, rng=None):\n        #swap a random step in the pipeline with the corresponding step in the other pipeline\n        rng = np.random.default_rng(rng)\n\n        cx_funcs = [self._crossover_node, self._crossover_swap_node]\n        rng.shuffle(cx_funcs)\n        for cx_func in cx_funcs:\n            if cx_func(other, rng):\n                return True\n\n        return False\n\n    def _crossover_swap_node(self, other, rng):\n        rng = np.random.default_rng(rng)\n        idx = rng.integers(1,len(self.pipeline))\n\n        self.pipeline[idx], other.pipeline[idx] = other.pipeline[idx], self.pipeline[idx]\n        return True\n\n    def _crossover_node(self, other, rng):\n        rng = np.random.default_rng(rng)\n\n        crossover_success = False\n        for idx in range(len(self.pipeline)):\n            if rng.random() &lt; 0.5:\n                if self.pipeline[idx].crossover(other.pipeline[idx], rng):\n                    crossover_success = True\n\n        return crossover_success\n\n    def export_pipeline(self, **kwargs):\n        return sklearn.pipeline.make_union(*[step.export_pipeline(**kwargs) for step in self.pipeline])\n\n    def unique_id(self):\n        l = [step.unique_id() for step in self.pipeline]\n        l = [\"FeatureUnion\"] + l\n        return TupleIndex(tuple(l))\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.WrapperPipeline","title":"<code>WrapperPipeline</code>","text":"<p>               Bases: <code>SearchSpace</code></p> Source code in <code>tpot/search_spaces/pipelines/wrapper.py</code> <pre><code>class WrapperPipeline(SearchSpace):\n    def __init__(\n            self, \n            method: type, \n            space: ConfigurationSpace,\n            estimator_search_space: SearchSpace,\n            hyperparameter_parser: callable = None, \n            wrapped_param_name: str = None\n            ) -&gt; None:\n\n        \"\"\"\n        This search space is for wrapping a sklearn estimator with a method that takes another estimator and hyperparameters as arguments.\n        For example, this can be used with sklearn.ensemble.BaggingClassifier or sklearn.ensemble.AdaBoostClassifier.\n\n        \"\"\"\n\n\n        self.estimator_search_space = estimator_search_space\n        self.method = method\n        self.space = space\n        self.hyperparameter_parser=hyperparameter_parser\n        self.wrapped_param_name = wrapped_param_name\n\n    def generate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        return WrapperPipelineIndividual(method=self.method, space=self.space, estimator_search_space=self.estimator_search_space, hyperparameter_parser=self.hyperparameter_parser, wrapped_param_name=self.wrapped_param_name,  rng=rng)\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.WrapperPipeline.__init__","title":"<code>__init__(method, space, estimator_search_space, hyperparameter_parser=None, wrapped_param_name=None)</code>","text":"<p>This search space is for wrapping a sklearn estimator with a method that takes another estimator and hyperparameters as arguments. For example, this can be used with sklearn.ensemble.BaggingClassifier or sklearn.ensemble.AdaBoostClassifier.</p> Source code in <code>tpot/search_spaces/pipelines/wrapper.py</code> <pre><code>def __init__(\n        self, \n        method: type, \n        space: ConfigurationSpace,\n        estimator_search_space: SearchSpace,\n        hyperparameter_parser: callable = None, \n        wrapped_param_name: str = None\n        ) -&gt; None:\n\n    \"\"\"\n    This search space is for wrapping a sklearn estimator with a method that takes another estimator and hyperparameters as arguments.\n    For example, this can be used with sklearn.ensemble.BaggingClassifier or sklearn.ensemble.AdaBoostClassifier.\n\n    \"\"\"\n\n\n    self.estimator_search_space = estimator_search_space\n    self.method = method\n    self.space = space\n    self.hyperparameter_parser=hyperparameter_parser\n    self.wrapped_param_name = wrapped_param_name\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.get_template_search_spaces","title":"<code>get_template_search_spaces(search_space, classification=True, inner_predictors=None, cross_val_predict_cv=None, **get_search_space_params)</code>","text":"<p>Returns a search space which can be optimized by TPOT.</p> <p>Parameters:</p> Name Type Description Default <code>search_space</code> <p>The default search space to use. If a string, it should be one of the following:     - 'linear': A search space for linear pipelines     - 'linear-light': A search space for linear pipelines with a smaller, faster search space     - 'graph': A search space for graph pipelines     - 'graph-light': A search space for graph pipelines with a smaller, faster search space     - 'mdr': A search space for MDR pipelines If a SearchSpace object, it should be a valid search space object for TPOT.</p> required <code>classification</code> <p>Whether the problem is a classification problem or a regression problem.</p> <code>True</code> <code>inner_predictors</code> <p>Whether to include additional classifiers/regressors before the final classifier/regressor (allowing for ensembles).  Defaults to False for 'linear-light' and 'graph-light' search spaces, and True otherwise. (Not used for 'mdr' search space)</p> <code>None</code> <code>cross_val_predict_cv</code> <p>The number of folds to use for cross_val_predict.  Defaults to 0 for 'linear-light' and 'graph-light' search spaces, and 5 otherwise. (Not used for 'mdr' search space)</p> <code>None</code> <code>get_search_space_params</code> <p>Additional parameters to pass to the get_search_space function.</p> <code>{}</code> Source code in <code>tpot/config/template_search_spaces.py</code> <pre><code>def get_template_search_spaces(search_space, classification=True, inner_predictors=None, cross_val_predict_cv=None, **get_search_space_params):\n    \"\"\"\n    Returns a search space which can be optimized by TPOT.\n\n    Parameters\n    ----------\n    search_space: str or SearchSpace\n        The default search space to use. If a string, it should be one of the following:\n            - 'linear': A search space for linear pipelines\n            - 'linear-light': A search space for linear pipelines with a smaller, faster search space\n            - 'graph': A search space for graph pipelines\n            - 'graph-light': A search space for graph pipelines with a smaller, faster search space\n            - 'mdr': A search space for MDR pipelines\n        If a SearchSpace object, it should be a valid search space object for TPOT.\n\n    classification: bool, default=True\n        Whether the problem is a classification problem or a regression problem.\n\n    inner_predictors: bool, default=None\n        Whether to include additional classifiers/regressors before the final classifier/regressor (allowing for ensembles). \n        Defaults to False for 'linear-light' and 'graph-light' search spaces, and True otherwise. (Not used for 'mdr' search space)\n\n    cross_val_predict_cv: int, default=None\n        The number of folds to use for cross_val_predict. \n        Defaults to 0 for 'linear-light' and 'graph-light' search spaces, and 5 otherwise. (Not used for 'mdr' search space)\n\n    get_search_space_params: dict\n        Additional parameters to pass to the get_search_space function.\n\n    \"\"\"\n    if inner_predictors is None:\n        if search_space == \"light\" or search_space == \"graph_light\":\n            inner_predictors = False\n        else:\n            inner_predictors = True\n\n    if cross_val_predict_cv is None:\n        if search_space == \"light\" or search_space == \"graph_light\":\n            cross_val_predict_cv = 0\n        else:\n            if classification:\n                cross_val_predict_cv = sklearn.model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n            else:\n                cross_val_predict_cv = sklearn.model_selection.KFold(n_splits=5, shuffle=True, random_state=42)\n\n    if isinstance(search_space, str):\n        if search_space == \"linear\":\n            return get_linear_search_space(classification, inner_predictors, cross_val_predict_cv=cross_val_predict_cv, **get_search_space_params)\n        elif search_space == \"graph\":\n            return get_graph_search_space(classification, inner_predictors, cross_val_predict_cv=cross_val_predict_cv, **get_search_space_params)\n        elif search_space == \"graph-light\":\n            return get_graph_search_space_light(classification, inner_predictors, cross_val_predict_cv=cross_val_predict_cv, **get_search_space_params)\n        elif search_space == \"linear-light\":\n            return get_light_search_space(classification, inner_predictors, cross_val_predict_cv=cross_val_predict_cv, **get_search_space_params)\n        elif search_space == \"mdr\":\n            return get_mdr_search_space(classification, **get_search_space_params)\n        else:\n            raise ValueError(\"Invalid search space\")\n    else:\n        return search_space\n</code></pre>"},{"location":"documentation/tpot/config/transformers/","title":"Transformers","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/evolvers/base_evolver/","title":"Base evolver","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/evolvers/base_evolver/#tpot.evolvers.base_evolver.BaseEvolver","title":"<code>BaseEvolver</code>","text":"Source code in <code>tpot/evolvers/base_evolver.py</code> <pre><code>class BaseEvolver():\n    def __init__(   self,\n                    individual_generator ,\n\n                    objective_functions,\n                    objective_function_weights,\n                    objective_names = None,\n                    objective_kwargs = None,\n                    bigger_is_better = True,\n\n                    population_size = 50,\n                    initial_population_size = None,\n                    population_scaling = .5,\n                    generations_until_end_population = 1,\n                    generations = 50,\n                    early_stop = None,\n                    early_stop_tol = 0.001,\n\n\n                    max_time_mins=float(\"inf\"),\n                    max_eval_time_mins=5,\n\n                    n_jobs=1,\n                    memory_limit=\"4GB\",\n                    client=None,\n\n                    survival_percentage = 1,\n                    crossover_probability=.2,\n                    mutate_probability=.7,\n                    mutate_then_crossover_probability=.05,\n                    crossover_then_mutate_probability=.05,\n\n                    mutation_functions = [ind_mutate],\n                    crossover_functions = [ind_crossover],\n\n                    mutation_function_weights = None,\n                    crossover_function_weights = None,\n\n                    n_parents=2,\n\n                    survival_selector = survival_select_NSGA2,\n                    parent_selector = tournament_selection_dominated,\n\n                    budget_range = None,\n                    budget_scaling = .5,\n                    generations_until_end_budget = 1,\n                    stepwise_steps = 5,\n\n                    threshold_evaluation_pruning = None,\n                    threshold_evaluation_scaling = .5,\n                    min_history_threshold = 20,\n                    selection_evaluation_pruning = None,\n                    selection_evaluation_scaling = .5,\n                    evaluation_early_stop_steps = None,\n                    final_score_strategy = \"mean\",\n\n                    verbose = 0,\n                    periodic_checkpoint_folder = None,\n                    callback = None,\n                    rng=None,\n\n                    ) -&gt; None:\n        \"\"\"\n        Uses mutation, crossover, and optimization functions to evolve a population of individuals towards the given objective functions.\n\n        Parameters\n        ----------\n        individual_generator : generator\n            Generator that yields new base individuals. Used to generate initial population.\n        objective_functions : list of callables\n            list of functions that get applied to the individual and return a float or list of floats\n            If an objective function returns multiple values, they are all concatenated in order\n            with respect to objective_function_weights and early_stop_tol.\n        objective_function_weights : list of floats\n            list of weights for each objective function. Sign flips whether bigger is better or not\n        objective_names : list of strings, default=None\n            Names of the objectives. If None, objective0, objective1, etc. will be used\n        objective_kwargs : dict, default=None\n            Dictionary of keyword arguments to pass to the objective function\n        bigger_is_better : bool, default=True\n            If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n        population_size : int, default=50\n            Size of the population\n        initial_population_size : int, default=None\n            Size of the initial population. If None, population_size will be used.\n        population_scaling : int, default=0.5\n            Scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n        generations_until_end_population : int, default=1\n            Number of generations until the population size reaches population_size\n        generations : int, default=50\n            Number of generations to run\n        early_stop : int, default=None\n            Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered. In general a value of around 5-20 is good.\n        early_stop_tol : float, list of floats, or None, default=0.001\n            -list of floats\n                list of tolerances for each objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n                If an index of the list is None, that item will not be used for early stopping\n            -int\n                If an int is given, it will be used as the tolerance for all objectives\n        max_time_mins : float, default=float(\"inf\")\n            Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n        max_eval_time_mins : float, default=10\n            Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n        n_jobs : int, default=1\n            Number of processes to run in parallel.\n        memory_limit : str, default=None\n            Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n        client : dask.distributed.Client, default=None\n            A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.\n        survival_percentage : float, default=1\n            Percentage of the population size to utilize for mutation and crossover at the beginning of the generation. The rest are discarded. Individuals are selected with the selector passed into survival_selector. The value of this parameter must be between 0 and 1, inclusive.\n            For example, if the population size is 100 and the survival percentage is .5, 50 individuals will be selected with NSGA2 from the existing population. These will be used for mutation and crossover to generate the next 100 individuals for the next generation. The remainder are discarded from the live population. In the next generation, there will now be the 50 parents + the 100 individuals for a total of 150. Surivival percentage is based of the population size parameter and not the existing population size (current population size when using successive halving). Therefore, in the next generation we will still select 50 individuals from the currently existing 150.\n        crossover_probability : float, default=.2\n            Probability of generating a new individual by crossover between two individuals.\n        mutate_probability : float, default=.7\n            Probability of generating a new individual by crossover between one individuals.\n        mutate_then_crossover_probability : float, default=.05\n            Probability of generating a new individual by mutating two individuals followed by crossover.\n        crossover_then_mutate_probability : float, default=.05\n            Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.\n        n_parents : int, default=2\n            Number of parents to use for crossover. Must be greater than 1.\n        survival_selector : function, default=survival_select_NSGA2\n            Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes.\n            Used to selected population_size * survival_percentage individuals at the start of each generation to use for mutation and crossover.\n        parent_selector : function, default=parent_select_NSGA2\n            Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.\n        budget_range : list [start, end], default=None\n            This parameter is used for the successive halving algorithm.\n            A starting and ending budget to use for the budget scaling. The evolver will interpolate between these values over the generations_until_end_budget.\n            Use is dependent on the objective functions. (In TPOTEstimator this corresponds to the percentage of the data to sample.)\n        budget_scaling float : [0,1], default=0.5\n            A scaling factor to use when determining how fast we move the budget from the start to end budget.\n        generations_until_end_budget : int, default=1\n            The number of generations to run before reaching the max budget.\n        stepwise_steps : int, default=1\n            The number of staircase steps to take when interpolating the budget and population size.\n        threshold_evaluation_pruning : list [start, end], default=None\n            Starting and ending percentile to use as a threshold for the evaluation early stopping. The evolver will interpolate between these values over the evaluation_early_stop_steps.\n            Values between 0 and 100.\n            At each step of the evaluation, a threshold is calculated based on the previous evaluations. All individuals that are below the performance threshold are not evaluated for further steps.\n            For example, if the threshold is set to the 90th percentile of the previous evaluations, all individuals that are below the 90th percentile are not evaluated further. This can save computation by not evaluating all individuals for all steps of cross validation.\n        threshold_evaluation_scaling : float [0,inf), default=0.5\n            A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n            Must be greater than zero. Higher numbers will move the threshold to the end faster.\n        min_history_threshold : int, default=0\n            The minimum number of previous scores needed before using threshold early stopping.\n        selection_evaluation_pruning : list, default=None\n            A lower and upper percent of the population size to select each round of CV.\n            Values between 0 and 1.\n            Selects a percentage of the population to evaluate at each step of the evaluation. \n            For example, one strategy is to evaluate different steps of cross validation one at a time, and only select the best N individuals for subsequent steps. \n            This can save computation by not evaluating all individuals for all steps of cross validation. By default this selection is done with the NSGA2 selector.\n        selection_evaluation_scaling : float, default=0.5\n            A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n            Must be greater than zero. Higher numbers will move the threshold to the end faster.\n        evaluation_early_stop_steps : int, default=1\n            The number of steps that will be taken from the objective function. (e.g., the number of CV folds to evaluate)\n        final_score_strategy : str, default=\"mean\"\n            The strategy to use when determining the final score for an individual.\n            \"mean\": The mean of all objective scores\n            \"last\": The score returned by the last call. Currently each objective is evaluated with a clone of the individual.\n        verbose : int, default=0\n            How much information to print during the optimization process. Higher values include the information from lower values.\n            0. nothing\n            1. progress bar\n            2. evaluations progress bar\n            3. best individual\n            4. warnings\n            &gt;=5. full warnings trace\n        periodic_checkpoint_folder : str, default=None\n            Folder to save the population to periodically. If None, no periodic saving will be done.\n            If provided, training will resume from this checkpoint.\n        callback : tpot.CallBackInterface, default=None\n            Callback object. Not implemented\n        rng : Numpy.Random.Generator, None, default=None\n            An object for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes\n\n            - Numpy.Random.Generator\n                Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'. Note this will be the same Generator passed in.\n            - None\n                Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS\n\n        Attributes\n        ----------\n        population : tpot.Population\n            The population of individuals.\n            Use population.population to access the individuals in the current population.\n            Use population.evaluated_individuals to access a data frame of all individuals that have been explored.\n\n        \"\"\"\n\n        self.rng = np.random.default_rng(rng)\n\n        if threshold_evaluation_pruning is not None or selection_evaluation_pruning is not None:\n            if evaluation_early_stop_steps is None:\n                raise ValueError(\"evaluation_early_stop_steps must be set when using threshold_evaluation_pruning or selection_evaluation_pruning\")\n\n        self.individual_generator = individual_generator\n        self.population_size = population_size\n        self.objective_functions = objective_functions\n        self.objective_function_weights = np.array(objective_function_weights)\n        self.bigger_is_better = bigger_is_better\n        if not bigger_is_better:\n            self.objective_function_weights = np.array(self.objective_function_weights)*-1\n\n        self.initial_population_size = initial_population_size\n        if self.initial_population_size is None:\n            self.cur_population_size = population_size\n        else:\n            self.cur_population_size = initial_population_size\n\n        self.population_scaling = population_scaling\n        self.generations_until_end_population = generations_until_end_population\n\n        self.population_size_list = None\n\n\n        self.periodic_checkpoint_folder = periodic_checkpoint_folder\n        self.verbose  = verbose\n        self.callback = callback\n        self.generations = generations\n        self.n_jobs = n_jobs\n\n\n\n        if max_time_mins is None:\n            self.max_time_mins = float(\"inf\")\n        else:\n            self.max_time_mins = max_time_mins\n\n        #functools requires none for infinite time, doesn't support inf\n        if max_eval_time_mins is not None and math.isinf(max_eval_time_mins ):\n            self.max_eval_time_mins = None\n        else:\n            self.max_eval_time_mins = max_eval_time_mins\n\n\n\n\n        self.generation = 0\n\n\n        self.threshold_evaluation_pruning =threshold_evaluation_pruning\n        self.threshold_evaluation_scaling =  max(0.00001,threshold_evaluation_scaling )\n        self.min_history_threshold = min_history_threshold\n\n        self.selection_evaluation_pruning = selection_evaluation_pruning\n        self.selection_evaluation_scaling =  max(0.00001,selection_evaluation_scaling )\n        self.evaluation_early_stop_steps = evaluation_early_stop_steps\n        self.final_score_strategy = final_score_strategy\n\n        self.budget_range = budget_range\n        self.budget_scaling = budget_scaling\n        self.generations_until_end_budget = generations_until_end_budget\n        self.stepwise_steps = stepwise_steps\n\n        self.memory_limit = memory_limit\n\n        self.client = client\n\n\n        self.survival_selector=survival_selector\n        self.parent_selector=parent_selector\n        self.survival_percentage = survival_percentage\n\n        total_var_p = crossover_probability + mutate_probability + mutate_then_crossover_probability + crossover_then_mutate_probability\n        self.crossover_probability = crossover_probability / total_var_p\n        self.mutate_probability = mutate_probability  / total_var_p\n        self.mutate_then_crossover_probability= mutate_then_crossover_probability / total_var_p\n        self.crossover_then_mutate_probability= crossover_then_mutate_probability / total_var_p\n\n\n        self.mutation_functions = mutation_functions\n        self.crossover_functions = crossover_functions\n\n        if mutation_function_weights is None:\n            self.mutation_function_weights = [1 for _ in range(len(mutation_functions))]\n        else:\n            self.mutation_function_weights = mutation_function_weights\n\n        if mutation_function_weights is None:\n            self.crossover_function_weights = [1 for _ in range(len(mutation_functions))]\n        else:\n            self.crossover_function_weights = crossover_function_weights\n\n        self.n_parents = n_parents\n\n        if objective_kwargs is None:\n            self.objective_kwargs = {}\n        else:\n            self.objective_kwargs = objective_kwargs\n\n        # if objective_kwargs is None:\n        #     self.objective_kwargs = [{}] * len(self.objective_functions)\n        # elif isinstance(objective_kwargs, dict):\n        #     self.objective_kwargs = [objective_kwargs] * len(self.objective_functions)\n        # else:\n        #     self.objective_kwargs = objective_kwargs\n\n        ###########\n\n        if self.initial_population_size != self.population_size:\n            self.population_size_list = beta_interpolation(start=self.cur_population_size, end=self.population_size, scale=self.population_scaling, n=generations_until_end_population, n_steps=self.stepwise_steps)\n            self.population_size_list = np.round(self.population_size_list).astype(int)\n\n        if self.budget_range is None:\n            self.budget_list = None\n        else:\n            self.budget_list = beta_interpolation(start=self.budget_range[0], end=self.budget_range[1], n=self.generations_until_end_budget, scale=self.budget_scaling, n_steps=self.stepwise_steps)\n\n        if objective_names is None:\n            self.objective_names = [\"objective\"+str(i) for i in range(len(objective_function_weights))]\n        else:\n            self.objective_names = objective_names\n\n        if self.budget_list is not None:\n            if len(self.budget_list) &lt;= self.generation:\n                self.budget = self.budget_list[-1]\n            else:\n                self.budget = self.budget_list[self.generation]\n        else:\n            self.budget = None\n\n\n        self.early_stop_tol = early_stop_tol\n        self.early_stop = early_stop\n\n        if isinstance(self.early_stop_tol, float):\n            self.early_stop_tol = [self.early_stop_tol for _ in range(len(self.objective_names))]\n\n        self.early_stop_tol = [np.inf if tol is None else tol for tol in self.early_stop_tol]\n\n        self.population = None\n        self.population_file = None\n        if self.periodic_checkpoint_folder is not None:\n            self.population_file = os.path.join(self.periodic_checkpoint_folder, \"population.pkl\")\n            if not os.path.exists(self.periodic_checkpoint_folder):\n                os.makedirs(self.periodic_checkpoint_folder)\n            if os.path.exists(self.population_file):\n                self.population = pickle.load(open(self.population_file, \"rb\"))\n\n                if len(self.population.evaluated_individuals)&gt;0 and \"Generation\" in self.population.evaluated_individuals.columns:\n                    self.generation = self.population.evaluated_individuals['Generation'].max() + 1 #TODO check if this is empty?\n\n        init_names = self.objective_names\n        if self.budget_range is not None:\n            init_names = init_names + [\"Budget\"]\n        if self.population is None:\n            self.population = tpot.Population(column_names=init_names)\n            initial_population = [next(self.individual_generator) for _ in range(self.cur_population_size)]\n            self.population.add_to_population(initial_population, self.rng)\n            self.population.update_column(self.population.population, column_names=\"Generation\", data=self.generation)\n\n\n    def optimize(self, generations=None):\n        \"\"\"\n        Creates an initial population and runs the evolutionary algorithm for the given number of generations. \n        If generations is None, will use self.generations.\n\n        Parameters\n        ----------\n        generations : int, default=None\n            Number of generations to run. If None, will use self.generations.\n\n        \"\"\"\n\n        if self.client is not None: #If user passed in a client manually\n           self._client = self.client\n        else:\n\n            if self.verbose &gt;= 4:\n                silence_logs = 30\n            elif self.verbose &gt;=5:\n                silence_logs = 40\n            else:\n                silence_logs = 50\n            self._cluster = LocalCluster(n_workers=self.n_jobs, #if no client is passed in and no global client exists, create our own\n                    threads_per_worker=1,\n                    silence_logs=silence_logs,\n                    processes=True,\n                    memory_limit=self.memory_limit)\n            self._client = Client(self._cluster)\n\n\n\n        if generations is None:\n            generations = self.generations\n\n        start_time = time.time()\n\n        generations_without_improvement = np.array([0 for _ in range(len(self.objective_function_weights))])\n        best_scores = [-np.inf for _ in range(len(self.objective_function_weights))]\n\n\n        self.scheduled_timeout_time = time.time() + self.max_time_mins*60\n\n\n        try:\n            #for gen in tnrange(generations,desc=\"Generation\", disable=self.verbose&lt;1):\n            done = False\n            gen = 0\n            if self.verbose &gt;= 1:\n                if generations is None or np.isinf(generations):\n                    pbar = tqdm.tqdm(total=0)\n                else:\n                    pbar = tqdm.tqdm(total=generations)\n                pbar.set_description(\"Generation\")\n            while not done:\n                # Generation 0 is the initial population\n                if self.generation == 0:\n                    if self.population_file is not None:\n                        pickle.dump(self.population, open(self.population_file, \"wb\"))\n                    self.evaluate_population()\n                    if self.population_file is not None:\n                        pickle.dump(self.population, open(self.population_file, \"wb\"))\n\n                    attempts = 2\n                    while len(self.population.population) == 0 and attempts &gt; 0:\n                        new_initial_population = [next(self.individual_generator) for _ in range(self.cur_population_size)]\n                        self.population.add_to_population(new_initial_population, rng=self.rng)\n                        attempts -= 1\n                        self.evaluate_population()\n\n                    if len(self.population.population) == 0:\n                        raise Exception(\"No individuals could be evaluated in the initial population. This may indicate a bug in the configuration, included models, or objective functions. Set verbose&gt;=4 to see the errors that caused individuals to fail.\")\n\n                    self.generation += 1\n                # Generation 1 is the first generation after the initial population\n                else:\n                    if time.time() - start_time &gt; self.max_time_mins*60:\n                        if self.population.evaluated_individuals[self.objective_names].isnull().all().iloc[0]:\n                            raise Exception(\"No individuals could be evaluated in the initial population as the max_eval_mins time limit was reached before any individuals could be evaluated.\")\n                        break\n                    self.step()\n\n                if self.verbose &gt;= 3:\n                    sign = np.sign(self.objective_function_weights)\n                    valid_df = self.population.evaluated_individuals[~self.population.evaluated_individuals[self.objective_names].isin([\"TIMEOUT\",\"INVALID\"]).any(axis=1)][self.objective_names]*sign\n                    cur_best_scores = valid_df.max(axis=0)*sign\n                    cur_best_scores = cur_best_scores.to_numpy()\n                    print(\"Generation: \", self.generation)\n                    for i, obj in enumerate(self.objective_names):\n                        print(f\"Best {obj} score: {cur_best_scores[i]}\")\n\n\n                if self.early_stop:\n                    if self.budget is None or self.budget&gt;=self.budget_range[-1]: #self.budget&gt;=1:\n                        #get sign of objective_function_weights\n                        sign = np.sign(self.objective_function_weights)\n                        #get best score for each objective\n                        valid_df = self.population.evaluated_individuals[~self.population.evaluated_individuals[self.objective_names].isin([\"TIMEOUT\",\"INVALID\"]).any(axis=1)][self.objective_names]*sign\n                        cur_best_scores = valid_df.max(axis=0)\n                        cur_best_scores = cur_best_scores.to_numpy()\n                        #cur_best_scores =  self.population.get_column(self.population.population, column_names=self.objective_names).max(axis=0)*sign #TODO this assumes the current population is the best\n\n                        improved = ( np.array(cur_best_scores) - np.array(best_scores) &gt;= np.array(self.early_stop_tol) )\n                        not_improved = np.logical_not(improved)\n                        generations_without_improvement = generations_without_improvement * not_improved + not_improved #set to zero if not improved, else increment\n                        pass\n                        #update best score\n                        best_scores = [max(best_scores[i], cur_best_scores[i]) for i in range(len(self.objective_names))]\n\n                        if all(generations_without_improvement&gt;self.early_stop):\n                            if self.verbose &gt;= 3:\n                                print(\"Early stop\")\n                            break\n\n                #save population\n                if self.population_file is not None: # and time.time() - last_save_time &gt; 60*10:\n                    pickle.dump(self.population, open(self.population_file, \"wb\"))\n\n                gen += 1\n                if self.verbose &gt;= 1:\n                    pbar.update(1)\n\n                if generations is not None and gen &gt;= generations:\n                    done = True\n\n        except KeyboardInterrupt:\n            if self.verbose &gt;= 3:\n                print(\"KeyboardInterrupt\")\n            self.population.remove_invalid_from_population(column_names=self.objective_names, invalid_value=\"INVALID\")\n            self.population.remove_invalid_from_population(column_names=self.objective_names, invalid_value=\"TIMEOUT\")\n            self.population.remove_invalid_from_population(column_names=\"Eval Error\", invalid_value=\"INVALID\")\n            self.population.remove_invalid_from_population(column_names=\"Eval Error\", invalid_value=\"TIMEOUT\")\n\n\n\n\n        if self.population_file is not None:\n            pickle.dump(self.population, open(self.population_file, \"wb\"))\n\n        if self.client is None: #If we created our own client, close it\n            self._client.close()\n            self._cluster.close()\n\n        tpot.utils.get_pareto_frontier(self.population.evaluated_individuals, column_names=self.objective_names, weights=self.objective_function_weights)\n\n    def step(self,):\n        \"\"\"\n        Runs a single generation of the evolutionary algorithm. This includes selecting individuals for survival, generating offspring, and evaluating the offspring.\n\n        \"\"\"\n\n\n        if self.population_size_list is not None:\n            if self.generation &lt; len(self.population_size_list):\n                self.cur_population_size = self.population_size_list[self.generation]\n            else:\n                self.cur_population_size = self.population_size\n\n        if self.budget_list is not None:\n            if len(self.budget_list) &lt;= self.generation:\n                self.budget = self.budget_range[-1]\n            else:\n                self.budget = self.budget_list[self.generation]\n        else:\n            self.budget = None\n\n        if self.survival_selector is not None:\n            n_survivors = max(1,int(self.cur_population_size*self.survival_percentage)) #always keep at least one individual\n            self.population.survival_select(    selector=self.survival_selector,\n                                                weights=self.objective_function_weights,\n                                                columns_names=self.objective_names,\n                                                n_survivors=n_survivors,\n                                                inplace=True,\n                                                rng=self.rng,)\n\n        self.generate_offspring()\n        self.evaluate_population()\n\n        self.generation += 1\n\n    def generate_offspring(self, ): #your EA Algorithm goes here\n        \"\"\"\n        Create population_size new individuals from the current population. \n        This includes selecting parents, applying mutation and crossover, and adding the new individuals to the population.\n        \"\"\"\n        parents = self.population.parent_select(selector=self.parent_selector, weights=self.objective_function_weights, columns_names=self.objective_names, k=self.cur_population_size, n_parents=2, rng=self.rng)\n        p = np.array([self.crossover_probability, self.mutate_then_crossover_probability, self.crossover_then_mutate_probability, self.mutate_probability])\n        p = p / p.sum()\n        var_op_list = self.rng.choice([\"crossover\", \"mutate_then_crossover\", \"crossover_then_mutate\", \"mutate\"], size=self.cur_population_size, p=p)\n\n        for i, op in enumerate(var_op_list):\n            if op == \"mutate\":\n                parents[i] = parents[i][0] #mutations take a single individual\n\n        offspring = self.population.create_offspring2(parents, var_op_list, self.mutation_functions, self.mutation_function_weights, self.crossover_functions, self.crossover_function_weights, add_to_population=True, keep_repeats=False, mutate_until_unique=True, rng=self.rng)\n\n        self.population.update_column(offspring, column_names=\"Generation\", data=self.generation, )\n\n    # Gets a list of unevaluated individuals in the livepopulation, evaluates them, and removes failed attempts\n    # TODO This could probably be an independent function?\n    def evaluate_population(self,):\n        \"\"\"\n        Evaluates the individuals in the population that have not been evaluated yet.\n        \"\"\"\n        #Update the sliding scales and thresholds\n        # Save population, TODO remove some of these\n        if self.population_file is not None: # and time.time() - last_save_time &gt; 60*10:\n            pickle.dump(self.population, open(self.population_file, \"wb\"))\n            last_save_time = time.time()\n\n\n        #Get the current thresholds per step\n        self.thresholds = None\n        if self.threshold_evaluation_pruning is not None:\n            old_data = self.population.evaluated_individuals[self.objective_names]\n            old_data = old_data[old_data[self.objective_names].notnull().all(axis=1)]\n            if len(old_data) &gt;= self.min_history_threshold:\n                self.thresholds = np.array([get_thresholds(old_data[obj_name],\n                                                            start=self.threshold_evaluation_pruning[0],\n                                                            end=self.threshold_evaluation_pruning[1],\n                                                            scale=self.threshold_evaluation_scaling,\n                                                            n=self.evaluation_early_stop_steps)\n                                        for obj_name in self.objective_names]).T\n\n        #Get the selectors survival rates per step\n        if self.selection_evaluation_pruning is not None:\n            lower = self.cur_population_size*self.selection_evaluation_pruning[0]\n            upper = self.cur_population_size*self.selection_evaluation_pruning[1]\n            #survival_counts = self.cur_population_size*(scipy.special.betainc(1,self.selection_evaluation_scaling,np.linspace(0,1,self.evaluation_early_stop_steps))*(upper-lower)+lower)\n\n            survival_counts = np.array(beta_interpolation(start=lower, end=upper, scale=self.selection_evaluation_scaling, n=self.evaluation_early_stop_steps, n_steps=self.evaluation_early_stop_steps))\n            self.survival_counts = survival_counts.astype(int)\n        else:\n            self.survival_counts = None\n\n\n\n        if self.evaluation_early_stop_steps is not None:\n            if self.survival_counts is None:\n                #TODO if we are not using selection method for each step, we can create single threads that run all steps for an individual. No need to come back each step.\n                self.evaluate_population_selection_early_stop(survival_counts=self.survival_counts, thresholds=self.thresholds, budget=self.budget)\n            else:\n                #parallelize one step at a time. After each step, come together and select the next individuals to run the next step on.\n                self.evaluate_population_selection_early_stop(survival_counts=self.survival_counts, thresholds=self.thresholds, budget=self.budget)\n        else:\n            self.evaluate_population_full(budget=self.budget)\n\n\n        # Save population, TODO remove some of these\n        if self.population_file is not None: # and time.time() - last_save_time &gt; 60*10:\n            pickle.dump(self.population, open(self.population_file, \"wb\"))\n            last_save_time = time.time()\n\n    def evaluate_population_full(self, budget=None):\n        \"\"\"\n        Evaluates all individuals in the population that have not been evaluated yet.\n        This is the normal/default strategy for evaluating individuals without any early stopping of individual evaluation functions. (e.g., no threshold or selection early stopping). Early stopping by generation is still possible.\n        \"\"\"\n        individuals_to_evaluate = self.get_unevaluated_individuals(self.objective_names, budget=budget,)\n\n        #print(\"evaluating this many individuals: \", len(individuals_to_evaluate))\n\n        if len(individuals_to_evaluate) == 0:\n            if self.verbose &gt; 3:\n                print(\"No new individuals to evaluate\")\n            return\n\n        if self.max_eval_time_mins is not None:\n            theoretical_timeout = self.max_eval_time_mins * math.ceil(len(individuals_to_evaluate) / self.n_jobs)\n            theoretical_timeout = theoretical_timeout*2\n        else:\n            theoretical_timeout = np.inf\n        scheduled_timeout_time_left = self.scheduled_timeout_time - time.time()\n        parallel_timeout = min(theoretical_timeout, scheduled_timeout_time_left)\n        if parallel_timeout &lt; 0:\n            parallel_timeout = 10\n\n        scores, start_times, end_times, eval_errors = tpot.utils.eval_utils.parallel_eval_objective_list(individuals_to_evaluate, self.objective_functions, verbose=self.verbose, max_eval_time_mins=self.max_eval_time_mins, budget=budget, n_expected_columns=len(self.objective_names), client=self._client, scheduled_timeout_time=self.scheduled_timeout_time, **self.objective_kwargs)\n\n        self.population.update_column(individuals_to_evaluate, column_names=self.objective_names, data=scores)\n        if budget is not None:\n            self.population.update_column(individuals_to_evaluate, column_names=\"Budget\", data=budget)\n\n        self.population.update_column(individuals_to_evaluate, column_names=\"Submitted Timestamp\", data=start_times)\n        self.population.update_column(individuals_to_evaluate, column_names=\"Completed Timestamp\", data=end_times)\n        self.population.update_column(individuals_to_evaluate, column_names=\"Eval Error\", data=eval_errors)\n        self.population.remove_invalid_from_population(column_names=\"Eval Error\")\n        self.population.remove_invalid_from_population(column_names=\"Eval Error\", invalid_value=\"TIMEOUT\")\n\n    def get_unevaluated_individuals(self, column_names, budget=None, individual_list=None):\n        \"\"\"\n        This function is used to get a list of individuals in the current population that have not been evaluated yet.\n\n        Parameters\n        ----------\n        column_names : list of strings\n            Names of the columns to check for unevaluated individuals (generally objective functions).\n        budget : float, default=None\n            Budget to use when checking for unevaluated individuals. If None, will not check the budget column.\n            Finds individuals who have not been evaluated with the given budget on column names.\n        individual_list : list of individuals, default=None\n            List of individuals to check for unevaluated individuals. If None, will use the current population.\n        \"\"\"\n        if individual_list is not None:\n            cur_pop = np.array(individual_list)\n        else:\n            cur_pop = np.array(self.population.population)\n\n        if all([name_step in self.population.evaluated_individuals.columns for name_step in column_names]):\n            if budget is not None:\n                offspring_scores = self.population.get_column(cur_pop, column_names=column_names+[\"Budget\"], to_numpy=False)\n                #Individuals are unevaluated if we have a higher budget OR if any of the objectives are nan\n                unevaluated_filter = lambda i: any(offspring_scores.loc[offspring_scores.index[i]][column_names].isna()) or (offspring_scores.loc[offspring_scores.index[i]][\"Budget\"] &lt; budget)\n            else:\n                offspring_scores = self.population.get_column(cur_pop, column_names=column_names, to_numpy=False)\n                unevaluated_filter = lambda i: any(offspring_scores.loc[offspring_scores.index[i]][column_names].isna())\n            unevaluated_individuals_this_step = [i for i in range(len(cur_pop)) if unevaluated_filter(i)]\n            return cur_pop[unevaluated_individuals_this_step]\n\n        else: #if column names are not in the evaluated_individuals, then we have not evaluated any individuals yet\n            for name_step in column_names:\n                self.population.evaluated_individuals[name_step] = np.nan\n            return cur_pop\n\n    def evaluate_population_selection_early_stop(self,survival_counts, thresholds=None, budget=None):\n        \"\"\"\n        This function tries to save computation by partially evaluating the individuals and then selecting which individuals to evaluate further based on the results of the partial evaluation. \n\n        Two strategies are implemented:\n            1.  Selection early stopping: Selects a percentage of the population to evaluate at each step of the evaluation. \n                for example, one strategy is to evaluate different steps of cross validation one at a time, and only select the best N individuals for subsequent steps. \n                This can save computation by not evaluating all individuals for all steps of cross validation. By default this selection is done with the NSGA2 selector.\n            2.  Threshold early stopping: At each step of the evaluation, a threshold is calculated based on the previous evaluations. All individuals that are below the performance threshold are not evaluated for further steps.\n                For example, if the threshold is set to the 90th percentile of the previous evaluations, all individuals that are below the 90th percentile are not evaluated further. This can save computation by not evaluating all individuals for all steps of cross validation.\n\n        Both of these strategies can be used simultaneously. Individuals must pass both the selection and threshold criteria to be evaluated further.\n\n        Parameters\n        ----------\n        survival_counts : list of ints, default=None\n            Number of individuals to select for survival at each step of the evaluation. If None, will not use selection early stopping.\n            For example: [10, 5, 2] would select 10 individuals for the first step, 5 for the second, and 2 for the third.\n        thresholds : list of floats, default=None\n            Thresholds to use for early stopping at each step of the evaluation. If None, will not use threshold early stopping.\n        budget : float, default=None\n            Budget to use when evaluating individuals. Use is dependent on the objective functions. (In TPOTEstimator this corresponds to the percentage of the data to sample.)\n        \"\"\"\n\n        survival_selector = tpot.selectors.survival_select_NSGA2\n\n        ################\n\n        objective_function_signs = np.sign(self.objective_function_weights)\n\n\n        cur_individuals = self.population.population.copy()\n\n        all_step_names = []\n        for step in range(self.evaluation_early_stop_steps):\n            if budget is None:\n                this_step_names = [f\"{n}_step_{step}\" for n in self.objective_names]\n            else:\n                this_step_names = [f\"{n}_budget_{budget}_step_{step}\" for n in self.objective_names]\n\n            all_step_names.append(this_step_names)\n\n            unevaluated_individuals_this_step = self.get_unevaluated_individuals(this_step_names, budget=None, individual_list=cur_individuals)\n\n            if len(unevaluated_individuals_this_step) == 0:\n                if self.verbose &gt; 3:\n                    print(\"No new individuals to evaluate\")\n                continue\n\n            if self.max_eval_time_mins is not None:\n                theoretical_timeout = self.max_eval_time_mins * math.ceil(len(unevaluated_individuals_this_step) / self.n_jobs)*60\n                theoretical_timeout = theoretical_timeout*2\n            else:\n                theoretical_timeout = np.inf\n            scheduled_timeout_time_left = self.scheduled_timeout_time - time.time()\n            parallel_timeout = min(theoretical_timeout, scheduled_timeout_time_left)\n            if parallel_timeout &lt; 0:\n                parallel_timeout = 10\n\n            scores, start_times, end_times, eval_errors = tpot.utils.eval_utils.parallel_eval_objective_list(individual_list=unevaluated_individuals_this_step,\n                                    objective_list=self.objective_functions,\n                                    verbose=self.verbose,\n                                    max_eval_time_mins=self.max_eval_time_mins,\n                                    step=step,\n                                    budget = self.budget,\n                                    generation = self.generation,\n                                    n_expected_columns=len(self.objective_names),\n                                    client=self._client,\n                                    scheduled_timeout_time=self.scheduled_timeout_time,\n                                    **self.objective_kwargs,\n                                    )\n\n            self.population.update_column(unevaluated_individuals_this_step, column_names=this_step_names, data=scores)\n            self.population.update_column(unevaluated_individuals_this_step, column_names=\"Submitted Timestamp\", data=start_times)\n            self.population.update_column(unevaluated_individuals_this_step, column_names=\"Completed Timestamp\", data=end_times)\n            self.population.update_column(unevaluated_individuals_this_step, column_names=\"Eval Error\", data=eval_errors)\n\n            self.population.remove_invalid_from_population(column_names=\"Eval Error\")\n            self.population.remove_invalid_from_population(column_names=\"Eval Error\", invalid_value=\"TIMEOUT\")\n\n            #remove invalids:\n            invalids = []\n            #find indeces of invalids\n\n            for j in range(len(scores)):\n                if  any([s==\"INVALID\" for s in scores[j]]):\n                    invalids.append(j)\n\n            for j in range(len(scores)):\n                if  any([s==\"TIMEOUT\" for s in scores[j]]):\n                    invalids.append(j)\n\n\n            #already evaluated\n            already_evaluated = list(set(cur_individuals) - set(unevaluated_individuals_this_step))\n            #evaluated and valid\n            valid_evaluations_this_step = remove_items(unevaluated_individuals_this_step,invalids)\n            #update cur_individuals with current individuals with valid scores\n            cur_individuals = np.concatenate([already_evaluated, valid_evaluations_this_step])\n\n\n            #Get average scores\n\n            #array of shape (steps, individuals, objectives)\n            offspring_scores = [self.population.get_column(cur_individuals, column_names=step_names) for step_names in all_step_names]\n            offspring_scores = np.array(offspring_scores)\n            if self.final_score_strategy == 'mean':\n                offspring_scores  = offspring_scores.mean(axis=0)\n            elif self.final_score_strategy == 'last':\n                offspring_scores = offspring_scores[-1]\n\n            #remove individuals with nan scores\n            invalids = []\n            for i in range(len(offspring_scores)):\n                if any(np.isnan(offspring_scores[i])):\n                    invalids.append(i)\n\n            cur_individuals = remove_items(cur_individuals,invalids)\n            offspring_scores = remove_items(offspring_scores,invalids)\n\n            #if last step, add the final metrics\n            if step == self.evaluation_early_stop_steps-1:\n                self.population.update_column(cur_individuals, column_names=self.objective_names, data=offspring_scores)\n                if budget is not None:\n                    self.population.update_column(cur_individuals, column_names=\"Budget\", data=budget)\n                return\n\n            #If we have more threads than remaining individuals, we may as well evaluate the extras too\n            if self.n_jobs &lt; len(cur_individuals):\n                #Remove based on thresholds\n                if thresholds is not None:\n                    threshold = thresholds[step]\n                    invalids = []\n                    for i in range(len(offspring_scores)):\n\n                        if all([s*w&gt;t*w for s,t,w in zip(offspring_scores[i],threshold,objective_function_signs)  ]):\n                            invalids.append(i)\n\n                    if len(invalids) &gt; 0:\n\n                        max_to_remove = min(len(cur_individuals) - self.n_jobs, len(invalids))\n\n                        if max_to_remove &lt; len(invalids):\n                            # invalids = np.random.choice(invalids, max_to_remove, replace=False)\n                            invalids = self.rng.choice(invalids, max_to_remove, replace=False)\n\n                        cur_individuals = remove_items(cur_individuals,invalids)\n                        offspring_scores = remove_items(offspring_scores,invalids)\n\n                # Remove based on selection\n                if survival_counts is not None:\n                    if step &lt; self.evaluation_early_stop_steps - 1 and survival_counts[step]&gt;1: #don't do selection for the last loop since they are completed\n                        k = survival_counts[step] + len(invalids) #TODO can remove the min if the selections method can ignore k&gt;population size\n                        if len(cur_individuals)&gt; 1 and k &gt; self.n_jobs and k &lt; len(cur_individuals):\n                            weighted_scores = np.array([s * self.objective_function_weights for s in offspring_scores ])\n\n                            new_population_index = survival_selector(weighted_scores, k=k)\n                            cur_individuals = np.array(cur_individuals)[new_population_index]\n                            offspring_scores = offspring_scores[new_population_index]\n</code></pre>"},{"location":"documentation/tpot/evolvers/base_evolver/#tpot.evolvers.base_evolver.BaseEvolver.__init__","title":"<code>__init__(individual_generator, objective_functions, objective_function_weights, objective_names=None, objective_kwargs=None, bigger_is_better=True, population_size=50, initial_population_size=None, population_scaling=0.5, generations_until_end_population=1, generations=50, early_stop=None, early_stop_tol=0.001, max_time_mins=float('inf'), max_eval_time_mins=5, n_jobs=1, memory_limit='4GB', client=None, survival_percentage=1, crossover_probability=0.2, mutate_probability=0.7, mutate_then_crossover_probability=0.05, crossover_then_mutate_probability=0.05, mutation_functions=[ind_mutate], crossover_functions=[ind_crossover], mutation_function_weights=None, crossover_function_weights=None, n_parents=2, survival_selector=survival_select_NSGA2, parent_selector=tournament_selection_dominated, budget_range=None, budget_scaling=0.5, generations_until_end_budget=1, stepwise_steps=5, threshold_evaluation_pruning=None, threshold_evaluation_scaling=0.5, min_history_threshold=20, selection_evaluation_pruning=None, selection_evaluation_scaling=0.5, evaluation_early_stop_steps=None, final_score_strategy='mean', verbose=0, periodic_checkpoint_folder=None, callback=None, rng=None)</code>","text":"<p>Uses mutation, crossover, and optimization functions to evolve a population of individuals towards the given objective functions.</p> <p>Parameters:</p> Name Type Description Default <code>individual_generator</code> <code>generator</code> <p>Generator that yields new base individuals. Used to generate initial population.</p> required <code>objective_functions</code> <code>list of callables</code> <p>list of functions that get applied to the individual and return a float or list of floats If an objective function returns multiple values, they are all concatenated in order with respect to objective_function_weights and early_stop_tol.</p> required <code>objective_function_weights</code> <code>list of floats</code> <p>list of weights for each objective function. Sign flips whether bigger is better or not</p> required <code>objective_names</code> <code>list of strings</code> <p>Names of the objectives. If None, objective0, objective1, etc. will be used</p> <code>None</code> <code>objective_kwargs</code> <code>dict</code> <p>Dictionary of keyword arguments to pass to the objective function</p> <code>None</code> <code>bigger_is_better</code> <code>bool</code> <p>If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.</p> <code>True</code> <code>population_size</code> <code>int</code> <p>Size of the population</p> <code>50</code> <code>initial_population_size</code> <code>int</code> <p>Size of the initial population. If None, population_size will be used.</p> <code>None</code> <code>population_scaling</code> <code>int</code> <p>Scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.</p> <code>0.5</code> <code>generations_until_end_population</code> <code>int</code> <p>Number of generations until the population size reaches population_size</p> <code>1</code> <code>generations</code> <code>int</code> <p>Number of generations to run</p> <code>50</code> <code>early_stop</code> <code>int</code> <p>Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered. In general a value of around 5-20 is good.</p> <code>None</code> <code>early_stop_tol</code> <code>float, list of floats, or None</code> <p>-list of floats     list of tolerances for each objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged     If an index of the list is None, that item will not be used for early stopping -int     If an int is given, it will be used as the tolerance for all objectives</p> <code>0.001</code> <code>max_time_mins</code> <code>float</code> <p>Maximum time to run the optimization. If none or inf, will run until the end of the generations.</p> <code>float(\"inf\")</code> <code>max_eval_time_mins</code> <code>float</code> <p>Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.</p> <code>10</code> <code>n_jobs</code> <code>int</code> <p>Number of processes to run in parallel.</p> <code>1</code> <code>memory_limit</code> <code>str</code> <p>Memory limit for each job. See Dask LocalCluster documentation for more information.</p> <code>None</code> <code>client</code> <code>Client</code> <p>A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.</p> <code>None</code> <code>survival_percentage</code> <code>float</code> <p>Percentage of the population size to utilize for mutation and crossover at the beginning of the generation. The rest are discarded. Individuals are selected with the selector passed into survival_selector. The value of this parameter must be between 0 and 1, inclusive. For example, if the population size is 100 and the survival percentage is .5, 50 individuals will be selected with NSGA2 from the existing population. These will be used for mutation and crossover to generate the next 100 individuals for the next generation. The remainder are discarded from the live population. In the next generation, there will now be the 50 parents + the 100 individuals for a total of 150. Surivival percentage is based of the population size parameter and not the existing population size (current population size when using successive halving). Therefore, in the next generation we will still select 50 individuals from the currently existing 150.</p> <code>1</code> <code>crossover_probability</code> <code>float</code> <p>Probability of generating a new individual by crossover between two individuals.</p> <code>.2</code> <code>mutate_probability</code> <code>float</code> <p>Probability of generating a new individual by crossover between one individuals.</p> <code>.7</code> <code>mutate_then_crossover_probability</code> <code>float</code> <p>Probability of generating a new individual by mutating two individuals followed by crossover.</p> <code>.05</code> <code>crossover_then_mutate_probability</code> <code>float</code> <p>Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.</p> <code>.05</code> <code>n_parents</code> <code>int</code> <p>Number of parents to use for crossover. Must be greater than 1.</p> <code>2</code> <code>survival_selector</code> <code>function</code> <p>Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes. Used to selected population_size * survival_percentage individuals at the start of each generation to use for mutation and crossover.</p> <code>survival_select_NSGA2</code> <code>parent_selector</code> <code>function</code> <p>Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.</p> <code>parent_select_NSGA2</code> <code>budget_range</code> <code>list[start, end]</code> <p>This parameter is used for the successive halving algorithm. A starting and ending budget to use for the budget scaling. The evolver will interpolate between these values over the generations_until_end_budget. Use is dependent on the objective functions. (In TPOTEstimator this corresponds to the percentage of the data to sample.)</p> <code>None</code> <code>budget_scaling</code> <p>A scaling factor to use when determining how fast we move the budget from the start to end budget.</p> <code>0.5</code> <code>generations_until_end_budget</code> <code>int</code> <p>The number of generations to run before reaching the max budget.</p> <code>1</code> <code>stepwise_steps</code> <code>int</code> <p>The number of staircase steps to take when interpolating the budget and population size.</p> <code>1</code> <code>threshold_evaluation_pruning</code> <code>list[start, end]</code> <p>Starting and ending percentile to use as a threshold for the evaluation early stopping. The evolver will interpolate between these values over the evaluation_early_stop_steps. Values between 0 and 100. At each step of the evaluation, a threshold is calculated based on the previous evaluations. All individuals that are below the performance threshold are not evaluated for further steps. For example, if the threshold is set to the 90th percentile of the previous evaluations, all individuals that are below the 90th percentile are not evaluated further. This can save computation by not evaluating all individuals for all steps of cross validation.</p> <code>None</code> <code>threshold_evaluation_scaling</code> <code>float [0,inf)</code> <p>A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile. Must be greater than zero. Higher numbers will move the threshold to the end faster.</p> <code>0.5</code> <code>min_history_threshold</code> <code>int</code> <p>The minimum number of previous scores needed before using threshold early stopping.</p> <code>0</code> <code>selection_evaluation_pruning</code> <code>list</code> <p>A lower and upper percent of the population size to select each round of CV. Values between 0 and 1. Selects a percentage of the population to evaluate at each step of the evaluation.  For example, one strategy is to evaluate different steps of cross validation one at a time, and only select the best N individuals for subsequent steps.  This can save computation by not evaluating all individuals for all steps of cross validation. By default this selection is done with the NSGA2 selector.</p> <code>None</code> <code>selection_evaluation_scaling</code> <code>float</code> <p>A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile. Must be greater than zero. Higher numbers will move the threshold to the end faster.</p> <code>0.5</code> <code>evaluation_early_stop_steps</code> <code>int</code> <p>The number of steps that will be taken from the objective function. (e.g., the number of CV folds to evaluate)</p> <code>1</code> <code>final_score_strategy</code> <code>str</code> <p>The strategy to use when determining the final score for an individual. \"mean\": The mean of all objective scores \"last\": The score returned by the last call. Currently each objective is evaluated with a clone of the individual.</p> <code>\"mean\"</code> <code>verbose</code> <code>int</code> <p>How much information to print during the optimization process. Higher values include the information from lower values. 0. nothing 1. progress bar 2. evaluations progress bar 3. best individual 4. warnings</p> <p>=5. full warnings trace</p> <code>0</code> <code>periodic_checkpoint_folder</code> <code>str</code> <p>Folder to save the population to periodically. If None, no periodic saving will be done. If provided, training will resume from this checkpoint.</p> <code>None</code> <code>callback</code> <code>CallBackInterface</code> <p>Callback object. Not implemented</p> <code>None</code> <code>rng</code> <code>(Generator, None)</code> <p>An object for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes</p> <ul> <li>Numpy.Random.Generator     Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'. Note this will be the same Generator passed in.</li> <li>None     Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS</li> </ul> <code>None</code> <p>Attributes:</p> Name Type Description <code>population</code> <code>Population</code> <p>The population of individuals. Use population.population to access the individuals in the current population. Use population.evaluated_individuals to access a data frame of all individuals that have been explored.</p> Source code in <code>tpot/evolvers/base_evolver.py</code> <pre><code>def __init__(   self,\n                individual_generator ,\n\n                objective_functions,\n                objective_function_weights,\n                objective_names = None,\n                objective_kwargs = None,\n                bigger_is_better = True,\n\n                population_size = 50,\n                initial_population_size = None,\n                population_scaling = .5,\n                generations_until_end_population = 1,\n                generations = 50,\n                early_stop = None,\n                early_stop_tol = 0.001,\n\n\n                max_time_mins=float(\"inf\"),\n                max_eval_time_mins=5,\n\n                n_jobs=1,\n                memory_limit=\"4GB\",\n                client=None,\n\n                survival_percentage = 1,\n                crossover_probability=.2,\n                mutate_probability=.7,\n                mutate_then_crossover_probability=.05,\n                crossover_then_mutate_probability=.05,\n\n                mutation_functions = [ind_mutate],\n                crossover_functions = [ind_crossover],\n\n                mutation_function_weights = None,\n                crossover_function_weights = None,\n\n                n_parents=2,\n\n                survival_selector = survival_select_NSGA2,\n                parent_selector = tournament_selection_dominated,\n\n                budget_range = None,\n                budget_scaling = .5,\n                generations_until_end_budget = 1,\n                stepwise_steps = 5,\n\n                threshold_evaluation_pruning = None,\n                threshold_evaluation_scaling = .5,\n                min_history_threshold = 20,\n                selection_evaluation_pruning = None,\n                selection_evaluation_scaling = .5,\n                evaluation_early_stop_steps = None,\n                final_score_strategy = \"mean\",\n\n                verbose = 0,\n                periodic_checkpoint_folder = None,\n                callback = None,\n                rng=None,\n\n                ) -&gt; None:\n    \"\"\"\n    Uses mutation, crossover, and optimization functions to evolve a population of individuals towards the given objective functions.\n\n    Parameters\n    ----------\n    individual_generator : generator\n        Generator that yields new base individuals. Used to generate initial population.\n    objective_functions : list of callables\n        list of functions that get applied to the individual and return a float or list of floats\n        If an objective function returns multiple values, they are all concatenated in order\n        with respect to objective_function_weights and early_stop_tol.\n    objective_function_weights : list of floats\n        list of weights for each objective function. Sign flips whether bigger is better or not\n    objective_names : list of strings, default=None\n        Names of the objectives. If None, objective0, objective1, etc. will be used\n    objective_kwargs : dict, default=None\n        Dictionary of keyword arguments to pass to the objective function\n    bigger_is_better : bool, default=True\n        If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n    population_size : int, default=50\n        Size of the population\n    initial_population_size : int, default=None\n        Size of the initial population. If None, population_size will be used.\n    population_scaling : int, default=0.5\n        Scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n    generations_until_end_population : int, default=1\n        Number of generations until the population size reaches population_size\n    generations : int, default=50\n        Number of generations to run\n    early_stop : int, default=None\n        Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered. In general a value of around 5-20 is good.\n    early_stop_tol : float, list of floats, or None, default=0.001\n        -list of floats\n            list of tolerances for each objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n            If an index of the list is None, that item will not be used for early stopping\n        -int\n            If an int is given, it will be used as the tolerance for all objectives\n    max_time_mins : float, default=float(\"inf\")\n        Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n    max_eval_time_mins : float, default=10\n        Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n    n_jobs : int, default=1\n        Number of processes to run in parallel.\n    memory_limit : str, default=None\n        Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n    client : dask.distributed.Client, default=None\n        A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.\n    survival_percentage : float, default=1\n        Percentage of the population size to utilize for mutation and crossover at the beginning of the generation. The rest are discarded. Individuals are selected with the selector passed into survival_selector. The value of this parameter must be between 0 and 1, inclusive.\n        For example, if the population size is 100 and the survival percentage is .5, 50 individuals will be selected with NSGA2 from the existing population. These will be used for mutation and crossover to generate the next 100 individuals for the next generation. The remainder are discarded from the live population. In the next generation, there will now be the 50 parents + the 100 individuals for a total of 150. Surivival percentage is based of the population size parameter and not the existing population size (current population size when using successive halving). Therefore, in the next generation we will still select 50 individuals from the currently existing 150.\n    crossover_probability : float, default=.2\n        Probability of generating a new individual by crossover between two individuals.\n    mutate_probability : float, default=.7\n        Probability of generating a new individual by crossover between one individuals.\n    mutate_then_crossover_probability : float, default=.05\n        Probability of generating a new individual by mutating two individuals followed by crossover.\n    crossover_then_mutate_probability : float, default=.05\n        Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.\n    n_parents : int, default=2\n        Number of parents to use for crossover. Must be greater than 1.\n    survival_selector : function, default=survival_select_NSGA2\n        Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes.\n        Used to selected population_size * survival_percentage individuals at the start of each generation to use for mutation and crossover.\n    parent_selector : function, default=parent_select_NSGA2\n        Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.\n    budget_range : list [start, end], default=None\n        This parameter is used for the successive halving algorithm.\n        A starting and ending budget to use for the budget scaling. The evolver will interpolate between these values over the generations_until_end_budget.\n        Use is dependent on the objective functions. (In TPOTEstimator this corresponds to the percentage of the data to sample.)\n    budget_scaling float : [0,1], default=0.5\n        A scaling factor to use when determining how fast we move the budget from the start to end budget.\n    generations_until_end_budget : int, default=1\n        The number of generations to run before reaching the max budget.\n    stepwise_steps : int, default=1\n        The number of staircase steps to take when interpolating the budget and population size.\n    threshold_evaluation_pruning : list [start, end], default=None\n        Starting and ending percentile to use as a threshold for the evaluation early stopping. The evolver will interpolate between these values over the evaluation_early_stop_steps.\n        Values between 0 and 100.\n        At each step of the evaluation, a threshold is calculated based on the previous evaluations. All individuals that are below the performance threshold are not evaluated for further steps.\n        For example, if the threshold is set to the 90th percentile of the previous evaluations, all individuals that are below the 90th percentile are not evaluated further. This can save computation by not evaluating all individuals for all steps of cross validation.\n    threshold_evaluation_scaling : float [0,inf), default=0.5\n        A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n        Must be greater than zero. Higher numbers will move the threshold to the end faster.\n    min_history_threshold : int, default=0\n        The minimum number of previous scores needed before using threshold early stopping.\n    selection_evaluation_pruning : list, default=None\n        A lower and upper percent of the population size to select each round of CV.\n        Values between 0 and 1.\n        Selects a percentage of the population to evaluate at each step of the evaluation. \n        For example, one strategy is to evaluate different steps of cross validation one at a time, and only select the best N individuals for subsequent steps. \n        This can save computation by not evaluating all individuals for all steps of cross validation. By default this selection is done with the NSGA2 selector.\n    selection_evaluation_scaling : float, default=0.5\n        A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n        Must be greater than zero. Higher numbers will move the threshold to the end faster.\n    evaluation_early_stop_steps : int, default=1\n        The number of steps that will be taken from the objective function. (e.g., the number of CV folds to evaluate)\n    final_score_strategy : str, default=\"mean\"\n        The strategy to use when determining the final score for an individual.\n        \"mean\": The mean of all objective scores\n        \"last\": The score returned by the last call. Currently each objective is evaluated with a clone of the individual.\n    verbose : int, default=0\n        How much information to print during the optimization process. Higher values include the information from lower values.\n        0. nothing\n        1. progress bar\n        2. evaluations progress bar\n        3. best individual\n        4. warnings\n        &gt;=5. full warnings trace\n    periodic_checkpoint_folder : str, default=None\n        Folder to save the population to periodically. If None, no periodic saving will be done.\n        If provided, training will resume from this checkpoint.\n    callback : tpot.CallBackInterface, default=None\n        Callback object. Not implemented\n    rng : Numpy.Random.Generator, None, default=None\n        An object for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes\n\n        - Numpy.Random.Generator\n            Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'. Note this will be the same Generator passed in.\n        - None\n            Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS\n\n    Attributes\n    ----------\n    population : tpot.Population\n        The population of individuals.\n        Use population.population to access the individuals in the current population.\n        Use population.evaluated_individuals to access a data frame of all individuals that have been explored.\n\n    \"\"\"\n\n    self.rng = np.random.default_rng(rng)\n\n    if threshold_evaluation_pruning is not None or selection_evaluation_pruning is not None:\n        if evaluation_early_stop_steps is None:\n            raise ValueError(\"evaluation_early_stop_steps must be set when using threshold_evaluation_pruning or selection_evaluation_pruning\")\n\n    self.individual_generator = individual_generator\n    self.population_size = population_size\n    self.objective_functions = objective_functions\n    self.objective_function_weights = np.array(objective_function_weights)\n    self.bigger_is_better = bigger_is_better\n    if not bigger_is_better:\n        self.objective_function_weights = np.array(self.objective_function_weights)*-1\n\n    self.initial_population_size = initial_population_size\n    if self.initial_population_size is None:\n        self.cur_population_size = population_size\n    else:\n        self.cur_population_size = initial_population_size\n\n    self.population_scaling = population_scaling\n    self.generations_until_end_population = generations_until_end_population\n\n    self.population_size_list = None\n\n\n    self.periodic_checkpoint_folder = periodic_checkpoint_folder\n    self.verbose  = verbose\n    self.callback = callback\n    self.generations = generations\n    self.n_jobs = n_jobs\n\n\n\n    if max_time_mins is None:\n        self.max_time_mins = float(\"inf\")\n    else:\n        self.max_time_mins = max_time_mins\n\n    #functools requires none for infinite time, doesn't support inf\n    if max_eval_time_mins is not None and math.isinf(max_eval_time_mins ):\n        self.max_eval_time_mins = None\n    else:\n        self.max_eval_time_mins = max_eval_time_mins\n\n\n\n\n    self.generation = 0\n\n\n    self.threshold_evaluation_pruning =threshold_evaluation_pruning\n    self.threshold_evaluation_scaling =  max(0.00001,threshold_evaluation_scaling )\n    self.min_history_threshold = min_history_threshold\n\n    self.selection_evaluation_pruning = selection_evaluation_pruning\n    self.selection_evaluation_scaling =  max(0.00001,selection_evaluation_scaling )\n    self.evaluation_early_stop_steps = evaluation_early_stop_steps\n    self.final_score_strategy = final_score_strategy\n\n    self.budget_range = budget_range\n    self.budget_scaling = budget_scaling\n    self.generations_until_end_budget = generations_until_end_budget\n    self.stepwise_steps = stepwise_steps\n\n    self.memory_limit = memory_limit\n\n    self.client = client\n\n\n    self.survival_selector=survival_selector\n    self.parent_selector=parent_selector\n    self.survival_percentage = survival_percentage\n\n    total_var_p = crossover_probability + mutate_probability + mutate_then_crossover_probability + crossover_then_mutate_probability\n    self.crossover_probability = crossover_probability / total_var_p\n    self.mutate_probability = mutate_probability  / total_var_p\n    self.mutate_then_crossover_probability= mutate_then_crossover_probability / total_var_p\n    self.crossover_then_mutate_probability= crossover_then_mutate_probability / total_var_p\n\n\n    self.mutation_functions = mutation_functions\n    self.crossover_functions = crossover_functions\n\n    if mutation_function_weights is None:\n        self.mutation_function_weights = [1 for _ in range(len(mutation_functions))]\n    else:\n        self.mutation_function_weights = mutation_function_weights\n\n    if mutation_function_weights is None:\n        self.crossover_function_weights = [1 for _ in range(len(mutation_functions))]\n    else:\n        self.crossover_function_weights = crossover_function_weights\n\n    self.n_parents = n_parents\n\n    if objective_kwargs is None:\n        self.objective_kwargs = {}\n    else:\n        self.objective_kwargs = objective_kwargs\n\n    # if objective_kwargs is None:\n    #     self.objective_kwargs = [{}] * len(self.objective_functions)\n    # elif isinstance(objective_kwargs, dict):\n    #     self.objective_kwargs = [objective_kwargs] * len(self.objective_functions)\n    # else:\n    #     self.objective_kwargs = objective_kwargs\n\n    ###########\n\n    if self.initial_population_size != self.population_size:\n        self.population_size_list = beta_interpolation(start=self.cur_population_size, end=self.population_size, scale=self.population_scaling, n=generations_until_end_population, n_steps=self.stepwise_steps)\n        self.population_size_list = np.round(self.population_size_list).astype(int)\n\n    if self.budget_range is None:\n        self.budget_list = None\n    else:\n        self.budget_list = beta_interpolation(start=self.budget_range[0], end=self.budget_range[1], n=self.generations_until_end_budget, scale=self.budget_scaling, n_steps=self.stepwise_steps)\n\n    if objective_names is None:\n        self.objective_names = [\"objective\"+str(i) for i in range(len(objective_function_weights))]\n    else:\n        self.objective_names = objective_names\n\n    if self.budget_list is not None:\n        if len(self.budget_list) &lt;= self.generation:\n            self.budget = self.budget_list[-1]\n        else:\n            self.budget = self.budget_list[self.generation]\n    else:\n        self.budget = None\n\n\n    self.early_stop_tol = early_stop_tol\n    self.early_stop = early_stop\n\n    if isinstance(self.early_stop_tol, float):\n        self.early_stop_tol = [self.early_stop_tol for _ in range(len(self.objective_names))]\n\n    self.early_stop_tol = [np.inf if tol is None else tol for tol in self.early_stop_tol]\n\n    self.population = None\n    self.population_file = None\n    if self.periodic_checkpoint_folder is not None:\n        self.population_file = os.path.join(self.periodic_checkpoint_folder, \"population.pkl\")\n        if not os.path.exists(self.periodic_checkpoint_folder):\n            os.makedirs(self.periodic_checkpoint_folder)\n        if os.path.exists(self.population_file):\n            self.population = pickle.load(open(self.population_file, \"rb\"))\n\n            if len(self.population.evaluated_individuals)&gt;0 and \"Generation\" in self.population.evaluated_individuals.columns:\n                self.generation = self.population.evaluated_individuals['Generation'].max() + 1 #TODO check if this is empty?\n\n    init_names = self.objective_names\n    if self.budget_range is not None:\n        init_names = init_names + [\"Budget\"]\n    if self.population is None:\n        self.population = tpot.Population(column_names=init_names)\n        initial_population = [next(self.individual_generator) for _ in range(self.cur_population_size)]\n        self.population.add_to_population(initial_population, self.rng)\n        self.population.update_column(self.population.population, column_names=\"Generation\", data=self.generation)\n</code></pre>"},{"location":"documentation/tpot/evolvers/base_evolver/#tpot.evolvers.base_evolver.BaseEvolver.evaluate_population","title":"<code>evaluate_population()</code>","text":"<p>Evaluates the individuals in the population that have not been evaluated yet.</p> Source code in <code>tpot/evolvers/base_evolver.py</code> <pre><code>def evaluate_population(self,):\n    \"\"\"\n    Evaluates the individuals in the population that have not been evaluated yet.\n    \"\"\"\n    #Update the sliding scales and thresholds\n    # Save population, TODO remove some of these\n    if self.population_file is not None: # and time.time() - last_save_time &gt; 60*10:\n        pickle.dump(self.population, open(self.population_file, \"wb\"))\n        last_save_time = time.time()\n\n\n    #Get the current thresholds per step\n    self.thresholds = None\n    if self.threshold_evaluation_pruning is not None:\n        old_data = self.population.evaluated_individuals[self.objective_names]\n        old_data = old_data[old_data[self.objective_names].notnull().all(axis=1)]\n        if len(old_data) &gt;= self.min_history_threshold:\n            self.thresholds = np.array([get_thresholds(old_data[obj_name],\n                                                        start=self.threshold_evaluation_pruning[0],\n                                                        end=self.threshold_evaluation_pruning[1],\n                                                        scale=self.threshold_evaluation_scaling,\n                                                        n=self.evaluation_early_stop_steps)\n                                    for obj_name in self.objective_names]).T\n\n    #Get the selectors survival rates per step\n    if self.selection_evaluation_pruning is not None:\n        lower = self.cur_population_size*self.selection_evaluation_pruning[0]\n        upper = self.cur_population_size*self.selection_evaluation_pruning[1]\n        #survival_counts = self.cur_population_size*(scipy.special.betainc(1,self.selection_evaluation_scaling,np.linspace(0,1,self.evaluation_early_stop_steps))*(upper-lower)+lower)\n\n        survival_counts = np.array(beta_interpolation(start=lower, end=upper, scale=self.selection_evaluation_scaling, n=self.evaluation_early_stop_steps, n_steps=self.evaluation_early_stop_steps))\n        self.survival_counts = survival_counts.astype(int)\n    else:\n        self.survival_counts = None\n\n\n\n    if self.evaluation_early_stop_steps is not None:\n        if self.survival_counts is None:\n            #TODO if we are not using selection method for each step, we can create single threads that run all steps for an individual. No need to come back each step.\n            self.evaluate_population_selection_early_stop(survival_counts=self.survival_counts, thresholds=self.thresholds, budget=self.budget)\n        else:\n            #parallelize one step at a time. After each step, come together and select the next individuals to run the next step on.\n            self.evaluate_population_selection_early_stop(survival_counts=self.survival_counts, thresholds=self.thresholds, budget=self.budget)\n    else:\n        self.evaluate_population_full(budget=self.budget)\n\n\n    # Save population, TODO remove some of these\n    if self.population_file is not None: # and time.time() - last_save_time &gt; 60*10:\n        pickle.dump(self.population, open(self.population_file, \"wb\"))\n        last_save_time = time.time()\n</code></pre>"},{"location":"documentation/tpot/evolvers/base_evolver/#tpot.evolvers.base_evolver.BaseEvolver.evaluate_population_full","title":"<code>evaluate_population_full(budget=None)</code>","text":"<p>Evaluates all individuals in the population that have not been evaluated yet. This is the normal/default strategy for evaluating individuals without any early stopping of individual evaluation functions. (e.g., no threshold or selection early stopping). Early stopping by generation is still possible.</p> Source code in <code>tpot/evolvers/base_evolver.py</code> <pre><code>def evaluate_population_full(self, budget=None):\n    \"\"\"\n    Evaluates all individuals in the population that have not been evaluated yet.\n    This is the normal/default strategy for evaluating individuals without any early stopping of individual evaluation functions. (e.g., no threshold or selection early stopping). Early stopping by generation is still possible.\n    \"\"\"\n    individuals_to_evaluate = self.get_unevaluated_individuals(self.objective_names, budget=budget,)\n\n    #print(\"evaluating this many individuals: \", len(individuals_to_evaluate))\n\n    if len(individuals_to_evaluate) == 0:\n        if self.verbose &gt; 3:\n            print(\"No new individuals to evaluate\")\n        return\n\n    if self.max_eval_time_mins is not None:\n        theoretical_timeout = self.max_eval_time_mins * math.ceil(len(individuals_to_evaluate) / self.n_jobs)\n        theoretical_timeout = theoretical_timeout*2\n    else:\n        theoretical_timeout = np.inf\n    scheduled_timeout_time_left = self.scheduled_timeout_time - time.time()\n    parallel_timeout = min(theoretical_timeout, scheduled_timeout_time_left)\n    if parallel_timeout &lt; 0:\n        parallel_timeout = 10\n\n    scores, start_times, end_times, eval_errors = tpot.utils.eval_utils.parallel_eval_objective_list(individuals_to_evaluate, self.objective_functions, verbose=self.verbose, max_eval_time_mins=self.max_eval_time_mins, budget=budget, n_expected_columns=len(self.objective_names), client=self._client, scheduled_timeout_time=self.scheduled_timeout_time, **self.objective_kwargs)\n\n    self.population.update_column(individuals_to_evaluate, column_names=self.objective_names, data=scores)\n    if budget is not None:\n        self.population.update_column(individuals_to_evaluate, column_names=\"Budget\", data=budget)\n\n    self.population.update_column(individuals_to_evaluate, column_names=\"Submitted Timestamp\", data=start_times)\n    self.population.update_column(individuals_to_evaluate, column_names=\"Completed Timestamp\", data=end_times)\n    self.population.update_column(individuals_to_evaluate, column_names=\"Eval Error\", data=eval_errors)\n    self.population.remove_invalid_from_population(column_names=\"Eval Error\")\n    self.population.remove_invalid_from_population(column_names=\"Eval Error\", invalid_value=\"TIMEOUT\")\n</code></pre>"},{"location":"documentation/tpot/evolvers/base_evolver/#tpot.evolvers.base_evolver.BaseEvolver.evaluate_population_selection_early_stop","title":"<code>evaluate_population_selection_early_stop(survival_counts, thresholds=None, budget=None)</code>","text":"<p>This function tries to save computation by partially evaluating the individuals and then selecting which individuals to evaluate further based on the results of the partial evaluation. </p> <p>Two strategies are implemented:     1.  Selection early stopping: Selects a percentage of the population to evaluate at each step of the evaluation.          for example, one strategy is to evaluate different steps of cross validation one at a time, and only select the best N individuals for subsequent steps.          This can save computation by not evaluating all individuals for all steps of cross validation. By default this selection is done with the NSGA2 selector.     2.  Threshold early stopping: At each step of the evaluation, a threshold is calculated based on the previous evaluations. All individuals that are below the performance threshold are not evaluated for further steps.         For example, if the threshold is set to the 90th percentile of the previous evaluations, all individuals that are below the 90th percentile are not evaluated further. This can save computation by not evaluating all individuals for all steps of cross validation.</p> <p>Both of these strategies can be used simultaneously. Individuals must pass both the selection and threshold criteria to be evaluated further.</p> <p>Parameters:</p> Name Type Description Default <code>survival_counts</code> <code>list of ints</code> <p>Number of individuals to select for survival at each step of the evaluation. If None, will not use selection early stopping. For example: [10, 5, 2] would select 10 individuals for the first step, 5 for the second, and 2 for the third.</p> <code>None</code> <code>thresholds</code> <code>list of floats</code> <p>Thresholds to use for early stopping at each step of the evaluation. If None, will not use threshold early stopping.</p> <code>None</code> <code>budget</code> <code>float</code> <p>Budget to use when evaluating individuals. Use is dependent on the objective functions. (In TPOTEstimator this corresponds to the percentage of the data to sample.)</p> <code>None</code> Source code in <code>tpot/evolvers/base_evolver.py</code> <pre><code>def evaluate_population_selection_early_stop(self,survival_counts, thresholds=None, budget=None):\n    \"\"\"\n    This function tries to save computation by partially evaluating the individuals and then selecting which individuals to evaluate further based on the results of the partial evaluation. \n\n    Two strategies are implemented:\n        1.  Selection early stopping: Selects a percentage of the population to evaluate at each step of the evaluation. \n            for example, one strategy is to evaluate different steps of cross validation one at a time, and only select the best N individuals for subsequent steps. \n            This can save computation by not evaluating all individuals for all steps of cross validation. By default this selection is done with the NSGA2 selector.\n        2.  Threshold early stopping: At each step of the evaluation, a threshold is calculated based on the previous evaluations. All individuals that are below the performance threshold are not evaluated for further steps.\n            For example, if the threshold is set to the 90th percentile of the previous evaluations, all individuals that are below the 90th percentile are not evaluated further. This can save computation by not evaluating all individuals for all steps of cross validation.\n\n    Both of these strategies can be used simultaneously. Individuals must pass both the selection and threshold criteria to be evaluated further.\n\n    Parameters\n    ----------\n    survival_counts : list of ints, default=None\n        Number of individuals to select for survival at each step of the evaluation. If None, will not use selection early stopping.\n        For example: [10, 5, 2] would select 10 individuals for the first step, 5 for the second, and 2 for the third.\n    thresholds : list of floats, default=None\n        Thresholds to use for early stopping at each step of the evaluation. If None, will not use threshold early stopping.\n    budget : float, default=None\n        Budget to use when evaluating individuals. Use is dependent on the objective functions. (In TPOTEstimator this corresponds to the percentage of the data to sample.)\n    \"\"\"\n\n    survival_selector = tpot.selectors.survival_select_NSGA2\n\n    ################\n\n    objective_function_signs = np.sign(self.objective_function_weights)\n\n\n    cur_individuals = self.population.population.copy()\n\n    all_step_names = []\n    for step in range(self.evaluation_early_stop_steps):\n        if budget is None:\n            this_step_names = [f\"{n}_step_{step}\" for n in self.objective_names]\n        else:\n            this_step_names = [f\"{n}_budget_{budget}_step_{step}\" for n in self.objective_names]\n\n        all_step_names.append(this_step_names)\n\n        unevaluated_individuals_this_step = self.get_unevaluated_individuals(this_step_names, budget=None, individual_list=cur_individuals)\n\n        if len(unevaluated_individuals_this_step) == 0:\n            if self.verbose &gt; 3:\n                print(\"No new individuals to evaluate\")\n            continue\n\n        if self.max_eval_time_mins is not None:\n            theoretical_timeout = self.max_eval_time_mins * math.ceil(len(unevaluated_individuals_this_step) / self.n_jobs)*60\n            theoretical_timeout = theoretical_timeout*2\n        else:\n            theoretical_timeout = np.inf\n        scheduled_timeout_time_left = self.scheduled_timeout_time - time.time()\n        parallel_timeout = min(theoretical_timeout, scheduled_timeout_time_left)\n        if parallel_timeout &lt; 0:\n            parallel_timeout = 10\n\n        scores, start_times, end_times, eval_errors = tpot.utils.eval_utils.parallel_eval_objective_list(individual_list=unevaluated_individuals_this_step,\n                                objective_list=self.objective_functions,\n                                verbose=self.verbose,\n                                max_eval_time_mins=self.max_eval_time_mins,\n                                step=step,\n                                budget = self.budget,\n                                generation = self.generation,\n                                n_expected_columns=len(self.objective_names),\n                                client=self._client,\n                                scheduled_timeout_time=self.scheduled_timeout_time,\n                                **self.objective_kwargs,\n                                )\n\n        self.population.update_column(unevaluated_individuals_this_step, column_names=this_step_names, data=scores)\n        self.population.update_column(unevaluated_individuals_this_step, column_names=\"Submitted Timestamp\", data=start_times)\n        self.population.update_column(unevaluated_individuals_this_step, column_names=\"Completed Timestamp\", data=end_times)\n        self.population.update_column(unevaluated_individuals_this_step, column_names=\"Eval Error\", data=eval_errors)\n\n        self.population.remove_invalid_from_population(column_names=\"Eval Error\")\n        self.population.remove_invalid_from_population(column_names=\"Eval Error\", invalid_value=\"TIMEOUT\")\n\n        #remove invalids:\n        invalids = []\n        #find indeces of invalids\n\n        for j in range(len(scores)):\n            if  any([s==\"INVALID\" for s in scores[j]]):\n                invalids.append(j)\n\n        for j in range(len(scores)):\n            if  any([s==\"TIMEOUT\" for s in scores[j]]):\n                invalids.append(j)\n\n\n        #already evaluated\n        already_evaluated = list(set(cur_individuals) - set(unevaluated_individuals_this_step))\n        #evaluated and valid\n        valid_evaluations_this_step = remove_items(unevaluated_individuals_this_step,invalids)\n        #update cur_individuals with current individuals with valid scores\n        cur_individuals = np.concatenate([already_evaluated, valid_evaluations_this_step])\n\n\n        #Get average scores\n\n        #array of shape (steps, individuals, objectives)\n        offspring_scores = [self.population.get_column(cur_individuals, column_names=step_names) for step_names in all_step_names]\n        offspring_scores = np.array(offspring_scores)\n        if self.final_score_strategy == 'mean':\n            offspring_scores  = offspring_scores.mean(axis=0)\n        elif self.final_score_strategy == 'last':\n            offspring_scores = offspring_scores[-1]\n\n        #remove individuals with nan scores\n        invalids = []\n        for i in range(len(offspring_scores)):\n            if any(np.isnan(offspring_scores[i])):\n                invalids.append(i)\n\n        cur_individuals = remove_items(cur_individuals,invalids)\n        offspring_scores = remove_items(offspring_scores,invalids)\n\n        #if last step, add the final metrics\n        if step == self.evaluation_early_stop_steps-1:\n            self.population.update_column(cur_individuals, column_names=self.objective_names, data=offspring_scores)\n            if budget is not None:\n                self.population.update_column(cur_individuals, column_names=\"Budget\", data=budget)\n            return\n\n        #If we have more threads than remaining individuals, we may as well evaluate the extras too\n        if self.n_jobs &lt; len(cur_individuals):\n            #Remove based on thresholds\n            if thresholds is not None:\n                threshold = thresholds[step]\n                invalids = []\n                for i in range(len(offspring_scores)):\n\n                    if all([s*w&gt;t*w for s,t,w in zip(offspring_scores[i],threshold,objective_function_signs)  ]):\n                        invalids.append(i)\n\n                if len(invalids) &gt; 0:\n\n                    max_to_remove = min(len(cur_individuals) - self.n_jobs, len(invalids))\n\n                    if max_to_remove &lt; len(invalids):\n                        # invalids = np.random.choice(invalids, max_to_remove, replace=False)\n                        invalids = self.rng.choice(invalids, max_to_remove, replace=False)\n\n                    cur_individuals = remove_items(cur_individuals,invalids)\n                    offspring_scores = remove_items(offspring_scores,invalids)\n\n            # Remove based on selection\n            if survival_counts is not None:\n                if step &lt; self.evaluation_early_stop_steps - 1 and survival_counts[step]&gt;1: #don't do selection for the last loop since they are completed\n                    k = survival_counts[step] + len(invalids) #TODO can remove the min if the selections method can ignore k&gt;population size\n                    if len(cur_individuals)&gt; 1 and k &gt; self.n_jobs and k &lt; len(cur_individuals):\n                        weighted_scores = np.array([s * self.objective_function_weights for s in offspring_scores ])\n\n                        new_population_index = survival_selector(weighted_scores, k=k)\n                        cur_individuals = np.array(cur_individuals)[new_population_index]\n                        offspring_scores = offspring_scores[new_population_index]\n</code></pre>"},{"location":"documentation/tpot/evolvers/base_evolver/#tpot.evolvers.base_evolver.BaseEvolver.generate_offspring","title":"<code>generate_offspring()</code>","text":"<p>Create population_size new individuals from the current population.  This includes selecting parents, applying mutation and crossover, and adding the new individuals to the population.</p> Source code in <code>tpot/evolvers/base_evolver.py</code> <pre><code>def generate_offspring(self, ): #your EA Algorithm goes here\n    \"\"\"\n    Create population_size new individuals from the current population. \n    This includes selecting parents, applying mutation and crossover, and adding the new individuals to the population.\n    \"\"\"\n    parents = self.population.parent_select(selector=self.parent_selector, weights=self.objective_function_weights, columns_names=self.objective_names, k=self.cur_population_size, n_parents=2, rng=self.rng)\n    p = np.array([self.crossover_probability, self.mutate_then_crossover_probability, self.crossover_then_mutate_probability, self.mutate_probability])\n    p = p / p.sum()\n    var_op_list = self.rng.choice([\"crossover\", \"mutate_then_crossover\", \"crossover_then_mutate\", \"mutate\"], size=self.cur_population_size, p=p)\n\n    for i, op in enumerate(var_op_list):\n        if op == \"mutate\":\n            parents[i] = parents[i][0] #mutations take a single individual\n\n    offspring = self.population.create_offspring2(parents, var_op_list, self.mutation_functions, self.mutation_function_weights, self.crossover_functions, self.crossover_function_weights, add_to_population=True, keep_repeats=False, mutate_until_unique=True, rng=self.rng)\n\n    self.population.update_column(offspring, column_names=\"Generation\", data=self.generation, )\n</code></pre>"},{"location":"documentation/tpot/evolvers/base_evolver/#tpot.evolvers.base_evolver.BaseEvolver.get_unevaluated_individuals","title":"<code>get_unevaluated_individuals(column_names, budget=None, individual_list=None)</code>","text":"<p>This function is used to get a list of individuals in the current population that have not been evaluated yet.</p> <p>Parameters:</p> Name Type Description Default <code>column_names</code> <code>list of strings</code> <p>Names of the columns to check for unevaluated individuals (generally objective functions).</p> required <code>budget</code> <code>float</code> <p>Budget to use when checking for unevaluated individuals. If None, will not check the budget column. Finds individuals who have not been evaluated with the given budget on column names.</p> <code>None</code> <code>individual_list</code> <code>list of individuals</code> <p>List of individuals to check for unevaluated individuals. If None, will use the current population.</p> <code>None</code> Source code in <code>tpot/evolvers/base_evolver.py</code> <pre><code>def get_unevaluated_individuals(self, column_names, budget=None, individual_list=None):\n    \"\"\"\n    This function is used to get a list of individuals in the current population that have not been evaluated yet.\n\n    Parameters\n    ----------\n    column_names : list of strings\n        Names of the columns to check for unevaluated individuals (generally objective functions).\n    budget : float, default=None\n        Budget to use when checking for unevaluated individuals. If None, will not check the budget column.\n        Finds individuals who have not been evaluated with the given budget on column names.\n    individual_list : list of individuals, default=None\n        List of individuals to check for unevaluated individuals. If None, will use the current population.\n    \"\"\"\n    if individual_list is not None:\n        cur_pop = np.array(individual_list)\n    else:\n        cur_pop = np.array(self.population.population)\n\n    if all([name_step in self.population.evaluated_individuals.columns for name_step in column_names]):\n        if budget is not None:\n            offspring_scores = self.population.get_column(cur_pop, column_names=column_names+[\"Budget\"], to_numpy=False)\n            #Individuals are unevaluated if we have a higher budget OR if any of the objectives are nan\n            unevaluated_filter = lambda i: any(offspring_scores.loc[offspring_scores.index[i]][column_names].isna()) or (offspring_scores.loc[offspring_scores.index[i]][\"Budget\"] &lt; budget)\n        else:\n            offspring_scores = self.population.get_column(cur_pop, column_names=column_names, to_numpy=False)\n            unevaluated_filter = lambda i: any(offspring_scores.loc[offspring_scores.index[i]][column_names].isna())\n        unevaluated_individuals_this_step = [i for i in range(len(cur_pop)) if unevaluated_filter(i)]\n        return cur_pop[unevaluated_individuals_this_step]\n\n    else: #if column names are not in the evaluated_individuals, then we have not evaluated any individuals yet\n        for name_step in column_names:\n            self.population.evaluated_individuals[name_step] = np.nan\n        return cur_pop\n</code></pre>"},{"location":"documentation/tpot/evolvers/base_evolver/#tpot.evolvers.base_evolver.BaseEvolver.optimize","title":"<code>optimize(generations=None)</code>","text":"<p>Creates an initial population and runs the evolutionary algorithm for the given number of generations.  If generations is None, will use self.generations.</p> <p>Parameters:</p> Name Type Description Default <code>generations</code> <code>int</code> <p>Number of generations to run. If None, will use self.generations.</p> <code>None</code> Source code in <code>tpot/evolvers/base_evolver.py</code> <pre><code>def optimize(self, generations=None):\n    \"\"\"\n    Creates an initial population and runs the evolutionary algorithm for the given number of generations. \n    If generations is None, will use self.generations.\n\n    Parameters\n    ----------\n    generations : int, default=None\n        Number of generations to run. If None, will use self.generations.\n\n    \"\"\"\n\n    if self.client is not None: #If user passed in a client manually\n       self._client = self.client\n    else:\n\n        if self.verbose &gt;= 4:\n            silence_logs = 30\n        elif self.verbose &gt;=5:\n            silence_logs = 40\n        else:\n            silence_logs = 50\n        self._cluster = LocalCluster(n_workers=self.n_jobs, #if no client is passed in and no global client exists, create our own\n                threads_per_worker=1,\n                silence_logs=silence_logs,\n                processes=True,\n                memory_limit=self.memory_limit)\n        self._client = Client(self._cluster)\n\n\n\n    if generations is None:\n        generations = self.generations\n\n    start_time = time.time()\n\n    generations_without_improvement = np.array([0 for _ in range(len(self.objective_function_weights))])\n    best_scores = [-np.inf for _ in range(len(self.objective_function_weights))]\n\n\n    self.scheduled_timeout_time = time.time() + self.max_time_mins*60\n\n\n    try:\n        #for gen in tnrange(generations,desc=\"Generation\", disable=self.verbose&lt;1):\n        done = False\n        gen = 0\n        if self.verbose &gt;= 1:\n            if generations is None or np.isinf(generations):\n                pbar = tqdm.tqdm(total=0)\n            else:\n                pbar = tqdm.tqdm(total=generations)\n            pbar.set_description(\"Generation\")\n        while not done:\n            # Generation 0 is the initial population\n            if self.generation == 0:\n                if self.population_file is not None:\n                    pickle.dump(self.population, open(self.population_file, \"wb\"))\n                self.evaluate_population()\n                if self.population_file is not None:\n                    pickle.dump(self.population, open(self.population_file, \"wb\"))\n\n                attempts = 2\n                while len(self.population.population) == 0 and attempts &gt; 0:\n                    new_initial_population = [next(self.individual_generator) for _ in range(self.cur_population_size)]\n                    self.population.add_to_population(new_initial_population, rng=self.rng)\n                    attempts -= 1\n                    self.evaluate_population()\n\n                if len(self.population.population) == 0:\n                    raise Exception(\"No individuals could be evaluated in the initial population. This may indicate a bug in the configuration, included models, or objective functions. Set verbose&gt;=4 to see the errors that caused individuals to fail.\")\n\n                self.generation += 1\n            # Generation 1 is the first generation after the initial population\n            else:\n                if time.time() - start_time &gt; self.max_time_mins*60:\n                    if self.population.evaluated_individuals[self.objective_names].isnull().all().iloc[0]:\n                        raise Exception(\"No individuals could be evaluated in the initial population as the max_eval_mins time limit was reached before any individuals could be evaluated.\")\n                    break\n                self.step()\n\n            if self.verbose &gt;= 3:\n                sign = np.sign(self.objective_function_weights)\n                valid_df = self.population.evaluated_individuals[~self.population.evaluated_individuals[self.objective_names].isin([\"TIMEOUT\",\"INVALID\"]).any(axis=1)][self.objective_names]*sign\n                cur_best_scores = valid_df.max(axis=0)*sign\n                cur_best_scores = cur_best_scores.to_numpy()\n                print(\"Generation: \", self.generation)\n                for i, obj in enumerate(self.objective_names):\n                    print(f\"Best {obj} score: {cur_best_scores[i]}\")\n\n\n            if self.early_stop:\n                if self.budget is None or self.budget&gt;=self.budget_range[-1]: #self.budget&gt;=1:\n                    #get sign of objective_function_weights\n                    sign = np.sign(self.objective_function_weights)\n                    #get best score for each objective\n                    valid_df = self.population.evaluated_individuals[~self.population.evaluated_individuals[self.objective_names].isin([\"TIMEOUT\",\"INVALID\"]).any(axis=1)][self.objective_names]*sign\n                    cur_best_scores = valid_df.max(axis=0)\n                    cur_best_scores = cur_best_scores.to_numpy()\n                    #cur_best_scores =  self.population.get_column(self.population.population, column_names=self.objective_names).max(axis=0)*sign #TODO this assumes the current population is the best\n\n                    improved = ( np.array(cur_best_scores) - np.array(best_scores) &gt;= np.array(self.early_stop_tol) )\n                    not_improved = np.logical_not(improved)\n                    generations_without_improvement = generations_without_improvement * not_improved + not_improved #set to zero if not improved, else increment\n                    pass\n                    #update best score\n                    best_scores = [max(best_scores[i], cur_best_scores[i]) for i in range(len(self.objective_names))]\n\n                    if all(generations_without_improvement&gt;self.early_stop):\n                        if self.verbose &gt;= 3:\n                            print(\"Early stop\")\n                        break\n\n            #save population\n            if self.population_file is not None: # and time.time() - last_save_time &gt; 60*10:\n                pickle.dump(self.population, open(self.population_file, \"wb\"))\n\n            gen += 1\n            if self.verbose &gt;= 1:\n                pbar.update(1)\n\n            if generations is not None and gen &gt;= generations:\n                done = True\n\n    except KeyboardInterrupt:\n        if self.verbose &gt;= 3:\n            print(\"KeyboardInterrupt\")\n        self.population.remove_invalid_from_population(column_names=self.objective_names, invalid_value=\"INVALID\")\n        self.population.remove_invalid_from_population(column_names=self.objective_names, invalid_value=\"TIMEOUT\")\n        self.population.remove_invalid_from_population(column_names=\"Eval Error\", invalid_value=\"INVALID\")\n        self.population.remove_invalid_from_population(column_names=\"Eval Error\", invalid_value=\"TIMEOUT\")\n\n\n\n\n    if self.population_file is not None:\n        pickle.dump(self.population, open(self.population_file, \"wb\"))\n\n    if self.client is None: #If we created our own client, close it\n        self._client.close()\n        self._cluster.close()\n\n    tpot.utils.get_pareto_frontier(self.population.evaluated_individuals, column_names=self.objective_names, weights=self.objective_function_weights)\n</code></pre>"},{"location":"documentation/tpot/evolvers/base_evolver/#tpot.evolvers.base_evolver.BaseEvolver.step","title":"<code>step()</code>","text":"<p>Runs a single generation of the evolutionary algorithm. This includes selecting individuals for survival, generating offspring, and evaluating the offspring.</p> Source code in <code>tpot/evolvers/base_evolver.py</code> <pre><code>def step(self,):\n    \"\"\"\n    Runs a single generation of the evolutionary algorithm. This includes selecting individuals for survival, generating offspring, and evaluating the offspring.\n\n    \"\"\"\n\n\n    if self.population_size_list is not None:\n        if self.generation &lt; len(self.population_size_list):\n            self.cur_population_size = self.population_size_list[self.generation]\n        else:\n            self.cur_population_size = self.population_size\n\n    if self.budget_list is not None:\n        if len(self.budget_list) &lt;= self.generation:\n            self.budget = self.budget_range[-1]\n        else:\n            self.budget = self.budget_list[self.generation]\n    else:\n        self.budget = None\n\n    if self.survival_selector is not None:\n        n_survivors = max(1,int(self.cur_population_size*self.survival_percentage)) #always keep at least one individual\n        self.population.survival_select(    selector=self.survival_selector,\n                                            weights=self.objective_function_weights,\n                                            columns_names=self.objective_names,\n                                            n_survivors=n_survivors,\n                                            inplace=True,\n                                            rng=self.rng,)\n\n    self.generate_offspring()\n    self.evaluate_population()\n\n    self.generation += 1\n</code></pre>"},{"location":"documentation/tpot/evolvers/base_evolver/#tpot.evolvers.base_evolver.ind_crossover","title":"<code>ind_crossover(ind1, ind2, rng)</code>","text":"<p>Calls the ind1.crossover(ind2, rng=rng)</p> <p>Parameters:</p> Name Type Description Default <code>ind1</code> <code>BaseIndividual</code> required <code>ind2</code> <code>BaseIndividual</code> required <code>rng</code> <code>int or Generator</code> <p>A numpy random generator to use for reproducibility</p> required Source code in <code>tpot/evolvers/base_evolver.py</code> <pre><code>def ind_crossover(ind1, ind2, rng):\n    \"\"\"\n    Calls the ind1.crossover(ind2, rng=rng)\n    Parameters\n    ----------\n    ind1 : tpot.BaseIndividual\n    ind2 : tpot.BaseIndividual\n    rng : int or numpy.random.Generator\n        A numpy random generator to use for reproducibility\n    \"\"\"\n    rng = np.random.default_rng(rng)\n    return ind1.crossover(ind2, rng=rng)\n</code></pre>"},{"location":"documentation/tpot/evolvers/base_evolver/#tpot.evolvers.base_evolver.ind_mutate","title":"<code>ind_mutate(ind, rng)</code>","text":"<p>Calls the ind.mutate method on the individual</p> <p>Parameters:</p> Name Type Description Default <code>ind</code> <code>BaseIndividual</code> <p>The individual to mutate</p> required <code>rng</code> <code>int or Generator</code> <p>A numpy random generator to use for reproducibility</p> required Source code in <code>tpot/evolvers/base_evolver.py</code> <pre><code>def ind_mutate(ind, rng):\n    \"\"\"\n    Calls the ind.mutate method on the individual\n\n    Parameters\n    ----------\n    ind : tpot.BaseIndividual\n        The individual to mutate\n    rng : int or numpy.random.Generator\n        A numpy random generator to use for reproducibility\n    \"\"\"\n    rng = np.random.default_rng(rng)\n    return ind.mutate(rng=rng)\n</code></pre>"},{"location":"documentation/tpot/evolvers/steady_state_evolver/","title":"Steady state evolver","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/evolvers/steady_state_evolver/#tpot.evolvers.steady_state_evolver.SteadyStateEvolver","title":"<code>SteadyStateEvolver</code>","text":"Source code in <code>tpot/evolvers/steady_state_evolver.py</code> <pre><code>class SteadyStateEvolver():\n    def __init__(   self,\n                    individual_generator ,\n\n                    objective_functions,\n                    objective_function_weights,\n                    objective_names = None,\n                    objective_kwargs = None,\n                    bigger_is_better = True,\n\n                    initial_population_size = 50,\n                    population_size = 300,\n                    max_evaluated_individuals = None,\n                    early_stop = None,\n                    early_stop_mins = None,\n                    early_stop_tol = 0.001,\n\n\n                    max_time_mins=float(\"inf\"),\n                    max_eval_time_mins=10,\n\n                    n_jobs=1,\n                    memory_limit=\"4GB\",\n                    client=None,\n\n                    crossover_probability=.2,\n                    mutate_probability=.7,\n                    mutate_then_crossover_probability=.05,\n                    crossover_then_mutate_probability=.05,\n                    n_parents=2,\n\n                    survival_selector = survival_select_NSGA2,\n                    parent_selector = tournament_selection_dominated,\n\n                    budget_range = None,\n                    budget_scaling = .5,\n                    individuals_until_end_budget = 1,\n                    stepwise_steps = 5,\n\n                    verbose = 0,\n                    periodic_checkpoint_folder = None,\n                    callback = None,\n\n                    rng=None\n                    ) -&gt; None:\n        \"\"\"\n        Whereas the base_evolver uses a generational approach, the steady state evolver continuously generates individuals as resources become available.\n\n        This evolver will simultaneously evaluated n_jobs individuals. As soon as one individual is evaluated, the current population is updated with survival_selector, \n        a new individual is generated from parents selected with parent_selector, and the new individual is immediately submitted for evaluation.\n        In contrast, the base_evolver batches evaluations in generations, and only updates the population and creates new individuals after all individuals in the current generation are evaluated.\n\n        In practice, this means that steady state evolver is more likely to use all cores at all times, allowing for flexibility is duration of evaluations and number of evaluations. However, it \n        may also generate less diverse populations as a result.\n\n        Parameters\n        ----------\n        individual_generator : generator\n            Generator that yields new base individuals. Used to generate initial population.\n        objective_functions : list of callables\n            list of functions that get applied to the individual and return a float or list of floats\n            If an objective function returns multiple values, they are all concatenated in order\n            with respect to objective_function_weights and early_stop_tol.\n        objective_function_weights : list of floats\n            list of weights for each objective function. Sign flips whether bigger is better or not\n        objective_names : list of strings, default=None\n            Names of the objectives. If None, objective0, objective1, etc. will be used\n        objective_kwargs : dict, default=None\n            Dictionary of keyword arguments to pass to the objective function\n        bigger_is_better : bool, default=True\n            If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n\n        initial_population_size : int, default=50\n            Number of random individuals to generate in the initial population. These will all be randomly sampled, all other subsequent individuals will be generated from the population.\n        population_size : int, default=50\n            Note: This is different from the base_evolver. \n            In steady_state_evolver, the population_size is the number of individuals to keep in the live population. This is the total number of best individuals (as determined by survival_selector) to keep in the population.\n            New individuals are generated from this population size.\n            In base evolver, this is also the number of individuals to generate in each generation, however, here, we generate individuals as resources become available so there is no concept of a generation.\n            It is recommended to use a higher population_size to ensure diversity in the population.\n        max_evaluated_individuals : int, default=None\n            Maximum number of individuals to evaluate after which training is terminated. If None, will evaluate until time limit is reached.\n        early_stop : int, default=None\n            If the best individual has not improved in this many evaluations, stop training.\n            Note: Also different from base_evolver. In base evolver, this is the number of generations without improvement. Here, it is the number of individuals evaluated without improvement. Naturally, a higher value is recommended.\n        early_stop_mins : int, default=None\n            If the best individual has not improved in this many minutes, stop training.\n                early_stop_tol : float, list of floats, or None, default=0.001\n            -list of floats\n                list of tolerances for each objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n                If an index of the list is None, that item will not be used for early stopping\n            -int\n                If an int is given, it will be used as the tolerance for all objectives\n        max_time_mins : float, default=float(\"inf\")\n            Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n        max_eval_time_mins : float, default=10\n            Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n        n_jobs : int, default=1\n            Number of processes to run in parallel.\n        memory_limit : str, default=None\n            Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n        client : dask.distributed.Client, default=None\n            A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.\n        crossover_probability : float, default=.2\n            Probability of generating a new individual by crossover between two individuals.\n        mutate_probability : float, default=.7\n            Probability of generating a new individual by crossover between one individuals.\n        mutate_then_crossover_probability : float, default=.05\n            Probability of generating a new individual by mutating two individuals followed by crossover.\n        crossover_then_mutate_probability : float, default=.05\n            Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.\n        n_parents : int, default=2\n            Number of parents to use for crossover. Must be greater than 1.\n        survival_selector : function, default=survival_select_NSGA2\n            Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes.\n            Used to selected population_size * survival_percentage individuals at the start of each generation to use for mutation and crossover.\n        parent_selector : function, default=parent_select_NSGA2\n            Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.     \n\n        budget_range : list [start, end], default=None\n            This parameter is used for the successive halving algorithm.\n            A starting and ending budget to use for the budget scaling. The evolver will interpolate between these values over the generations_until_end_budget.\n            Use is dependent on the objective functions. (In TPOTEstimator this corresponds to the percentage of the data to sample.)\n        budget_scaling float : [0,1], default=0.5\n            A scaling factor to use when determining how fast we move the budget from the start to end budget.\n        evaluations_until_end_budget : int, default=1\n            The number of evaluations to run before reaching the max budget.\n        stepwise_steps : int, default=1\n            The number of staircase steps to take when interpolating the budget.\n        verbose : int, default=0\n            How much information to print during the optimization process. Higher values include the information from lower values.\n            0. nothing\n            1. progress bar\n            2. evaluations progress bar\n            3. best individual\n            4. warnings\n            &gt;=5. full warnings trace\n        periodic_checkpoint_folder : str, default=None\n            Folder to save the population to periodically. If None, no periodic saving will be done.\n            If provided, training will resume from this checkpoint.\n        callback : tpot.CallBackInterface, default=None\n            Callback object. Not implemented\n        rng : Numpy.Random.Generator, None, default=None\n            An object for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes\n\n            - Numpy.Random.Generator\n                Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'. Note this will be the same Generator passed in.\n            - None\n                Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS\n\n        Attributes\n        ----------\n        population : tpot.Population\n            The population of individuals.\n            Use population.population to access the individuals in the current population.\n            Use population.evaluated_individuals to access a data frame of all individuals that have been explored.\n\n        \"\"\"\n\n        self.rng = np.random.default_rng(rng)\n\n        self.max_evaluated_individuals = max_evaluated_individuals\n        self.individuals_until_end_budget = individuals_until_end_budget\n\n        self.individual_generator = individual_generator\n        self.population_size = population_size\n        self.objective_functions = objective_functions\n        self.objective_function_weights = np.array(objective_function_weights)\n        self.bigger_is_better = bigger_is_better\n        if not bigger_is_better:\n            self.objective_function_weights = np.array(self.objective_function_weights)*-1\n\n        self.population_size_list = None\n\n\n        self.periodic_checkpoint_folder = periodic_checkpoint_folder\n        self.verbose  = verbose\n        self.callback = callback\n        self.n_jobs = n_jobs\n\n        if max_time_mins is None:\n            self.max_time_mins = float(\"inf\")\n        else:\n            self.max_time_mins = max_time_mins\n\n        #functools requires none for infinite time, doesn't support inf\n        if max_eval_time_mins is not None and math.isinf(max_eval_time_mins ):\n            self.max_eval_time_mins = None\n        else:\n            self.max_eval_time_mins = max_eval_time_mins\n\n        self.initial_population_size = initial_population_size\n        self.budget_range = budget_range\n        self.budget_scaling = budget_scaling\n        self.stepwise_steps = stepwise_steps\n\n        self.memory_limit = memory_limit\n\n        self.client = client\n\n\n        self.survival_selector=survival_selector\n        self.parent_selector=parent_selector\n\n\n        total_var_p = crossover_probability + mutate_probability + mutate_then_crossover_probability + crossover_then_mutate_probability\n        self.crossover_probability = crossover_probability / total_var_p\n        self.mutate_probability = mutate_probability  / total_var_p\n        self.mutate_then_crossover_probability= mutate_then_crossover_probability / total_var_p\n        self.crossover_then_mutate_probability= crossover_then_mutate_probability / total_var_p\n\n        self.n_parents = n_parents\n\n        if objective_kwargs is None:\n            self.objective_kwargs = {}\n        else:\n            self.objective_kwargs = objective_kwargs\n\n        ###########\n\n\n        if self.budget_range is None:\n            self.budget_list = None\n        else:\n            self.budget_list = beta_interpolation(start=self.budget_range[0], end=self.budget_range[1], n=self.generations_until_end_budget, scale=self.budget_scaling, n_steps=self.stepwise_steps)\n\n        if objective_names is None:\n            self.objective_names = [\"objective\"+str(i) for i in range(len(objective_function_weights))]\n        else:\n            self.objective_names = objective_names\n\n        if self.budget_list is not None:\n            if len(self.budget_list) &lt;= self.generation:\n                self.budget = self.budget_list[-1]\n            else:\n                self.budget = self.budget_list[self.generation]\n        else:\n            self.budget = None\n\n\n        self.early_stop_tol = early_stop_tol\n        self.early_stop_mins = early_stop_mins\n        self.early_stop = early_stop\n\n        if isinstance(self.early_stop_tol, float):\n            self.early_stop_tol = [self.early_stop_tol for _ in range(len(self.objective_names))]\n\n        self.early_stop_tol = [np.inf if tol is None else tol for tol in self.early_stop_tol]\n\n        self.population = None\n        self.population_file = None\n        if self.periodic_checkpoint_folder is not None:\n            self.population_file = os.path.join(self.periodic_checkpoint_folder, \"population.pkl\")\n            if not os.path.exists(self.periodic_checkpoint_folder):\n                os.makedirs(self.periodic_checkpoint_folder)\n            if os.path.exists(self.population_file):\n                self.population = pickle.load(open(self.population_file, \"rb\"))\n\n\n        init_names = self.objective_names\n        if self.budget_range is not None:\n            init_names = init_names + [\"Budget\"]\n        if self.population is None:\n            self.population = tpot.Population(column_names=init_names)\n            initial_population = [next(self.individual_generator) for _ in range(self.initial_population_size)]\n            self.population.add_to_population(initial_population, rng=self.rng)\n\n\n    def optimize(self):\n        \"\"\"\n        Creates an initial population and runs the evolutionary algorithm for the given number of generations. \n        If generations is None, will use self.generations.\n        \"\"\"\n\n        #intialize the client\n        if self.client is not None: #If user passed in a client manually\n           self._client = self.client\n        else:\n\n            if self.verbose &gt;= 4:\n                silence_logs = 30\n            elif self.verbose &gt;=5:\n                silence_logs = 40\n            else:\n                silence_logs = 50\n            self._cluster = LocalCluster(n_workers=self.n_jobs, #if no client is passed in and no global client exists, create our own\n                    threads_per_worker=1,\n                    silence_logs=silence_logs,\n                    processes=False,\n                    memory_limit=self.memory_limit)\n            self._client = Client(self._cluster)\n\n\n        self.max_queue_size = len(self._client.cluster.workers)\n\n        #set up logging params\n        evaluated_count = 0\n        generations_without_improvement = np.array([0 for _ in range(len(self.objective_function_weights))])\n        timestamp_of_last_improvement = np.array([time.time() for _ in range(len(self.objective_function_weights))])\n        best_scores = [-np.inf for _ in range(len(self.objective_function_weights))]\n        scheduled_timeout_time = time.time() + self.max_time_mins*60\n        budget = None\n\n        submitted_futures = {}\n        submitted_inds = set()\n\n        start_time = time.time()\n\n        try:\n\n\n            if self.verbose &gt;= 1:\n                if self.max_evaluated_individuals is not None:\n                    pbar = tqdm.tqdm(total=self.max_evaluated_individuals, miniters=1)\n                else:\n                    pbar = tqdm.tqdm(total=0, miniters=1)\n                pbar.set_description(\"Evaluations\")\n\n            #submit initial population\n            individuals_to_evaluate = self.get_unevaluated_individuals(self.objective_names, budget=budget,)\n\n            for individual in individuals_to_evaluate:\n                if len(submitted_futures) &gt;= self.max_queue_size:\n                    break\n                future = self._client.submit(tpot.utils.eval_utils.eval_objective_list, individual,  self.objective_functions, verbose=self.verbose, timeout=self.max_eval_time_mins*60,**self.objective_kwargs)\n\n                submitted_futures[future] = {\"individual\": individual,\n                                            \"time\": time.time(),\n                                            \"budget\": budget,}\n                submitted_inds.add(individual.unique_id())\n                self.population.update_column(individual, column_names=\"Submitted Timestamp\", data=time.time())\n\n            done = False\n            start_time = time.time()\n\n            enough_parents_evaluated=False\n            while not done:\n\n                ###############################\n                # Step 1: Check for finished futures\n                ###############################\n\n                #wait for at least one future to finish or timeout\n                try:\n                    next(distributed.as_completed(submitted_futures, timeout=self.max_eval_time_mins*60))\n                except dask.distributed.TimeoutError:\n                    pass\n                except dask.distributed.CancelledError:\n                    pass\n\n                #Loop through all futures, collect completed and timeout futures.\n                for completed_future in list(submitted_futures.keys()):\n                    eval_error = None\n                    #get scores and update\n                    if completed_future.done(): #if future is done\n                        #If the future is done but threw and error, record the error\n                        if completed_future.exception() or completed_future.status == \"error\": #if the future is done and threw an error\n                            print(\"Exception in future\")\n                            print(completed_future.exception())\n                            scores = [np.nan for _ in range(len(self.objective_names))]\n                            eval_error = \"INVALID\"\n                        elif completed_future.cancelled(): #if the future is done and was cancelled\n                            print(\"Cancelled future (likely memory related)\")\n                            scores = [np.nan for _ in range(len(self.objective_names))]\n                            eval_error = \"INVALID\"\n                            client.run(gc.collect)\n                        else: #if the future is done and did not throw an error, get the scores\n                            try:\n                                scores = completed_future.result()\n\n                                #check if scores contain \"INVALID\" or \"TIMEOUT\"\n                                if \"INVALID\" in scores:\n                                    eval_error = \"INVALID\"\n                                    scores = [np.nan]\n                                elif \"TIMEOUT\" in scores:\n                                    eval_error = \"TIMEOUT\"\n                                    scores = [np.nan]\n\n                            except Exception as e:\n                                print(\"Exception in future, but not caught by dask\")\n                                print(e)\n                                print(completed_future.exception())\n                                print(completed_future)\n                                print(\"status\", completed_future.status)\n                                print(\"done\", completed_future.done())\n                                print(\"cancelld \", completed_future.cancelled())\n                                scores = [np.nan for _ in range(len(self.objective_names))]\n                                eval_error = \"INVALID\"\n                        completed_future.release() #release the future\n                    else: #if future is not done\n\n                        if self.max_eval_time_mins is not None:\n                            #check if the future has been running for too long, cancel the future\n                            if time.time() - submitted_futures[completed_future][\"time\"] &gt; self.max_eval_time_mins*1.25*60:\n                                completed_future.cancel()\n                                completed_future.release() #release the future\n                                if self.verbose &gt;= 4:\n                                    print(f'WARNING AN INDIVIDUAL TIMED OUT (Fallback): \\n {submitted_futures[completed_future]} \\n')\n\n                                scores = [np.nan for _ in range(len(self.objective_names))]\n                                eval_error = \"TIMEOUT\"\n                            else:\n                                continue #otherwise, continue to next future\n\n\n\n                    #update population\n                    this_individual = submitted_futures[completed_future][\"individual\"]\n                    this_budget = submitted_futures[completed_future][\"budget\"]\n                    this_time = submitted_futures[completed_future][\"time\"]\n\n                    if len(scores) &lt; len(self.objective_names):\n                        scores = [scores[0] for _ in range(len(self.objective_names))]\n                    self.population.update_column(this_individual, column_names=self.objective_names, data=scores)\n                    self.population.update_column(this_individual, column_names=\"Completed Timestamp\", data=time.time())\n                    self.population.update_column(this_individual, column_names=\"Eval Error\", data=eval_error)\n                    if budget is not None:\n                        self.population.update_column(this_individual, column_names=\"Budget\", data=this_budget)\n\n                    submitted_futures.pop(completed_future)\n                    submitted_inds.add(this_individual.unique_id())\n                    if self.verbose &gt;= 1:\n                        pbar.update(1)\n\n                #now we have a list of completed futures\n\n                self.population.remove_invalid_from_population(column_names=\"Eval Error\", invalid_value=\"INVALID\")\n                self.population.remove_invalid_from_population(column_names=\"Eval Error\", invalid_value=\"TIMEOUT\")\n\n                #I am not entirely sure if this is necessary. I believe that calling release on the futures should be enough to free up memory. If memory issues persist, this may be a good place to start.\n                #client.run(gc.collect) #run garbage collection to free up memory\n\n                ###############################\n                # Step 2: Early Stopping\n                ###############################\n                if self.verbose &gt;= 3:\n                    sign = np.sign(self.objective_function_weights)\n                    valid_df = self.population.evaluated_individuals[~self.population.evaluated_individuals[[\"Eval Error\"]].isin([\"TIMEOUT\",\"INVALID\"]).any(axis=1)][self.objective_names]*sign\n                    cur_best_scores = valid_df.max(axis=0)*sign\n                    cur_best_scores = cur_best_scores.to_numpy()\n                    for i, obj in enumerate(self.objective_names):\n                        print(f\"Best {obj} score: {cur_best_scores[i]}\")\n\n                if self.early_stop or self.early_stop_mins:\n                    if self.budget is None or self.budget&gt;=self.budget_range[-1]: #self.budget&gt;=1:\n                        #get sign of objective_function_weights\n                        sign = np.sign(self.objective_function_weights)\n                        #get best score for each objective\n                        valid_df = self.population.evaluated_individuals[~self.population.evaluated_individuals[[\"Eval Error\"]].isin([\"TIMEOUT\",\"INVALID\"]).any(axis=1)][self.objective_names]*sign\n                        cur_best_scores = valid_df.max(axis=0)\n                        cur_best_scores = cur_best_scores.to_numpy()\n                        #cur_best_scores =  self.population.get_column(self.population.population, column_names=self.objective_names).max(axis=0)*sign #TODO this assumes the current population is the best\n\n                        improved = ( np.array(cur_best_scores) - np.array(best_scores) &gt;= np.array(self.early_stop_tol) )\n                        not_improved = np.logical_not(improved)\n                        generations_without_improvement = generations_without_improvement * not_improved + not_improved #set to zero if not improved, else increment\n\n                        timestamp_of_last_improvement = timestamp_of_last_improvement * not_improved + time.time()*improved #set to current time if improved\n\n                        pass\n                        #update best score\n                        best_scores = [max(best_scores[i], cur_best_scores[i]) for i in range(len(self.objective_names))]\n\n                        if self.early_stop:\n                            if all(generations_without_improvement&gt;self.early_stop):\n                                if self.verbose &gt;= 3:\n                                    print(f\"Early stop ({self.early_stop} individuals evaluated without improvement)\")\n                                break\n\n                        if self.early_stop_mins:\n                            if any(time.time() - timestamp_of_last_improvement &gt; self.early_stop_mins*60):\n                                if self.verbose &gt;= 3:\n                                    print(f\"Early stop  ({self.early_stop_mins} seconds passed without improvement)\")\n                                break\n\n                #if we evaluated enough individuals or time is up, stop\n                if self.max_time_mins is not None and time.time() - start_time &gt; self.max_time_mins*60:\n                    if self.verbose &gt;= 3:\n                        print(\"Time limit reached\")\n                    done = True\n                    break\n\n                if self.max_evaluated_individuals is not None and len(self.population.evaluated_individuals.dropna(subset=self.objective_names)) &gt;= self.max_evaluated_individuals:\n                    print(\"Evaluated enough individuals\")\n                    done = True\n                    break\n\n                ###############################\n                # Step 3: Submit unevaluated individuals from the initial population\n                ###############################\n                individuals_to_evaluate = self.get_unevaluated_individuals(self.objective_names, budget=budget,)\n                individuals_to_evaluate = [ind for ind in individuals_to_evaluate if ind.unique_id() not in submitted_inds]\n                for individual in individuals_to_evaluate:\n                    if self.max_queue_size &gt; len(submitted_futures):\n                        future = self._client.submit(tpot.utils.eval_utils.eval_objective_list, individual,  self.objective_functions, verbose=self.verbose, timeout=self.max_eval_time_mins*60,**self.objective_kwargs)\n\n                        submitted_futures[future] = {\"individual\": individual,\n                                                    \"time\": time.time(),\n                                                    \"budget\": budget,}\n                        submitted_inds.add(individual.unique_id())\n\n                        self.population.update_column(individual, column_names=\"Submitted Timestamp\", data=time.time())\n\n\n                ###############################\n                # Step 4: Survival Selection\n                ###############################\n                if self.survival_selector is not None:\n                    parents_df = self.population.get_column(self.population.population, column_names=self.objective_names + [\"Individual\"], to_numpy=False)\n                    evaluated = parents_df[~parents_df[self.objective_names].isna().any(axis=1)]\n                    if len(evaluated) &gt; self.population_size:\n                        unevaluated = parents_df[parents_df[self.objective_names].isna().any(axis=1)]\n\n                        cur_evaluated_population = parents_df[\"Individual\"].to_numpy()\n                        if len(cur_evaluated_population) &gt; self.population_size:\n                            scores = evaluated[self.objective_names].to_numpy()\n                            weighted_scores = scores * self.objective_function_weights\n                            new_population_index = np.ravel(self.survival_selector(weighted_scores, k=self.population_size, rng=self.rng)) #TODO make it clear that we are concatenating scores...\n\n                            #set new population\n                            try:\n                                cur_evaluated_population = np.array(cur_evaluated_population)[new_population_index]\n                                cur_evaluated_population = np.concatenate([cur_evaluated_population, unevaluated[\"Individual\"].to_numpy()])\n                                self.population.set_population(cur_evaluated_population, rng=self.rng)\n                            except Exception as e:\n                                print(\"Exception in survival selection\")\n                                print(e)\n                                print(\"new_population_index\", new_population_index)\n                                print(\"cur_evaluated_population\", cur_evaluated_population)\n                                print(\"unevaluated\", unevaluated)\n                                print(\"evaluated\", evaluated)\n                                print(\"scores\", scores)\n                                print(\"weighted_scores\", weighted_scores)\n                                print(\"self.objective_function_weights\", self.objective_function_weights)\n                                print(\"self.population_size\", self.population_size)\n                                print(\"parents_df\", parents_df)\n\n                ###############################\n                # Step 5: Parent Selection and Variation\n                ###############################\n                n_individuals_to_submit = self.max_queue_size - len(submitted_futures)\n                if n_individuals_to_submit &gt; 0:\n                    #count non-nan values in the objective columns\n                    if not enough_parents_evaluated:\n                        parents_df = self.population.get_column(self.population.population, column_names=self.objective_names, to_numpy=False)\n                        scores = parents_df[self.objective_names[0]].to_numpy()\n                        #count non-nan values in the objective columns\n                        n_evaluated = np.count_nonzero(~np.isnan(scores))\n                        if n_evaluated &gt;0 :\n                            enough_parents_evaluated=True\n\n                    # parents_df = self.population.get_column(self.population.population, column_names=self.objective_names+ [\"Individual\"], to_numpy=False)\n                    # parents_df = parents_df[~parents_df[self.objective_names].isin([\"TIMEOUT\",\"INVALID\"]).any(axis=1)]\n                    # parents_df = parents_df[~parents_df[self.objective_names].isna().any(axis=1)]\n\n                    # cur_evaluated_population = parents_df[\"Individual\"].to_numpy()\n                    # if len(cur_evaluated_population) &gt; 0:\n                    #     scores = parents_df[self.objective_names].to_numpy()\n                    #     weighted_scores = scores * self.objective_function_weights\n                    #     #number of crossover pairs and mutation only parent to generate\n\n                    #     if len(parents_df) &lt; 2:\n                    #         var_ops = [\"mutate\" for _ in range(n_individuals_to_submit)]\n                    #     else:\n                    #         var_ops = [self.rng.choice([\"crossover\",\"mutate_then_crossover\",\"crossover_then_mutate\",'mutate'],p=[self.crossover_probability,self.mutate_then_crossover_probability, self.crossover_then_mutate_probability,self.mutate_probability]) for _ in range(n_individuals_to_submit)]\n\n                    #     parents = []\n                    #     for op in var_ops:\n                    #         if op == \"mutate\":\n                    #             parents.extend(np.array(cur_evaluated_population)[self.parent_selector(weighted_scores, k=1, n_parents=1, rng=self.rng)])\n                    #         else:\n                    #             parents.extend(np.array(cur_evaluated_population)[self.parent_selector(weighted_scores, k=1, n_parents=2, rng=self.rng)])\n\n                    #     #_offspring = self.population.create_offspring2(parents, var_ops, rng=self.rng, add_to_population=True)\n                    #     offspring = self.population.create_offspring2(parents, var_ops, [ind_mutate], None, [ind_crossover], None, add_to_population=True, keep_repeats=False, mutate_until_unique=True, rng=self.rng)\n\n                    if enough_parents_evaluated:\n\n                        parents = self.population.parent_select(selector=self.parent_selector, weights=self.objective_function_weights, columns_names=self.objective_names, k=n_individuals_to_submit, n_parents=2, rng=self.rng)\n                        p = np.array([self.crossover_probability, self.mutate_then_crossover_probability, self.crossover_then_mutate_probability, self.mutate_probability])\n                        p = p / p.sum()\n                        var_op_list = self.rng.choice([\"crossover\", \"mutate_then_crossover\", \"crossover_then_mutate\", \"mutate\"], size=n_individuals_to_submit, p=p)\n\n                        for i, op in enumerate(var_op_list):\n                            if op == \"mutate\":\n                                parents[i] = parents[i][0] #mutations take a single individual\n\n                        offspring = self.population.create_offspring2(parents, var_op_list, [ind_mutate], None, [ind_crossover], None, add_to_population=True, keep_repeats=False, mutate_until_unique=True, rng=self.rng)\n\n                    # If we don't have enough evaluated individuals to use as parents for variation, we create new individuals randomly\n                    # This can happen if the individuals in the initial population are invalid\n                    elif len(submitted_futures) &lt; self.max_queue_size:\n\n                        initial_population = self.population.evaluated_individuals.iloc[:self.initial_population_size*3]\n                        invalid_initial_population = initial_population[initial_population[[\"Eval Error\"]].isin([\"TIMEOUT\",\"INVALID\"]).any(axis=1)]\n                        if len(invalid_initial_population) &gt;= self.initial_population_size*3: #if all individuals in the 3*initial population are invalid\n                            raise Exception(\"No individuals could be evaluated in the initial population. This may indicate a bug in the configuration, included models, or objective functions. Set verbose&gt;=4 to see the errors that caused individuals to fail.\")\n\n                        n_individuals_to_create = self.max_queue_size - len(submitted_futures)\n                        initial_population = [next(self.individual_generator) for _ in range(n_individuals_to_create)]\n                        self.population.add_to_population(initial_population, rng=self.rng)\n\n\n\n\n                ###############################\n                # Step 6: Add Unevaluated Individuals Generated by Variation\n                ###############################\n                individuals_to_evaluate = self.get_unevaluated_individuals(self.objective_names, budget=budget,)\n                individuals_to_evaluate = [ind for ind in individuals_to_evaluate if ind.unique_id() not in submitted_inds]\n                for individual in individuals_to_evaluate:\n                    if self.max_queue_size &gt; len(submitted_futures):\n                        future = self._client.submit(tpot.utils.eval_utils.eval_objective_list, individual,  self.objective_functions, verbose=self.verbose, timeout=self.max_eval_time_mins*60,**self.objective_kwargs)\n\n                        submitted_futures[future] = {\"individual\": individual,\n                                                    \"time\": time.time(),\n                                                    \"budget\": budget,}\n                        submitted_inds.add(individual.unique_id())\n                        self.population.update_column(individual, column_names=\"Submitted Timestamp\", data=time.time())\n\n\n                #Checkpointing\n                if self.population_file is not None: # and time.time() - last_save_time &gt; 60*10:\n                    pickle.dump(self.population, open(self.population_file, \"wb\"))\n\n\n\n        except KeyboardInterrupt:\n            if self.verbose &gt;= 3:\n                print(\"KeyboardInterrupt\")\n\n        ###############################\n        # Step 7: Cleanup\n        ###############################\n\n        self.population.remove_invalid_from_population(column_names=\"Eval Error\", invalid_value=\"INVALID\")\n        self.population.remove_invalid_from_population(column_names=\"Eval Error\", invalid_value=\"TIMEOUT\")\n\n\n        #done, cleanup futures\n        for future in submitted_futures.keys():\n            future.cancel()\n            future.release() #release the future\n\n        #I am not entirely sure if this is necessary. I believe that calling release on the futures should be enough to free up memory. If memory issues persist, this may be a good place to start.\n        #client.run(gc.collect) #run garbage collection to free up memory\n\n        #checkpoint\n        if self.population_file is not None:\n            pickle.dump(self.population, open(self.population_file, \"wb\"))\n\n        if self.client is None: #If we created our own client, close it\n            self._client.close()\n            self._cluster.close()\n\n        tpot.utils.get_pareto_frontier(self.population.evaluated_individuals, column_names=self.objective_names, weights=self.objective_function_weights)\n\n\n    def get_unevaluated_individuals(self, column_names, budget=None, individual_list=None):\n        \"\"\"\n        This function is used to get a list of individuals in the current population that have not been evaluated yet.\n\n        Parameters\n        ----------\n        column_names : list of strings\n            Names of the columns to check for unevaluated individuals (generally objective functions).\n        budget : float, default=None\n            Budget to use when checking for unevaluated individuals. If None, will not check the budget column.\n            Finds individuals who have not been evaluated with the given budget on column names.\n        individual_list : list of individuals, default=None\n            List of individuals to check for unevaluated individuals. If None, will use the current population.\n        \"\"\"\n        if individual_list is not None:\n            cur_pop = np.array(individual_list)\n        else:\n            cur_pop = np.array(self.population.population)\n\n        if all([name_step in self.population.evaluated_individuals.columns for name_step in column_names]):\n            if budget is not None:\n                offspring_scores = self.population.get_column(cur_pop, column_names=column_names+[\"Budget\"], to_numpy=False)\n                #Individuals are unevaluated if we have a higher budget OR if any of the objectives are nan\n                unevaluated_filter = lambda i: any(offspring_scores.loc[offspring_scores.index[i]][column_names].isna()) or (offspring_scores.loc[offspring_scores.index[i]][\"Budget\"] &lt; budget)\n            else:\n                offspring_scores = self.population.get_column(cur_pop, column_names=column_names, to_numpy=False)\n                unevaluated_filter = lambda i: any(offspring_scores.loc[offspring_scores.index[i]][column_names].isna())\n            unevaluated_individuals_this_step = [i for i in range(len(cur_pop)) if unevaluated_filter(i)]\n            return cur_pop[unevaluated_individuals_this_step]\n\n        else: #if column names are not in the evaluated_individuals, then we have not evaluated any individuals yet\n            for name_step in column_names:\n                self.population.evaluated_individuals[name_step] = np.nan\n            return cur_pop\n</code></pre>"},{"location":"documentation/tpot/evolvers/steady_state_evolver/#tpot.evolvers.steady_state_evolver.SteadyStateEvolver.__init__","title":"<code>__init__(individual_generator, objective_functions, objective_function_weights, objective_names=None, objective_kwargs=None, bigger_is_better=True, initial_population_size=50, population_size=300, max_evaluated_individuals=None, early_stop=None, early_stop_mins=None, early_stop_tol=0.001, max_time_mins=float('inf'), max_eval_time_mins=10, n_jobs=1, memory_limit='4GB', client=None, crossover_probability=0.2, mutate_probability=0.7, mutate_then_crossover_probability=0.05, crossover_then_mutate_probability=0.05, n_parents=2, survival_selector=survival_select_NSGA2, parent_selector=tournament_selection_dominated, budget_range=None, budget_scaling=0.5, individuals_until_end_budget=1, stepwise_steps=5, verbose=0, periodic_checkpoint_folder=None, callback=None, rng=None)</code>","text":"<p>Whereas the base_evolver uses a generational approach, the steady state evolver continuously generates individuals as resources become available.</p> <p>This evolver will simultaneously evaluated n_jobs individuals. As soon as one individual is evaluated, the current population is updated with survival_selector,  a new individual is generated from parents selected with parent_selector, and the new individual is immediately submitted for evaluation. In contrast, the base_evolver batches evaluations in generations, and only updates the population and creates new individuals after all individuals in the current generation are evaluated.</p> <p>In practice, this means that steady state evolver is more likely to use all cores at all times, allowing for flexibility is duration of evaluations and number of evaluations. However, it  may also generate less diverse populations as a result.</p> <p>Parameters:</p> Name Type Description Default <code>individual_generator</code> <code>generator</code> <p>Generator that yields new base individuals. Used to generate initial population.</p> required <code>objective_functions</code> <code>list of callables</code> <p>list of functions that get applied to the individual and return a float or list of floats If an objective function returns multiple values, they are all concatenated in order with respect to objective_function_weights and early_stop_tol.</p> required <code>objective_function_weights</code> <code>list of floats</code> <p>list of weights for each objective function. Sign flips whether bigger is better or not</p> required <code>objective_names</code> <code>list of strings</code> <p>Names of the objectives. If None, objective0, objective1, etc. will be used</p> <code>None</code> <code>objective_kwargs</code> <code>dict</code> <p>Dictionary of keyword arguments to pass to the objective function</p> <code>None</code> <code>bigger_is_better</code> <code>bool</code> <p>If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.</p> <code>True</code> <code>initial_population_size</code> <code>int</code> <p>Number of random individuals to generate in the initial population. These will all be randomly sampled, all other subsequent individuals will be generated from the population.</p> <code>50</code> <code>population_size</code> <code>int</code> <p>Note: This is different from the base_evolver.  In steady_state_evolver, the population_size is the number of individuals to keep in the live population. This is the total number of best individuals (as determined by survival_selector) to keep in the population. New individuals are generated from this population size. In base evolver, this is also the number of individuals to generate in each generation, however, here, we generate individuals as resources become available so there is no concept of a generation. It is recommended to use a higher population_size to ensure diversity in the population.</p> <code>50</code> <code>max_evaluated_individuals</code> <code>int</code> <p>Maximum number of individuals to evaluate after which training is terminated. If None, will evaluate until time limit is reached.</p> <code>None</code> <code>early_stop</code> <code>int</code> <p>If the best individual has not improved in this many evaluations, stop training. Note: Also different from base_evolver. In base evolver, this is the number of generations without improvement. Here, it is the number of individuals evaluated without improvement. Naturally, a higher value is recommended.</p> <code>None</code> <code>early_stop_mins</code> <code>int</code> <p>If the best individual has not improved in this many minutes, stop training.     early_stop_tol : float, list of floats, or None, default=0.001 -list of floats     list of tolerances for each objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged     If an index of the list is None, that item will not be used for early stopping -int     If an int is given, it will be used as the tolerance for all objectives</p> <code>None</code> <code>max_time_mins</code> <code>float</code> <p>Maximum time to run the optimization. If none or inf, will run until the end of the generations.</p> <code>float(\"inf\")</code> <code>max_eval_time_mins</code> <code>float</code> <p>Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.</p> <code>10</code> <code>n_jobs</code> <code>int</code> <p>Number of processes to run in parallel.</p> <code>1</code> <code>memory_limit</code> <code>str</code> <p>Memory limit for each job. See Dask LocalCluster documentation for more information.</p> <code>None</code> <code>client</code> <code>Client</code> <p>A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.</p> <code>None</code> <code>crossover_probability</code> <code>float</code> <p>Probability of generating a new individual by crossover between two individuals.</p> <code>.2</code> <code>mutate_probability</code> <code>float</code> <p>Probability of generating a new individual by crossover between one individuals.</p> <code>.7</code> <code>mutate_then_crossover_probability</code> <code>float</code> <p>Probability of generating a new individual by mutating two individuals followed by crossover.</p> <code>.05</code> <code>crossover_then_mutate_probability</code> <code>float</code> <p>Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.</p> <code>.05</code> <code>n_parents</code> <code>int</code> <p>Number of parents to use for crossover. Must be greater than 1.</p> <code>2</code> <code>survival_selector</code> <code>function</code> <p>Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes. Used to selected population_size * survival_percentage individuals at the start of each generation to use for mutation and crossover.</p> <code>survival_select_NSGA2</code> <code>parent_selector</code> <code>function</code> <p>Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.</p> <code>parent_select_NSGA2</code> <code>budget_range</code> <code>list[start, end]</code> <p>This parameter is used for the successive halving algorithm. A starting and ending budget to use for the budget scaling. The evolver will interpolate between these values over the generations_until_end_budget. Use is dependent on the objective functions. (In TPOTEstimator this corresponds to the percentage of the data to sample.)</p> <code>None</code> <code>budget_scaling</code> <p>A scaling factor to use when determining how fast we move the budget from the start to end budget.</p> <code>0.5</code> <code>evaluations_until_end_budget</code> <code>int</code> <p>The number of evaluations to run before reaching the max budget.</p> <code>1</code> <code>stepwise_steps</code> <code>int</code> <p>The number of staircase steps to take when interpolating the budget.</p> <code>1</code> <code>verbose</code> <code>int</code> <p>How much information to print during the optimization process. Higher values include the information from lower values. 0. nothing 1. progress bar 2. evaluations progress bar 3. best individual 4. warnings</p> <p>=5. full warnings trace</p> <code>0</code> <code>periodic_checkpoint_folder</code> <code>str</code> <p>Folder to save the population to periodically. If None, no periodic saving will be done. If provided, training will resume from this checkpoint.</p> <code>None</code> <code>callback</code> <code>CallBackInterface</code> <p>Callback object. Not implemented</p> <code>None</code> <code>rng</code> <code>(Generator, None)</code> <p>An object for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes</p> <ul> <li>Numpy.Random.Generator     Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'. Note this will be the same Generator passed in.</li> <li>None     Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS</li> </ul> <code>None</code> <p>Attributes:</p> Name Type Description <code>population</code> <code>Population</code> <p>The population of individuals. Use population.population to access the individuals in the current population. Use population.evaluated_individuals to access a data frame of all individuals that have been explored.</p> Source code in <code>tpot/evolvers/steady_state_evolver.py</code> <pre><code>def __init__(   self,\n                individual_generator ,\n\n                objective_functions,\n                objective_function_weights,\n                objective_names = None,\n                objective_kwargs = None,\n                bigger_is_better = True,\n\n                initial_population_size = 50,\n                population_size = 300,\n                max_evaluated_individuals = None,\n                early_stop = None,\n                early_stop_mins = None,\n                early_stop_tol = 0.001,\n\n\n                max_time_mins=float(\"inf\"),\n                max_eval_time_mins=10,\n\n                n_jobs=1,\n                memory_limit=\"4GB\",\n                client=None,\n\n                crossover_probability=.2,\n                mutate_probability=.7,\n                mutate_then_crossover_probability=.05,\n                crossover_then_mutate_probability=.05,\n                n_parents=2,\n\n                survival_selector = survival_select_NSGA2,\n                parent_selector = tournament_selection_dominated,\n\n                budget_range = None,\n                budget_scaling = .5,\n                individuals_until_end_budget = 1,\n                stepwise_steps = 5,\n\n                verbose = 0,\n                periodic_checkpoint_folder = None,\n                callback = None,\n\n                rng=None\n                ) -&gt; None:\n    \"\"\"\n    Whereas the base_evolver uses a generational approach, the steady state evolver continuously generates individuals as resources become available.\n\n    This evolver will simultaneously evaluated n_jobs individuals. As soon as one individual is evaluated, the current population is updated with survival_selector, \n    a new individual is generated from parents selected with parent_selector, and the new individual is immediately submitted for evaluation.\n    In contrast, the base_evolver batches evaluations in generations, and only updates the population and creates new individuals after all individuals in the current generation are evaluated.\n\n    In practice, this means that steady state evolver is more likely to use all cores at all times, allowing for flexibility is duration of evaluations and number of evaluations. However, it \n    may also generate less diverse populations as a result.\n\n    Parameters\n    ----------\n    individual_generator : generator\n        Generator that yields new base individuals. Used to generate initial population.\n    objective_functions : list of callables\n        list of functions that get applied to the individual and return a float or list of floats\n        If an objective function returns multiple values, they are all concatenated in order\n        with respect to objective_function_weights and early_stop_tol.\n    objective_function_weights : list of floats\n        list of weights for each objective function. Sign flips whether bigger is better or not\n    objective_names : list of strings, default=None\n        Names of the objectives. If None, objective0, objective1, etc. will be used\n    objective_kwargs : dict, default=None\n        Dictionary of keyword arguments to pass to the objective function\n    bigger_is_better : bool, default=True\n        If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n\n    initial_population_size : int, default=50\n        Number of random individuals to generate in the initial population. These will all be randomly sampled, all other subsequent individuals will be generated from the population.\n    population_size : int, default=50\n        Note: This is different from the base_evolver. \n        In steady_state_evolver, the population_size is the number of individuals to keep in the live population. This is the total number of best individuals (as determined by survival_selector) to keep in the population.\n        New individuals are generated from this population size.\n        In base evolver, this is also the number of individuals to generate in each generation, however, here, we generate individuals as resources become available so there is no concept of a generation.\n        It is recommended to use a higher population_size to ensure diversity in the population.\n    max_evaluated_individuals : int, default=None\n        Maximum number of individuals to evaluate after which training is terminated. If None, will evaluate until time limit is reached.\n    early_stop : int, default=None\n        If the best individual has not improved in this many evaluations, stop training.\n        Note: Also different from base_evolver. In base evolver, this is the number of generations without improvement. Here, it is the number of individuals evaluated without improvement. Naturally, a higher value is recommended.\n    early_stop_mins : int, default=None\n        If the best individual has not improved in this many minutes, stop training.\n            early_stop_tol : float, list of floats, or None, default=0.001\n        -list of floats\n            list of tolerances for each objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n            If an index of the list is None, that item will not be used for early stopping\n        -int\n            If an int is given, it will be used as the tolerance for all objectives\n    max_time_mins : float, default=float(\"inf\")\n        Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n    max_eval_time_mins : float, default=10\n        Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n    n_jobs : int, default=1\n        Number of processes to run in parallel.\n    memory_limit : str, default=None\n        Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n    client : dask.distributed.Client, default=None\n        A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.\n    crossover_probability : float, default=.2\n        Probability of generating a new individual by crossover between two individuals.\n    mutate_probability : float, default=.7\n        Probability of generating a new individual by crossover between one individuals.\n    mutate_then_crossover_probability : float, default=.05\n        Probability of generating a new individual by mutating two individuals followed by crossover.\n    crossover_then_mutate_probability : float, default=.05\n        Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.\n    n_parents : int, default=2\n        Number of parents to use for crossover. Must be greater than 1.\n    survival_selector : function, default=survival_select_NSGA2\n        Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes.\n        Used to selected population_size * survival_percentage individuals at the start of each generation to use for mutation and crossover.\n    parent_selector : function, default=parent_select_NSGA2\n        Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.     \n\n    budget_range : list [start, end], default=None\n        This parameter is used for the successive halving algorithm.\n        A starting and ending budget to use for the budget scaling. The evolver will interpolate between these values over the generations_until_end_budget.\n        Use is dependent on the objective functions. (In TPOTEstimator this corresponds to the percentage of the data to sample.)\n    budget_scaling float : [0,1], default=0.5\n        A scaling factor to use when determining how fast we move the budget from the start to end budget.\n    evaluations_until_end_budget : int, default=1\n        The number of evaluations to run before reaching the max budget.\n    stepwise_steps : int, default=1\n        The number of staircase steps to take when interpolating the budget.\n    verbose : int, default=0\n        How much information to print during the optimization process. Higher values include the information from lower values.\n        0. nothing\n        1. progress bar\n        2. evaluations progress bar\n        3. best individual\n        4. warnings\n        &gt;=5. full warnings trace\n    periodic_checkpoint_folder : str, default=None\n        Folder to save the population to periodically. If None, no periodic saving will be done.\n        If provided, training will resume from this checkpoint.\n    callback : tpot.CallBackInterface, default=None\n        Callback object. Not implemented\n    rng : Numpy.Random.Generator, None, default=None\n        An object for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes\n\n        - Numpy.Random.Generator\n            Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'. Note this will be the same Generator passed in.\n        - None\n            Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS\n\n    Attributes\n    ----------\n    population : tpot.Population\n        The population of individuals.\n        Use population.population to access the individuals in the current population.\n        Use population.evaluated_individuals to access a data frame of all individuals that have been explored.\n\n    \"\"\"\n\n    self.rng = np.random.default_rng(rng)\n\n    self.max_evaluated_individuals = max_evaluated_individuals\n    self.individuals_until_end_budget = individuals_until_end_budget\n\n    self.individual_generator = individual_generator\n    self.population_size = population_size\n    self.objective_functions = objective_functions\n    self.objective_function_weights = np.array(objective_function_weights)\n    self.bigger_is_better = bigger_is_better\n    if not bigger_is_better:\n        self.objective_function_weights = np.array(self.objective_function_weights)*-1\n\n    self.population_size_list = None\n\n\n    self.periodic_checkpoint_folder = periodic_checkpoint_folder\n    self.verbose  = verbose\n    self.callback = callback\n    self.n_jobs = n_jobs\n\n    if max_time_mins is None:\n        self.max_time_mins = float(\"inf\")\n    else:\n        self.max_time_mins = max_time_mins\n\n    #functools requires none for infinite time, doesn't support inf\n    if max_eval_time_mins is not None and math.isinf(max_eval_time_mins ):\n        self.max_eval_time_mins = None\n    else:\n        self.max_eval_time_mins = max_eval_time_mins\n\n    self.initial_population_size = initial_population_size\n    self.budget_range = budget_range\n    self.budget_scaling = budget_scaling\n    self.stepwise_steps = stepwise_steps\n\n    self.memory_limit = memory_limit\n\n    self.client = client\n\n\n    self.survival_selector=survival_selector\n    self.parent_selector=parent_selector\n\n\n    total_var_p = crossover_probability + mutate_probability + mutate_then_crossover_probability + crossover_then_mutate_probability\n    self.crossover_probability = crossover_probability / total_var_p\n    self.mutate_probability = mutate_probability  / total_var_p\n    self.mutate_then_crossover_probability= mutate_then_crossover_probability / total_var_p\n    self.crossover_then_mutate_probability= crossover_then_mutate_probability / total_var_p\n\n    self.n_parents = n_parents\n\n    if objective_kwargs is None:\n        self.objective_kwargs = {}\n    else:\n        self.objective_kwargs = objective_kwargs\n\n    ###########\n\n\n    if self.budget_range is None:\n        self.budget_list = None\n    else:\n        self.budget_list = beta_interpolation(start=self.budget_range[0], end=self.budget_range[1], n=self.generations_until_end_budget, scale=self.budget_scaling, n_steps=self.stepwise_steps)\n\n    if objective_names is None:\n        self.objective_names = [\"objective\"+str(i) for i in range(len(objective_function_weights))]\n    else:\n        self.objective_names = objective_names\n\n    if self.budget_list is not None:\n        if len(self.budget_list) &lt;= self.generation:\n            self.budget = self.budget_list[-1]\n        else:\n            self.budget = self.budget_list[self.generation]\n    else:\n        self.budget = None\n\n\n    self.early_stop_tol = early_stop_tol\n    self.early_stop_mins = early_stop_mins\n    self.early_stop = early_stop\n\n    if isinstance(self.early_stop_tol, float):\n        self.early_stop_tol = [self.early_stop_tol for _ in range(len(self.objective_names))]\n\n    self.early_stop_tol = [np.inf if tol is None else tol for tol in self.early_stop_tol]\n\n    self.population = None\n    self.population_file = None\n    if self.periodic_checkpoint_folder is not None:\n        self.population_file = os.path.join(self.periodic_checkpoint_folder, \"population.pkl\")\n        if not os.path.exists(self.periodic_checkpoint_folder):\n            os.makedirs(self.periodic_checkpoint_folder)\n        if os.path.exists(self.population_file):\n            self.population = pickle.load(open(self.population_file, \"rb\"))\n\n\n    init_names = self.objective_names\n    if self.budget_range is not None:\n        init_names = init_names + [\"Budget\"]\n    if self.population is None:\n        self.population = tpot.Population(column_names=init_names)\n        initial_population = [next(self.individual_generator) for _ in range(self.initial_population_size)]\n        self.population.add_to_population(initial_population, rng=self.rng)\n</code></pre>"},{"location":"documentation/tpot/evolvers/steady_state_evolver/#tpot.evolvers.steady_state_evolver.SteadyStateEvolver.get_unevaluated_individuals","title":"<code>get_unevaluated_individuals(column_names, budget=None, individual_list=None)</code>","text":"<p>This function is used to get a list of individuals in the current population that have not been evaluated yet.</p> <p>Parameters:</p> Name Type Description Default <code>column_names</code> <code>list of strings</code> <p>Names of the columns to check for unevaluated individuals (generally objective functions).</p> required <code>budget</code> <code>float</code> <p>Budget to use when checking for unevaluated individuals. If None, will not check the budget column. Finds individuals who have not been evaluated with the given budget on column names.</p> <code>None</code> <code>individual_list</code> <code>list of individuals</code> <p>List of individuals to check for unevaluated individuals. If None, will use the current population.</p> <code>None</code> Source code in <code>tpot/evolvers/steady_state_evolver.py</code> <pre><code>def get_unevaluated_individuals(self, column_names, budget=None, individual_list=None):\n    \"\"\"\n    This function is used to get a list of individuals in the current population that have not been evaluated yet.\n\n    Parameters\n    ----------\n    column_names : list of strings\n        Names of the columns to check for unevaluated individuals (generally objective functions).\n    budget : float, default=None\n        Budget to use when checking for unevaluated individuals. If None, will not check the budget column.\n        Finds individuals who have not been evaluated with the given budget on column names.\n    individual_list : list of individuals, default=None\n        List of individuals to check for unevaluated individuals. If None, will use the current population.\n    \"\"\"\n    if individual_list is not None:\n        cur_pop = np.array(individual_list)\n    else:\n        cur_pop = np.array(self.population.population)\n\n    if all([name_step in self.population.evaluated_individuals.columns for name_step in column_names]):\n        if budget is not None:\n            offspring_scores = self.population.get_column(cur_pop, column_names=column_names+[\"Budget\"], to_numpy=False)\n            #Individuals are unevaluated if we have a higher budget OR if any of the objectives are nan\n            unevaluated_filter = lambda i: any(offspring_scores.loc[offspring_scores.index[i]][column_names].isna()) or (offspring_scores.loc[offspring_scores.index[i]][\"Budget\"] &lt; budget)\n        else:\n            offspring_scores = self.population.get_column(cur_pop, column_names=column_names, to_numpy=False)\n            unevaluated_filter = lambda i: any(offspring_scores.loc[offspring_scores.index[i]][column_names].isna())\n        unevaluated_individuals_this_step = [i for i in range(len(cur_pop)) if unevaluated_filter(i)]\n        return cur_pop[unevaluated_individuals_this_step]\n\n    else: #if column names are not in the evaluated_individuals, then we have not evaluated any individuals yet\n        for name_step in column_names:\n            self.population.evaluated_individuals[name_step] = np.nan\n        return cur_pop\n</code></pre>"},{"location":"documentation/tpot/evolvers/steady_state_evolver/#tpot.evolvers.steady_state_evolver.SteadyStateEvolver.optimize","title":"<code>optimize()</code>","text":"<p>Creates an initial population and runs the evolutionary algorithm for the given number of generations.  If generations is None, will use self.generations.</p> Source code in <code>tpot/evolvers/steady_state_evolver.py</code> <pre><code>def optimize(self):\n    \"\"\"\n    Creates an initial population and runs the evolutionary algorithm for the given number of generations. \n    If generations is None, will use self.generations.\n    \"\"\"\n\n    #intialize the client\n    if self.client is not None: #If user passed in a client manually\n       self._client = self.client\n    else:\n\n        if self.verbose &gt;= 4:\n            silence_logs = 30\n        elif self.verbose &gt;=5:\n            silence_logs = 40\n        else:\n            silence_logs = 50\n        self._cluster = LocalCluster(n_workers=self.n_jobs, #if no client is passed in and no global client exists, create our own\n                threads_per_worker=1,\n                silence_logs=silence_logs,\n                processes=False,\n                memory_limit=self.memory_limit)\n        self._client = Client(self._cluster)\n\n\n    self.max_queue_size = len(self._client.cluster.workers)\n\n    #set up logging params\n    evaluated_count = 0\n    generations_without_improvement = np.array([0 for _ in range(len(self.objective_function_weights))])\n    timestamp_of_last_improvement = np.array([time.time() for _ in range(len(self.objective_function_weights))])\n    best_scores = [-np.inf for _ in range(len(self.objective_function_weights))]\n    scheduled_timeout_time = time.time() + self.max_time_mins*60\n    budget = None\n\n    submitted_futures = {}\n    submitted_inds = set()\n\n    start_time = time.time()\n\n    try:\n\n\n        if self.verbose &gt;= 1:\n            if self.max_evaluated_individuals is not None:\n                pbar = tqdm.tqdm(total=self.max_evaluated_individuals, miniters=1)\n            else:\n                pbar = tqdm.tqdm(total=0, miniters=1)\n            pbar.set_description(\"Evaluations\")\n\n        #submit initial population\n        individuals_to_evaluate = self.get_unevaluated_individuals(self.objective_names, budget=budget,)\n\n        for individual in individuals_to_evaluate:\n            if len(submitted_futures) &gt;= self.max_queue_size:\n                break\n            future = self._client.submit(tpot.utils.eval_utils.eval_objective_list, individual,  self.objective_functions, verbose=self.verbose, timeout=self.max_eval_time_mins*60,**self.objective_kwargs)\n\n            submitted_futures[future] = {\"individual\": individual,\n                                        \"time\": time.time(),\n                                        \"budget\": budget,}\n            submitted_inds.add(individual.unique_id())\n            self.population.update_column(individual, column_names=\"Submitted Timestamp\", data=time.time())\n\n        done = False\n        start_time = time.time()\n\n        enough_parents_evaluated=False\n        while not done:\n\n            ###############################\n            # Step 1: Check for finished futures\n            ###############################\n\n            #wait for at least one future to finish or timeout\n            try:\n                next(distributed.as_completed(submitted_futures, timeout=self.max_eval_time_mins*60))\n            except dask.distributed.TimeoutError:\n                pass\n            except dask.distributed.CancelledError:\n                pass\n\n            #Loop through all futures, collect completed and timeout futures.\n            for completed_future in list(submitted_futures.keys()):\n                eval_error = None\n                #get scores and update\n                if completed_future.done(): #if future is done\n                    #If the future is done but threw and error, record the error\n                    if completed_future.exception() or completed_future.status == \"error\": #if the future is done and threw an error\n                        print(\"Exception in future\")\n                        print(completed_future.exception())\n                        scores = [np.nan for _ in range(len(self.objective_names))]\n                        eval_error = \"INVALID\"\n                    elif completed_future.cancelled(): #if the future is done and was cancelled\n                        print(\"Cancelled future (likely memory related)\")\n                        scores = [np.nan for _ in range(len(self.objective_names))]\n                        eval_error = \"INVALID\"\n                        client.run(gc.collect)\n                    else: #if the future is done and did not throw an error, get the scores\n                        try:\n                            scores = completed_future.result()\n\n                            #check if scores contain \"INVALID\" or \"TIMEOUT\"\n                            if \"INVALID\" in scores:\n                                eval_error = \"INVALID\"\n                                scores = [np.nan]\n                            elif \"TIMEOUT\" in scores:\n                                eval_error = \"TIMEOUT\"\n                                scores = [np.nan]\n\n                        except Exception as e:\n                            print(\"Exception in future, but not caught by dask\")\n                            print(e)\n                            print(completed_future.exception())\n                            print(completed_future)\n                            print(\"status\", completed_future.status)\n                            print(\"done\", completed_future.done())\n                            print(\"cancelld \", completed_future.cancelled())\n                            scores = [np.nan for _ in range(len(self.objective_names))]\n                            eval_error = \"INVALID\"\n                    completed_future.release() #release the future\n                else: #if future is not done\n\n                    if self.max_eval_time_mins is not None:\n                        #check if the future has been running for too long, cancel the future\n                        if time.time() - submitted_futures[completed_future][\"time\"] &gt; self.max_eval_time_mins*1.25*60:\n                            completed_future.cancel()\n                            completed_future.release() #release the future\n                            if self.verbose &gt;= 4:\n                                print(f'WARNING AN INDIVIDUAL TIMED OUT (Fallback): \\n {submitted_futures[completed_future]} \\n')\n\n                            scores = [np.nan for _ in range(len(self.objective_names))]\n                            eval_error = \"TIMEOUT\"\n                        else:\n                            continue #otherwise, continue to next future\n\n\n\n                #update population\n                this_individual = submitted_futures[completed_future][\"individual\"]\n                this_budget = submitted_futures[completed_future][\"budget\"]\n                this_time = submitted_futures[completed_future][\"time\"]\n\n                if len(scores) &lt; len(self.objective_names):\n                    scores = [scores[0] for _ in range(len(self.objective_names))]\n                self.population.update_column(this_individual, column_names=self.objective_names, data=scores)\n                self.population.update_column(this_individual, column_names=\"Completed Timestamp\", data=time.time())\n                self.population.update_column(this_individual, column_names=\"Eval Error\", data=eval_error)\n                if budget is not None:\n                    self.population.update_column(this_individual, column_names=\"Budget\", data=this_budget)\n\n                submitted_futures.pop(completed_future)\n                submitted_inds.add(this_individual.unique_id())\n                if self.verbose &gt;= 1:\n                    pbar.update(1)\n\n            #now we have a list of completed futures\n\n            self.population.remove_invalid_from_population(column_names=\"Eval Error\", invalid_value=\"INVALID\")\n            self.population.remove_invalid_from_population(column_names=\"Eval Error\", invalid_value=\"TIMEOUT\")\n\n            #I am not entirely sure if this is necessary. I believe that calling release on the futures should be enough to free up memory. If memory issues persist, this may be a good place to start.\n            #client.run(gc.collect) #run garbage collection to free up memory\n\n            ###############################\n            # Step 2: Early Stopping\n            ###############################\n            if self.verbose &gt;= 3:\n                sign = np.sign(self.objective_function_weights)\n                valid_df = self.population.evaluated_individuals[~self.population.evaluated_individuals[[\"Eval Error\"]].isin([\"TIMEOUT\",\"INVALID\"]).any(axis=1)][self.objective_names]*sign\n                cur_best_scores = valid_df.max(axis=0)*sign\n                cur_best_scores = cur_best_scores.to_numpy()\n                for i, obj in enumerate(self.objective_names):\n                    print(f\"Best {obj} score: {cur_best_scores[i]}\")\n\n            if self.early_stop or self.early_stop_mins:\n                if self.budget is None or self.budget&gt;=self.budget_range[-1]: #self.budget&gt;=1:\n                    #get sign of objective_function_weights\n                    sign = np.sign(self.objective_function_weights)\n                    #get best score for each objective\n                    valid_df = self.population.evaluated_individuals[~self.population.evaluated_individuals[[\"Eval Error\"]].isin([\"TIMEOUT\",\"INVALID\"]).any(axis=1)][self.objective_names]*sign\n                    cur_best_scores = valid_df.max(axis=0)\n                    cur_best_scores = cur_best_scores.to_numpy()\n                    #cur_best_scores =  self.population.get_column(self.population.population, column_names=self.objective_names).max(axis=0)*sign #TODO this assumes the current population is the best\n\n                    improved = ( np.array(cur_best_scores) - np.array(best_scores) &gt;= np.array(self.early_stop_tol) )\n                    not_improved = np.logical_not(improved)\n                    generations_without_improvement = generations_without_improvement * not_improved + not_improved #set to zero if not improved, else increment\n\n                    timestamp_of_last_improvement = timestamp_of_last_improvement * not_improved + time.time()*improved #set to current time if improved\n\n                    pass\n                    #update best score\n                    best_scores = [max(best_scores[i], cur_best_scores[i]) for i in range(len(self.objective_names))]\n\n                    if self.early_stop:\n                        if all(generations_without_improvement&gt;self.early_stop):\n                            if self.verbose &gt;= 3:\n                                print(f\"Early stop ({self.early_stop} individuals evaluated without improvement)\")\n                            break\n\n                    if self.early_stop_mins:\n                        if any(time.time() - timestamp_of_last_improvement &gt; self.early_stop_mins*60):\n                            if self.verbose &gt;= 3:\n                                print(f\"Early stop  ({self.early_stop_mins} seconds passed without improvement)\")\n                            break\n\n            #if we evaluated enough individuals or time is up, stop\n            if self.max_time_mins is not None and time.time() - start_time &gt; self.max_time_mins*60:\n                if self.verbose &gt;= 3:\n                    print(\"Time limit reached\")\n                done = True\n                break\n\n            if self.max_evaluated_individuals is not None and len(self.population.evaluated_individuals.dropna(subset=self.objective_names)) &gt;= self.max_evaluated_individuals:\n                print(\"Evaluated enough individuals\")\n                done = True\n                break\n\n            ###############################\n            # Step 3: Submit unevaluated individuals from the initial population\n            ###############################\n            individuals_to_evaluate = self.get_unevaluated_individuals(self.objective_names, budget=budget,)\n            individuals_to_evaluate = [ind for ind in individuals_to_evaluate if ind.unique_id() not in submitted_inds]\n            for individual in individuals_to_evaluate:\n                if self.max_queue_size &gt; len(submitted_futures):\n                    future = self._client.submit(tpot.utils.eval_utils.eval_objective_list, individual,  self.objective_functions, verbose=self.verbose, timeout=self.max_eval_time_mins*60,**self.objective_kwargs)\n\n                    submitted_futures[future] = {\"individual\": individual,\n                                                \"time\": time.time(),\n                                                \"budget\": budget,}\n                    submitted_inds.add(individual.unique_id())\n\n                    self.population.update_column(individual, column_names=\"Submitted Timestamp\", data=time.time())\n\n\n            ###############################\n            # Step 4: Survival Selection\n            ###############################\n            if self.survival_selector is not None:\n                parents_df = self.population.get_column(self.population.population, column_names=self.objective_names + [\"Individual\"], to_numpy=False)\n                evaluated = parents_df[~parents_df[self.objective_names].isna().any(axis=1)]\n                if len(evaluated) &gt; self.population_size:\n                    unevaluated = parents_df[parents_df[self.objective_names].isna().any(axis=1)]\n\n                    cur_evaluated_population = parents_df[\"Individual\"].to_numpy()\n                    if len(cur_evaluated_population) &gt; self.population_size:\n                        scores = evaluated[self.objective_names].to_numpy()\n                        weighted_scores = scores * self.objective_function_weights\n                        new_population_index = np.ravel(self.survival_selector(weighted_scores, k=self.population_size, rng=self.rng)) #TODO make it clear that we are concatenating scores...\n\n                        #set new population\n                        try:\n                            cur_evaluated_population = np.array(cur_evaluated_population)[new_population_index]\n                            cur_evaluated_population = np.concatenate([cur_evaluated_population, unevaluated[\"Individual\"].to_numpy()])\n                            self.population.set_population(cur_evaluated_population, rng=self.rng)\n                        except Exception as e:\n                            print(\"Exception in survival selection\")\n                            print(e)\n                            print(\"new_population_index\", new_population_index)\n                            print(\"cur_evaluated_population\", cur_evaluated_population)\n                            print(\"unevaluated\", unevaluated)\n                            print(\"evaluated\", evaluated)\n                            print(\"scores\", scores)\n                            print(\"weighted_scores\", weighted_scores)\n                            print(\"self.objective_function_weights\", self.objective_function_weights)\n                            print(\"self.population_size\", self.population_size)\n                            print(\"parents_df\", parents_df)\n\n            ###############################\n            # Step 5: Parent Selection and Variation\n            ###############################\n            n_individuals_to_submit = self.max_queue_size - len(submitted_futures)\n            if n_individuals_to_submit &gt; 0:\n                #count non-nan values in the objective columns\n                if not enough_parents_evaluated:\n                    parents_df = self.population.get_column(self.population.population, column_names=self.objective_names, to_numpy=False)\n                    scores = parents_df[self.objective_names[0]].to_numpy()\n                    #count non-nan values in the objective columns\n                    n_evaluated = np.count_nonzero(~np.isnan(scores))\n                    if n_evaluated &gt;0 :\n                        enough_parents_evaluated=True\n\n                # parents_df = self.population.get_column(self.population.population, column_names=self.objective_names+ [\"Individual\"], to_numpy=False)\n                # parents_df = parents_df[~parents_df[self.objective_names].isin([\"TIMEOUT\",\"INVALID\"]).any(axis=1)]\n                # parents_df = parents_df[~parents_df[self.objective_names].isna().any(axis=1)]\n\n                # cur_evaluated_population = parents_df[\"Individual\"].to_numpy()\n                # if len(cur_evaluated_population) &gt; 0:\n                #     scores = parents_df[self.objective_names].to_numpy()\n                #     weighted_scores = scores * self.objective_function_weights\n                #     #number of crossover pairs and mutation only parent to generate\n\n                #     if len(parents_df) &lt; 2:\n                #         var_ops = [\"mutate\" for _ in range(n_individuals_to_submit)]\n                #     else:\n                #         var_ops = [self.rng.choice([\"crossover\",\"mutate_then_crossover\",\"crossover_then_mutate\",'mutate'],p=[self.crossover_probability,self.mutate_then_crossover_probability, self.crossover_then_mutate_probability,self.mutate_probability]) for _ in range(n_individuals_to_submit)]\n\n                #     parents = []\n                #     for op in var_ops:\n                #         if op == \"mutate\":\n                #             parents.extend(np.array(cur_evaluated_population)[self.parent_selector(weighted_scores, k=1, n_parents=1, rng=self.rng)])\n                #         else:\n                #             parents.extend(np.array(cur_evaluated_population)[self.parent_selector(weighted_scores, k=1, n_parents=2, rng=self.rng)])\n\n                #     #_offspring = self.population.create_offspring2(parents, var_ops, rng=self.rng, add_to_population=True)\n                #     offspring = self.population.create_offspring2(parents, var_ops, [ind_mutate], None, [ind_crossover], None, add_to_population=True, keep_repeats=False, mutate_until_unique=True, rng=self.rng)\n\n                if enough_parents_evaluated:\n\n                    parents = self.population.parent_select(selector=self.parent_selector, weights=self.objective_function_weights, columns_names=self.objective_names, k=n_individuals_to_submit, n_parents=2, rng=self.rng)\n                    p = np.array([self.crossover_probability, self.mutate_then_crossover_probability, self.crossover_then_mutate_probability, self.mutate_probability])\n                    p = p / p.sum()\n                    var_op_list = self.rng.choice([\"crossover\", \"mutate_then_crossover\", \"crossover_then_mutate\", \"mutate\"], size=n_individuals_to_submit, p=p)\n\n                    for i, op in enumerate(var_op_list):\n                        if op == \"mutate\":\n                            parents[i] = parents[i][0] #mutations take a single individual\n\n                    offspring = self.population.create_offspring2(parents, var_op_list, [ind_mutate], None, [ind_crossover], None, add_to_population=True, keep_repeats=False, mutate_until_unique=True, rng=self.rng)\n\n                # If we don't have enough evaluated individuals to use as parents for variation, we create new individuals randomly\n                # This can happen if the individuals in the initial population are invalid\n                elif len(submitted_futures) &lt; self.max_queue_size:\n\n                    initial_population = self.population.evaluated_individuals.iloc[:self.initial_population_size*3]\n                    invalid_initial_population = initial_population[initial_population[[\"Eval Error\"]].isin([\"TIMEOUT\",\"INVALID\"]).any(axis=1)]\n                    if len(invalid_initial_population) &gt;= self.initial_population_size*3: #if all individuals in the 3*initial population are invalid\n                        raise Exception(\"No individuals could be evaluated in the initial population. This may indicate a bug in the configuration, included models, or objective functions. Set verbose&gt;=4 to see the errors that caused individuals to fail.\")\n\n                    n_individuals_to_create = self.max_queue_size - len(submitted_futures)\n                    initial_population = [next(self.individual_generator) for _ in range(n_individuals_to_create)]\n                    self.population.add_to_population(initial_population, rng=self.rng)\n\n\n\n\n            ###############################\n            # Step 6: Add Unevaluated Individuals Generated by Variation\n            ###############################\n            individuals_to_evaluate = self.get_unevaluated_individuals(self.objective_names, budget=budget,)\n            individuals_to_evaluate = [ind for ind in individuals_to_evaluate if ind.unique_id() not in submitted_inds]\n            for individual in individuals_to_evaluate:\n                if self.max_queue_size &gt; len(submitted_futures):\n                    future = self._client.submit(tpot.utils.eval_utils.eval_objective_list, individual,  self.objective_functions, verbose=self.verbose, timeout=self.max_eval_time_mins*60,**self.objective_kwargs)\n\n                    submitted_futures[future] = {\"individual\": individual,\n                                                \"time\": time.time(),\n                                                \"budget\": budget,}\n                    submitted_inds.add(individual.unique_id())\n                    self.population.update_column(individual, column_names=\"Submitted Timestamp\", data=time.time())\n\n\n            #Checkpointing\n            if self.population_file is not None: # and time.time() - last_save_time &gt; 60*10:\n                pickle.dump(self.population, open(self.population_file, \"wb\"))\n\n\n\n    except KeyboardInterrupt:\n        if self.verbose &gt;= 3:\n            print(\"KeyboardInterrupt\")\n\n    ###############################\n    # Step 7: Cleanup\n    ###############################\n\n    self.population.remove_invalid_from_population(column_names=\"Eval Error\", invalid_value=\"INVALID\")\n    self.population.remove_invalid_from_population(column_names=\"Eval Error\", invalid_value=\"TIMEOUT\")\n\n\n    #done, cleanup futures\n    for future in submitted_futures.keys():\n        future.cancel()\n        future.release() #release the future\n\n    #I am not entirely sure if this is necessary. I believe that calling release on the futures should be enough to free up memory. If memory issues persist, this may be a good place to start.\n    #client.run(gc.collect) #run garbage collection to free up memory\n\n    #checkpoint\n    if self.population_file is not None:\n        pickle.dump(self.population, open(self.population_file, \"wb\"))\n\n    if self.client is None: #If we created our own client, close it\n        self._client.close()\n        self._cluster.close()\n\n    tpot.utils.get_pareto_frontier(self.population.evaluated_individuals, column_names=self.objective_names, weights=self.objective_function_weights)\n</code></pre>"},{"location":"documentation/tpot/evolvers/steady_state_evolver/#tpot.evolvers.steady_state_evolver.ind_crossover","title":"<code>ind_crossover(ind1, ind2, rng)</code>","text":"<p>Calls the ind1.crossover(ind2, rng=rng)</p> <p>Parameters:</p> Name Type Description Default <code>ind1</code> <code>BaseIndividual</code> required <code>ind2</code> <code>BaseIndividual</code> required <code>rng</code> <code>int or Generator</code> <p>A numpy random generator to use for reproducibility</p> required Source code in <code>tpot/evolvers/steady_state_evolver.py</code> <pre><code>def ind_crossover(ind1, ind2, rng):\n    \"\"\"\n    Calls the ind1.crossover(ind2, rng=rng)\n    Parameters\n    ----------\n    ind1 : tpot.BaseIndividual\n    ind2 : tpot.BaseIndividual\n    rng : int or numpy.random.Generator\n        A numpy random generator to use for reproducibility\n    \"\"\"\n    rng = np.random.default_rng(rng)\n    return ind1.crossover(ind2, rng=rng)\n</code></pre>"},{"location":"documentation/tpot/evolvers/steady_state_evolver/#tpot.evolvers.steady_state_evolver.ind_mutate","title":"<code>ind_mutate(ind, rng)</code>","text":"<p>Calls the ind.mutate method on the individual</p> <p>Parameters:</p> Name Type Description Default <code>ind</code> <code>BaseIndividual</code> <p>The individual to mutate</p> required <code>rng</code> <code>int or Generator</code> <p>A numpy random generator to use for reproducibility</p> required Source code in <code>tpot/evolvers/steady_state_evolver.py</code> <pre><code>def ind_mutate(ind, rng):\n    \"\"\"\n    Calls the ind.mutate method on the individual\n\n    Parameters\n    ----------\n    ind : tpot.BaseIndividual\n        The individual to mutate\n    rng : int or numpy.random.Generator\n        A numpy random generator to use for reproducibility\n    \"\"\"\n    rng = np.random.default_rng(rng)\n    return ind.mutate(rng=rng)\n</code></pre>"},{"location":"documentation/tpot/objectives/average_path_length/","title":"Average path length","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/objectives/average_path_length/#tpot.objectives.average_path_length.average_path_length_objective","title":"<code>average_path_length_objective(graph_pipeline)</code>","text":"<p>Computes the average shortest path from all nodes to the root/final estimator (only supported for GraphPipeline)</p> <p>Parameters:</p> Name Type Description Default <code>graph_pipeline</code> <p>The pipeline to compute the average path length for</p> required Source code in <code>tpot/objectives/average_path_length.py</code> <pre><code>def average_path_length_objective(graph_pipeline):\n    \"\"\"\n    Computes the average shortest path from all nodes to the root/final estimator (only supported for GraphPipeline)\n\n    Parameters\n    ----------\n    graph_pipeline: GraphPipeline\n        The pipeline to compute the average path length for\n\n    \"\"\"\n\n    path_lengths =  nx.shortest_path_length(graph_pipeline.graph, source=graph_pipeline.root)\n    return np.mean(np.array(list(path_lengths.values())))+1\n</code></pre>"},{"location":"documentation/tpot/objectives/complexity/","title":"Complexity","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/objectives/complexity/#tpot.objectives.complexity.complexity_scorer","title":"<code>complexity_scorer(est, X=None, y=None)</code>","text":"<p>Estimates the number of learned parameters across all classifiers and regressors in the pipelines.  Additionally, currently transformers add 1 point and selectors add 0 points (since they don't affect the complexity of the \"final\" predictive pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>est</code> <p>The estimator or pipeline to compute the complexity for</p> required <code>X</code> <p>The input samples (unused)</p> <code>None</code> <code>y</code> <p>The target values (unused)</p> <code>None</code> Source code in <code>tpot/objectives/complexity.py</code> <pre><code>def complexity_scorer(est, X=None, y=None):\n    \"\"\"\n    Estimates the number of learned parameters across all classifiers and regressors in the pipelines. \n    Additionally, currently transformers add 1 point and selectors add 0 points (since they don't affect the complexity of the \"final\" predictive pipeline.\n\n    Parameters\n    ----------\n    est: sklearn.base.BaseEstimator\n        The estimator or pipeline to compute the complexity for\n    X: array-like\n        The input samples (unused)\n    y: array-like\n        The target values (unused)\n\n    \"\"\"\n    return calculate_model_complexity(est)\n</code></pre>"},{"location":"documentation/tpot/objectives/number_of_leaves/","title":"Number of leaves","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/objectives/number_of_leaves/#tpot.objectives.number_of_leaves.number_of_leaves_objective","title":"<code>number_of_leaves_objective(est)</code>","text":"<p>Calculates the number of leaves (input nodes) in a GraphPipeline</p> <p>Parameters:</p> Name Type Description Default <code>est</code> <p>The pipeline to compute the number of leaves for</p> required Source code in <code>tpot/objectives/number_of_leaves.py</code> <pre><code>def number_of_leaves_objective(est):\n    \"\"\"\n    Calculates the number of leaves (input nodes) in a GraphPipeline\n\n    Parameters\n    ----------\n    est: GraphPipeline\n        The pipeline to compute the number of leaves for\n    \"\"\"\n    return len([v for v, d in est.graph.out_degree() if d == 0])\n</code></pre>"},{"location":"documentation/tpot/objectives/number_of_nodes/","title":"Number of nodes","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/objectives/number_of_nodes/#tpot.objectives.number_of_nodes.number_of_nodes_objective","title":"<code>number_of_nodes_objective(est)</code>","text":"<p>Calculates the number of leaves (input nodes) in an sklearn pipeline</p> <p>Parameters:</p> Name Type Description Default <code>est</code> <p>The pipeline to compute the number of nodes from.</p> required Source code in <code>tpot/objectives/number_of_nodes.py</code> <pre><code>def number_of_nodes_objective(est):\n    \"\"\"\n    Calculates the number of leaves (input nodes) in an sklearn pipeline\n\n    Parameters\n    ----------\n    est: GraphPipeline | Pipeline | FeatureUnion | BaseEstimator\n        The pipeline to compute the number of nodes from.\n    \"\"\"\n\n    if isinstance(est, GraphPipeline):\n        return sum(number_of_nodes_objective(est.graph.nodes[node][\"instance\"]) for node in est.graph.nodes)\n    if isinstance(est, Pipeline):\n        return sum(number_of_nodes_objective(estimator) for _,estimator in est.steps)\n    if isinstance(est, sklearn.pipeline.FeatureUnion):\n        return sum(number_of_nodes_objective(estimator) for _,estimator in est.transformer_list)\n\n    return 1\n</code></pre>"},{"location":"documentation/tpot/old_config_utils/old_config_utils/","title":"Old config utils","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/old_config_utils/old_config_utils/#tpot.old_config_utils.old_config_utils.convert_config_dict_to_choicepipeline","title":"<code>convert_config_dict_to_choicepipeline(config_dict)</code>","text":"<p>Takes in a TPOT config dictionary and returns a ChoicePipeline search space that represents the config_dict. This space will sample from all included modules in the config_dict.</p> <p>Parameters:</p> Name Type Description Default <code>config_dict</code> <code>dict</code> <p>The dictionary representation of the TPOT config.</p> required <p>Returns:</p> Type Description <code>ChoicePipeline</code> <p>A ChoicePipeline search space that represents the config_dict.</p> Source code in <code>tpot/old_config_utils/old_config_utils.py</code> <pre><code>def convert_config_dict_to_choicepipeline(config_dict):\n    \"\"\"\n    Takes in a TPOT config dictionary and returns a ChoicePipeline search space that represents the config_dict.\n    This space will sample from all included modules in the config_dict.\n\n    Parameters\n    ----------\n    config_dict : dict\n        The dictionary representation of the TPOT config.\n\n    Returns\n    -------\n    ChoicePipeline\n        A ChoicePipeline search space that represents the config_dict.\n    \"\"\"\n    search_spaces = []\n    for key, value in config_dict.items():\n        search_spaces.append(get_node_space(key, value))\n    return ChoicePipeline(search_spaces)\n</code></pre>"},{"location":"documentation/tpot/old_config_utils/old_config_utils/#tpot.old_config_utils.old_config_utils.convert_config_dict_to_graphpipeline","title":"<code>convert_config_dict_to_graphpipeline(config_dict)</code>","text":"<p>Takes in a TPOT config dictionary and returns a GraphSearchPipeline search space that represents the config_dict. This space will sample from all included modules in the config_dict. It will also identify classifiers/regressors to set the search space for the root node.</p> <p>Note doesn't convert estimators so they passthrough inputs like in TPOT1</p> <p>Parameters:</p> Name Type Description Default <code>config_dict</code> <code>dict</code> <p>The dictionary representation of the TPOT config.</p> required <p>Returns:</p> Type Description <code>GraphSearchPipeline</code> <p>A GraphSearchPipeline search space that represents the config_dict.</p> Source code in <code>tpot/old_config_utils/old_config_utils.py</code> <pre><code>def convert_config_dict_to_graphpipeline(config_dict):\n    \"\"\"\n    Takes in a TPOT config dictionary and returns a GraphSearchPipeline search space that represents the config_dict.\n    This space will sample from all included modules in the config_dict. It will also identify classifiers/regressors to set the search space for the root node.\n\n    Note doesn't convert estimators so they passthrough inputs like in TPOT1\n    Parameters\n    ----------\n    config_dict : dict\n        The dictionary representation of the TPOT config.\n\n    Returns\n    -------\n    GraphSearchPipeline\n        A GraphSearchPipeline search space that represents the config_dict.\n    \"\"\"\n    root_search_spaces = []\n    inner_search_spaces = []\n\n    for key, value in config_dict.items():\n        #if root\n        if issubclass(load_get_module_from_string(key), sklearn.base.ClassifierMixin) or issubclass(load_get_module_from_string(key), sklearn.base.RegressorMixin):\n            root_search_spaces.append(get_node_space(key, value))\n        else:\n            inner_search_spaces.append(get_node_space(key, value))\n\n    if len(root_search_spaces) == 0:\n        Warning(\"No classifiers or regressors found, allowing any estimator to be the root node\")\n        root_search_spaces = inner_search_spaces\n\n    #merge inner and root search spaces\n\n    inner_space = np.concatenate([root_search_spaces,inner_search_spaces])\n\n    root_space = ChoicePipeline(root_search_spaces)\n    inner_space = ChoicePipeline(inner_search_spaces)\n\n    final_space = GraphSearchPipeline(root_search_space=root_space, inner_search_space=inner_space)\n    return final_space\n</code></pre>"},{"location":"documentation/tpot/old_config_utils/old_config_utils/#tpot.old_config_utils.old_config_utils.convert_config_dict_to_linearpipeline","title":"<code>convert_config_dict_to_linearpipeline(config_dict)</code>","text":"<p>Takes in a TPOT config dictionary and returns a GraphSearchPipeline search space that represents the config_dict. This space will sample from all included modules in the config_dict. It will also identify classifiers/regressors to set the search space for the root node.</p> <p>Note doesn't convert estimators so they passthrough inputs like in TPOT1</p> <p>Parameters:</p> Name Type Description Default <code>config_dict</code> <code>dict</code> <p>The dictionary representation of the TPOT config.</p> required <p>Returns:</p> Type Description <code>GraphSearchPipeline</code> <p>A GraphSearchPipeline search space that represents the config_dict.</p> Source code in <code>tpot/old_config_utils/old_config_utils.py</code> <pre><code>def convert_config_dict_to_linearpipeline(config_dict):\n    \"\"\"\n    Takes in a TPOT config dictionary and returns a GraphSearchPipeline search space that represents the config_dict.\n    This space will sample from all included modules in the config_dict. It will also identify classifiers/regressors to set the search space for the root node.\n\n    Note doesn't convert estimators so they passthrough inputs like in TPOT1\n    Parameters\n    ----------\n    config_dict : dict\n        The dictionary representation of the TPOT config.\n\n    Returns\n    -------\n    GraphSearchPipeline\n        A GraphSearchPipeline search space that represents the config_dict.\n    \"\"\"\n    root_search_spaces = []\n    inner_search_spaces = []\n\n    for key, value in config_dict.items():\n        #if root\n        if issubclass(load_get_module_from_string(key), sklearn.base.ClassifierMixin) or issubclass(load_get_module_from_string(key), sklearn.base.RegressorMixin):\n            root_search_spaces.append(get_node_space(key, value))\n        else:\n            inner_search_spaces.append(get_node_space(key, value))\n\n    if len(root_search_spaces) == 0:\n        Warning(\"No classifiers or regressors found, allowing any estimator to be the root node\")\n        root_search_spaces = inner_search_spaces\n\n    #merge inner and root search spaces\n\n    inner_space = np.concatenate([root_search_spaces,inner_search_spaces])\n\n    root_space = ChoicePipeline(root_search_spaces)\n    inner_space = ChoicePipeline(inner_search_spaces)\n\n    final_space = SequentialPipeline([\n        DynamicLinearPipeline(inner_space, 10),\n        root_space\n    ])\n    return final_space\n</code></pre>"},{"location":"documentation/tpot/old_config_utils/old_config_utils/#tpot.old_config_utils.old_config_utils.convert_config_dict_to_list","title":"<code>convert_config_dict_to_list(config_dict)</code>","text":"<p>Takes in a TPOT config dictionary and returns a list of search spaces (EstimatorNode, WrapperPipeline)</p> <p>Parameters:</p> Name Type Description Default <code>config_dict</code> <code>dict</code> <p>The dictionary representation of the TPOT config.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of search spaces (EstimatorNode, WrapperPipeline) that represent the config_dict.</p> Source code in <code>tpot/old_config_utils/old_config_utils.py</code> <pre><code>def convert_config_dict_to_list(config_dict):\n    \"\"\"\n    Takes in a TPOT config dictionary and returns a list of search spaces (EstimatorNode, WrapperPipeline)\n\n    Parameters\n    ----------\n    config_dict : dict\n        The dictionary representation of the TPOT config.\n\n    Returns\n    -------\n    list\n        A list of search spaces (EstimatorNode, WrapperPipeline) that represent the config_dict.\n    \"\"\"\n    search_spaces = []\n    for key, value in config_dict.items():\n        search_spaces.append(get_node_space(key, value))\n    return search_spaces\n</code></pre>"},{"location":"documentation/tpot/old_config_utils/old_config_utils/#tpot.old_config_utils.old_config_utils.get_node_space","title":"<code>get_node_space(module_string, params)</code>","text":"<p>Create the search space for a single node in the TPOT config.</p> <p>Parameters:</p> Name Type Description Default <code>module_string</code> <code>str</code> <p>The string representation of the module and class to load. E.g. 'sklearn.ensemble.RandomForestClassifier'</p> required <code>params</code> <code>dict</code> <p>The dictionary representation of the hyperparameter search space for the module_string.</p> required <p>Returns:</p> Type Description <code>EstimatorNode or WrapperPipeline</code> Source code in <code>tpot/old_config_utils/old_config_utils.py</code> <pre><code>def get_node_space(module_string, params):\n    \"\"\"\n    Create the search space for a single node in the TPOT config.\n\n    Parameters\n    ----------\n    module_string : str\n        The string representation of the module and class to load. E.g. 'sklearn.ensemble.RandomForestClassifier'\n    params : dict\n        The dictionary representation of the hyperparameter search space for the module_string.\n\n    Returns\n    -------\n    EstimatorNode or WrapperPipeline\n    \"\"\"\n    method = load_get_module_from_string(module_string)\n    config_space = ConfigurationSpace()\n    sub_space = None\n    sub_space_name = None\n\n    function_params_conversion_dict = {}\n\n    if params is None:\n        return EstimatorNode(method=method, space=config_space)\n\n    for param_name, param in params.items():\n        if param is None:\n            config_space.add(Categorical(param_name, [None]))\n\n        if isinstance(param, range):\n            param = list(param)\n\n        if isinstance(param, list) or isinstance(param, np.ndarray):\n            if len(param) == 1:\n                p = param[0]\n                config_space.add(ConfigSpace.hyperparameters.Constant(param_name, p))\n            else:\n                config_space.add(Categorical(param_name, param))\n            # if all(isinstance(i, int) for i in param):\n            #     config_space.add_hyperparameter(Integer(param_name, (min(param), max(param))))\n            # elif all(isinstance(i, float) for i in param):\n            #     config_space.add_hyperparameter(Float(param_name, (min(param), max(param))))\n            # else:\n            #     config_space.add_hyperparameter(Categorical(param_name, param))\n        elif isinstance(param, dict): #TPOT1 config dicts have dictionaries for values of hyperparameters that are either a function or an estimator\n            if len(param) &gt; 1:\n                    raise ValueError(f\"Multiple items in dictionary entry for {param_name}\")\n\n            key = list(param.keys())[0]\n\n            innermethod = load_get_module_from_string(key)\n\n            if inspect.isclass(innermethod) and issubclass(innermethod, sklearn.base.BaseEstimator): #is an estimator\n                if sub_space is None:\n                    sub_space_name = param_name\n                    sub_space = get_node_space(key, param[key])   \n                else:\n                    raise ValueError(\"Only multiple hyperparameters are estimators. Only one parameter \")\n\n            else: #assume the key is a function and ignore the value\n                function_params_conversion_dict[param_name] = innermethod\n\n        else:\n            # config_space.add_hyperparameter(Categorical(param_name, param))\n            config_space.add(ConfigSpace.hyperparameters.Constant(param_name, param))\n\n    parser=None\n    if len(function_params_conversion_dict) &gt; 0:\n        parser = partial(hyperparameter_parser, function_params_conversion_dict)\n\n\n    if sub_space is None:\n\n        if parser is not None:\n            return EstimatorNode(method=method, space=config_space, hyperparameter_parser=parser)\n        else:\n            return EstimatorNode(method=method, space=config_space)\n\n\n    else:\n        if parser is not None:\n            return WrapperPipeline(method=method, space=config_space, estimator_search_space=sub_space, wrapped_param_name=sub_space_name, hyperparameter_parser=parser)\n        else:\n            return WrapperPipeline(method=method, space=config_space, estimator_search_space=sub_space, wrapped_param_name=sub_space_name)\n</code></pre>"},{"location":"documentation/tpot/old_config_utils/old_config_utils/#tpot.old_config_utils.old_config_utils.load_get_module_from_string","title":"<code>load_get_module_from_string(module_string)</code>","text":"<p>Takes a string in the form of 'module.submodule.class' and returns the class.</p> <p>Parameters:</p> Name Type Description Default <code>module_string</code> <code>str</code> <p>The string representation of the module and class to load.</p> required <p>Returns:</p> Type Description <code>class</code> <p>The class that was loaded from the module string.</p> Source code in <code>tpot/old_config_utils/old_config_utils.py</code> <pre><code>def load_get_module_from_string(module_string):\n    \"\"\"\n    Takes a string in the form of 'module.submodule.class' and returns the class.\n\n    Parameters\n    ----------\n    module_string : str\n        The string representation of the module and class to load.\n\n    Returns\n    -------\n    class\n        The class that was loaded from the module string.\n    \"\"\"\n    module_name, class_name = module_string.rsplit('.', 1)\n    module = __import__(module_name, fromlist=[class_name])\n    return getattr(module, class_name)\n</code></pre>"},{"location":"documentation/tpot/search_spaces/base/","title":"Base","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/search_spaces/base/#tpot.search_spaces.base.SklearnIndividual","title":"<code>SklearnIndividual</code>","text":"<p>               Bases: <code>BaseIndividual</code></p> Source code in <code>tpot/search_spaces/base.py</code> <pre><code>class SklearnIndividual(tpot.BaseIndividual):\n\n    def __init_subclass__(cls):\n        cls.crossover = cls.validate_same_type(cls.crossover)\n\n\n    def __init__(self,) -&gt; None:\n        super().__init__()\n\n    def mutate(self, rng=None):\n        return\n\n    def crossover(self, other, rng=None, **kwargs):\n        return \n\n    @final\n    def validate_same_type(func):\n\n        def wrapper(self, other, rng=None, **kwargs):\n            if not isinstance(other, type(self)):\n                return False\n            return func(self, other, rng=rng, **kwargs)\n\n        return wrapper\n\n    def export_pipeline(self, **kwargs) -&gt; BaseEstimator:\n        return\n\n    def unique_id(self):\n        \"\"\"\n        Returns a unique identifier for the individual. Used for preventing duplicate individuals from being evaluated.\n        \"\"\"\n        return self\n\n    #TODO currently TPOT population class manually uses the unique_id to generate the index for the population data frame.\n    #alternatively, the index could be the individual itself, with the __eq__ and __hash__ methods implemented.\n\n    # Though this breaks the graphpipeline. When a mutation is called, it changes the __eq__ and __hash__ outputs.\n    # Since networkx uses the hash and eq to determine if a node is already in the graph, this causes the graph thing that \n    # This is a new node not in the graph. But this could be changed if when the graphpipeline mutates nodes, \n    # it \"replaces\" the existing node with the mutated node. This would require a change in the graphpipeline class.\n\n    # def __eq__(self, other):\n    #     return self.unique_id() == other.unique_id()\n\n    # def __hash__(self):\n    #     return hash(self.unique_id())\n\n    #number of components in the pipeline\n    def get_size(self):\n        return 1\n\n    @final\n    def export_flattened_graphpipeline(self, **graphpipeline_kwargs) -&gt; tpot.GraphPipeline:\n        return flatten_to_graphpipeline(self.export_pipeline(), **graphpipeline_kwargs)\n</code></pre>"},{"location":"documentation/tpot/search_spaces/base/#tpot.search_spaces.base.SklearnIndividual.unique_id","title":"<code>unique_id()</code>","text":"<p>Returns a unique identifier for the individual. Used for preventing duplicate individuals from being evaluated.</p> Source code in <code>tpot/search_spaces/base.py</code> <pre><code>def unique_id(self):\n    \"\"\"\n    Returns a unique identifier for the individual. Used for preventing duplicate individuals from being evaluated.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"documentation/tpot/search_spaces/tuple_index/","title":"Tuple index","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/search_spaces/tuple_index/#tpot.search_spaces.tuple_index.TupleIndex","title":"<code>TupleIndex</code>","text":"<p>TPOT uses tuples to create a unique id for some pipeline search spaces. However, tuples sometimes don't interact correctly with pandas indexes. This class is a wrapper around a tuple that allows it to be used as a key in a dictionary, without it being an itereable.</p> <p>An alternative could be to make unique id return a string, but this would not work with graphpipelines, which require a special object. This class allows linear pipelines to contain graph pipelines while still being able to be used as a key in a dictionary.</p> Source code in <code>tpot/search_spaces/tuple_index.py</code> <pre><code>class TupleIndex():\n    \"\"\"\n    TPOT uses tuples to create a unique id for some pipeline search spaces. However, tuples sometimes don't interact correctly with pandas indexes.\n    This class is a wrapper around a tuple that allows it to be used as a key in a dictionary, without it being an itereable.\n\n    An alternative could be to make unique id return a string, but this would not work with graphpipelines, which require a special object.\n    This class allows linear pipelines to contain graph pipelines while still being able to be used as a key in a dictionary.\n\n    \"\"\"\n    def __init__(self, tup):\n        self.tup = tup\n\n    def __eq__(self,other) -&gt; bool:\n        return self.tup == other\n\n    def __hash__(self) -&gt; int:\n        return self.tup.__hash__()\n\n    def __str__(self) -&gt; str:\n        return self.tup.__str__()\n\n    def __repr__(self) -&gt; str:\n        return self.tup.__repr__()\n</code></pre>"},{"location":"documentation/tpot/search_spaces/nodes/estimator_node/","title":"Estimator node","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/search_spaces/nodes/estimator_node/#tpot.search_spaces.nodes.estimator_node.EstimatorNodeIndividual","title":"<code>EstimatorNodeIndividual</code>","text":"<p>               Bases: <code>SklearnIndividual</code></p> <p>Note that ConfigurationSpace does not support None as a parameter. Instead, use the special string \"\". TPOT will automatically replace instances of this string with the Python None.  <p>Parameters:</p> Name Type Description Default <code>method</code> <code>type</code> <p>The class of the estimator to be used</p> required <code>space</code> <code>ConfigurationSpace | dict</code> <p>The hyperparameter space to be used. If a dict is passed, hyperparameters are fixed and not learned.</p> required Source code in <code>tpot/search_spaces/nodes/estimator_node.py</code> <pre><code>class EstimatorNodeIndividual(SklearnIndividual):\n    \"\"\"\n    Note that ConfigurationSpace does not support None as a parameter. Instead, use the special string \"&lt;NONE&gt;\". TPOT will automatically replace instances of this string with the Python None. \n\n    Parameters\n    ----------\n    method : type\n        The class of the estimator to be used\n\n    space : ConfigurationSpace|dict\n        The hyperparameter space to be used. If a dict is passed, hyperparameters are fixed and not learned.\n\n    \"\"\"\n    def __init__(self, method: type, \n                        space: ConfigurationSpace|dict, #TODO If a dict is passed, hyperparameters are fixed and not learned. Is this confusing? Should we make a second node type?\n                        hyperparameter_parser: callable = None,\n                        rng=None) -&gt; None:\n        super().__init__()\n        self.method = method\n        self.space = space\n\n        if hyperparameter_parser is None:\n            self.hyperparameter_parser = default_hyperparameter_parser\n        else:\n            self.hyperparameter_parser = hyperparameter_parser\n\n        if isinstance(space, dict):\n            self.hyperparameters = space\n        else:\n            rng = np.random.default_rng(rng)\n            self.space.seed(rng.integers(0, 2**32))\n            self.hyperparameters = dict(self.space.sample_configuration())\n\n    def mutate(self, rng=None):\n        if isinstance(self.space, dict): \n            return False\n\n        rng = np.random.default_rng(rng)\n        self.space.seed(rng.integers(0, 2**32))\n        self.hyperparameters = dict(self.space.sample_configuration())\n        return True\n\n    def crossover(self, other, rng=None):\n        if isinstance(self.space, dict):\n            return False\n\n        rng = np.random.default_rng(rng)\n        if self.method != other.method:\n            return False\n\n        #loop through hyperparameters, randomly swap items in self.hyperparameters with items in other.hyperparameters\n        for hyperparameter in self.space:\n            if rng.choice([True, False]):\n                if hyperparameter in other.hyperparameters:\n                    self.hyperparameters[hyperparameter] = other.hyperparameters[hyperparameter]\n\n        return True\n\n\n\n    @final #this method should not be overridden, instead override hyperparameter_parser\n    def export_pipeline(self, **kwargs):\n        return self.method(**self.hyperparameter_parser(self.hyperparameters))\n\n    def unique_id(self):\n        #return a dictionary of the method and the hyperparameters\n        method_str = self.method.__name__\n        params = list(self.hyperparameters.keys())\n        params = sorted(params)\n\n        id_str = f\"{method_str}({', '.join([f'{param}={self.hyperparameters[param]}' for param in params])})\"\n\n        return id_str\n</code></pre>"},{"location":"documentation/tpot/search_spaces/nodes/estimator_node_gradual/","title":"Estimator node gradual","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/search_spaces/nodes/estimator_node_gradual/#tpot.search_spaces.nodes.estimator_node_gradual.EstimatorNodeIndividual_gradual","title":"<code>EstimatorNodeIndividual_gradual</code>","text":"<p>               Bases: <code>SklearnIndividual</code></p> <p>Note that ConfigurationSpace does not support None as a parameter. Instead, use the special string \"\". TPOT will automatically replace instances of this string with the Python None.  <p>Parameters:</p> Name Type Description Default <code>method</code> <code>type</code> <p>The class of the estimator to be used</p> required <code>space</code> <code>ConfigurationSpace | dict</code> <p>The hyperparameter space to be used. If a dict is passed, hyperparameters are fixed and not learned.</p> required Source code in <code>tpot/search_spaces/nodes/estimator_node_gradual.py</code> <pre><code>class EstimatorNodeIndividual_gradual(SklearnIndividual):\n    \"\"\"\n    Note that ConfigurationSpace does not support None as a parameter. Instead, use the special string \"&lt;NONE&gt;\". TPOT will automatically replace instances of this string with the Python None. \n\n    Parameters\n    ----------\n    method : type\n        The class of the estimator to be used\n\n    space : ConfigurationSpace|dict\n        The hyperparameter space to be used. If a dict is passed, hyperparameters are fixed and not learned.\n\n    \"\"\"\n    def __init__(self, method: type, \n                        space: ConfigurationSpace|dict, #TODO If a dict is passed, hyperparameters are fixed and not learned. Is this confusing? Should we make a second node type?\n                        hyperparameter_parser: callable = None,\n                        rng=None) -&gt; None:\n        super().__init__()\n        self.method = method\n        self.space = space\n\n        if hyperparameter_parser is None:\n            self.hyperparameter_parser = default_hyperparameter_parser\n        else:\n            self.hyperparameter_parser = hyperparameter_parser\n\n        if isinstance(space, dict):\n            self.hyperparameters = space\n        else:\n            rng = np.random.default_rng(rng)\n            self.space.seed(rng.integers(0, 2**32))\n            self.hyperparameters = dict(self.space.sample_configuration())\n\n    def mutate(self, rng=None):\n        if isinstance(self.space, dict): \n            return False\n        self.hyperparameters = gradual_hyperparameter_update(params=self.hyperparameters, configspace=self.space, rng=rng)\n        return True\n\n    def crossover(self, other, rng=None):\n        if isinstance(self.space, dict):\n            return False\n\n        rng = np.random.default_rng(rng)\n        if self.method != other.method:\n            return False\n\n        #loop through hyperparameters, randomly swap items in self.hyperparameters with items in other.hyperparameters\n        for hyperparameter in self.space:\n            if rng.choice([True, False]):\n                if hyperparameter in other.hyperparameters:\n                    self.hyperparameters[hyperparameter] = other.hyperparameters[hyperparameter]\n\n        return True\n\n\n    @final #this method should not be overridden, instead override hyperparameter_parser\n    def export_pipeline(self, **kwargs):\n        return self.method(**self.hyperparameter_parser(self.hyperparameters))\n\n    def unique_id(self):\n        #return a dictionary of the method and the hyperparameters\n        method_str = self.method.__name__\n        params = list(self.hyperparameters.keys())\n        params = sorted(params)\n\n        id_str = f\"{method_str}({', '.join([f'{param}={self.hyperparameters[param]}' for param in params])})\"\n\n        return id_str\n</code></pre>"},{"location":"documentation/tpot/search_spaces/nodes/fss_node/","title":"Fss node","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/search_spaces/nodes/fss_node/#tpot.search_spaces.nodes.fss_node.FSSIndividual","title":"<code>FSSIndividual</code>","text":"<p>               Bases: <code>SklearnIndividual</code></p> Source code in <code>tpot/search_spaces/nodes/fss_node.py</code> <pre><code>class FSSIndividual(SklearnIndividual):\n    def __init__(   self,\n                    subsets,\n                    rng=None,\n                ):\n\n        \"\"\"\n        An individual for representing a specific FeatureSetSelector. \n        The FeatureSetSelector selects a feature list of list of predefined feature subsets.\n\n        This instance will select one set initially. Mutation and crossover can swap the selected subset with another.\n\n        Parameters\n        ----------\n        subsets : str or list, default=None\n            Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries. \n            Features are defined by column names if using a Pandas data frame, or ints corresponding to indexes if using numpy arrays.\n            - str : If a string, it is assumed to be a path to a csv file with the subsets. \n                The first column is assumed to be the name of the subset and the remaining columns are the features in the subset.\n            - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets (i.e a list of lists).\n            - dict : A dictionary where keys are the names of the subsets and the values are the list of features.\n            - int : If an int, it is assumed to be the number of subsets to generate. Each subset will contain one feature.\n            - None : If None, each column will be treated as a subset. One column will be selected per subset.\n        rng : int, np.random.Generator, optional\n            The random number generator. The default is None.\n            Only used to select the first subset.\n\n        Returns\n        -------\n        None    \n        \"\"\"\n\n        subsets = subsets\n        rng = np.random.default_rng(rng)\n\n        if isinstance(subsets, str):\n            df = pd.read_csv(subsets,header=None,index_col=0)\n            df['features'] = df.apply(lambda x: list([x[c] for c in df.columns]),axis=1)\n            self.subset_dict = {}\n            for row in df.index:\n                self.subset_dict[row] = df.loc[row]['features']\n        elif isinstance(subsets, dict):\n            self.subset_dict = subsets\n        elif isinstance(subsets, list) or isinstance(subsets, np.ndarray):\n            self.subset_dict = {str(i):subsets[i] for i in range(len(subsets))}\n        elif isinstance(subsets, int):\n            self.subset_dict = {\"{0}\".format(i):i for i in range(subsets)}\n        else:\n            raise ValueError(\"Subsets must be a string, dictionary, list, int, or numpy array\")\n\n        self.names_list = list(self.subset_dict.keys())\n\n\n        self.selected_subset_name = rng.choice(self.names_list)\n        self.sel_subset = self.subset_dict[self.selected_subset_name]\n\n\n    def mutate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        #get list of names not including the current one\n        names = [name for name in self.names_list if name != self.selected_subset_name]\n        self.selected_subset_name = rng.choice(names)\n        self.sel_subset = self.subset_dict[self.selected_subset_name]\n\n\n    def crossover(self, other, rng=None):\n        self.selected_subset_name = other.selected_subset_name\n        self.sel_subset = other.sel_subset\n\n    def export_pipeline(self, **kwargs):\n        return FeatureSetSelector(sel_subset=self.sel_subset, name=self.selected_subset_name)\n\n\n    def unique_id(self):\n        id_str = \"FeatureSetSelector({0})\".format(self.selected_subset_name)\n        return id_str\n</code></pre>"},{"location":"documentation/tpot/search_spaces/nodes/fss_node/#tpot.search_spaces.nodes.fss_node.FSSIndividual.__init__","title":"<code>__init__(subsets, rng=None)</code>","text":"<p>An individual for representing a specific FeatureSetSelector.  The FeatureSetSelector selects a feature list of list of predefined feature subsets.</p> <p>This instance will select one set initially. Mutation and crossover can swap the selected subset with another.</p> <p>Parameters:</p> Name Type Description Default <code>subsets</code> <code>str or list</code> <p>Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries.  Features are defined by column names if using a Pandas data frame, or ints corresponding to indexes if using numpy arrays. - str : If a string, it is assumed to be a path to a csv file with the subsets.      The first column is assumed to be the name of the subset and the remaining columns are the features in the subset. - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets (i.e a list of lists). - dict : A dictionary where keys are the names of the subsets and the values are the list of features. - int : If an int, it is assumed to be the number of subsets to generate. Each subset will contain one feature. - None : If None, each column will be treated as a subset. One column will be selected per subset.</p> <code>None</code> <code>rng</code> <code>(int, Generator)</code> <p>The random number generator. The default is None. Only used to select the first subset.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>tpot/search_spaces/nodes/fss_node.py</code> <pre><code>def __init__(   self,\n                subsets,\n                rng=None,\n            ):\n\n    \"\"\"\n    An individual for representing a specific FeatureSetSelector. \n    The FeatureSetSelector selects a feature list of list of predefined feature subsets.\n\n    This instance will select one set initially. Mutation and crossover can swap the selected subset with another.\n\n    Parameters\n    ----------\n    subsets : str or list, default=None\n        Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries. \n        Features are defined by column names if using a Pandas data frame, or ints corresponding to indexes if using numpy arrays.\n        - str : If a string, it is assumed to be a path to a csv file with the subsets. \n            The first column is assumed to be the name of the subset and the remaining columns are the features in the subset.\n        - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets (i.e a list of lists).\n        - dict : A dictionary where keys are the names of the subsets and the values are the list of features.\n        - int : If an int, it is assumed to be the number of subsets to generate. Each subset will contain one feature.\n        - None : If None, each column will be treated as a subset. One column will be selected per subset.\n    rng : int, np.random.Generator, optional\n        The random number generator. The default is None.\n        Only used to select the first subset.\n\n    Returns\n    -------\n    None    \n    \"\"\"\n\n    subsets = subsets\n    rng = np.random.default_rng(rng)\n\n    if isinstance(subsets, str):\n        df = pd.read_csv(subsets,header=None,index_col=0)\n        df['features'] = df.apply(lambda x: list([x[c] for c in df.columns]),axis=1)\n        self.subset_dict = {}\n        for row in df.index:\n            self.subset_dict[row] = df.loc[row]['features']\n    elif isinstance(subsets, dict):\n        self.subset_dict = subsets\n    elif isinstance(subsets, list) or isinstance(subsets, np.ndarray):\n        self.subset_dict = {str(i):subsets[i] for i in range(len(subsets))}\n    elif isinstance(subsets, int):\n        self.subset_dict = {\"{0}\".format(i):i for i in range(subsets)}\n    else:\n        raise ValueError(\"Subsets must be a string, dictionary, list, int, or numpy array\")\n\n    self.names_list = list(self.subset_dict.keys())\n\n\n    self.selected_subset_name = rng.choice(self.names_list)\n    self.sel_subset = self.subset_dict[self.selected_subset_name]\n</code></pre>"},{"location":"documentation/tpot/search_spaces/nodes/fss_node/#tpot.search_spaces.nodes.fss_node.FSSNode","title":"<code>FSSNode</code>","text":"<p>               Bases: <code>SearchSpace</code></p> Source code in <code>tpot/search_spaces/nodes/fss_node.py</code> <pre><code>class FSSNode(SearchSpace):\n    def __init__(self,                     \n                    subsets,\n                ):\n        \"\"\"\n        A search space for a FeatureSetSelector. \n        The FeatureSetSelector selects a feature list of list of predefined feature subsets.\n\n        Parameters\n        ----------\n        subsets : str or list, default=None\n            Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries. \n            Features are defined by column names if using a Pandas data frame, or ints corresponding to indexes if using numpy arrays.\n            - str : If a string, it is assumed to be a path to a csv file with the subsets. \n                The first column is assumed to be the name of the subset and the remaining columns are the features in the subset.\n            - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets (i.e a list of lists).\n            - dict : A dictionary where keys are the names of the subsets and the values are the list of features.\n            - int : If an int, it is assumed to be the number of subsets to generate. Each subset will contain one feature.\n            - None : If None, each column will be treated as a subset. One column will be selected per subset.\n\n        Returns\n        -------\n        None    \n\n        \"\"\"\n\n        self.subsets = subsets\n\n    def generate(self, rng=None) -&gt; SklearnIndividual:\n        return FSSIndividual(   \n            subsets=self.subsets,\n            rng=rng,\n            )\n</code></pre>"},{"location":"documentation/tpot/search_spaces/nodes/fss_node/#tpot.search_spaces.nodes.fss_node.FSSNode.__init__","title":"<code>__init__(subsets)</code>","text":"<p>A search space for a FeatureSetSelector.  The FeatureSetSelector selects a feature list of list of predefined feature subsets.</p> <p>Parameters:</p> Name Type Description Default <code>subsets</code> <code>str or list</code> <p>Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries.  Features are defined by column names if using a Pandas data frame, or ints corresponding to indexes if using numpy arrays. - str : If a string, it is assumed to be a path to a csv file with the subsets.      The first column is assumed to be the name of the subset and the remaining columns are the features in the subset. - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets (i.e a list of lists). - dict : A dictionary where keys are the names of the subsets and the values are the list of features. - int : If an int, it is assumed to be the number of subsets to generate. Each subset will contain one feature. - None : If None, each column will be treated as a subset. One column will be selected per subset.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>tpot/search_spaces/nodes/fss_node.py</code> <pre><code>def __init__(self,                     \n                subsets,\n            ):\n    \"\"\"\n    A search space for a FeatureSetSelector. \n    The FeatureSetSelector selects a feature list of list of predefined feature subsets.\n\n    Parameters\n    ----------\n    subsets : str or list, default=None\n        Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries. \n        Features are defined by column names if using a Pandas data frame, or ints corresponding to indexes if using numpy arrays.\n        - str : If a string, it is assumed to be a path to a csv file with the subsets. \n            The first column is assumed to be the name of the subset and the remaining columns are the features in the subset.\n        - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets (i.e a list of lists).\n        - dict : A dictionary where keys are the names of the subsets and the values are the list of features.\n        - int : If an int, it is assumed to be the number of subsets to generate. Each subset will contain one feature.\n        - None : If None, each column will be treated as a subset. One column will be selected per subset.\n\n    Returns\n    -------\n    None    \n\n    \"\"\"\n\n    self.subsets = subsets\n</code></pre>"},{"location":"documentation/tpot/search_spaces/nodes/genetic_feature_selection/","title":"Genetic feature selection","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/search_spaces/nodes/genetic_feature_selection/#tpot.search_spaces.nodes.genetic_feature_selection.GeneticFeatureSelectorNode","title":"<code>GeneticFeatureSelectorNode</code>","text":"<p>               Bases: <code>SearchSpace</code></p> Source code in <code>tpot/search_spaces/nodes/genetic_feature_selection.py</code> <pre><code>class GeneticFeatureSelectorNode(SearchSpace):\n    def __init__(self,                     \n                    n_features,\n                    start_p=0.2,\n                    mutation_rate = 0.1,\n                    crossover_rate = 0.1,\n                    mutation_rate_rate = 0, # These are still experimental but seem to help. Theory is that it takes slower steps as it gets closer to the optimal solution.\n                    crossover_rate_rate = 0,# Otherwise is mutation_rate is too small, it takes forever, and if its too large, it never converges.\n                    ):\n        \"\"\"\n        A node that generates a GeneticFeatureSelectorIndividual. Uses genetic algorithm to select novel subsets of features.\n\n        Parameters\n        ----------\n        n_features : int\n            Number of features in the dataset.\n        start_p : float\n            Probability of selecting a given feature for the initial subset of features.\n        mutation_rate : float\n            Probability of adding/removing a feature from the subset of features.\n        crossover_rate : float\n            Probability of swapping a feature between two subsets of features.\n        mutation_rate_rate : float\n            Probability of changing the mutation rate. (experimental)\n        crossover_rate_rate : float\n            Probability of changing the crossover rate. (experimental)\n\n        \"\"\"\n\n        self.n_features = n_features\n        self.start_p = start_p\n        self.mutation_rate = mutation_rate\n        self.crossover_rate = crossover_rate\n        self.mutation_rate_rate = mutation_rate_rate\n        self.crossover_rate_rate = crossover_rate_rate\n\n\n    def generate(self, rng=None) -&gt; SklearnIndividual:\n        return GeneticFeatureSelectorIndividual(   mask=self.n_features,\n                                                    start_p=self.start_p,\n                                                    mutation_rate=self.mutation_rate,\n                                                    crossover_rate=self.crossover_rate,\n                                                    mutation_rate_rate=self.mutation_rate_rate,\n                                                    crossover_rate_rate=self.crossover_rate_rate,\n                                                    rng=rng\n                                                )\n</code></pre>"},{"location":"documentation/tpot/search_spaces/nodes/genetic_feature_selection/#tpot.search_spaces.nodes.genetic_feature_selection.GeneticFeatureSelectorNode.__init__","title":"<code>__init__(n_features, start_p=0.2, mutation_rate=0.1, crossover_rate=0.1, mutation_rate_rate=0, crossover_rate_rate=0)</code>","text":"<p>A node that generates a GeneticFeatureSelectorIndividual. Uses genetic algorithm to select novel subsets of features.</p> <p>Parameters:</p> Name Type Description Default <code>n_features</code> <code>int</code> <p>Number of features in the dataset.</p> required <code>start_p</code> <code>float</code> <p>Probability of selecting a given feature for the initial subset of features.</p> <code>0.2</code> <code>mutation_rate</code> <code>float</code> <p>Probability of adding/removing a feature from the subset of features.</p> <code>0.1</code> <code>crossover_rate</code> <code>float</code> <p>Probability of swapping a feature between two subsets of features.</p> <code>0.1</code> <code>mutation_rate_rate</code> <code>float</code> <p>Probability of changing the mutation rate. (experimental)</p> <code>0</code> <code>crossover_rate_rate</code> <code>float</code> <p>Probability of changing the crossover rate. (experimental)</p> <code>0</code> Source code in <code>tpot/search_spaces/nodes/genetic_feature_selection.py</code> <pre><code>def __init__(self,                     \n                n_features,\n                start_p=0.2,\n                mutation_rate = 0.1,\n                crossover_rate = 0.1,\n                mutation_rate_rate = 0, # These are still experimental but seem to help. Theory is that it takes slower steps as it gets closer to the optimal solution.\n                crossover_rate_rate = 0,# Otherwise is mutation_rate is too small, it takes forever, and if its too large, it never converges.\n                ):\n    \"\"\"\n    A node that generates a GeneticFeatureSelectorIndividual. Uses genetic algorithm to select novel subsets of features.\n\n    Parameters\n    ----------\n    n_features : int\n        Number of features in the dataset.\n    start_p : float\n        Probability of selecting a given feature for the initial subset of features.\n    mutation_rate : float\n        Probability of adding/removing a feature from the subset of features.\n    crossover_rate : float\n        Probability of swapping a feature between two subsets of features.\n    mutation_rate_rate : float\n        Probability of changing the mutation rate. (experimental)\n    crossover_rate_rate : float\n        Probability of changing the crossover rate. (experimental)\n\n    \"\"\"\n\n    self.n_features = n_features\n    self.start_p = start_p\n    self.mutation_rate = mutation_rate\n    self.crossover_rate = crossover_rate\n    self.mutation_rate_rate = mutation_rate_rate\n    self.crossover_rate_rate = crossover_rate_rate\n</code></pre>"},{"location":"documentation/tpot/search_spaces/nodes/genetic_feature_selection/#tpot.search_spaces.nodes.genetic_feature_selection.MaskSelector","title":"<code>MaskSelector</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>SelectorMixin</code></p> <p>Select predefined feature subsets.</p> Source code in <code>tpot/search_spaces/nodes/genetic_feature_selection.py</code> <pre><code>class MaskSelector(BaseEstimator, SelectorMixin):\n    \"\"\"Select predefined feature subsets.\"\"\"\n\n    def __init__(self, mask, set_output_transform=None):\n        self.mask = mask\n        self.set_output_transform = set_output_transform\n        if set_output_transform is not None:\n            self.set_output(transform=set_output_transform)\n\n    def fit(self, X, y=None):\n        self.n_features_in_ = X.shape[1]\n        if isinstance(X, pd.DataFrame):\n            self.feature_names_in_ = X.columns\n        #     self.set_output(transform=\"pandas\")\n        self.is_fitted_ = True #so sklearn knows it's fitted\n        return self\n\n    def _get_tags(self):\n        tags = {\"allow_nan\": True, \"requires_y\": False}\n        return tags\n\n    def _get_support_mask(self):\n        return np.array(self.mask)\n\n    def get_feature_names_out(self, input_features=None):\n        return self.feature_names_in_[self.get_support()]\n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/choice/","title":"Choice","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/search_spaces/pipelines/dynamic_linear/","title":"Dynamic linear","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/search_spaces/pipelines/dynamicunion/","title":"Dynamicunion","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/search_spaces/pipelines/dynamicunion/#tpot.search_spaces.pipelines.dynamicunion.DynamicUnionPipeline","title":"<code>DynamicUnionPipeline</code>","text":"<p>               Bases: <code>SearchSpace</code></p> Source code in <code>tpot/search_spaces/pipelines/dynamicunion.py</code> <pre><code>class DynamicUnionPipeline(SearchSpace):\n    def __init__(self, search_space : SearchSpace, max_estimators=None, allow_repeats=False ) -&gt; None:\n        \"\"\"\n        Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.\n        \"\"\"\n\n        self.search_space = search_space\n        self.max_estimators = max_estimators\n        self.allow_repeats = allow_repeats\n\n    def generate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        return DynamicUnionPipelineIndividual(self.search_space, max_estimators=self.max_estimators, allow_repeats=self.allow_repeats, rng=rng)\n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/dynamicunion/#tpot.search_spaces.pipelines.dynamicunion.DynamicUnionPipeline.__init__","title":"<code>__init__(search_space, max_estimators=None, allow_repeats=False)</code>","text":"<p>Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.</p> Source code in <code>tpot/search_spaces/pipelines/dynamicunion.py</code> <pre><code>def __init__(self, search_space : SearchSpace, max_estimators=None, allow_repeats=False ) -&gt; None:\n    \"\"\"\n    Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.\n    \"\"\"\n\n    self.search_space = search_space\n    self.max_estimators = max_estimators\n    self.allow_repeats = allow_repeats\n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/dynamicunion/#tpot.search_spaces.pipelines.dynamicunion.DynamicUnionPipelineIndividual","title":"<code>DynamicUnionPipelineIndividual</code>","text":"<p>               Bases: <code>SklearnIndividual</code></p> <p>Takes in one search space. Will produce a FeatureUnion of up to max_estimators number of steps. The output of the FeatureUnion will the all of the steps concatenated together.</p> Source code in <code>tpot/search_spaces/pipelines/dynamicunion.py</code> <pre><code>class DynamicUnionPipelineIndividual(SklearnIndividual):\n    \"\"\"\n    Takes in one search space.\n    Will produce a FeatureUnion of up to max_estimators number of steps.\n    The output of the FeatureUnion will the all of the steps concatenated together.\n\n    \"\"\"\n\n    def __init__(self, search_space : SearchSpace, max_estimators=None, allow_repeats=False, rng=None) -&gt; None:\n        super().__init__()\n        self.search_space = search_space\n\n        if max_estimators is None:\n            self.max_estimators = np.inf\n        else:\n            self.max_estimators = max_estimators\n\n        self.allow_repeats = allow_repeats\n\n        self.union_dict = {}\n\n        if self.max_estimators == np.inf:\n            init_max = 3\n        else:\n            init_max = self.max_estimators\n\n        rng = np.random.default_rng(rng)\n\n        for _ in range(rng.integers(1, init_max)):\n            self._mutate_add_step(rng)\n\n\n    def mutate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        mutation_funcs = [self._mutate_add_step, self._mutate_remove_step, self._mutate_replace_step, self._mutate_note]\n        rng.shuffle(mutation_funcs)\n        for mutation_func in mutation_funcs:\n            if mutation_func(rng):\n                return True\n\n    def _mutate_add_step(self, rng):\n        rng = np.random.default_rng(rng)\n        max_attempts = 10\n        if len(self.union_dict) &lt; self.max_estimators:\n            for _ in range(max_attempts):\n                new_step = self.search_space.generate(rng)\n                if new_step.unique_id() not in self.union_dict:\n                    self.union_dict[new_step.unique_id()] = new_step\n                    return True\n        return False\n\n    def _mutate_remove_step(self, rng):\n        rng = np.random.default_rng(rng)\n        if len(self.union_dict) &gt; 1:\n            self.union_dict.pop( rng.choice(list(self.union_dict.keys())))  \n            return True\n        return False\n\n    def _mutate_replace_step(self, rng):\n        rng = np.random.default_rng(rng)        \n        changed = self._mutate_remove_step(rng) or self._mutate_add_step(rng)\n        return changed\n\n    #TODO mutate one step or multiple?\n    def _mutate_note(self, rng):\n        rng = np.random.default_rng(rng)\n        changed = False\n        values = list(self.union_dict.values())\n        for step in values:\n            if rng.random() &lt; 0.5:\n                changed = step.mutate(rng) or changed\n\n        self.union_dict = {step.unique_id(): step for step in values}\n\n        return changed\n\n\n    def crossover(self, other, rng=None):\n        rng = np.random.default_rng(rng)\n\n        cx_funcs = [self._crossover_swap_multiple_nodes, self._crossover_node]\n        rng.shuffle(cx_funcs)\n        for cx_func in cx_funcs:\n            if cx_func(other, rng):\n                return True\n\n        return False\n\n\n    def _crossover_swap_multiple_nodes(self, other, rng):\n        rng = np.random.default_rng(rng)\n        self_values = list(self.union_dict.values())\n        other_values = list(other.union_dict.values())\n\n        rng.shuffle(self_values)\n        rng.shuffle(other_values)\n\n        self_idx = rng.integers(0,len(self_values))\n        other_idx = rng.integers(0,len(other_values))\n\n        #Note that this is not one-point-crossover since the sequence doesn't matter. this is just a quick way to swap multiple random items\n        self_values[:self_idx], other_values[:other_idx] = other_values[:other_idx], self_values[:self_idx]\n\n        self.union_dict = {step.unique_id(): step for step in self_values}\n        other.union_dict = {step.unique_id(): step for step in other_values}\n\n        return True\n\n\n    def _crossover_node(self, other, rng):\n        rng = np.random.default_rng(rng)\n\n        changed = False\n        self_values = list(self.union_dict.values())\n        other_values = list(other.union_dict.values())\n\n        rng.shuffle(self_values)\n        rng.shuffle(other_values)\n\n        for self_step, other_step in zip(self_values, other_values):\n            if rng.random() &lt; 0.5:\n                changed = self_step.crossover(other_step, rng) or changed\n\n        self.union_dict = {step.unique_id(): step for step in self_values}\n        other.union_dict = {step.unique_id(): step for step in other_values}\n\n        return changed\n\n    def export_pipeline(self, **kwargs):\n        values = list(self.union_dict.values())\n        return sklearn.pipeline.make_union(*[step.export_pipeline(**kwargs) for step in values])\n\n    def unique_id(self):\n        values = list(self.union_dict.values())\n        l = [step.unique_id() for step in values]\n        # if all items are strings, then sort them\n        if all([isinstance(x, str) for x in l]):\n            l.sort()\n        l = [\"FeatureUnion\"] + l\n        return TupleIndex(frozenset(l))\n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/graph/","title":"Graph","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/search_spaces/pipelines/graph/#tpot.search_spaces.pipelines.graph.GraphKey","title":"<code>GraphKey</code>","text":"<p>A class that can be used as a key for a graph.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>Graph</code> <p>The graph to use as a key. Node Attributes are used for the hash.</p> required <code>matched_label</code> <code>str</code> <p>The node attribute to consider for the hash.</p> <code>'label'</code> Source code in <code>tpot/search_spaces/pipelines/graph.py</code> <pre><code>class GraphKey():\n    '''\n    A class that can be used as a key for a graph.\n\n    Parameters\n    ----------\n    graph : (nx.Graph)\n        The graph to use as a key. Node Attributes are used for the hash.\n    matched_label : (str)\n        The node attribute to consider for the hash.\n    '''\n\n    def __init__(self, graph, matched_label='label') -&gt; None:#['hyperparameters', 'method_class']) -&gt; None:\n\n\n        self.graph = graph\n        self.matched_label = matched_label\n        self.node_match = partial(node_match, matched_labels=[matched_label])\n        self.key = int(nx.weisfeiler_lehman_graph_hash(self.graph, node_attr=self.matched_label),16) #hash(tuple(sorted([val for (node, val) in self.graph.degree()])))\n\n\n    #If hash is different, node is definitely different\n    # https://arxiv.org/pdf/2002.06653.pdf\n    def __hash__(self) -&gt; int:\n\n        return self.key\n\n    #If hash is same, use __eq__ to know if they are actually different\n    def __eq__(self, other):\n        return nx.is_isomorphic(self.graph, other.graph, node_match=self.node_match)\n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/graph/#tpot.search_spaces.pipelines.graph.GraphPipelineIndividual","title":"<code>GraphPipelineIndividual</code>","text":"<p>               Bases: <code>SklearnIndividual</code></p> <p>Defines a search space of pipelines in the shape of a Directed Acyclic Graphs. The search spaces for root, leaf, and inner nodes can be defined separately if desired. Each graph will have a single root serving as the final estimator which is drawn from the <code>root_search_space</code>. If the <code>leaf_search_space</code> is defined, all leaves  in the pipeline will be drawn from that search space. If the <code>leaf_search_space</code> is not defined, all leaves will be drawn from the <code>inner_search_space</code>. Nodes that are not leaves or roots will be drawn from the <code>inner_search_space</code>. If the <code>inner_search_space</code> is not defined, there will be no inner nodes.</p> <p><code>cross_val_predict_cv</code>, <code>method</code>, <code>memory</code>, and <code>use_label_encoder</code> are passed to the GraphPipeline object when the pipeline is exported and not directly used in the search space.</p> <p>Exports to a GraphPipeline object.</p> <p>Parameters:</p> Name Type Description Default <code>root_search_space</code> <code>SearchSpace</code> <p>The search space for the root node of the graph. This node will be the final estimator in the pipeline.</p> required <code>inner_search_space</code> <code>SearchSpace</code> <p>The search space for the inner nodes of the graph. If not defined, there will be no inner nodes.</p> <code>None</code> <code>leaf_search_space</code> <code>SearchSpace</code> <p>The search space for the leaf nodes of the graph. If not defined, the leaf nodes will be drawn from the inner_search_space.</p> <code>None</code> <code>crossover_same_depth</code> <code>bool</code> <p>If True, crossover will only occur between nodes at the same depth in the graph. If False, crossover will occur between nodes at any depth.</p> <code>False</code> <code>cross_val_predict_cv</code> <code>Union[int, Callable]</code> <p>Determines the cross-validation splitting strategy used in inner classifiers or regressors</p> <code>0</code> <code>method</code> <code>str</code> <p>The prediction method to use for the inner classifiers or regressors. If 'auto', it will try to use predict_proba, decision_function, or predict in that order.</p> <code>'auto'</code> <code>memory</code> <p>Used to cache the input and outputs of nodes to prevent refitting or computationally heavy transformations. By default, no caching is performed. If a string is given, it is the path to the caching directory.</p> required <code>use_label_encoder</code> <code>bool</code> <p>If True, the label encoder is used to encode the labels to be 0 to N. If False, the label encoder is not used. Mainly useful for classifiers (XGBoost) that require labels to be ints from 0 to N. Can also be a sklearn.preprocessing.LabelEncoder object. If so, that label encoder is used.</p> <code>False</code> <code>rng</code> <p>Seed for sampling the first graph instance.</p> <code>None</code> Source code in <code>tpot/search_spaces/pipelines/graph.py</code> <pre><code>class GraphPipelineIndividual(SklearnIndividual):\n    \"\"\"\n        Defines a search space of pipelines in the shape of a Directed Acyclic Graphs. The search spaces for root, leaf, and inner nodes can be defined separately if desired.\n        Each graph will have a single root serving as the final estimator which is drawn from the `root_search_space`. If the `leaf_search_space` is defined, all leaves \n        in the pipeline will be drawn from that search space. If the `leaf_search_space` is not defined, all leaves will be drawn from the `inner_search_space`.\n        Nodes that are not leaves or roots will be drawn from the `inner_search_space`. If the `inner_search_space` is not defined, there will be no inner nodes.\n\n        `cross_val_predict_cv`, `method`, `memory`, and `use_label_encoder` are passed to the GraphPipeline object when the pipeline is exported and not directly used in the search space.\n\n        Exports to a GraphPipeline object.\n\n        Parameters\n        ----------\n\n        root_search_space: SearchSpace\n            The search space for the root node of the graph. This node will be the final estimator in the pipeline.\n\n        inner_search_space: SearchSpace, optional\n            The search space for the inner nodes of the graph. If not defined, there will be no inner nodes.\n\n        leaf_search_space: SearchSpace, optional\n            The search space for the leaf nodes of the graph. If not defined, the leaf nodes will be drawn from the inner_search_space.\n\n        crossover_same_depth: bool, optional\n            If True, crossover will only occur between nodes at the same depth in the graph. If False, crossover will occur between nodes at any depth.\n\n        cross_val_predict_cv: int, cross-validation generator or an iterable, optional\n            Determines the cross-validation splitting strategy used in inner classifiers or regressors\n\n        method: str, optional\n            The prediction method to use for the inner classifiers or regressors. If 'auto', it will try to use predict_proba, decision_function, or predict in that order.\n\n        memory: str or object with the joblib.Memory interface, optional\n            Used to cache the input and outputs of nodes to prevent refitting or computationally heavy transformations. By default, no caching is performed. If a string is given, it is the path to the caching directory.\n\n        use_label_encoder: bool, optional\n            If True, the label encoder is used to encode the labels to be 0 to N. If False, the label encoder is not used.\n            Mainly useful for classifiers (XGBoost) that require labels to be ints from 0 to N.\n            Can also be a sklearn.preprocessing.LabelEncoder object. If so, that label encoder is used.\n\n        rng: int, RandomState instance or None, optional\n            Seed for sampling the first graph instance. \n\n        \"\"\"\n\n    def __init__(\n            self,  \n            root_search_space: SearchSpace, \n            leaf_search_space: SearchSpace = None, \n            inner_search_space: SearchSpace = None, \n            max_size: int = np.inf,\n            crossover_same_depth: bool = False,\n            cross_val_predict_cv: Union[int, Callable] = 0, #signature function(estimator, X, y=none)\n            method: str = 'auto',\n            use_label_encoder: bool = False,\n            rng=None):\n\n        super().__init__()\n\n        self.__debug = False\n\n        rng = np.random.default_rng(rng)\n\n        self.root_search_space = root_search_space\n        self.leaf_search_space = leaf_search_space\n        self.inner_search_space = inner_search_space\n        self.max_size = max_size\n        self.crossover_same_depth = crossover_same_depth\n\n        self.cross_val_predict_cv = cross_val_predict_cv\n        self.method = method\n        self.use_label_encoder = use_label_encoder\n\n        self.root = self.root_search_space.generate(rng)\n        self.graph = nx.DiGraph()\n        self.graph.add_node(self.root)\n\n        if self.leaf_search_space is not None:\n            self.leaf = self.leaf_search_space.generate(rng)\n            self.graph.add_node(self.leaf)\n            self.graph.add_edge(self.root, self.leaf)\n\n        if self.inner_search_space is None and self.leaf_search_space is None:\n            self.mutate_methods_list = [self._mutate_node]\n            self.crossover_methods_list = [self._crossover_swap_branch,]#[self._crossover_swap_branch, self._crossover_swap_node, self._crossover_take_branch]  #TODO self._crossover_nodes, \n\n        else:\n            self.mutate_methods_list = [self._mutate_insert_leaf, self._mutate_insert_inner_node, self._mutate_remove_node, self._mutate_node, self._mutate_insert_bypass_node]\n            self.crossover_methods_list = [self._crossover_swap_branch, self._crossover_nodes, self._crossover_take_branch ]#[self._crossover_swap_branch, self._crossover_swap_node, self._crossover_take_branch]  #TODO self._crossover_nodes, \n\n        self.merge_duplicated_nodes_toggle = True\n\n        self.graphkey = None\n\n\n    def mutate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        rng.shuffle(self.mutate_methods_list)\n        for mutate_method in self.mutate_methods_list:\n            if mutate_method(rng=rng):\n\n                if self.merge_duplicated_nodes_toggle:\n                    self._merge_duplicated_nodes()\n\n                if self.__debug:\n                    print(mutate_method)\n\n                    if self.root not in self.graph.nodes:\n                        print('lost root something went wrong with ', mutate_method)\n\n                    if len(self.graph.predecessors(self.root)) &gt; 0:\n                        print('root has parents ', mutate_method)\n\n                    if any([n in nx.ancestors(self.graph,n) for n in self.graph.nodes]):\n                        print('a node is connecting to itself...')\n\n                    if self.__debug:\n                        try:\n                            nx.find_cycle(self.graph)\n                            print('something went wrong with ', mutate_method)\n                        except:\n                            pass\n\n                self.graphkey = None\n\n        return False\n\n\n\n\n    def _mutate_insert_leaf(self, rng=None):\n        rng = np.random.default_rng(rng)\n        if self.max_size &gt; self.graph.number_of_nodes():\n            sorted_nodes_list = list(self.graph.nodes)\n            rng.shuffle(sorted_nodes_list) #TODO: sort by number of children and/or parents? bias model one way or another\n            for node in sorted_nodes_list:\n                #if leafs are protected, check if node is a leaf\n                #if node is a leaf, skip because we don't want to add node on top of node\n                if (self.leaf_search_space is not None #if leafs are protected\n                    and   len(list(self.graph.successors(node))) == 0 #if node is leaf\n                    and  len(list(self.graph.predecessors(node))) &gt; 0 #except if node is root, in which case we want to add a leaf even if it happens to be a leaf too\n                    ):\n\n                    continue\n\n                #If node *is* the root or is not a leaf, add leaf node. (dont want to add leaf on top of leaf)\n                if self.leaf_search_space is not None:\n                    new_node = self.leaf_search_space.generate(rng)\n                else:\n                    new_node = self.inner_search_space.generate(rng)\n\n                self.graph.add_node(new_node)\n                self.graph.add_edge(node, new_node)\n                return True\n\n        return False\n\n    def _mutate_insert_inner_node(self, rng=None):\n        \"\"\"\n        Finds an edge in the graph and inserts a new node between the two nodes. Removes the edge between the two nodes.\n        \"\"\"\n        rng = np.random.default_rng(rng)\n        if self.max_size &gt; self.graph.number_of_nodes():\n            sorted_nodes_list = list(self.graph.nodes)\n            sorted_nodes_list2 = list(self.graph.nodes)\n            rng.shuffle(sorted_nodes_list) #TODO: sort by number of children and/or parents? bias model one way or another\n            rng.shuffle(sorted_nodes_list2)\n            for node in sorted_nodes_list:\n                #loop through children of node\n                for child_node in list(self.graph.successors(node)):\n\n                    if child_node is not node and child_node not in nx.ancestors(self.graph, node):\n                        if self.leaf_search_space is not None:\n                            #If if we are protecting leafs, dont add connection into a leaf\n                            if len(list(nx.descendants(self.graph,node))) ==0 :\n                                continue\n\n                        new_node = self.inner_search_space.generate(rng)\n\n                        self.graph.add_node(new_node)\n                        self.graph.add_edges_from([(node, new_node), (new_node, child_node)])\n                        self.graph.remove_edge(node, child_node)\n                        return True\n\n        return False\n\n\n    def _mutate_remove_node(self, rng=None):\n        '''\n        Removes a randomly chosen node and connects its parents to its children.\n        If the node is the only leaf for an inner node and 'leaf_search_space' is not none, we do not remove it.\n        '''\n        rng = np.random.default_rng(rng)\n        nodes_list = list(self.graph.nodes)\n        nodes_list.remove(self.root)\n        leaves = get_leaves(self.graph)\n\n        while len(nodes_list) &gt; 0:\n            node = rng.choice(nodes_list)\n            nodes_list.remove(node)\n\n            if self.leaf_search_space is not None and len(list(nx.descendants(self.graph,node))) == 0 : #if the node is a leaf\n                if len(leaves) &lt;= 1:\n                    continue #dont remove the last leaf\n                leaf_parents = self.graph.predecessors(node)\n\n                # if any of the parents of the node has one one child, continue\n                if any([len(list(self.graph.successors(lp))) &lt; 2 for lp in leaf_parents]): #dont remove a leaf if it is the only input into another node.\n                    continue\n\n                remove_and_stitch(self.graph, node)\n                remove_nodes_disconnected_from_node(self.graph, self.root)\n                return True\n\n            else:\n                remove_and_stitch(self.graph, node)\n                remove_nodes_disconnected_from_node(self.graph, self.root)\n                return True\n\n        return False\n\n\n\n    def _mutate_node(self, rng=None):\n        '''\n        Mutates the hyperparameters for a randomly chosen node in the graph.\n        '''\n        rng = np.random.default_rng(rng)\n        sorted_nodes_list = list(self.graph.nodes)\n        rng.shuffle(sorted_nodes_list)\n        completed_one = False\n        for node in sorted_nodes_list:\n            if node.mutate(rng):\n                return True\n        return False\n\n    def _mutate_remove_edge(self, rng=None):\n        '''\n        Deletes an edge as long as deleting that edge does not make the graph disconnected.\n        '''\n        rng = np.random.default_rng(rng)\n        sorted_nodes_list = list(self.graph.nodes)\n        rng.shuffle(sorted_nodes_list)\n        for child_node in sorted_nodes_list:\n            parents = list(self.graph.predecessors(child_node))\n            if len(parents) &gt; 1: # if it has more than one parent, you can remove an edge (if this is the only child of a node, it will become a leaf)\n\n                for parent_node in parents:\n                    # if removing the egde will make the parent_node a leaf node, skip\n                    if self.leaf_search_space is not None and len(list(self.graph.successors(parent_node))) &lt; 2:\n                        continue\n\n                    self.graph.remove_edge(parent_node, child_node)\n                    return True\n        return False   \n\n    def _mutate_add_edge(self, rng=None):\n        '''\n        Randomly add an edge from a node to another node that is not an ancestor of the first node.\n        '''\n        rng = np.random.default_rng(rng)\n        sorted_nodes_list = list(self.graph.nodes)\n        rng.shuffle(sorted_nodes_list)\n        for child_node in sorted_nodes_list:\n            for parent_node in sorted_nodes_list:\n                if self.leaf_search_space is not None:\n                    if len(list(self.graph.successors(parent_node))) == 0:\n                        continue\n\n                # skip if\n                # - parent and child are the same node\n                # - edge already exists\n                # - child is an ancestor of parent\n                if  (child_node is not parent_node) and not self.graph.has_edge(parent_node,child_node) and (child_node not in nx.ancestors(self.graph, parent_node)):\n                    self.graph.add_edge(parent_node,child_node)\n                    return True\n\n        return False\n\n    def _mutate_insert_bypass_node(self, rng=None):\n        \"\"\"\n        Pick two nodes (doesn't necessarily need to be connected). Create a new node. connect one node to the new node and the new node to the other node.\n        Does not remove any edges.\n        \"\"\"\n        rng = np.random.default_rng(rng)\n        if self.max_size &gt; self.graph.number_of_nodes():\n            sorted_nodes_list = list(self.graph.nodes)\n            sorted_nodes_list2 = list(self.graph.nodes)\n            rng.shuffle(sorted_nodes_list) #TODO: sort by number of children and/or parents? bias model one way or another\n            rng.shuffle(sorted_nodes_list2)\n            for node in sorted_nodes_list:\n                for child_node in sorted_nodes_list2:\n                    if child_node is not node and child_node not in nx.ancestors(self.graph, node):\n                        if self.leaf_search_space is not None:\n                            #If if we are protecting leafs, dont add connection into a leaf\n                            if len(list(nx.descendants(self.graph,node))) ==0 :\n                                continue\n\n                        new_node = self.inner_search_space.generate(rng)\n\n                        self.graph.add_node(new_node)\n                        self.graph.add_edges_from([(node, new_node), (new_node, child_node)])\n                        return True\n\n        return False\n\n\n    def crossover(self, ind2, rng=None):\n        '''\n        self is the first individual, ind2 is the second individual\n        If crossover_same_depth, it will select graphindividuals at the same recursive depth.\n        Otherwise, it will select graphindividuals randomly from the entire graph and its subgraphs.\n\n        This does not impact graphs without subgraphs. And it does not impacts nodes that are not graphindividuals. Cros\n        '''\n\n        rng = np.random.default_rng(rng)\n\n        rng.shuffle(self.crossover_methods_list)\n\n        finished = False\n\n        for crossover_method in self.crossover_methods_list:\n            if crossover_method(ind2, rng=rng):\n                self._merge_duplicated_nodes()\n                finished = True\n                break\n\n        if self.__debug:\n            try:\n                nx.find_cycle(self.graph)\n                print('something went wrong with ', crossover_method)\n            except:\n                pass\n\n        if finished:\n            self.graphkey = None\n\n        return finished\n\n\n    def _crossover_swap_branch(self, G2, rng=None):\n        '''\n        swaps a branch from parent1 with a branch from parent2. does not modify parent2\n        '''\n        rng = np.random.default_rng(rng)\n\n        if self.crossover_same_depth:\n            pair_gen = select_nodes_same_depth(self.graph, self.root, G2.graph, G2.root, rng=rng)\n        else:\n            pair_gen = select_nodes_randomly(self.graph, G2.graph, rng=rng)\n\n        for node1, node2 in pair_gen:\n            #TODO: if root is in inner_search_space, then do use it?\n            if node1 is self.root or node2 is G2.root: #dont want to add root as inner node\n                continue\n\n            #check if node1 is a leaf and leafs are protected, don't add an input to the leave\n            if self.leaf_search_space is not None: #if we are protecting leaves,\n                node1_is_leaf = len(list(self.graph.successors(node1))) == 0\n                node2_is_leaf = len(list(G2.graph.successors(node2))) == 0\n                #if not ((node1_is_leaf and node1_is_leaf) or (not node1_is_leaf and not node2_is_leaf)): #if node1 is a leaf\n                #if (node1_is_leaf and (not node2_is_leaf)) or ( (not node1_is_leaf) and node2_is_leaf):\n                if not node1_is_leaf:\n                    #only continue if node1 and node2 are both leaves or both not leaves\n                    continue\n\n            temp_graph_1 = self.graph.copy()\n            temp_graph_1.remove_node(node1)\n            remove_nodes_disconnected_from_node(temp_graph_1, self.root)\n\n            #isolating the branch\n            branch2 = G2.graph.copy()\n            n2_descendants = nx.descendants(branch2,node2)\n            for n in list(branch2.nodes):\n                if n not in n2_descendants and n is not node2: #removes all nodes not in the branch\n                    branch2.remove_node(n)\n\n            branch2 = copy.deepcopy(branch2)\n            branch2_root = get_roots(branch2)[0]\n            temp_graph_1.add_edges_from(branch2.edges)\n            for p in list(self.graph.predecessors(node1)):\n                temp_graph_1.add_edge(p,branch2_root)\n\n            if temp_graph_1.number_of_nodes() &gt; self.max_size:\n                continue\n\n            self.graph = temp_graph_1\n\n            return True\n        return False\n\n\n    def _crossover_take_branch(self, G2, rng=None):\n        '''\n        Takes a subgraph from Parent2 and add it to a randomly chosen node in Parent1.\n        '''\n        rng = np.random.default_rng(rng)\n\n        if self.crossover_same_depth:\n            pair_gen = select_nodes_same_depth(self.graph, self.root, G2.graph, G2.root, rng=rng)\n        else:\n            pair_gen = select_nodes_randomly(self.graph, G2.graph, rng=rng)\n\n        for node1, node2 in pair_gen:\n            #TODO: if root is in inner_search_space, then do use it?\n            if node2 is G2.root: #dont want to add root as inner node\n                continue\n\n\n            #check if node1 is a leaf and leafs are protected, don't add an input to the leave\n            if self.leaf_search_space is not None and len(list(self.graph.successors(node1))) == 0:\n                continue\n\n            #icheck if node2 is graph individual\n            # if isinstance(node2,GraphIndividual):\n            #     if not ((isinstance(node2,GraphIndividual) and (\"Recursive\" in self.inner_search_space or \"Recursive\" in self.leaf_search_space))):\n            #         continue\n\n            #isolating the branch\n            branch2 = G2.graph.copy()\n            n2_descendants = nx.descendants(branch2,node2)\n            for n in list(branch2.nodes):\n                if n not in n2_descendants and n is not node2: #removes all nodes not in the branch\n                    branch2.remove_node(n)\n\n            #if node1 plus node2 branch has more than max_children, skip\n            if branch2.number_of_nodes() + self.graph.number_of_nodes() &gt; self.max_size:\n                continue\n\n            branch2 = copy.deepcopy(branch2)\n            branch2_root = get_roots(branch2)[0]\n            self.graph.add_edges_from(branch2.edges)\n            self.graph.add_edge(node1,branch2_root)\n\n            return True\n        return False\n\n\n\n    def _crossover_nodes(self, G2, rng=None):\n        '''\n        Swaps the hyperparamters of one randomly chosen node in Parent1 with the hyperparameters of randomly chosen node in Parent2.\n        '''\n        rng = np.random.default_rng(rng)\n\n        if self.crossover_same_depth:\n            pair_gen = select_nodes_same_depth(self.graph, self.root, G2.graph, G2.root, rng=rng)\n        else:\n            pair_gen = select_nodes_randomly(self.graph, G2.graph, rng=rng)\n\n        for node1, node2 in pair_gen:\n\n            #if both nodes are leaves\n            if len(list(self.graph.successors(node1)))==0 and len(list(G2.graph.successors(node2)))==0:\n                if node1.crossover(node2):\n                    return True\n\n\n            #if both nodes are inner nodes\n            if len(list(self.graph.successors(node1)))&gt;0 and len(list(G2.graph.successors(node2)))&gt;0:\n                if len(list(self.graph.predecessors(node1)))&gt;0 and len(list(G2.graph.predecessors(node2)))&gt;0:\n                    if node1.crossover(node2):\n                        return True\n\n            #if both nodes are root nodes\n            if node1 is self.root and node2 is G2.root:\n                if node1.crossover(node2):\n                    return True\n\n\n        return False\n\n    #not including the nodes, just their children\n    #Finds leaves attached to nodes and swaps them\n    def _crossover_swap_leaf_at_node(self, G2, rng=None):\n        rng = np.random.default_rng(rng)\n\n        if self.crossover_same_depth:\n            pair_gen = select_nodes_same_depth(self.graph, self.root, G2.graph, G2.root, rng=rng)\n        else:\n            pair_gen = select_nodes_randomly(self.graph, G2.graph, rng=rng)\n\n        success = False\n        for node1, node2 in pair_gen:\n            # if leaves are protected node1 and node2 must both be leaves or both be inner nodes\n            if self.leaf_search_space is not None and not (len(list(self.graph.successors(node1)))==0 ^ len(list(G2.graph.successors(node2)))==0):\n                continue\n            #self_leafs = [c for c in nx.descendants(self.graph,node1) if len(list(self.graph.successors(c)))==0 and c is not node1]\n            node_leafs = [c for c in nx.descendants(G2.graph,node2) if len(list(G2.graph.successors(c)))==0 and c is not node2]\n\n            # if len(self_leafs) &gt;0:\n            #     for c in self_leafs:\n            #         if random.choice([True,False]):\n            #             self.graph.remove_node(c)\n            #             G2.graph.add_edge(node2, c)\n            #             success = True\n\n            if len(node_leafs) &gt;0:\n                for c in node_leafs:\n                    if rng.choice([True,False]):\n                        G2.graph.remove_node(c)\n                        self.graph.add_edge(node1, c)\n                        success = True\n\n        return success\n\n\n\n    #TODO edit so that G2 is not modified\n    def _crossover_swap_node(self, G2, rng=None):\n        '''\n        Swaps randomly chosen node from Parent1 with a randomly chosen node from Parent2.\n        '''\n        rng = np.random.default_rng(rng)\n\n        if self.crossover_same_depth:\n            pair_gen = select_nodes_same_depth(self.graph, self.root, G2.graph, G2.root, rng=rng)\n        else:\n            pair_gen = select_nodes_randomly(self.graph, G2.graph, rng=rng)\n\n        for node1, node2 in pair_gen:\n            if node1 is self.root or node2 is G2.root: #TODO: allow root\n                continue\n\n            #if leaves are protected\n            if self.leaf_search_space is not None:\n                #if one node is a leaf, the other must be a leaf\n                if not((len(list(self.graph.successors(node1)))==0) ^ (len(list(G2.graph.successors(node2)))==0)):\n                    continue #only continue if both are leaves, or both are not leaves\n\n\n            n1_s = self.graph.successors(node1)\n            n1_p = self.graph.predecessors(node1)\n\n            n2_s = G2.graph.successors(node2)\n            n2_p = G2.graph.predecessors(node2)\n\n            self.graph.remove_node(node1)\n            G2.graph.remove_node(node2)\n\n            self.graph.add_node(node2)\n\n            self.graph.add_edges_from([ (node2, n) for n in n1_s])\n            G2.graph.add_edges_from([ (node1, n) for n in n2_s])\n\n            self.graph.add_edges_from([ (n, node2) for n in n1_p])\n            G2.graph.add_edges_from([ (n, node1) for n in n2_p])\n\n            return True\n\n        return False\n\n\n    def _merge_duplicated_nodes(self):\n\n        graph_changed = False\n        merged = False\n        while(not merged):\n            node_list = list(self.graph.nodes)\n            merged = True\n            for node, other_node in itertools.product(node_list, node_list):\n                if node is other_node:\n                    continue\n\n                #If nodes are same class/hyperparameters\n                if node.unique_id() == other_node.unique_id():\n                    node_children = set(self.graph.successors(node))\n                    other_node_children = set(self.graph.successors(other_node))\n                    #if nodes have identical children, they can be merged\n                    if node_children == other_node_children:\n                        for other_node_parent in list(self.graph.predecessors(other_node)):\n                            if other_node_parent not in self.graph.predecessors(node):\n                                self.graph.add_edge(other_node_parent,node)\n\n                        self.graph.remove_node(other_node)\n                        merged=False\n                        graph_changed = True\n                        break\n\n        return graph_changed\n\n\n    def export_pipeline(self, memory=None, **kwargs):\n        estimator_graph = self.graph.copy()\n\n        #mapping = {node:node.method_class(**node.hyperparameters) for node in estimator_graph}\n        label_remapping = {}\n        label_to_instance = {}\n\n        for node in estimator_graph:\n            this_pipeline_node = node.export_pipeline(memory=memory, **kwargs)\n            found_unique_label = False\n            i=1\n            while not found_unique_label:\n                label = \"{0}_{1}\".format(this_pipeline_node.__class__.__name__, i)\n                if label not in label_to_instance:\n                    found_unique_label = True\n                else:\n                    i+=1\n\n            label_remapping[node] = label\n            label_to_instance[label] = this_pipeline_node\n\n        estimator_graph = nx.relabel_nodes(estimator_graph, label_remapping)\n\n        for label, instance in label_to_instance.items():\n            estimator_graph.nodes[label][\"instance\"] = instance\n\n        return tpot.GraphPipeline(graph=estimator_graph, memory=memory, use_label_encoder=self.use_label_encoder, method=self.method, cross_val_predict_cv=self.cross_val_predict_cv)\n\n\n    def plot(self):\n        G = self.graph.reverse()\n        #TODO clean this up\n        try:\n            pos = nx.planar_layout(G)  # positions for all nodes\n        except:\n            pos = nx.shell_layout(G)\n        # nodes\n        options = {'edgecolors': 'tab:gray', 'node_size': 800, 'alpha': 0.9}\n        nodelist = list(G.nodes)\n        node_color = [plt.cm.Set1(G.nodes[n]['recursive depth']) for n in G]\n\n        fig, ax = plt.subplots()\n\n        nx.draw(G, pos, nodelist=nodelist, node_color=node_color, ax=ax,  **options)\n\n\n        '''edgelist = []\n        for n in n1.node_set:\n            for child in n.children:\n                edgelist.append((n,child))'''\n\n        # edges\n        #nx.draw_networkx_edges(G, pos, width=3.0, arrows=True)\n        '''nx.draw_networkx_edges(\n            G,\n            pos,\n            edgelist=[edgelist],\n            width=8,\n            alpha=0.5,\n            edge_color='tab:red',\n        )'''\n\n\n\n        # some math labels\n        labels = {}\n        for i, n in enumerate(G.nodes):\n            labels[n] = n.method_class.__name__ + \"\\n\" + str(n.hyperparameters)\n\n\n        nx.draw_networkx_labels(G, pos, labels,ax=ax, font_size=7, font_color='black')\n\n        plt.tight_layout()\n        plt.axis('off')\n        plt.show()\n\n\n    def unique_id(self):\n        if self.graphkey is None:\n            #copy self.graph\n            new_graph = self.graph.copy()\n            for n in new_graph.nodes:\n                new_graph.nodes[n]['label'] = n.unique_id()\n\n            new_graph = nx.convert_node_labels_to_integers(new_graph)\n            self.graphkey = GraphKey(new_graph)\n\n        return self.graphkey\n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/graph/#tpot.search_spaces.pipelines.graph.GraphPipelineIndividual.crossover","title":"<code>crossover(ind2, rng=None)</code>","text":"<p>self is the first individual, ind2 is the second individual If crossover_same_depth, it will select graphindividuals at the same recursive depth. Otherwise, it will select graphindividuals randomly from the entire graph and its subgraphs.</p> <p>This does not impact graphs without subgraphs. And it does not impacts nodes that are not graphindividuals. Cros</p> Source code in <code>tpot/search_spaces/pipelines/graph.py</code> <pre><code>def crossover(self, ind2, rng=None):\n    '''\n    self is the first individual, ind2 is the second individual\n    If crossover_same_depth, it will select graphindividuals at the same recursive depth.\n    Otherwise, it will select graphindividuals randomly from the entire graph and its subgraphs.\n\n    This does not impact graphs without subgraphs. And it does not impacts nodes that are not graphindividuals. Cros\n    '''\n\n    rng = np.random.default_rng(rng)\n\n    rng.shuffle(self.crossover_methods_list)\n\n    finished = False\n\n    for crossover_method in self.crossover_methods_list:\n        if crossover_method(ind2, rng=rng):\n            self._merge_duplicated_nodes()\n            finished = True\n            break\n\n    if self.__debug:\n        try:\n            nx.find_cycle(self.graph)\n            print('something went wrong with ', crossover_method)\n        except:\n            pass\n\n    if finished:\n        self.graphkey = None\n\n    return finished\n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/graph/#tpot.search_spaces.pipelines.graph.GraphSearchPipeline","title":"<code>GraphSearchPipeline</code>","text":"<p>               Bases: <code>SearchSpace</code></p> Source code in <code>tpot/search_spaces/pipelines/graph.py</code> <pre><code>class GraphSearchPipeline(SearchSpace):\n    def __init__(self, \n        root_search_space: SearchSpace, \n        leaf_search_space: SearchSpace = None, \n        inner_search_space: SearchSpace = None, \n        max_size: int = np.inf,\n        crossover_same_depth: bool = False,\n        cross_val_predict_cv: Union[int, Callable] = 0, #signature function(estimator, X, y=none)\n        method: str = 'auto',\n        use_label_encoder: bool = False):\n\n        \"\"\"\n        Defines a search space of pipelines in the shape of a Directed Acyclic Graphs. The search spaces for root, leaf, and inner nodes can be defined separately if desired.\n        Each graph will have a single root serving as the final estimator which is drawn from the `root_search_space`. If the `leaf_search_space` is defined, all leaves \n        in the pipeline will be drawn from that search space. If the `leaf_search_space` is not defined, all leaves will be drawn from the `inner_search_space`.\n        Nodes that are not leaves or roots will be drawn from the `inner_search_space`. If the `inner_search_space` is not defined, there will be no inner nodes.\n\n        `cross_val_predict_cv`, `method`, `memory`, and `use_label_encoder` are passed to the GraphPipeline object when the pipeline is exported and not directly used in the search space.\n\n        Exports to a GraphPipeline object.\n\n        Parameters\n        ----------\n\n        root_search_space: SearchSpace\n            The search space for the root node of the graph. This node will be the final estimator in the pipeline.\n\n        inner_search_space: SearchSpace, optional\n            The search space for the inner nodes of the graph. If not defined, there will be no inner nodes.\n\n        leaf_search_space: SearchSpace, optional\n            The search space for the leaf nodes of the graph. If not defined, the leaf nodes will be drawn from the inner_search_space.\n\n        crossover_same_depth: bool, optional\n            If True, crossover will only occur between nodes at the same depth in the graph. If False, crossover will occur between nodes at any depth.\n\n        cross_val_predict_cv : int, default=0\n            Number of folds to use for the cross_val_predict function for inner classifiers and regressors. Estimators will still be fit on the full dataset, but the following node will get the outputs from cross_val_predict.\n\n            - 0-1 : When set to 0 or 1, the cross_val_predict function will not be used. The next layer will get the outputs from fitting and transforming the full dataset.\n            - &gt;=2 : When fitting pipelines with inner classifiers or regressors, they will still be fit on the full dataset.\n                    However, the output to the next node will come from cross_val_predict with the specified number of folds.\n\n        method: str, optional\n            The prediction method to use for the inner classifiers or regressors. If 'auto', it will try to use predict_proba, decision_function, or predict in that order.\n\n        memory: str or object with the joblib.Memory interface, optional\n            Used to cache the input and outputs of nodes to prevent refitting or computationally heavy transformations. By default, no caching is performed. If a string is given, it is the path to the caching directory.\n\n        use_label_encoder: bool, optional\n            If True, the label encoder is used to encode the labels to be 0 to N. If False, the label encoder is not used.\n            Mainly useful for classifiers (XGBoost) that require labels to be ints from 0 to N.\n            Can also be a sklearn.preprocessing.LabelEncoder object. If so, that label encoder is used.\n\n        \"\"\"\n\n\n        self.root_search_space = root_search_space\n        self.leaf_search_space = leaf_search_space\n        self.inner_search_space = inner_search_space\n        self.max_size = max_size\n        self.crossover_same_depth = crossover_same_depth\n\n        self.cross_val_predict_cv = cross_val_predict_cv\n        self.method = method\n        self.use_label_encoder = use_label_encoder\n\n    def generate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        ind =  GraphPipelineIndividual(self.root_search_space, self.leaf_search_space, self.inner_search_space, self.max_size, self.crossover_same_depth, \n                                       self.cross_val_predict_cv, self.method, self.use_label_encoder, rng=rng)  \n            # if user specified limit, grab a random number between that limit\n\n        if self.max_size is None or self.max_size == np.inf:\n            n_nodes = rng.integers(1, 5)\n        else:\n            n_nodes = min(rng.integers(1, self.max_size), 5)\n\n        starting_ops = []\n        if self.inner_search_space is not None:\n            starting_ops.append(ind._mutate_insert_inner_node)\n        if self.leaf_search_space is not None or self.inner_search_space is not None:\n            starting_ops.append(ind._mutate_insert_leaf)\n            n_nodes -= 1\n\n        if len(starting_ops) &gt; 0:\n            for _ in range(n_nodes-1):\n                func = rng.choice(starting_ops)\n                func(rng=rng)\n\n        ind._merge_duplicated_nodes()\n\n        return ind\n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/graph/#tpot.search_spaces.pipelines.graph.GraphSearchPipeline.__init__","title":"<code>__init__(root_search_space, leaf_search_space=None, inner_search_space=None, max_size=np.inf, crossover_same_depth=False, cross_val_predict_cv=0, method='auto', use_label_encoder=False)</code>","text":"<p>Defines a search space of pipelines in the shape of a Directed Acyclic Graphs. The search spaces for root, leaf, and inner nodes can be defined separately if desired. Each graph will have a single root serving as the final estimator which is drawn from the <code>root_search_space</code>. If the <code>leaf_search_space</code> is defined, all leaves  in the pipeline will be drawn from that search space. If the <code>leaf_search_space</code> is not defined, all leaves will be drawn from the <code>inner_search_space</code>. Nodes that are not leaves or roots will be drawn from the <code>inner_search_space</code>. If the <code>inner_search_space</code> is not defined, there will be no inner nodes.</p> <p><code>cross_val_predict_cv</code>, <code>method</code>, <code>memory</code>, and <code>use_label_encoder</code> are passed to the GraphPipeline object when the pipeline is exported and not directly used in the search space.</p> <p>Exports to a GraphPipeline object.</p> <p>Parameters:</p> Name Type Description Default <code>root_search_space</code> <code>SearchSpace</code> <p>The search space for the root node of the graph. This node will be the final estimator in the pipeline.</p> required <code>inner_search_space</code> <code>SearchSpace</code> <p>The search space for the inner nodes of the graph. If not defined, there will be no inner nodes.</p> <code>None</code> <code>leaf_search_space</code> <code>SearchSpace</code> <p>The search space for the leaf nodes of the graph. If not defined, the leaf nodes will be drawn from the inner_search_space.</p> <code>None</code> <code>crossover_same_depth</code> <code>bool</code> <p>If True, crossover will only occur between nodes at the same depth in the graph. If False, crossover will occur between nodes at any depth.</p> <code>False</code> <code>cross_val_predict_cv</code> <code>int</code> <p>Number of folds to use for the cross_val_predict function for inner classifiers and regressors. Estimators will still be fit on the full dataset, but the following node will get the outputs from cross_val_predict.</p> <ul> <li>0-1 : When set to 0 or 1, the cross_val_predict function will not be used. The next layer will get the outputs from fitting and transforming the full dataset.</li> <li> <p>=2 : When fitting pipelines with inner classifiers or regressors, they will still be fit on the full dataset.         However, the output to the next node will come from cross_val_predict with the specified number of folds.</p> </li> </ul> <code>0</code> <code>method</code> <code>str</code> <p>The prediction method to use for the inner classifiers or regressors. If 'auto', it will try to use predict_proba, decision_function, or predict in that order.</p> <code>'auto'</code> <code>memory</code> <p>Used to cache the input and outputs of nodes to prevent refitting or computationally heavy transformations. By default, no caching is performed. If a string is given, it is the path to the caching directory.</p> required <code>use_label_encoder</code> <code>bool</code> <p>If True, the label encoder is used to encode the labels to be 0 to N. If False, the label encoder is not used. Mainly useful for classifiers (XGBoost) that require labels to be ints from 0 to N. Can also be a sklearn.preprocessing.LabelEncoder object. If so, that label encoder is used.</p> <code>False</code> Source code in <code>tpot/search_spaces/pipelines/graph.py</code> <pre><code>def __init__(self, \n    root_search_space: SearchSpace, \n    leaf_search_space: SearchSpace = None, \n    inner_search_space: SearchSpace = None, \n    max_size: int = np.inf,\n    crossover_same_depth: bool = False,\n    cross_val_predict_cv: Union[int, Callable] = 0, #signature function(estimator, X, y=none)\n    method: str = 'auto',\n    use_label_encoder: bool = False):\n\n    \"\"\"\n    Defines a search space of pipelines in the shape of a Directed Acyclic Graphs. The search spaces for root, leaf, and inner nodes can be defined separately if desired.\n    Each graph will have a single root serving as the final estimator which is drawn from the `root_search_space`. If the `leaf_search_space` is defined, all leaves \n    in the pipeline will be drawn from that search space. If the `leaf_search_space` is not defined, all leaves will be drawn from the `inner_search_space`.\n    Nodes that are not leaves or roots will be drawn from the `inner_search_space`. If the `inner_search_space` is not defined, there will be no inner nodes.\n\n    `cross_val_predict_cv`, `method`, `memory`, and `use_label_encoder` are passed to the GraphPipeline object when the pipeline is exported and not directly used in the search space.\n\n    Exports to a GraphPipeline object.\n\n    Parameters\n    ----------\n\n    root_search_space: SearchSpace\n        The search space for the root node of the graph. This node will be the final estimator in the pipeline.\n\n    inner_search_space: SearchSpace, optional\n        The search space for the inner nodes of the graph. If not defined, there will be no inner nodes.\n\n    leaf_search_space: SearchSpace, optional\n        The search space for the leaf nodes of the graph. If not defined, the leaf nodes will be drawn from the inner_search_space.\n\n    crossover_same_depth: bool, optional\n        If True, crossover will only occur between nodes at the same depth in the graph. If False, crossover will occur between nodes at any depth.\n\n    cross_val_predict_cv : int, default=0\n        Number of folds to use for the cross_val_predict function for inner classifiers and regressors. Estimators will still be fit on the full dataset, but the following node will get the outputs from cross_val_predict.\n\n        - 0-1 : When set to 0 or 1, the cross_val_predict function will not be used. The next layer will get the outputs from fitting and transforming the full dataset.\n        - &gt;=2 : When fitting pipelines with inner classifiers or regressors, they will still be fit on the full dataset.\n                However, the output to the next node will come from cross_val_predict with the specified number of folds.\n\n    method: str, optional\n        The prediction method to use for the inner classifiers or regressors. If 'auto', it will try to use predict_proba, decision_function, or predict in that order.\n\n    memory: str or object with the joblib.Memory interface, optional\n        Used to cache the input and outputs of nodes to prevent refitting or computationally heavy transformations. By default, no caching is performed. If a string is given, it is the path to the caching directory.\n\n    use_label_encoder: bool, optional\n        If True, the label encoder is used to encode the labels to be 0 to N. If False, the label encoder is not used.\n        Mainly useful for classifiers (XGBoost) that require labels to be ints from 0 to N.\n        Can also be a sklearn.preprocessing.LabelEncoder object. If so, that label encoder is used.\n\n    \"\"\"\n\n\n    self.root_search_space = root_search_space\n    self.leaf_search_space = leaf_search_space\n    self.inner_search_space = inner_search_space\n    self.max_size = max_size\n    self.crossover_same_depth = crossover_same_depth\n\n    self.cross_val_predict_cv = cross_val_predict_cv\n    self.method = method\n    self.use_label_encoder = use_label_encoder\n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/sequential/","title":"Sequential","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/search_spaces/pipelines/sequential/#tpot.search_spaces.pipelines.sequential.SequentialPipeline","title":"<code>SequentialPipeline</code>","text":"<p>               Bases: <code>SearchSpace</code></p> Source code in <code>tpot/search_spaces/pipelines/sequential.py</code> <pre><code>class SequentialPipeline(SearchSpace):\n    def __init__(self, search_spaces : List[SearchSpace] ) -&gt; None:\n        \"\"\"\n        Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.\n        \"\"\"\n\n        self.search_spaces = search_spaces\n\n    def generate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        return SequentialPipelineIndividual(self.search_spaces, rng=rng)\n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/sequential/#tpot.search_spaces.pipelines.sequential.SequentialPipeline.__init__","title":"<code>__init__(search_spaces)</code>","text":"<p>Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.</p> Source code in <code>tpot/search_spaces/pipelines/sequential.py</code> <pre><code>def __init__(self, search_spaces : List[SearchSpace] ) -&gt; None:\n    \"\"\"\n    Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.\n    \"\"\"\n\n    self.search_spaces = search_spaces\n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/tree/","title":"Tree","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/search_spaces/pipelines/tree/#tpot.search_spaces.pipelines.tree.TreePipeline","title":"<code>TreePipeline</code>","text":"<p>               Bases: <code>SearchSpace</code></p> Source code in <code>tpot/search_spaces/pipelines/tree.py</code> <pre><code>class TreePipeline(SearchSpace):\n    def __init__(self, root_search_space : SearchSpace, \n                        leaf_search_space : SearchSpace = None, \n                        inner_search_space : SearchSpace =None, \n                        min_size: int = 2, \n                        max_size: int = 10,\n                        crossover_same_depth=False) -&gt; None:\n\n        \"\"\"\n        Generates a pipeline of variable length. Pipeline will have a tree structure similar to TPOT1.\n\n        \"\"\"\n\n        self.search_space = root_search_space\n        self.leaf_search_space = leaf_search_space\n        self.inner_search_space = inner_search_space\n        self.min_size = min_size\n        self.max_size = max_size\n        self.crossover_same_depth = crossover_same_depth\n\n    def generate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        return TreePipelineIndividual(self.search_space, self.leaf_search_space, self.inner_search_space, self.min_size, self.max_size, self.crossover_same_depth, rng=rng) \n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/tree/#tpot.search_spaces.pipelines.tree.TreePipeline.__init__","title":"<code>__init__(root_search_space, leaf_search_space=None, inner_search_space=None, min_size=2, max_size=10, crossover_same_depth=False)</code>","text":"<p>Generates a pipeline of variable length. Pipeline will have a tree structure similar to TPOT1.</p> Source code in <code>tpot/search_spaces/pipelines/tree.py</code> <pre><code>def __init__(self, root_search_space : SearchSpace, \n                    leaf_search_space : SearchSpace = None, \n                    inner_search_space : SearchSpace =None, \n                    min_size: int = 2, \n                    max_size: int = 10,\n                    crossover_same_depth=False) -&gt; None:\n\n    \"\"\"\n    Generates a pipeline of variable length. Pipeline will have a tree structure similar to TPOT1.\n\n    \"\"\"\n\n    self.search_space = root_search_space\n    self.leaf_search_space = leaf_search_space\n    self.inner_search_space = inner_search_space\n    self.min_size = min_size\n    self.max_size = max_size\n    self.crossover_same_depth = crossover_same_depth\n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/union/","title":"Union","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/search_spaces/pipelines/union/#tpot.search_spaces.pipelines.union.UnionPipeline","title":"<code>UnionPipeline</code>","text":"<p>               Bases: <code>SearchSpace</code></p> Source code in <code>tpot/search_spaces/pipelines/union.py</code> <pre><code>class UnionPipeline(SearchSpace):\n    def __init__(self, search_spaces : List[SearchSpace] ) -&gt; None:\n        \"\"\"\n        Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.\n        \"\"\"\n\n        self.search_spaces = search_spaces\n\n    def generate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        return UnionPipelineIndividual(self.search_spaces, rng=rng)\n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/union/#tpot.search_spaces.pipelines.union.UnionPipeline.__init__","title":"<code>__init__(search_spaces)</code>","text":"<p>Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.</p> Source code in <code>tpot/search_spaces/pipelines/union.py</code> <pre><code>def __init__(self, search_spaces : List[SearchSpace] ) -&gt; None:\n    \"\"\"\n    Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.\n    \"\"\"\n\n    self.search_spaces = search_spaces\n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/union/#tpot.search_spaces.pipelines.union.UnionPipelineIndividual","title":"<code>UnionPipelineIndividual</code>","text":"<p>               Bases: <code>SklearnIndividual</code></p> <p>Takes in a list of search spaces. each space is a list of SearchSpaces. Will produce a FeatureUnion pipeline. Each step in the pipeline will correspond to the the search space provided in the same index. The resulting pipeline will be a FeatureUnion of the steps in the pipeline.</p> Source code in <code>tpot/search_spaces/pipelines/union.py</code> <pre><code>class UnionPipelineIndividual(SklearnIndividual):\n    \"\"\"\n    Takes in a list of search spaces. each space is a list of SearchSpaces.\n    Will produce a FeatureUnion pipeline. Each step in the pipeline will correspond to the the search space provided in the same index.\n    The resulting pipeline will be a FeatureUnion of the steps in the pipeline.\n\n    \"\"\"\n\n    def __init__(self, search_spaces : List[SearchSpace], rng=None) -&gt; None:\n        super().__init__()\n        self.search_spaces = search_spaces\n\n        self.pipeline = []\n        for space in self.search_spaces:\n            self.pipeline.append(space.generate(rng))\n\n    def mutate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        step = rng.choice(self.pipeline)\n        return step.mutate(rng)\n\n\n    def crossover(self, other, rng=None):\n        #swap a random step in the pipeline with the corresponding step in the other pipeline\n        rng = np.random.default_rng(rng)\n\n        cx_funcs = [self._crossover_node, self._crossover_swap_node]\n        rng.shuffle(cx_funcs)\n        for cx_func in cx_funcs:\n            if cx_func(other, rng):\n                return True\n\n        return False\n\n    def _crossover_swap_node(self, other, rng):\n        rng = np.random.default_rng(rng)\n        idx = rng.integers(1,len(self.pipeline))\n\n        self.pipeline[idx], other.pipeline[idx] = other.pipeline[idx], self.pipeline[idx]\n        return True\n\n    def _crossover_node(self, other, rng):\n        rng = np.random.default_rng(rng)\n\n        crossover_success = False\n        for idx in range(len(self.pipeline)):\n            if rng.random() &lt; 0.5:\n                if self.pipeline[idx].crossover(other.pipeline[idx], rng):\n                    crossover_success = True\n\n        return crossover_success\n\n    def export_pipeline(self, **kwargs):\n        return sklearn.pipeline.make_union(*[step.export_pipeline(**kwargs) for step in self.pipeline])\n\n    def unique_id(self):\n        l = [step.unique_id() for step in self.pipeline]\n        l = [\"FeatureUnion\"] + l\n        return TupleIndex(tuple(l))\n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/wrapper/","title":"Wrapper","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/search_spaces/pipelines/wrapper/#tpot.search_spaces.pipelines.wrapper.WrapperPipeline","title":"<code>WrapperPipeline</code>","text":"<p>               Bases: <code>SearchSpace</code></p> Source code in <code>tpot/search_spaces/pipelines/wrapper.py</code> <pre><code>class WrapperPipeline(SearchSpace):\n    def __init__(\n            self, \n            method: type, \n            space: ConfigurationSpace,\n            estimator_search_space: SearchSpace,\n            hyperparameter_parser: callable = None, \n            wrapped_param_name: str = None\n            ) -&gt; None:\n\n        \"\"\"\n        This search space is for wrapping a sklearn estimator with a method that takes another estimator and hyperparameters as arguments.\n        For example, this can be used with sklearn.ensemble.BaggingClassifier or sklearn.ensemble.AdaBoostClassifier.\n\n        \"\"\"\n\n\n        self.estimator_search_space = estimator_search_space\n        self.method = method\n        self.space = space\n        self.hyperparameter_parser=hyperparameter_parser\n        self.wrapped_param_name = wrapped_param_name\n\n    def generate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        return WrapperPipelineIndividual(method=self.method, space=self.space, estimator_search_space=self.estimator_search_space, hyperparameter_parser=self.hyperparameter_parser, wrapped_param_name=self.wrapped_param_name,  rng=rng)\n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/wrapper/#tpot.search_spaces.pipelines.wrapper.WrapperPipeline.__init__","title":"<code>__init__(method, space, estimator_search_space, hyperparameter_parser=None, wrapped_param_name=None)</code>","text":"<p>This search space is for wrapping a sklearn estimator with a method that takes another estimator and hyperparameters as arguments. For example, this can be used with sklearn.ensemble.BaggingClassifier or sklearn.ensemble.AdaBoostClassifier.</p> Source code in <code>tpot/search_spaces/pipelines/wrapper.py</code> <pre><code>def __init__(\n        self, \n        method: type, \n        space: ConfigurationSpace,\n        estimator_search_space: SearchSpace,\n        hyperparameter_parser: callable = None, \n        wrapped_param_name: str = None\n        ) -&gt; None:\n\n    \"\"\"\n    This search space is for wrapping a sklearn estimator with a method that takes another estimator and hyperparameters as arguments.\n    For example, this can be used with sklearn.ensemble.BaggingClassifier or sklearn.ensemble.AdaBoostClassifier.\n\n    \"\"\"\n\n\n    self.estimator_search_space = estimator_search_space\n    self.method = method\n    self.space = space\n    self.hyperparameter_parser=hyperparameter_parser\n    self.wrapped_param_name = wrapped_param_name\n</code></pre>"},{"location":"documentation/tpot/selectors/lexicase_selection/","title":"Lexicase selection","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/selectors/lexicase_selection/#tpot.selectors.lexicase_selection.lexicase_selection","title":"<code>lexicase_selection(scores, k, n_parents=1, rng=None)</code>","text":"<p>Select the best individual according to Lexicase Selection, k times. The returned list contains the indices of the chosen individuals.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>ndarray</code> <p>The score matrix, where rows the individuals and the columns are the corresponds to scores on different objectives.</p> required <code>k</code> <code>int</code> <p>The number of individuals to select.</p> required <code>n_parents</code> <code>int</code> <p>The number of parents to select per individual. The default is 1.</p> <code>1</code> <code>rng</code> <code>(int, Generator)</code> <p>The random number generator. The default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>    A array of indices of selected individuals of shape (k, n_parents).</code> Source code in <code>tpot/selectors/lexicase_selection.py</code> <pre><code>def lexicase_selection(scores, k, n_parents=1, rng=None):\n    \"\"\"\n    Select the best individual according to Lexicase Selection, *k* times.\n    The returned list contains the indices of the chosen *individuals*.\n\n    Parameters\n    ----------\n    scores : np.ndarray\n        The score matrix, where rows the individuals and the columns are the corresponds to scores on different objectives.\n    k : int\n        The number of individuals to select.\n    n_parents : int, optional\n        The number of parents to select per individual. The default is 1.\n    rng : int, np.random.Generator, optional\n        The random number generator. The default is None.\n    Returns\n    -------\n        A array of indices of selected individuals of shape (k, n_parents).\n    \"\"\"\n    rng = np.random.default_rng(rng)\n    chosen =[]\n    for i in range(k*n_parents):\n        candidates = list(range(len(scores)))\n        cases = list(range(len(scores[0])))\n        rng.shuffle(cases)\n\n        while len(cases) &gt; 0 and len(candidates) &gt; 1:\n            best_val_for_case = max(scores[candidates,cases[0]])\n            candidates = [x for x in candidates if scores[x, cases[0]] == best_val_for_case]\n            cases.pop(0)\n        chosen.append(rng.choice(candidates))\n\n    return np.reshape(chosen, (k, n_parents))\n</code></pre>"},{"location":"documentation/tpot/selectors/map_elites_selection/","title":"Map elites selection","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/selectors/map_elites_selection/#tpot.selectors.map_elites_selection.create_nd_matrix","title":"<code>create_nd_matrix(matrix, grid_steps=None, bins=None)</code>","text":"<p>Create an n-dimensional matrix with the highest score for each cell</p> <p>Parameters:</p> Name Type Description Default <code>matrix</code> <code>ndarray</code> <p>The score matrix, where the first column is the score and the rest are the features for the map-elites algorithm.</p> required <code>grid_steps</code> <code>int</code> <p>The number of steps to use for each feature to automatically create the bin thresholds. The default is None.</p> <code>None</code> <code>bins</code> <code>list</code> <p>A list of lists containing the bin edges for each feature (other than the score). The default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>An n-dimensional matrix with the highest score for each cell and the index of the individual with that score. The value in the cell is a dictionary with the keys \"score\" and \"idx\" containing the score and index of the individual respectively.</p> Source code in <code>tpot/selectors/map_elites_selection.py</code> <pre><code>def create_nd_matrix(matrix, grid_steps=None, bins=None):\n    \"\"\"\n    Create an n-dimensional matrix with the highest score for each cell\n\n    Parameters\n    ----------\n    matrix : np.ndarray\n        The score matrix, where the first column is the score and the rest are the features for the map-elites algorithm.\n    grid_steps : int, optional\n        The number of steps to use for each feature to automatically create the bin thresholds. The default is None.\n    bins : list, optional\n        A list of lists containing the bin edges for each feature (other than the score). The default is None.\n\n    Returns\n    -------\n    np.ndarray\n        An n-dimensional matrix with the highest score for each cell and the index of the individual with that score.\n        The value in the cell is a dictionary with the keys \"score\" and \"idx\" containing the score and index of the individual respectively.\n    \"\"\"\n    if grid_steps is not None and bins is not None:\n        raise ValueError(\"Either grid_steps or bins must be provided but not both\")\n\n    # Extract scores and features\n    scores = matrix[:, 0]\n    features = matrix[:, 1:]\n\n    # Determine the min and max of each feature\n    min_vals = np.min(features, axis=0)\n    max_vals = np.max(features, axis=0)\n\n    # Create bins for each feature\n    if bins is None:\n        bins = [np.linspace(min_vals[i], max_vals[i], grid_steps) for i in range(len(min_vals))]\n\n    # Initialize n-dimensional matrix with negative infinity\n    nd_matrix = np.full([len(b)+1 for b in bins], {\"score\": -np.inf, \"idx\": None})\n    # Fill in each cell with the highest score for that cell\n    for idx, (score, feature) in enumerate(zip(scores, features)):\n        indices = [np.digitize(f, bin) for f, bin in zip(feature, bins)]\n        cur_score = nd_matrix[tuple(indices)][\"score\"]\n        if score &gt; cur_score:\n            nd_matrix[tuple(indices)] = {\"score\": score, \"idx\": idx}\n\n    return nd_matrix\n</code></pre>"},{"location":"documentation/tpot/selectors/map_elites_selection/#tpot.selectors.map_elites_selection.get_bins","title":"<code>get_bins(arr, k)</code>","text":"<p>Get equally spaced bin thresholds between the min and max values for the array of scores.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>The list of values to calculate the bins for.</p> required <code>k</code> <code>int</code> <p>The number of bins to create.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of bin thresholds calculated to be k equally spaced bins between the min and max of the array.</p> Source code in <code>tpot/selectors/map_elites_selection.py</code> <pre><code>def get_bins(arr, k):\n    \"\"\"\n    Get equally spaced bin thresholds between the min and max values for the array of scores.\n\n    Parameters\n    ----------\n    arr : np.ndarray\n        The list of values to calculate the bins for.\n    k : int\n        The number of bins to create.\n\n    Returns\n    -------\n    list\n        A list of bin thresholds calculated to be k equally spaced bins between the min and max of the array.\n\n    \"\"\"\n    min_vals = np.min(arr, axis=0)\n    max_vals = np.max(arr, axis=0)\n    [np.linspace(min_vals[i], max_vals[i], k) for i in range(len(min_vals))]\n</code></pre>"},{"location":"documentation/tpot/selectors/map_elites_selection/#tpot.selectors.map_elites_selection.get_bins_quantiles","title":"<code>get_bins_quantiles(arr, k=None, q=None)</code>","text":"<p>Takes a matrix and returns the bin thresholds based on quantiles.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>The matrix to calculate the bins for.</p> required <code>k</code> <code>int</code> <p>The number of bins to create. This parameter creates k equally spaced quantiles.  For example, k=3 will create quantiles at array([0.25, 0.5 , 0.75]).</p> <code>None</code> <code>q</code> <code>ndarray</code> <p>Custom quantiles to use for the bins. This parameter creates bins based on the quantiles of the data. The default is None.</p> <code>None</code> Source code in <code>tpot/selectors/map_elites_selection.py</code> <pre><code>def get_bins_quantiles(arr, k=None, q=None):\n    \"\"\"\n    Takes a matrix and returns the bin thresholds based on quantiles.\n\n    Parameters\n    ----------\n    arr : np.ndarray\n        The matrix to calculate the bins for.\n    k : int, optional\n        The number of bins to create. This parameter creates k equally spaced quantiles. \n        For example, k=3 will create quantiles at array([0.25, 0.5 , 0.75]).\n    q : np.ndarray, optional\n        Custom quantiles to use for the bins. This parameter creates bins based on the quantiles of the data. The default is None.\n    \"\"\"\n    bins = []\n\n    if q is not None and k is not None:\n        raise ValueError(\"Only one of k or q can be specified\")\n\n    if q is not None:\n        final_q = q\n    elif k is not None:\n        final_q = np.linspace(0, 1, k+2)[1:-1]\n\n    for i in range(arr.shape[1]):\n        bins.append(np.quantile(arr[:,i], final_q))\n    return bins\n</code></pre>"},{"location":"documentation/tpot/selectors/map_elites_selection/#tpot.selectors.map_elites_selection.manhattan","title":"<code>manhattan(a, b)</code>","text":"<p>Calculate the Manhattan distance between two points.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>ndarray</code> <p>The first point.</p> required <code>b</code> <code>ndarray</code> <p>The second point.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The Manhattan distance between the two points.</p> Source code in <code>tpot/selectors/map_elites_selection.py</code> <pre><code>def manhattan(a, b):\n    \"\"\"\n    Calculate the Manhattan distance between two points.\n\n    Parameters\n    ----------\n    a : np.ndarray\n        The first point.\n    b : np.ndarray\n        The second point.\n\n    Returns\n    -------\n    float\n        The Manhattan distance between the two points.\n    \"\"\"\n    return sum(abs(val1-val2) for val1, val2 in zip(a,b))\n</code></pre>"},{"location":"documentation/tpot/selectors/map_elites_selection/#tpot.selectors.map_elites_selection.map_elites_parent_selector","title":"<code>map_elites_parent_selector(scores, k, n_parents=1, rng=None, manhattan_distance=2, grid_steps=10, bins=None)</code>","text":"<p>A parent selection algorithm for the map-elites algorithm. First creates a grid of the best individuals per cell and then selects parents based on the Manhattan distance between the cells of the best individuals.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>ndarray</code> <p>The score matrix, where the first column is the score and the rest are the features for the map-elites algorithm.</p> required <code>k</code> <code>int</code> <p>The number of individuals to select.</p> required <code>n_parents</code> <code>int</code> <p>The number of parents to select per individual. The default is 1.</p> <code>1</code> <code>rng</code> <code>(int, Generator)</code> <p>The random number generator. The default is None.</p> <code>None</code> <code>manhattan_distance</code> <code>int</code> <p>The maximum Manhattan distance between parents. The default is 2. If no parents are found within this distance, the distance is increased by 1 until at least one other parent is found.</p> <code>2</code> <code>grid_steps</code> <code>int</code> <p>The number of steps to use for each feature to automatically create the bin thresholds. The default is None.</p> <code>10</code> <code>bins</code> <code>list</code> <p>A list of lists containing the bin edges for each feature (other than the score). The default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>An array of indexes of the parents selected for each individual</p> Source code in <code>tpot/selectors/map_elites_selection.py</code> <pre><code>def map_elites_parent_selector(scores,  k, n_parents=1, rng=None, manhattan_distance = 2,  grid_steps= 10, bins=None):\n    \"\"\"\n    A parent selection algorithm for the map-elites algorithm. First creates a grid of the best individuals per cell and then selects parents based on the Manhattan distance between the cells of the best individuals.\n\n    Parameters\n    ----------\n    scores : np.ndarray\n        The score matrix, where the first column is the score and the rest are the features for the map-elites algorithm.\n    k : int\n        The number of individuals to select.\n    n_parents : int, optional\n        The number of parents to select per individual. The default is 1.\n    rng : int, np.random.Generator, optional\n        The random number generator. The default is None.\n    manhattan_distance : int, optional\n        The maximum Manhattan distance between parents. The default is 2. If no parents are found within this distance, the distance is increased by 1 until at least one other parent is found.\n    grid_steps : int, optional\n        The number of steps to use for each feature to automatically create the bin thresholds. The default is None.\n    bins : list, optional\n        A list of lists containing the bin edges for each feature (other than the score). The default is None.\n\n    Returns\n    -------\n    np.ndarray\n        An array of indexes of the parents selected for each individual\n\n    \"\"\"\n\n\n    if grid_steps is not None and bins is not None:\n        raise ValueError(\"Either grid_steps or bins must be provided but not both\")\n\n    rng = np.random.default_rng(rng)\n    scores = np.array(scores)\n    #create grid\n\n    matrix = create_nd_matrix(scores, grid_steps=grid_steps, bins=bins)\n\n    #return true if cell is not empty\n    f = np.vectorize(lambda x: x[\"idx\"] is not None)\n    valid_coordinates  = np.array(np.where(f(matrix))).T\n\n    idx_to_coordinates = {matrix[tuple(coordinates)][\"idx\"]: coordinates for coordinates in valid_coordinates}\n\n    idxes = [idx for idx in idx_to_coordinates.keys()] #all the indexes of best score per cell\n\n    distance_matrix = np.zeros((len(idxes), len(idxes)))\n\n    for i, idx1 in enumerate(idxes):\n        for j, idx2 in enumerate(idxes):\n            distance_matrix[i][j] = manhattan(idx_to_coordinates[idx1], idx_to_coordinates[idx2])\n\n\n    parents = []\n\n    for i in range(k):\n        #randomly select a cell\n        idx = rng.choice(idxes) #select random parent\n\n        #get the distance from this parent to all other parents \n        dm_idx = idxes.index(idx) \n        row = distance_matrix[dm_idx] \n\n        #get all second parents that are within manhattan distance. if none are found increase the distance\n        candidates = []\n        while len(candidates) == 0:\n            candidates = np.where(row &lt;= manhattan_distance)[0]\n            #remove self from candidates\n            candidates = candidates[candidates != dm_idx]\n            manhattan_distance += 1\n\n            if manhattan_distance &gt; np.max(distance_matrix):\n                break\n\n        if len(candidates) == 0:\n            parents.append([idx, idx]) #if no other parents are found, select the same parent twice. weird to crossover with itself though\n        else:\n            this_parents = [idx]\n            for p in range(n_parents-1):\n                idx2_cords = rng.choice(candidates)\n                this_parents.append(idxes[idx2_cords])\n            parents.append(this_parents)\n\n    return np.array(parents)\n</code></pre>"},{"location":"documentation/tpot/selectors/map_elites_selection/#tpot.selectors.map_elites_selection.map_elites_survival_selector","title":"<code>map_elites_survival_selector(scores, k=None, rng=None, grid_steps=10, bins=None)</code>","text":"<p>Takes a matrix of scores and returns the indexes of the individuals that are in the best cells of the map-elites grid. Can either take a grid_steps parameter to automatically create the bins or a bins parameter to specify the bins manually.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>ndarray</code> <p>The score matrix, where the first column is the score and the rest are the features for the map-elites algorithm.</p> required <code>k</code> <code>int</code> <p>The number of individuals to select. The default is None.</p> <code>None</code> <code>rng</code> <code>(int, Generator)</code> <p>The random number generator. The default is None.</p> <code>None</code> <code>grid_steps</code> <code>int</code> <p>The number of steps to use for each feature to automatically create the bin thresholds. The default is None.</p> <code>10</code> <code>bins</code> <code>list</code> <p>A list of lists containing the bin edges for each feature (other than the score). The default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>An array of indexes of the individuals in the best cells of the map-elites grid (without repeats).</p> Source code in <code>tpot/selectors/map_elites_selection.py</code> <pre><code>def map_elites_survival_selector(scores,  k=None, rng=None, grid_steps= 10, bins=None):\n    \"\"\"\n    Takes a matrix of scores and returns the indexes of the individuals that are in the best cells of the map-elites grid.\n    Can either take a grid_steps parameter to automatically create the bins or a bins parameter to specify the bins manually.\n\n    Parameters\n    ----------\n    scores : np.ndarray\n        The score matrix, where the first column is the score and the rest are the features for the map-elites algorithm.\n    k : int, optional\n        The number of individuals to select. The default is None.\n    rng : int, np.random.Generator, optional\n        The random number generator. The default is None.\n    grid_steps : int, optional\n        The number of steps to use for each feature to automatically create the bin thresholds. The default is None.\n    bins : list, optional\n        A list of lists containing the bin edges for each feature (other than the score). The default is None.\n\n    Returns\n    -------\n    np.ndarray\n        An array of indexes of the individuals in the best cells of the map-elites grid (without repeats).\n\n    \"\"\"\n\n    if grid_steps is not None and bins is not None:\n        raise ValueError(\"Either grid_steps or bins must be provided but not both\")\n\n    rng = np.random.default_rng(rng)\n    scores = np.array(scores)\n    #create grid\n\n    matrix = create_nd_matrix(scores, grid_steps=grid_steps, bins=bins)\n    matrix = matrix.flatten()\n\n    indexes =  [cell[\"idx\"] for cell in matrix if cell[\"idx\"] is not None]\n\n    return np.unique(indexes)\n</code></pre>"},{"location":"documentation/tpot/selectors/max_weighted_average_selector/","title":"Max weighted average selector","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/selectors/max_weighted_average_selector/#tpot.selectors.max_weighted_average_selector.max_weighted_average_selector","title":"<code>max_weighted_average_selector(scores, k, n_parents=1, rng=None)</code>","text":"<p>Select the best individual according to Max Weighted Average Selection, k times.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>ndarray</code> <p>The score matrix, where rows the individuals and the columns are the corresponds to scores on different objectives.</p> required <code>k</code> <code>int</code> <p>The number of individuals to select.</p> required <code>n_parents</code> <code>int</code> <p>The number of parents to select per individual. The default is 1.</p> <code>1</code> <code>rng</code> <code>(int, Generator)</code> <p>The random number generator. The default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>    A array of indices of selected individuals of shape (k, n_parents).</code> Source code in <code>tpot/selectors/max_weighted_average_selector.py</code> <pre><code>def max_weighted_average_selector(scores,k, n_parents=1, rng=None):\n    \"\"\"\n    Select the best individual according to Max Weighted Average Selection, *k* times.\n\n    Parameters\n    ----------\n    scores : np.ndarray\n        The score matrix, where rows the individuals and the columns are the corresponds to scores on different objectives.\n    k : int\n        The number of individuals to select.\n    n_parents : int, optional\n        The number of parents to select per individual. The default is 1.\n    rng : int, np.random.Generator, optional\n        The random number generator. The default is None.\n\n    Returns\n    -------\n        A array of indices of selected individuals of shape (k, n_parents).\n\n    \"\"\"\n    ave_scores = [np.nanmean(s ) for s in scores ] #TODO make this more efficient\n    chosen = np.argsort(ave_scores)[::-1][0:k] #TODO check this behavior with nans\n    return np.reshape(chosen, (k, n_parents))\n</code></pre>"},{"location":"documentation/tpot/selectors/nsgaii/","title":"Nsgaii","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/selectors/nsgaii/#tpot.selectors.nsgaii.crowding_distance","title":"<code>crowding_distance(matrix)</code>","text":"<p>Takes a matrix of scores and returns the crowding distance for each point.</p> <p>Parameters:</p> Name Type Description Default <code>matrix</code> <code>ndarray</code> <p>The score matrix, where rows the individuals and the columns are the corresponds to scores on different objectives.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of the crowding distances for each point in the score matrix.</p> Source code in <code>tpot/selectors/nsgaii.py</code> <pre><code>def crowding_distance(matrix):\n    \"\"\"\n    Takes a matrix of scores and returns the crowding distance for each point.\n\n    Parameters\n    ----------\n    matrix : np.ndarray\n        The score matrix, where rows the individuals and the columns are the corresponds to scores on different objectives.\n\n    Returns\n    -------\n    list\n        A list of the crowding distances for each point in the score matrix.\n    \"\"\"\n    matrix = np.array(matrix)\n    # Initialize the crowding distance for each point to zero\n    crowding_distances = [0 for _ in range(len(matrix))]\n\n    # Iterate over each objective\n    for objective_i in range(matrix.shape[1]):\n        # Sort the points according to the current objective\n        sorted_i = matrix[:, objective_i].argsort()\n\n        # Set the crowding distance of the first and last points to infinity\n        crowding_distances[sorted_i[0]] = float(\"inf\")\n        crowding_distances[sorted_i[-1]] = float(\"inf\")\n\n        if matrix[sorted_i[0]][objective_i] == matrix[sorted_i[-1]][objective_i]: # https://github.com/DEAP/deap/blob/f2a570567fa3dce156d7cfb0c50bc72f133258a1/deap/tools/emo.py#L135\n            continue\n\n        norm = matrix.shape[1] * float(matrix[sorted_i[0]][objective_i] - matrix[sorted_i[-1]][objective_i])\n        for prev, cur, following in zip(sorted_i[:-2], sorted_i[1:-1], sorted_i[2:]):\n            crowding_distances[cur] += (matrix[following][objective_i] - matrix[prev][objective_i]) / norm\n\n\n    return crowding_distances\n</code></pre>"},{"location":"documentation/tpot/selectors/nsgaii/#tpot.selectors.nsgaii.dominates","title":"<code>dominates(list1, list2)</code>","text":"<p>returns true is all values in list1 are not strictly worse than list2 AND at least one item in list1 is better than list2</p> <p>Parameters:</p> Name Type Description Default <code>list1</code> <code>list</code> <p>The first list of values to compare.</p> required <code>list2</code> <code>list</code> <p>The second list of values to compare.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if all values in list1 are not strictly worse than list2 AND at least one item in list1 is better than list2, False otherwise.</p> Source code in <code>tpot/selectors/nsgaii.py</code> <pre><code>def dominates(list1, list2):\n    \"\"\"\n    returns true is all values in list1 are not strictly worse than list2 AND at least one item in list1 is better than list2\n\n    Parameters\n    ----------\n    list1 : list\n        The first list of values to compare.\n    list2 : list\n        The second list of values to compare.\n\n    Returns\n    -------\n    bool\n        True if all values in list1 are not strictly worse than list2 AND at least one item in list1 is better than list2, False otherwise.\n\n    \"\"\"\n    return all(list1[i] &gt;= list2[i] for i in range(len(list1))) and any(list1[i] &gt; list2[i] for i in range(len(list1)))\n</code></pre>"},{"location":"documentation/tpot/selectors/nsgaii/#tpot.selectors.nsgaii.nondominated_sorting","title":"<code>nondominated_sorting(matrix)</code>","text":"<p>Returns the indices of the non-dominated rows in the scores matrix. Rows are considered samples, and columns are considered objectives.</p> <p>Parameters:</p> Name Type Description Default <code>matrix</code> <code>ndarray</code> <p>The score matrix, where rows the individuals and the columns are the corresponds to scores on different objectives.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of lists of indices of the non-dominated rows in the scores matrix.</p> Source code in <code>tpot/selectors/nsgaii.py</code> <pre><code>def nondominated_sorting(matrix):\n    \"\"\"\n    Returns the indices of the non-dominated rows in the scores matrix.\n    Rows are considered samples, and columns are considered objectives.\n\n    Parameters\n    ----------\n    matrix : np.ndarray\n        The score matrix, where rows the individuals and the columns are the corresponds to scores on different objectives.\n\n    Returns\n    -------\n    list\n        A list of lists of indices of the non-dominated rows in the scores matrix.\n\n    \"\"\"\n    # Initialize the front list and the rank list\n\n    # Initialize the current front\n    fronts = {0:set()}\n\n    # Initialize the list of dominated points\n    dominated = [set() for _ in range(len(matrix))] #si the set of solutions which solution i dominates\n\n    # Initialize the list of points that dominate the current point\n    dominating = [0 for _ in range(len(matrix))] #ni the number of solutions that denominate solution i\n\n\n    # Iterate over all points\n    for p, p_scores in enumerate(matrix):\n        # Iterate over all other points\n        for q, q_scores in enumerate(matrix):\n            # If the current point dominates the other point, increment the count of points dominated by the current point\n            if dominates(p_scores, q_scores):\n                dominated[p].add(q)\n            # If the current point is dominated by the other point, add it to the list of dominated points\n            elif dominates(q_scores, p_scores):\n                dominating[p] += 1\n\n        if dominating[p] == 0:\n            fronts[0].add(p)\n\n    i=0\n\n    # Iterate until all points have been added to a front\n    while len(fronts[i]) &gt; 0:\n        H = set()\n        for p in fronts[i]:\n            for q in dominated[p]:\n                dominating[q] -= 1\n                if dominating[q] == 0:\n                    H.add(q)\n\n        i += 1\n        fronts[i] = H\n\n\n    return [fronts[j] for j in range(i)]\n</code></pre>"},{"location":"documentation/tpot/selectors/nsgaii/#tpot.selectors.nsgaii.survival_select_NSGA2","title":"<code>survival_select_NSGA2(scores, k, rng=None)</code>","text":"<p>Select the top k individuals from the scores matrix using the NSGA-II algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>ndarray</code> <p>The score matrix, where rows the individuals and the columns are the corresponds to scores on different objectives.</p> required <code>k</code> <code>int</code> <p>The number of individuals to select.</p> required <code>rng</code> <code>(int, Generator)</code> <p>The random number generator. The default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of indices of the selected individuals (without repeats).</p> Source code in <code>tpot/selectors/nsgaii.py</code> <pre><code>def survival_select_NSGA2(scores, k, rng=None):\n    \"\"\"\n    Select the top k individuals from the scores matrix using the NSGA-II algorithm.\n\n    Parameters\n    ----------\n    scores : np.ndarray\n        The score matrix, where rows the individuals and the columns are the corresponds to scores on different objectives.\n    k : int\n        The number of individuals to select.\n    rng : int, np.random.Generator, optional\n        The random number generator. The default is None.\n\n    Returns\n    -------\n    list\n        A list of indices of the selected individuals (without repeats).\n\n    \"\"\"\n\n    pareto_fronts = nondominated_sorting(scores)\n\n    # chosen = list(itertools.chain.from_iterable(fronts))\n    # if len(chosen) &gt;= k:\n    #     return chosen[0:k]\n\n    chosen = []\n    current_front_number = 0\n    while len(chosen) &lt; k and current_front_number &lt; len(pareto_fronts):\n\n        current_front = np.array(list(pareto_fronts[current_front_number]))\n        front_scores = [scores[i] for i in current_front]\n        crowding_distances = crowding_distance(front_scores)\n\n        sorted_indeces = current_front[np.argsort(crowding_distances)[::-1]]\n\n        chosen.extend(sorted_indeces[0:(k-len(chosen))])\n\n        current_front_number += 1\n\n    return chosen\n</code></pre>"},{"location":"documentation/tpot/selectors/random_selector/","title":"Random selector","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/selectors/random_selector/#tpot.selectors.random_selector.random_selector","title":"<code>random_selector(scores, k, n_parents=1, rng=None)</code>","text":"<p>Randomly selects indeces of individuals from the scores matrix.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>ndarray</code> <p>The score matrix, where rows the individuals and the columns are the corresponds to scores on different objectives.</p> required <code>k</code> <code>int</code> <p>The number of individuals to select.</p> required <code>n_parents</code> <code>int</code> <p>The number of parents to select per individual. The default is 1.</p> <code>1</code> <code>rng</code> <code>(int, Generator)</code> <p>The random number generator. The default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>    A array of indices of randomly selected individuals (with replacement) of shape (k, n_parents).</code> Source code in <code>tpot/selectors/random_selector.py</code> <pre><code>def random_selector(scores,  k, n_parents=1, rng=None, ):\n    \"\"\"\n    Randomly selects indeces of individuals from the scores matrix.\n\n    Parameters\n    ----------\n    scores : np.ndarray\n        The score matrix, where rows the individuals and the columns are the corresponds to scores on different objectives.\n    k : int\n        The number of individuals to select.\n    n_parents : int, optional\n        The number of parents to select per individual. The default is 1.\n    rng : int, np.random.Generator, optional\n        The random number generator. The default is None.\n\n    Returns\n    -------\n        A array of indices of randomly selected individuals (with replacement) of shape (k, n_parents).\n\n    \"\"\"\n    rng = np.random.default_rng(rng)\n    chosen = rng.choice(list(range(0,len(scores))), size=k*n_parents)\n    return np.reshape(chosen, (k, n_parents))\n</code></pre>"},{"location":"documentation/tpot/selectors/tournament_selection/","title":"Tournament selection","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/selectors/tournament_selection/#tpot.selectors.tournament_selection.tournament_selection","title":"<code>tournament_selection(scores, k, n_parents=1, rng=None, tournament_size=2, score_index=0)</code>","text":"<p>Select the best individual among tournsize randomly chosen individuals, k times. The returned list contains the indices of the chosen individuals.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>ndarray</code> <p>The score matrix, where rows the individuals and the columns are the corresponds to scores on different objectives.</p> required <code>k</code> <code>int</code> <p>The number of individuals to select.</p> required <code>n_parents</code> <code>int</code> <p>The number of parents to select per individual. The default is 1.</p> <code>1</code> <code>rng</code> <code>(int, Generator)</code> <p>The random number generator. The default is None.</p> <code>None</code> <code>tournament_size</code> <code>int</code> <p>The number of individuals participating in each tournament.</p> <code>2</code> <code>score_index</code> <code>(int, str)</code> <p>The index of the score to use for selection. If \"average\" is passed, the average score is used. The default is 0 (only the first score is used).</p> <code>0</code> <p>Returns:</p> Type Description <code>    A array of indices of selected individuals of shape (k, n_parents).</code> Source code in <code>tpot/selectors/tournament_selection.py</code> <pre><code>def tournament_selection(scores, k, n_parents=1, rng=None, tournament_size=2, score_index=0):\n    \"\"\"\n    Select the best individual among *tournsize* randomly chosen\n    individuals, *k* times. The returned list contains the indices of the chosen *individuals*.\n\n    Parameters\n    ----------\n    scores : np.ndarray\n        The score matrix, where rows the individuals and the columns are the corresponds to scores on different objectives.\n    k : int\n        The number of individuals to select.\n    n_parents : int, optional\n        The number of parents to select per individual. The default is 1.\n    rng : int, np.random.Generator, optional\n        The random number generator. The default is None.\n    tournament_size : int, optional\n        The number of individuals participating in each tournament.\n    score_index : int, str, optional\n        The index of the score to use for selection. If \"average\" is passed, the average score is used. The default is 0 (only the first score is used).\n\n    Returns\n    -------\n        A array of indices of selected individuals of shape (k, n_parents).\n    \"\"\"\n\n    rng = np.random.default_rng(rng)\n\n    if isinstance(score_index,int):\n        key=lambda x:x[1][score_index]\n    elif score_index == \"average\":\n        key=lambda x:np.mean(x[1])\n\n    chosen = []\n    for i in range(k*n_parents):\n        aspirants_idx =[rng.choice(len(scores)) for i in range(tournament_size)]\n        aspirants  = list(zip(aspirants_idx, scores[aspirants_idx])) # Zip indices and elements together\n        chosen.append(max(aspirants, key=key)[0]) # Retrun the index of the maximum element\n\n    return np.reshape(chosen, (k, n_parents))\n</code></pre>"},{"location":"documentation/tpot/selectors/tournament_selection_dominated/","title":"Tournament selection dominated","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/selectors/tournament_selection_dominated/#tpot.selectors.tournament_selection_dominated.tournament_selection_dominated","title":"<code>tournament_selection_dominated(scores, k, n_parents=2, rng=None)</code>","text":"<p>Select the best individual among 2 randomly chosen individuals, k times. Selection is first attempted by checking if one individual dominates the other. Otherwise one with the highest crowding distance is selected. The returned list contains the indices of the chosen individuals.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>ndarray</code> <p>The score matrix, where rows the individuals and the columns are the corresponds to scores on different objectives.</p> required <code>k</code> <code>int</code> <p>The number of individuals to select.</p> required <code>n_parents</code> <code>int</code> <p>The number of parents to select per individual. The default is 2.</p> <code>2</code> <code>rng</code> <code>(int, Generator)</code> <p>The random number generator. The default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>    A array of indices of selected individuals of shape (k, n_parents).</code> Source code in <code>tpot/selectors/tournament_selection_dominated.py</code> <pre><code>def tournament_selection_dominated(scores, k, n_parents=2, rng=None):\n    \"\"\"\n    Select the best individual among 2 randomly chosen\n    individuals, *k* times. Selection is first attempted by checking if one individual dominates the other. Otherwise one with the highest crowding distance is selected.\n    The returned list contains the indices of the chosen *individuals*.\n\n    Parameters\n    ----------\n    scores : np.ndarray\n        The score matrix, where rows the individuals and the columns are the corresponds to scores on different objectives.\n    k : int\n        The number of individuals to select.\n    n_parents : int, optional\n        The number of parents to select per individual. The default is 2.\n    rng : int, np.random.Generator, optional\n        The random number generator. The default is None.\n\n    Returns\n    -------\n        A array of indices of selected individuals of shape (k, n_parents).\n\n    \"\"\"\n\n    rng = np.random.default_rng(rng)\n    pareto_fronts = nondominated_sorting(scores)\n\n    # chosen = list(itertools.chain.from_iterable(fronts))\n    # if len(chosen) &gt;= k:\n    #     return chosen[0:k]\n\n    crowding_dict = {}\n    chosen = []\n    current_front_number = 0\n    while current_front_number &lt; len(pareto_fronts):\n\n        current_front = np.array(list(pareto_fronts[current_front_number]))\n        front_scores = [scores[i] for i in current_front]\n        crowding_distances = crowding_distance(front_scores)\n        for i, crowding in zip(current_front,crowding_distances):\n            crowding_dict[i] = crowding\n\n        current_front_number += 1\n\n\n    chosen = []\n    for i in range(k*n_parents):\n        asp1 = rng.choice(len(scores))\n        asp2 = rng.choice(len(scores))\n\n        if dominates(scores[asp1], scores[asp2]):\n            chosen.append(asp1)\n        elif dominates(scores[asp2], scores[asp1]):\n            chosen.append(asp2)\n\n        elif crowding_dict[asp1] &gt; crowding_dict[asp2]:\n            chosen.append(asp1)\n        elif crowding_dict[asp1] &lt; crowding_dict[asp2]:\n            chosen.append(asp2)\n\n        else:\n            chosen.append(rng.choice([asp1,asp2]))\n\n    return np.reshape(chosen, (k, n_parents))\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/cross_val_utils/","title":"Cross val utils","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/tpot_estimator/cross_val_utils/#tpot.tpot_estimator.cross_val_utils.cross_val_score_objective","title":"<code>cross_val_score_objective(estimator, X, y, scorers, cv, fold=None)</code>","text":"<p>Compute the cross validated scores for a estimator. Only fits the estimator once per fold, and loops over the scorers to evaluate the estimator.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <p>The estimator to fit and score.</p> required <code>X</code> <p>The feature matrix.</p> required <code>y</code> <p>The target vector.</p> required <code>scorers</code> <p>The scorers to use.  If a list, will loop over the scorers and return a list of scorers. If a single scorer, will return a single score.</p> required <code>cv</code> <p>The cross-validator to use. For example, sklearn.model_selection.KFold or sklearn.model_selection.StratifiedKFold.</p> required <code>fold</code> <p>The fold to return the scores for. If None, will return the mean of all the scores (per scorer). Default is None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>scores</code> <code>ndarray or float</code> <p>The scores for the estimator per scorer. If fold is None, will return the mean of all the scores (per scorer). Returns a list if multiple scorers are used, otherwise returns a float for the single scorer.</p> Source code in <code>tpot/tpot_estimator/cross_val_utils.py</code> <pre><code>def cross_val_score_objective(estimator, X, y, scorers, cv, fold=None):\n    \"\"\"\n    Compute the cross validated scores for a estimator. Only fits the estimator once per fold, and loops over the scorers to evaluate the estimator.\n\n    Parameters\n    ----------\n    estimator: sklearn.base.BaseEstimator\n        The estimator to fit and score.\n    X: np.ndarray or pd.DataFrame\n        The feature matrix.\n    y: np.ndarray or pd.Series\n        The target vector.\n    scorers: list or scorer\n        The scorers to use. \n        If a list, will loop over the scorers and return a list of scorers.\n        If a single scorer, will return a single score.\n    cv: sklearn cross-validator\n        The cross-validator to use. For example, sklearn.model_selection.KFold or sklearn.model_selection.StratifiedKFold.\n    fold: int, optional\n        The fold to return the scores for. If None, will return the mean of all the scores (per scorer). Default is None.\n\n    Returns\n    -------\n    scores: np.ndarray or float\n        The scores for the estimator per scorer. If fold is None, will return the mean of all the scores (per scorer).\n        Returns a list if multiple scorers are used, otherwise returns a float for the single scorer.\n\n    \"\"\"\n\n    #check if scores is not iterable\n    if not isinstance(scorers, Iterable): \n        scorers = [scorers]\n    scores = []\n    if fold is None:\n        for train_index, test_index in cv.split(X, y):\n            this_fold_estimator = sklearn.base.clone(estimator)\n            if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n            else:\n                X_train, X_test = X[train_index], X[test_index]\n\n            if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n            else:\n                y_train, y_test = y[train_index], y[test_index]\n\n\n            start = time.time()\n            this_fold_estimator.fit(X_train,y_train)\n            duration = time.time() - start\n\n            this_fold_scores = [sklearn.metrics.get_scorer(scorer)(this_fold_estimator, X_test, y_test) for scorer in scorers] \n            scores.append(this_fold_scores)\n            del this_fold_estimator\n            del X_train\n            del X_test\n            del y_train\n            del y_test\n\n\n        return np.mean(scores,0)\n    else:\n        this_fold_estimator = sklearn.base.clone(estimator)\n        train_index, test_index = list(cv.split(X, y))[fold]\n        if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        else:\n            X_train, X_test = X[train_index], X[test_index]\n\n        if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n        else:\n            y_train, y_test = y[train_index], y[test_index]\n\n        start = time.time()\n        this_fold_estimator.fit(X_train,y_train)\n        duration = time.time() - start\n        this_fold_scores = [sklearn.metrics.get_scorer(scorer)(this_fold_estimator, X_test, y_test) for scorer in scorers] \n        return this_fold_scores\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/estimator/","title":"Estimator","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/tpot_estimator/estimator/#tpot.tpot_estimator.estimator.TPOTEstimator","title":"<code>TPOTEstimator</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> Source code in <code>tpot/tpot_estimator/estimator.py</code> <pre><code>class TPOTEstimator(BaseEstimator):\n    def __init__(self,  \n                        search_space,\n                        scorers,\n                        scorers_weights,\n                        classification,\n                        cv = 10,\n                        other_objective_functions=[],\n                        other_objective_functions_weights = [],\n                        objective_function_names = None,\n                        bigger_is_better = True,\n\n                        export_graphpipeline = False,\n                        memory = None,\n\n                        categorical_features = None,\n                        preprocessing = False,\n                        population_size = 50,\n                        initial_population_size = None,\n                        population_scaling = .5,\n                        generations_until_end_population = 1,\n                        generations = None,\n                        max_time_mins=60,\n                        max_eval_time_mins=10,\n                        validation_strategy = \"none\",\n                        validation_fraction = .2,\n                        disable_label_encoder = False,\n\n                        #early stopping parameters\n                        early_stop = None,\n                        scorers_early_stop_tol = 0.001,\n                        other_objectives_early_stop_tol =None,\n                        threshold_evaluation_pruning = None,\n                        threshold_evaluation_scaling = .5,\n                        selection_evaluation_pruning = None,\n                        selection_evaluation_scaling = .5,\n                        min_history_threshold = 20,\n\n                        #evolver parameters\n                        survival_percentage = 1,\n                        crossover_probability=.2,\n                        mutate_probability=.7,\n                        mutate_then_crossover_probability=.05,\n                        crossover_then_mutate_probability=.05,\n                        survival_selector = survival_select_NSGA2,\n                        parent_selector = tournament_selection_dominated,\n\n                        #budget parameters\n                        budget_range = None,\n                        budget_scaling = .5,\n                        generations_until_end_budget = 1,\n                        stepwise_steps = 5,\n\n                        #dask parameters\n                        n_jobs=1,\n                        memory_limit = None,\n                        client = None,\n                        processes = True,\n\n                        #debugging and logging parameters\n                        warm_start = False,\n                        periodic_checkpoint_folder = None,\n                        callback = None,\n\n                        verbose = 0,\n                        scatter = True,\n\n                         # random seed for random number generator (rng)\n                        random_state = None,\n\n                        ):\n\n        '''\n        An sklearn baseestimator that uses genetic programming to optimize a pipeline.\n\n        Parameters\n        ----------\n        search_space : (String, tpot.search_spaces.SearchSpace)\n            - String : The default search space to use for the optimization.\n            | String     | Description      |\n            | :---        |    :----:   |\n            | linear  | A linear pipeline with the structure of \"Selector-&gt;(transformers+Passthrough)-&gt;(classifiers/regressors+Passthrough)-&gt;final classifier/regressor.\" For both the transformer and inner estimator layers, TPOT may choose one or more transformers/classifiers, or it may choose none. The inner classifier/regressor layer is optional. |\n            | linear-light | Same search space as linear, but without the inner classifier/regressor layer and with a reduced set of faster running estimators. |\n            | graph | TPOT will optimize a pipeline in the shape of a directed acyclic graph. The nodes of the graph can include selectors, scalers, transformers, or classifiers/regressors (inner classifiers/regressors can optionally be not included). This will return a custom GraphPipeline rather than an sklearn Pipeline. More details in Tutorial 6. |\n            | graph-light | Same as graph search space, but without the inner classifier/regressors and with a reduced set of faster running estimators. |\n            | mdr |TPOT will search over a series of feature selectors and Multifactor Dimensionality Reduction models to find a series of operators that maximize prediction accuracy. The TPOT MDR configuration is specialized for genome-wide association studies (GWAS), and is described in detail online here.\n\n            Note that TPOT MDR may be slow to run because the feature selection routines are computationally expensive, especially on large datasets. |\n\n\n            - SearchSpace : The search space to use for the optimization. This should be an instance of a SearchSpace.\n                The search space to use for the optimization. This should be an instance of a SearchSpace.\n                TPOT has groups of search spaces found in the following folders, tpot.search_spaces.nodes for the nodes in the pipeline and tpot.search_spaces.pipelines for the pipeline structure.\n\n        scorers : (list, scorer)\n            A scorer or list of scorers to be used in the cross-validation process.\n            see https://scikit-learn.org/stable/modules/model_evaluation.html\n\n        scorers_weights : list\n            A list of weights to be applied to the scorers during the optimization process.\n\n        classification : bool\n            If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem.\n            Used to determine the CV strategy.\n\n        cv : int, cross-validator\n            - (int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.\n            - (sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.\n                - max_depth (int): The maximum depth from any node to the root of the pipelines to be generated.\n\n        other_objective_functions : list, default=[]\n            A list of other objective functions to apply to the pipeline. The function takes a single parameter for the graphpipeline estimator and returns either a single score or a list of scores.\n\n        other_objective_functions_weights : list, default=[]\n            A list of weights to be applied to the other objective functions.\n\n        objective_function_names : list, default=None\n            A list of names to be applied to the objective functions. If None, will use the names of the objective functions.\n\n        bigger_is_better : bool, default=True\n            If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n\n        memory: Memory object or string, default=None\n            If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. This feature\n            is used to avoid computing the fit transformers within a pipeline if the parameters\n            and input data are identical with another fitted pipeline during optimization process.\n            - String 'auto':\n                TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.\n            - String path of a caching directory\n                TPOT uses memory caching with the provided directory and TPOT does NOT clean\n                the caching directory up upon shutdown. If the directory does not exist, TPOT will\n                create it.\n            - Memory object:\n                TPOT uses the instance of joblib.Memory for memory caching,\n                and TPOT does NOT clean the caching directory up upon shutdown.\n            - None:\n                TPOT does not use memory caching.              \n\n        categorical_features: list or None\n            Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n            - None : If None, TPOT will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing.\n            - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices\n\n        preprocessing : bool or BaseEstimator/Pipeline,\n            EXPERIMENTAL - will be changed in future versions\n            A pipeline that will be used to preprocess the data before CV. Note that the parameters for these steps are not optimized. Add them to the search space to be optimized.\n            - bool : If True, will use a default preprocessing pipeline which includes imputation followed by one hot encoding.\n            - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.\n\n        population_size : int, default=50\n            Size of the population\n\n        initial_population_size : int, default=None\n            Size of the initial population. If None, population_size will be used.\n\n        population_scaling : int, default=0.5\n            Scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n\n        generations_until_end_population : int, default=1\n            Number of generations until the population size reaches population_size\n\n        generations : int, default=50\n            Number of generations to run\n\n        max_time_mins : float, default=float(\"inf\")\n            Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n\n        max_eval_time_mins : float, default=5\n            Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n\n        validation_strategy : str, default='none'\n            EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT may overfit the cross validation score. A second validation set can be used to select the final pipeline.\n            - 'auto' : Automatically determine the validation strategy based on the dataset shape.\n            - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets.\n            - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets.\n            - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.\n\n        validation_fraction : float, default=0.2\n          EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.\n\n        disable_label_encoder : bool, default=False\n            If True, TPOT will check if the target needs to be relabeled to be sequential ints from 0 to N. This is necessary for XGBoost compatibility. If the labels need to be encoded, TPOT will use sklearn.preprocessing.LabelEncoder to encode the labels. The encoder can be accessed via the self.label_encoder_ attribute.\n            If False, no additional label encoders will be used.\n\n        early_stop : int, default=None\n            Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered. In general a value of around 5-20 is good.\n\n        scorers_early_stop_tol :\n            -list of floats\n                list of tolerances for each scorer. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n                If an index of the list is None, that item will not be used for early stopping\n            -int\n                If an int is given, it will be used as the tolerance for all objectives\n\n        other_objectives_early_stop_tol :\n            -list of floats\n                list of tolerances for each of the other objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n                If an index of the list is None, that item will not be used for early stopping\n            -int\n                If an int is given, it will be used as the tolerance for all objectives\n\n        threshold_evaluation_pruning : list [start, end], default=None\n            starting and ending percentile to use as a threshold for the evaluation early stopping.\n            Values between 0 and 100.\n\n        threshold_evaluation_scaling : float [0,inf), default=0.5\n            A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n            Must be greater than zero. Higher numbers will move the threshold to the end faster.\n\n        selection_evaluation_pruning : list, default=None\n            A lower and upper percent of the population size to select each round of CV.\n            Values between 0 and 1.\n\n        selection_evaluation_scaling : float, default=0.5\n            A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n            Must be greater than zero. Higher numbers will move the threshold to the end faster.\n\n        min_history_threshold : int, default=0\n            The minimum number of previous scores needed before using threshold early stopping.\n\n        survival_percentage : float, default=1\n            Percentage of the population size to utilize for mutation and crossover at the beginning of the generation. The rest are discarded. Individuals are selected with the selector passed into survival_selector. The value of this parameter must be between 0 and 1, inclusive.\n            For example, if the population size is 100 and the survival percentage is .5, 50 individuals will be selected with NSGA2 from the existing population. These will be used for mutation and crossover to generate the next 100 individuals for the next generation. The remainder are discarded from the live population. In the next generation, there will now be the 50 parents + the 100 individuals for a total of 150. Surivival percentage is based of the population size parameter and not the existing population size (current population size when using successive halving). Therefore, in the next generation we will still select 50 individuals from the currently existing 150.\n\n        crossover_probability : float, default=.2\n            Probability of generating a new individual by crossover between two individuals.\n\n        mutate_probability : float, default=.7\n            Probability of generating a new individual by crossover between one individuals.\n\n        mutate_then_crossover_probability : float, default=.05\n            Probability of generating a new individual by mutating two individuals followed by crossover.\n\n        crossover_then_mutate_probability : float, default=.05\n            Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.\n\n        survival_selector : function, default=survival_select_NSGA2\n            Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes.\n            Used to selected population_size * survival_percentage individuals at the start of each generation to use for mutation and crossover.\n\n        parent_selector : function, default=parent_select_NSGA2\n            Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.\n\n        budget_range : list [start, end], default=None\n            A starting and ending budget to use for the budget scaling.\n\n        budget_scaling float : [0,1], default=0.5\n            A scaling factor to use when determining how fast we move the budget from the start to end budget.\n\n        generations_until_end_budget : int, default=1\n            The number of generations to run before reaching the max budget.\n\n        stepwise_steps : int, default=1\n            The number of staircase steps to take when scaling the budget and population size.\n\n        n_jobs : int, default=1\n            Number of processes to run in parallel.\n\n        memory_limit : str, default=None\n            Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n\n        client : dask.distributed.Client, default=None\n            A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.\n\n        processes : bool, default=True\n            If True, will use multiprocessing to parallelize the optimization process. If False, will use threading.\n            True seems to perform better. However, False is required for interactive debugging.\n\n        warm_start : bool, default=False\n            If True, will use the continue the evolutionary algorithm from the last generation of the previous run.\n\n        periodic_checkpoint_folder : str, default=None\n            Folder to save the population to periodically. If None, no periodic saving will be done.\n            If provided, training will resume from this checkpoint.\n\n        callback : tpot.CallBackInterface, default=None\n            Callback object. Not implemented\n\n        verbose : int, default=1\n            How much information to print during the optimization process. Higher values include the information from lower values.\n            0. nothing\n            1. progress bar\n\n            3. best individual\n            4. warnings\n            &gt;=5. full warnings trace\n            6. evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_mins in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)\n\n        scatter : bool, default=True\n            If True, will scatter the data to the dask workers. If False, will not scatter the data. This can be useful for debugging.\n\n        random_state : int, None, default=None\n            A seed for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes\n\n            - int\n                Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'\n            - None\n                Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS\n\n        Attributes\n        ----------\n\n        fitted_pipeline_ : GraphPipeline\n            A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.\n\n        evaluated_individuals : A pandas data frame containing data for all evaluated individuals in the run.\n            Columns:\n            - *objective functions : The first few columns correspond to the passed in scorers and objective functions\n            - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population.\n            - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population.\n            - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator.\n            - Generation : The generation the pipeline first appeared.\n            - Pareto_Front\t: The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.\n                            To save on computational time, the best frontier is updated iteratively each generation.\n                            The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.\n                            All other pipelines are set to NaN.\n            - Instance\t: The unfitted GraphPipeline BaseEstimator.\n            - *validation objective functions : Objective function scores evaluated on the validation set.\n            - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.\n\n        pareto_front : The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.\n        '''\n\n        # sklearn BaseEstimator must have a corresponding attribute for each parameter.\n        # These should not be modified once set.\n\n        self.scorers = scorers\n        self.scorers_weights = scorers_weights\n        self.classification = classification\n        self.cv = cv\n        self.other_objective_functions = other_objective_functions\n        self.other_objective_functions_weights = other_objective_functions_weights\n        self.objective_function_names = objective_function_names\n        self.bigger_is_better = bigger_is_better\n\n        self.search_space = search_space\n\n        self.export_graphpipeline = export_graphpipeline\n        self.memory = memory\n\n        self.categorical_features = categorical_features\n\n        self.preprocessing = preprocessing\n        self.validation_strategy = validation_strategy\n        self.validation_fraction = validation_fraction\n        self.disable_label_encoder = disable_label_encoder\n        self.population_size = population_size\n        self.initial_population_size = initial_population_size\n        self.population_scaling = population_scaling\n        self.generations_until_end_population = generations_until_end_population\n        self.generations = generations\n        self.early_stop = early_stop\n        self.scorers_early_stop_tol = scorers_early_stop_tol\n        self.other_objectives_early_stop_tol = other_objectives_early_stop_tol\n        self.max_time_mins = max_time_mins\n        self.max_eval_time_mins = max_eval_time_mins\n        self.n_jobs= n_jobs\n        self.memory_limit = memory_limit\n        self.client = client\n        self.survival_percentage = survival_percentage\n        self.crossover_probability = crossover_probability\n        self.mutate_probability = mutate_probability\n        self.mutate_then_crossover_probability= mutate_then_crossover_probability\n        self.crossover_then_mutate_probability= crossover_then_mutate_probability\n        self.survival_selector=survival_selector\n        self.parent_selector=parent_selector\n        self.budget_range = budget_range\n        self.budget_scaling = budget_scaling\n        self.generations_until_end_budget = generations_until_end_budget\n        self.stepwise_steps = stepwise_steps\n        self.threshold_evaluation_pruning =threshold_evaluation_pruning\n        self.threshold_evaluation_scaling =  threshold_evaluation_scaling\n        self.min_history_threshold = min_history_threshold\n        self.selection_evaluation_pruning = selection_evaluation_pruning\n        self.selection_evaluation_scaling =  selection_evaluation_scaling\n        self.warm_start = warm_start\n        self.verbose = verbose\n        self.periodic_checkpoint_folder = periodic_checkpoint_folder\n        self.callback = callback\n        self.processes = processes\n\n\n        self.scatter = scatter\n\n\n        timer_set = self.max_time_mins != float(\"inf\") and self.max_time_mins is not None\n        if self.generations is not None and timer_set:\n            warnings.warn(\"Both generations and max_time_mins are set. TPOT will terminate when the first condition is met.\")\n\n        # create random number generator based on rngseed\n        self.rng = np.random.default_rng(random_state)\n        # save random state passed to us for other functions that use random_state\n        self.random_state = random_state\n\n        #Initialize other used params\n\n\n        if self.initial_population_size is None:\n            self._initial_population_size = self.population_size\n        else:\n            self._initial_population_size = self.initial_population_size\n\n        if isinstance(self.scorers, str):\n            self._scorers = [self.scorers]\n\n        elif callable(self.scorers):\n            self._scorers = [self.scorers]\n        else:\n            self._scorers = self.scorers\n\n        self._scorers = [sklearn.metrics.get_scorer(scoring) for scoring in self._scorers]\n        self._scorers_early_stop_tol = self.scorers_early_stop_tol\n\n        self._evolver = tpot.evolvers.BaseEvolver\n\n        self.objective_function_weights = [*scorers_weights, *other_objective_functions_weights]\n\n\n        if self.objective_function_names is None:\n            obj_names = [f.__name__ for f in other_objective_functions]\n        else:\n            obj_names = self.objective_function_names\n        self.objective_names = [f._score_func.__name__ if hasattr(f,\"_score_func\") else f.__name__ for f in self._scorers] + obj_names\n\n\n        if not isinstance(self.other_objectives_early_stop_tol, list):\n            self._other_objectives_early_stop_tol = [self.other_objectives_early_stop_tol for _ in range(len(self.other_objective_functions))]\n        else:\n            self._other_objectives_early_stop_tol = self.other_objectives_early_stop_tol\n\n        if not isinstance(self._scorers_early_stop_tol, list):\n            self._scorers_early_stop_tol = [self._scorers_early_stop_tol for _ in range(len(self._scorers))]\n        else:\n            self._scorers_early_stop_tol = self._scorers_early_stop_tol\n\n        self.early_stop_tol = [*self._scorers_early_stop_tol, *self._other_objectives_early_stop_tol]\n\n        self._evolver_instance = None\n        self.evaluated_individuals = None\n\n\n        self.label_encoder_ = None\n\n\n        set_dask_settings()\n\n\n    def fit(self, X, y):\n        if self.client is not None: #If user passed in a client manually\n           _client = self.client\n        else:\n\n            if self.verbose &gt;= 4:\n                silence_logs = 30\n            elif self.verbose &gt;=5:\n                silence_logs = 40\n            else:\n                silence_logs = 50\n            cluster = LocalCluster(n_workers=self.n_jobs, #if no client is passed in and no global client exists, create our own\n                    threads_per_worker=1,\n                    processes=self.processes,\n                    silence_logs=silence_logs,\n                    memory_limit=self.memory_limit)\n            _client = Client(cluster)\n\n        if self.classification and not self.disable_label_encoder and not check_if_y_is_encoded(y):\n            warnings.warn(\"Labels are not encoded as ints from 0 to N. For compatibility with some classifiers such as sklearn, TPOT has encoded y with the sklearn LabelEncoder. When using pipelines outside the main TPOT estimator class, you can encode the labels with est.label_encoder_\")\n            self.label_encoder_ = LabelEncoder()\n            y = self.label_encoder_.fit_transform(y)\n\n        self.evaluated_individuals = None\n        #determine validation strategy\n        if self.validation_strategy == 'auto':\n            nrows = X.shape[0]\n            ncols = X.shape[1]\n\n            if nrows/ncols &lt; 20:\n                validation_strategy = 'reshuffled'\n            elif nrows/ncols &lt; 100:\n                validation_strategy = 'split'\n            else:\n                validation_strategy = 'none'\n        else:\n            validation_strategy = self.validation_strategy\n\n        if validation_strategy == 'split':\n            if self.classification:\n                X, X_val, y, y_val = train_test_split(X, y, test_size=self.validation_fraction, stratify=y, random_state=self.random_state)\n            else:\n                X, X_val, y, y_val = train_test_split(X, y, test_size=self.validation_fraction, random_state=self.random_state)\n\n\n        X_original = X\n        y_original = y\n        if isinstance(self.cv, int) or isinstance(self.cv, float):\n            n_folds = self.cv\n        else:\n            n_folds = self.cv.get_n_splits(X, y)\n\n        if self.classification:\n            X, y = remove_underrepresented_classes(X, y, n_folds)\n\n        if self.preprocessing:\n            #X = pd.DataFrame(X)\n\n            if not isinstance(self.preprocessing, bool) and isinstance(self.preprocessing, sklearn.base.BaseEstimator):\n                self._preprocessing_pipeline = sklearn.base.clone(self.preprocessing)\n\n            #TODO: check if there are missing values in X before imputation. If not, don't include imputation in pipeline. Check if there are categorical columns. If not, don't include one hot encoding in pipeline\n            else: #if self.preprocessing is True or not a sklearn estimator\n\n                pipeline_steps = []\n\n                if self.categorical_features is not None: #if categorical features are specified, use those\n                    pipeline_steps.append((\"impute_categorical\", tpot.builtin_modules.ColumnSimpleImputer(self.categorical_features, strategy='most_frequent')))\n                    pipeline_steps.append((\"impute_numeric\", tpot.builtin_modules.ColumnSimpleImputer(\"numeric\", strategy='mean')))\n                    pipeline_steps.append((\"ColumnOneHotEncoder\", tpot.builtin_modules.ColumnOneHotEncoder(self.categorical_features, min_frequency=0.0001))) # retain wrong param fix\n\n                else:\n                    if isinstance(X, pd.DataFrame):\n                        categorical_columns = X.select_dtypes(include=['object']).columns\n                        if len(categorical_columns) &gt; 0:\n                            pipeline_steps.append((\"impute_categorical\", tpot.builtin_modules.ColumnSimpleImputer(\"categorical\", strategy='most_frequent')))\n                            pipeline_steps.append((\"impute_numeric\", tpot.builtin_modules.ColumnSimpleImputer(\"numeric\", strategy='mean')))\n                            pipeline_steps.append((\"ColumnOneHotEncoder\", tpot.builtin_modules.ColumnOneHotEncoder(\"categorical\", min_frequency=0.0001))) # retain wrong param fix\n                        else:\n                            pipeline_steps.append((\"impute_numeric\", tpot.builtin_modules.ColumnSimpleImputer(\"all\", strategy='mean')))\n                    else:\n                        pipeline_steps.append((\"impute_numeric\", tpot.builtin_modules.ColumnSimpleImputer(\"all\", strategy='mean')))\n\n                self._preprocessing_pipeline = sklearn.pipeline.Pipeline(pipeline_steps)\n\n            X = self._preprocessing_pipeline.fit_transform(X, y)\n\n        else:\n            self._preprocessing_pipeline = None\n\n        #_, y = sklearn.utils.check_X_y(X, y, y_numeric=True)\n\n        #Set up the configuation dictionaries and the search spaces\n\n        #check if self.cv is a number\n        if isinstance(self.cv, int) or isinstance(self.cv, float):\n            if self.classification:\n                self.cv_gen = sklearn.model_selection.StratifiedKFold(n_splits=self.cv, shuffle=True, random_state=self.random_state)\n            else:\n                self.cv_gen = sklearn.model_selection.KFold(n_splits=self.cv, shuffle=True, random_state=self.random_state)\n\n        else:\n            self.cv_gen = sklearn.model_selection.check_cv(self.cv, y, classifier=self.classification)\n\n\n\n        n_samples= int(math.floor(X.shape[0]/n_folds))\n        n_features=X.shape[1]\n\n        if isinstance(X, pd.DataFrame):\n            self.feature_names = X.columns\n        else:\n            self.feature_names = None\n\n\n\n        def objective_function(pipeline_individual,\n                                            X,\n                                            y,\n                                            is_classification=self.classification,\n                                            scorers= self._scorers,\n                                            cv=self.cv_gen,\n                                            other_objective_functions=self.other_objective_functions,\n                                            export_graphpipeline=self.export_graphpipeline,\n                                            memory=self.memory,\n                                            **kwargs):\n            return objective_function_generator(\n                pipeline_individual,\n                X,\n                y,\n                is_classification=is_classification,\n                scorers= scorers,\n                cv=cv,\n                other_objective_functions=other_objective_functions,\n                export_graphpipeline=export_graphpipeline,\n                memory=memory,\n                **kwargs,\n            )\n\n\n\n        if self.threshold_evaluation_pruning is not None or self.selection_evaluation_pruning is not None:\n            evaluation_early_stop_steps = self.cv\n        else:\n            evaluation_early_stop_steps = None\n\n        if self.scatter:\n            X_future = _client.scatter(X)\n            y_future = _client.scatter(y)\n        else:\n            X_future = X\n            y_future = y\n\n        if self.classification:\n            n_classes = len(np.unique(y))\n        else:\n            n_classes = None\n\n        get_search_space_params = {\"n_classes\": n_classes, \n                        \"n_samples\":len(y), \n                        \"n_features\":X.shape[1], \n                        \"random_state\":self.random_state}\n\n        self._search_space = get_template_search_spaces(self.search_space, classification=self.classification, inner_predictors=True, **get_search_space_params)\n\n\n        # TODO : Add check for empty values in X and if so, add imputation to the search space\n        # make this depend on self.preprocessing\n        # if check_empty_values(X):\n        #     from sklearn.experimental import enable_iterative_imputer\n\n        #     from ConfigSpace import ConfigurationSpace\n        #     from ConfigSpace import ConfigurationSpace, Integer, Float, Categorical, Normal\n        #     iterative_imputer_cs = ConfigurationSpace(\n        #         space = {\n        #             'n_nearest_features' : Categorical('n_nearest_features', [100]),\n        #             'initial_strategy' : Categorical('initial_strategy', ['mean','median', 'most_frequent', ]),\n        #             'add_indicator' : Categorical('add_indicator', [True, False]),\n        #         }\n        #     )\n\n        #     imputation_search = tpot.search_spaces.pipelines.ChoicePipeline([\n        #         tpot.config.get_search_space(\"SimpleImputer\"),\n        #         tpot.search_spaces.nodes.EstimatorNode(sklearn.impute.IterativeImputer, iterative_imputer_cs)\n        #     ])\n\n\n\n\n        #     self.search_space_final = tpot.search_spaces.pipelines.SequentialPipeline(search_spaces=[ imputation_search, self._search_space], memory=\"sklearn_pipeline_memory\")\n        # else:\n        #     self.search_space_final = self._search_space\n\n        self.search_space_final = self._search_space\n\n        def ind_generator(rng):\n            rng = np.random.default_rng(rng)\n            while True:\n                yield self.search_space_final.generate(rng)\n\n        #If warm start and we have an evolver instance, use the existing one\n        if not(self.warm_start and self._evolver_instance is not None):\n            self._evolver_instance = self._evolver(   individual_generator=ind_generator(self.rng),\n                                            objective_functions= [objective_function],\n                                            objective_function_weights = self.objective_function_weights,\n                                            objective_names=self.objective_names,\n                                            bigger_is_better = self.bigger_is_better,\n                                            population_size= self.population_size,\n                                            generations=self.generations,\n                                            initial_population_size = self._initial_population_size,\n                                            n_jobs=self.n_jobs,\n                                            verbose = self.verbose,\n                                            max_time_mins =      self.max_time_mins ,\n                                            max_eval_time_mins = self.max_eval_time_mins,\n\n                                            periodic_checkpoint_folder = self.periodic_checkpoint_folder,\n                                            threshold_evaluation_pruning = self.threshold_evaluation_pruning,\n                                            threshold_evaluation_scaling =  self.threshold_evaluation_scaling,\n                                            min_history_threshold = self.min_history_threshold,\n\n                                            selection_evaluation_pruning = self.selection_evaluation_pruning,\n                                            selection_evaluation_scaling =  self.selection_evaluation_scaling,\n                                            evaluation_early_stop_steps = evaluation_early_stop_steps,\n\n                                            early_stop_tol = self.early_stop_tol,\n                                            early_stop= self.early_stop,\n\n                                            budget_range = self.budget_range,\n                                            budget_scaling = self.budget_scaling,\n                                            generations_until_end_budget = self.generations_until_end_budget,\n\n                                            population_scaling = self.population_scaling,\n                                            generations_until_end_population = self.generations_until_end_population,\n                                            stepwise_steps = self.stepwise_steps,\n                                            client = _client,\n                                            objective_kwargs = {\"X\": X_future, \"y\": y_future},\n                                            survival_selector=self.survival_selector,\n                                            parent_selector=self.parent_selector,\n                                            survival_percentage = self.survival_percentage,\n                                            crossover_probability = self.crossover_probability,\n                                            mutate_probability = self.mutate_probability,\n                                            mutate_then_crossover_probability= self.mutate_then_crossover_probability,\n                                            crossover_then_mutate_probability= self.crossover_then_mutate_probability,\n\n                                            rng=self.rng,\n                                            )\n\n\n        self._evolver_instance.optimize()\n        #self._evolver_instance.population.update_pareto_fronts(self.objective_names, self.objective_function_weights)\n        self.make_evaluated_individuals()\n\n\n\n\n        tpot.utils.get_pareto_frontier(self.evaluated_individuals, column_names=self.objective_names, weights=self.objective_function_weights)\n\n        if validation_strategy == 'reshuffled':\n            best_pareto_front_idx = list(self.pareto_front.index)\n            best_pareto_front = list(self.pareto_front.loc[best_pareto_front_idx]['Individual'])\n\n            #reshuffle rows\n            X, y = sklearn.utils.shuffle(X, y, random_state=self.random_state)\n\n            if self.scatter:\n                X_future = _client.scatter(X)\n                y_future = _client.scatter(y)\n            else:\n                X_future = X\n                y_future = y\n\n            val_objective_function_list = [lambda   ind,\n                                                    X,\n                                                    y,\n                                                    is_classification=self.classification,\n                                                    scorers= self._scorers,\n                                                    cv=self.cv_gen,\n                                                    other_objective_functions=self.other_objective_functions,\n                                                    export_graphpipeline=self.export_graphpipeline,\n                                                    memory=self.memory,\n                                                    **kwargs: objective_function_generator(\n                                                                                                ind,\n                                                                                                X,\n                                                                                                y,\n                                                                                                is_classification=is_classification,\n                                                                                                scorers= scorers,\n                                                                                                cv=cv,\n                                                                                                other_objective_functions=other_objective_functions,\n                                                                                                export_graphpipeline=export_graphpipeline,\n                                                                                                memory=memory,\n                                                                                                **kwargs,\n                                                                                                )]\n\n            objective_kwargs = {\"X\": X_future, \"y\": y_future}\n            val_scores, start_times, end_times, eval_errors = tpot.utils.eval_utils.parallel_eval_objective_list(best_pareto_front, val_objective_function_list, verbose=self.verbose, max_eval_time_mins=self.max_eval_time_mins, n_expected_columns=len(self.objective_names), client=_client, **objective_kwargs)\n\n\n\n            val_objective_names = ['validation_'+name for name in self.objective_names]\n            self.objective_names_for_selection = val_objective_names\n            self.evaluated_individuals.loc[best_pareto_front_idx,val_objective_names] = val_scores\n            self.evaluated_individuals.loc[best_pareto_front_idx,'validation_start_times'] = start_times\n            self.evaluated_individuals.loc[best_pareto_front_idx,'validation_end_times'] = end_times\n            self.evaluated_individuals.loc[best_pareto_front_idx,'validation_eval_errors'] = eval_errors\n\n            self.evaluated_individuals[\"Validation_Pareto_Front\"] = tpot.utils.get_pareto_frontier(self.evaluated_individuals, column_names=val_objective_names, weights=self.objective_function_weights)\n\n\n        elif validation_strategy == 'split':\n\n\n            if self.scatter:\n                X_future = _client.scatter(X)\n                y_future = _client.scatter(y)\n                X_val_future = _client.scatter(X_val)\n                y_val_future = _client.scatter(y_val)\n            else:\n                X_future = X\n                y_future = y\n                X_val_future = X_val\n                y_val_future = y_val\n\n            objective_kwargs = {\"X\": X_future, \"y\": y_future, \"X_val\" : X_val_future, \"y_val\":y_val_future }\n\n            best_pareto_front_idx = list(self.pareto_front.index)\n            best_pareto_front = list(self.pareto_front.loc[best_pareto_front_idx]['Individual'])\n            val_objective_function_list = [lambda   ind,\n                                                    X,\n                                                    y,\n                                                    X_val,\n                                                    y_val,\n                                                    scorers= self._scorers,\n                                                    other_objective_functions=self.other_objective_functions,\n                                                    export_graphpipeline=self.export_graphpipeline,\n                                                    memory=self.memory,\n                                                    **kwargs: val_objective_function_generator(\n                                                        ind,\n                                                        X,\n                                                        y,\n                                                        X_val,\n                                                        y_val,\n                                                        scorers= scorers,\n                                                        other_objective_functions=other_objective_functions,\n                                                        export_graphpipeline=export_graphpipeline,\n                                                        memory=memory,\n                                                        **kwargs,\n                                                        )]\n\n            val_scores, start_times, end_times, eval_errors = tpot.utils.eval_utils.parallel_eval_objective_list(best_pareto_front, val_objective_function_list, verbose=self.verbose, max_eval_time_mins=self.max_eval_time_mins, n_expected_columns=len(self.objective_names), client=_client, **objective_kwargs)\n\n\n\n            val_objective_names = ['validation_'+name for name in self.objective_names]\n            self.objective_names_for_selection = val_objective_names\n            self.evaluated_individuals.loc[best_pareto_front_idx,val_objective_names] = val_scores\n            self.evaluated_individuals.loc[best_pareto_front_idx,'validation_start_times'] = start_times\n            self.evaluated_individuals.loc[best_pareto_front_idx,'validation_end_times'] = end_times\n            self.evaluated_individuals.loc[best_pareto_front_idx,'validation_eval_errors'] = eval_errors\n\n            self.evaluated_individuals[\"Validation_Pareto_Front\"] = tpot.utils.get_pareto_frontier(self.evaluated_individuals, column_names=val_objective_names, weights=self.objective_function_weights)\n\n        else:\n            self.objective_names_for_selection = self.objective_names\n\n        val_scores = self.evaluated_individuals[self.evaluated_individuals[self.objective_names_for_selection].isna().all(1).ne(True)][self.objective_names_for_selection]\n        weighted_scores = val_scores*self.objective_function_weights\n\n        if self.bigger_is_better:\n            best_indices = list(weighted_scores.sort_values(by=self.objective_names_for_selection, ascending=False).index)\n        else:\n            best_indices = list(weighted_scores.sort_values(by=self.objective_names_for_selection, ascending=True).index)\n\n        for best_idx in best_indices:\n\n            best_individual = self.evaluated_individuals.loc[best_idx]['Individual']\n            self.selected_best_score =  self.evaluated_individuals.loc[best_idx]\n\n\n            #TODO\n            #best_individual_pipeline = best_individual.export_pipeline(memory=self.memory, cross_val_predict_cv=self.cross_val_predict_cv)\n            if self.export_graphpipeline:\n                best_individual_pipeline = best_individual.export_flattened_graphpipeline(memory=self.memory)\n            else:\n                best_individual_pipeline = best_individual.export_pipeline(memory=self.memory)\n\n            if self.preprocessing:\n                self.fitted_pipeline_ = sklearn.pipeline.make_pipeline(sklearn.base.clone(self._preprocessing_pipeline), best_individual_pipeline )\n            else:\n                self.fitted_pipeline_ = best_individual_pipeline\n\n            try:\n                self.fitted_pipeline_.fit(X_original,y_original) #TODO use y_original as well?\n                break\n            except Exception as e:\n                if self.verbose &gt;= 4:\n                    warnings.warn(\"Final pipeline failed to fit. Rarely, the pipeline might work on the objective function but fail on the full dataset. Generally due to interactions with different features being selected or transformations having different properties. Trying next pipeline\")\n                    print(e)\n                continue\n\n\n        if self.client is None: #no client was passed in\n            #close cluster and client\n            # _client.close()\n            # cluster.close()\n            try:\n                _client.shutdown()\n                cluster.close()\n            #catch exception\n            except Exception as e:\n                print(\"Error shutting down client and cluster\")\n                Warning(e)\n\n        return self\n\n    def _estimator_has(attr):\n        '''Check if we can delegate a method to the underlying estimator.\n        First, we check the first fitted final estimator if available, otherwise we\n        check the unfitted final estimator.\n        '''\n        return  lambda self: (self.fitted_pipeline_ is not None and\n            hasattr(self.fitted_pipeline_, attr)\n        )\n\n\n\n\n\n\n    @available_if(_estimator_has('predict'))\n    def predict(self, X, **predict_params):\n        check_is_fitted(self)\n        #X = check_array(X)\n\n        preds = self.fitted_pipeline_.predict(X,**predict_params)\n        if self.classification and self.label_encoder_:\n            preds = self.label_encoder_.inverse_transform(preds)\n\n        return preds\n\n    @available_if(_estimator_has('predict_proba'))\n    def predict_proba(self, X, **predict_params):\n        check_is_fitted(self)\n        #X = check_array(X)\n        return self.fitted_pipeline_.predict_proba(X,**predict_params)\n\n    @available_if(_estimator_has('decision_function'))\n    def decision_function(self, X, **predict_params):\n        check_is_fitted(self)\n        #X = check_array(X)\n        return self.fitted_pipeline_.decision_function(X,**predict_params)\n\n    @available_if(_estimator_has('transform'))\n    def transform(self, X, **predict_params):\n        check_is_fitted(self)\n        #X = check_array(X)\n        return self.fitted_pipeline_.transform(X,**predict_params)\n\n    @property\n    def classes_(self):\n        \"\"\"The classes labels. Only exist if the last step is a classifier.\"\"\"\n        if self.label_encoder_:\n            return self.label_encoder_.classes_\n        else:\n            return self.fitted_pipeline_.classes_\n\n\n    @property\n    def _estimator_type(self):\n        return self.fitted_pipeline_._estimator_type\n\n\n    def make_evaluated_individuals(self):\n        #check if _evolver_instance exists\n        if self.evaluated_individuals is None:\n            self.evaluated_individuals  =  self._evolver_instance.population.evaluated_individuals.copy()\n            objects = list(self.evaluated_individuals.index)\n            object_to_int = dict(zip(objects, range(len(objects))))\n            self.evaluated_individuals = self.evaluated_individuals.set_index(self.evaluated_individuals.index.map(object_to_int))\n            self.evaluated_individuals['Parents'] = self.evaluated_individuals['Parents'].apply(lambda row: convert_parents_tuples_to_integers(row, object_to_int))\n\n            self.evaluated_individuals[\"Instance\"] = self.evaluated_individuals[\"Individual\"].apply(lambda ind: apply_make_pipeline(ind, preprocessing_pipeline=self._preprocessing_pipeline, export_graphpipeline=self.export_graphpipeline, memory=self.memory))\n\n        return self.evaluated_individuals\n\n    @property\n    def pareto_front(self):\n        #check if _evolver_instance exists\n        if self.evaluated_individuals is None:\n            return None\n        else:\n            if \"Pareto_Front\" not in self.evaluated_individuals:\n                return self.evaluated_individuals\n            else:\n                return self.evaluated_individuals[self.evaluated_individuals[\"Pareto_Front\"]==1]\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/estimator/#tpot.tpot_estimator.estimator.TPOTEstimator.classes_","title":"<code>classes_</code>  <code>property</code>","text":"<p>The classes labels. Only exist if the last step is a classifier.</p>"},{"location":"documentation/tpot/tpot_estimator/estimator/#tpot.tpot_estimator.estimator.TPOTEstimator.__init__","title":"<code>__init__(search_space, scorers, scorers_weights, classification, cv=10, other_objective_functions=[], other_objective_functions_weights=[], objective_function_names=None, bigger_is_better=True, export_graphpipeline=False, memory=None, categorical_features=None, preprocessing=False, population_size=50, initial_population_size=None, population_scaling=0.5, generations_until_end_population=1, generations=None, max_time_mins=60, max_eval_time_mins=10, validation_strategy='none', validation_fraction=0.2, disable_label_encoder=False, early_stop=None, scorers_early_stop_tol=0.001, other_objectives_early_stop_tol=None, threshold_evaluation_pruning=None, threshold_evaluation_scaling=0.5, selection_evaluation_pruning=None, selection_evaluation_scaling=0.5, min_history_threshold=20, survival_percentage=1, crossover_probability=0.2, mutate_probability=0.7, mutate_then_crossover_probability=0.05, crossover_then_mutate_probability=0.05, survival_selector=survival_select_NSGA2, parent_selector=tournament_selection_dominated, budget_range=None, budget_scaling=0.5, generations_until_end_budget=1, stepwise_steps=5, n_jobs=1, memory_limit=None, client=None, processes=True, warm_start=False, periodic_checkpoint_folder=None, callback=None, verbose=0, scatter=True, random_state=None)</code>","text":"<p>An sklearn baseestimator that uses genetic programming to optimize a pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>search_space</code> <code>(String, SearchSpace)</code> <ul> <li>String : The default search space to use for the optimization. | String     | Description      | | :---        |    :----:   | | linear  | A linear pipeline with the structure of \"Selector-&gt;(transformers+Passthrough)-&gt;(classifiers/regressors+Passthrough)-&gt;final classifier/regressor.\" For both the transformer and inner estimator layers, TPOT may choose one or more transformers/classifiers, or it may choose none. The inner classifier/regressor layer is optional. | | linear-light | Same search space as linear, but without the inner classifier/regressor layer and with a reduced set of faster running estimators. | | graph | TPOT will optimize a pipeline in the shape of a directed acyclic graph. The nodes of the graph can include selectors, scalers, transformers, or classifiers/regressors (inner classifiers/regressors can optionally be not included). This will return a custom GraphPipeline rather than an sklearn Pipeline. More details in Tutorial 6. | | graph-light | Same as graph search space, but without the inner classifier/regressors and with a reduced set of faster running estimators. | | mdr |TPOT will search over a series of feature selectors and Multifactor Dimensionality Reduction models to find a series of operators that maximize prediction accuracy. The TPOT MDR configuration is specialized for genome-wide association studies (GWAS), and is described in detail online here.</li> </ul> <p>Note that TPOT MDR may be slow to run because the feature selection routines are computationally expensive, especially on large datasets. |</p> <ul> <li>SearchSpace : The search space to use for the optimization. This should be an instance of a SearchSpace.     The search space to use for the optimization. This should be an instance of a SearchSpace.     TPOT has groups of search spaces found in the following folders, tpot.search_spaces.nodes for the nodes in the pipeline and tpot.search_spaces.pipelines for the pipeline structure.</li> </ul> required <code>scorers</code> <code>(list, scorer)</code> <p>A scorer or list of scorers to be used in the cross-validation process. see https://scikit-learn.org/stable/modules/model_evaluation.html</p> required <code>scorers_weights</code> <code>list</code> <p>A list of weights to be applied to the scorers during the optimization process.</p> required <code>classification</code> <code>bool</code> <p>If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem. Used to determine the CV strategy.</p> required <code>cv</code> <code>(int, cross - validator)</code> <ul> <li>(int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.</li> <li>(sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.<ul> <li>max_depth (int): The maximum depth from any node to the root of the pipelines to be generated.</li> </ul> </li> </ul> <code>10</code> <code>other_objective_functions</code> <code>list</code> <p>A list of other objective functions to apply to the pipeline. The function takes a single parameter for the graphpipeline estimator and returns either a single score or a list of scores.</p> <code>[]</code> <code>other_objective_functions_weights</code> <code>list</code> <p>A list of weights to be applied to the other objective functions.</p> <code>[]</code> <code>objective_function_names</code> <code>list</code> <p>A list of names to be applied to the objective functions. If None, will use the names of the objective functions.</p> <code>None</code> <code>bigger_is_better</code> <code>bool</code> <p>If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.</p> <code>True</code> <code>memory</code> <p>If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. This feature is used to avoid computing the fit transformers within a pipeline if the parameters and input data are identical with another fitted pipeline during optimization process. - String 'auto':     TPOT uses memory caching with a temporary directory and cleans it up upon shutdown. - String path of a caching directory     TPOT uses memory caching with the provided directory and TPOT does NOT clean     the caching directory up upon shutdown. If the directory does not exist, TPOT will     create it. - Memory object:     TPOT uses the instance of joblib.Memory for memory caching,     and TPOT does NOT clean the caching directory up upon shutdown. - None:     TPOT does not use memory caching.</p> <code>None</code> <code>categorical_features</code> <p>Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False. - None : If None, TPOT will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing. - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices</p> <code>None</code> <code>preprocessing</code> <code>(bool or BaseEstimator / Pipeline)</code> <p>EXPERIMENTAL - will be changed in future versions A pipeline that will be used to preprocess the data before CV. Note that the parameters for these steps are not optimized. Add them to the search space to be optimized. - bool : If True, will use a default preprocessing pipeline which includes imputation followed by one hot encoding. - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.</p> <code>False</code> <code>population_size</code> <code>int</code> <p>Size of the population</p> <code>50</code> <code>initial_population_size</code> <code>int</code> <p>Size of the initial population. If None, population_size will be used.</p> <code>None</code> <code>population_scaling</code> <code>int</code> <p>Scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.</p> <code>0.5</code> <code>generations_until_end_population</code> <code>int</code> <p>Number of generations until the population size reaches population_size</p> <code>1</code> <code>generations</code> <code>int</code> <p>Number of generations to run</p> <code>50</code> <code>max_time_mins</code> <code>float</code> <p>Maximum time to run the optimization. If none or inf, will run until the end of the generations.</p> <code>float(\"inf\")</code> <code>max_eval_time_mins</code> <code>float</code> <p>Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.</p> <code>5</code> <code>validation_strategy</code> <code>str</code> <p>EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT may overfit the cross validation score. A second validation set can be used to select the final pipeline. - 'auto' : Automatically determine the validation strategy based on the dataset shape. - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets. - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets. - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.</p> <code>'none'</code> <code>validation_fraction</code> <code>float</code> <p>EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.</p> <code>0.2</code> <code>disable_label_encoder</code> <code>bool</code> <p>If True, TPOT will check if the target needs to be relabeled to be sequential ints from 0 to N. This is necessary for XGBoost compatibility. If the labels need to be encoded, TPOT will use sklearn.preprocessing.LabelEncoder to encode the labels. The encoder can be accessed via the self.label_encoder_ attribute. If False, no additional label encoders will be used.</p> <code>False</code> <code>early_stop</code> <code>int</code> <p>Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered. In general a value of around 5-20 is good.</p> <code>None</code> <code>scorers_early_stop_tol</code> <p>-list of floats     list of tolerances for each scorer. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged     If an index of the list is None, that item will not be used for early stopping -int     If an int is given, it will be used as the tolerance for all objectives</p> <code>0.001</code> <code>other_objectives_early_stop_tol</code> <p>-list of floats     list of tolerances for each of the other objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged     If an index of the list is None, that item will not be used for early stopping -int     If an int is given, it will be used as the tolerance for all objectives</p> <code>None</code> <code>threshold_evaluation_pruning</code> <code>list[start, end]</code> <p>starting and ending percentile to use as a threshold for the evaluation early stopping. Values between 0 and 100.</p> <code>None</code> <code>threshold_evaluation_scaling</code> <code>float [0,inf)</code> <p>A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile. Must be greater than zero. Higher numbers will move the threshold to the end faster.</p> <code>0.5</code> <code>selection_evaluation_pruning</code> <code>list</code> <p>A lower and upper percent of the population size to select each round of CV. Values between 0 and 1.</p> <code>None</code> <code>selection_evaluation_scaling</code> <code>float</code> <p>A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile. Must be greater than zero. Higher numbers will move the threshold to the end faster.</p> <code>0.5</code> <code>min_history_threshold</code> <code>int</code> <p>The minimum number of previous scores needed before using threshold early stopping.</p> <code>0</code> <code>survival_percentage</code> <code>float</code> <p>Percentage of the population size to utilize for mutation and crossover at the beginning of the generation. The rest are discarded. Individuals are selected with the selector passed into survival_selector. The value of this parameter must be between 0 and 1, inclusive. For example, if the population size is 100 and the survival percentage is .5, 50 individuals will be selected with NSGA2 from the existing population. These will be used for mutation and crossover to generate the next 100 individuals for the next generation. The remainder are discarded from the live population. In the next generation, there will now be the 50 parents + the 100 individuals for a total of 150. Surivival percentage is based of the population size parameter and not the existing population size (current population size when using successive halving). Therefore, in the next generation we will still select 50 individuals from the currently existing 150.</p> <code>1</code> <code>crossover_probability</code> <code>float</code> <p>Probability of generating a new individual by crossover between two individuals.</p> <code>.2</code> <code>mutate_probability</code> <code>float</code> <p>Probability of generating a new individual by crossover between one individuals.</p> <code>.7</code> <code>mutate_then_crossover_probability</code> <code>float</code> <p>Probability of generating a new individual by mutating two individuals followed by crossover.</p> <code>.05</code> <code>crossover_then_mutate_probability</code> <code>float</code> <p>Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.</p> <code>.05</code> <code>survival_selector</code> <code>function</code> <p>Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes. Used to selected population_size * survival_percentage individuals at the start of each generation to use for mutation and crossover.</p> <code>survival_select_NSGA2</code> <code>parent_selector</code> <code>function</code> <p>Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.</p> <code>parent_select_NSGA2</code> <code>budget_range</code> <code>list[start, end]</code> <p>A starting and ending budget to use for the budget scaling.</p> <code>None</code> <code>budget_scaling</code> <p>A scaling factor to use when determining how fast we move the budget from the start to end budget.</p> <code>0.5</code> <code>generations_until_end_budget</code> <code>int</code> <p>The number of generations to run before reaching the max budget.</p> <code>1</code> <code>stepwise_steps</code> <code>int</code> <p>The number of staircase steps to take when scaling the budget and population size.</p> <code>1</code> <code>n_jobs</code> <code>int</code> <p>Number of processes to run in parallel.</p> <code>1</code> <code>memory_limit</code> <code>str</code> <p>Memory limit for each job. See Dask LocalCluster documentation for more information.</p> <code>None</code> <code>client</code> <code>Client</code> <p>A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.</p> <code>None</code> <code>processes</code> <code>bool</code> <p>If True, will use multiprocessing to parallelize the optimization process. If False, will use threading. True seems to perform better. However, False is required for interactive debugging.</p> <code>True</code> <code>warm_start</code> <code>bool</code> <p>If True, will use the continue the evolutionary algorithm from the last generation of the previous run.</p> <code>False</code> <code>periodic_checkpoint_folder</code> <code>str</code> <p>Folder to save the population to periodically. If None, no periodic saving will be done. If provided, training will resume from this checkpoint.</p> <code>None</code> <code>callback</code> <code>CallBackInterface</code> <p>Callback object. Not implemented</p> <code>None</code> <code>verbose</code> <code>int</code> <p>How much information to print during the optimization process. Higher values include the information from lower values. 0. nothing 1. progress bar</p> <ol> <li>best individual</li> <li>warnings <p>=5. full warnings trace</p> </li> <li>evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_mins in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)</li> </ol> <code>1</code> <code>scatter</code> <code>bool</code> <p>If True, will scatter the data to the dask workers. If False, will not scatter the data. This can be useful for debugging.</p> <code>True</code> <code>random_state</code> <code>(int, None)</code> <p>A seed for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes</p> <ul> <li>int     Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'</li> <li>None     Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS</li> </ul> <code>None</code> <p>Attributes:</p> Name Type Description <code>fitted_pipeline_</code> <code>GraphPipeline</code> <p>A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.</p> <code>evaluated_individuals</code> <code>A pandas data frame containing data for all evaluated individuals in the run.</code> <p>Columns: - objective functions : The first few columns correspond to the passed in scorers and objective functions - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population. - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population. - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator. - Generation : The generation the pipeline first appeared. - Pareto_Front      : The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.                 To save on computational time, the best frontier is updated iteratively each generation.                 The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.                 All other pipelines are set to NaN. - Instance  : The unfitted GraphPipeline BaseEstimator. - validation objective functions : Objective function scores evaluated on the validation set. - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.</p> <code>pareto_front</code> <code>The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.</code> Source code in <code>tpot/tpot_estimator/estimator.py</code> <pre><code>def __init__(self,  \n                    search_space,\n                    scorers,\n                    scorers_weights,\n                    classification,\n                    cv = 10,\n                    other_objective_functions=[],\n                    other_objective_functions_weights = [],\n                    objective_function_names = None,\n                    bigger_is_better = True,\n\n                    export_graphpipeline = False,\n                    memory = None,\n\n                    categorical_features = None,\n                    preprocessing = False,\n                    population_size = 50,\n                    initial_population_size = None,\n                    population_scaling = .5,\n                    generations_until_end_population = 1,\n                    generations = None,\n                    max_time_mins=60,\n                    max_eval_time_mins=10,\n                    validation_strategy = \"none\",\n                    validation_fraction = .2,\n                    disable_label_encoder = False,\n\n                    #early stopping parameters\n                    early_stop = None,\n                    scorers_early_stop_tol = 0.001,\n                    other_objectives_early_stop_tol =None,\n                    threshold_evaluation_pruning = None,\n                    threshold_evaluation_scaling = .5,\n                    selection_evaluation_pruning = None,\n                    selection_evaluation_scaling = .5,\n                    min_history_threshold = 20,\n\n                    #evolver parameters\n                    survival_percentage = 1,\n                    crossover_probability=.2,\n                    mutate_probability=.7,\n                    mutate_then_crossover_probability=.05,\n                    crossover_then_mutate_probability=.05,\n                    survival_selector = survival_select_NSGA2,\n                    parent_selector = tournament_selection_dominated,\n\n                    #budget parameters\n                    budget_range = None,\n                    budget_scaling = .5,\n                    generations_until_end_budget = 1,\n                    stepwise_steps = 5,\n\n                    #dask parameters\n                    n_jobs=1,\n                    memory_limit = None,\n                    client = None,\n                    processes = True,\n\n                    #debugging and logging parameters\n                    warm_start = False,\n                    periodic_checkpoint_folder = None,\n                    callback = None,\n\n                    verbose = 0,\n                    scatter = True,\n\n                     # random seed for random number generator (rng)\n                    random_state = None,\n\n                    ):\n\n    '''\n    An sklearn baseestimator that uses genetic programming to optimize a pipeline.\n\n    Parameters\n    ----------\n    search_space : (String, tpot.search_spaces.SearchSpace)\n        - String : The default search space to use for the optimization.\n        | String     | Description      |\n        | :---        |    :----:   |\n        | linear  | A linear pipeline with the structure of \"Selector-&gt;(transformers+Passthrough)-&gt;(classifiers/regressors+Passthrough)-&gt;final classifier/regressor.\" For both the transformer and inner estimator layers, TPOT may choose one or more transformers/classifiers, or it may choose none. The inner classifier/regressor layer is optional. |\n        | linear-light | Same search space as linear, but without the inner classifier/regressor layer and with a reduced set of faster running estimators. |\n        | graph | TPOT will optimize a pipeline in the shape of a directed acyclic graph. The nodes of the graph can include selectors, scalers, transformers, or classifiers/regressors (inner classifiers/regressors can optionally be not included). This will return a custom GraphPipeline rather than an sklearn Pipeline. More details in Tutorial 6. |\n        | graph-light | Same as graph search space, but without the inner classifier/regressors and with a reduced set of faster running estimators. |\n        | mdr |TPOT will search over a series of feature selectors and Multifactor Dimensionality Reduction models to find a series of operators that maximize prediction accuracy. The TPOT MDR configuration is specialized for genome-wide association studies (GWAS), and is described in detail online here.\n\n        Note that TPOT MDR may be slow to run because the feature selection routines are computationally expensive, especially on large datasets. |\n\n\n        - SearchSpace : The search space to use for the optimization. This should be an instance of a SearchSpace.\n            The search space to use for the optimization. This should be an instance of a SearchSpace.\n            TPOT has groups of search spaces found in the following folders, tpot.search_spaces.nodes for the nodes in the pipeline and tpot.search_spaces.pipelines for the pipeline structure.\n\n    scorers : (list, scorer)\n        A scorer or list of scorers to be used in the cross-validation process.\n        see https://scikit-learn.org/stable/modules/model_evaluation.html\n\n    scorers_weights : list\n        A list of weights to be applied to the scorers during the optimization process.\n\n    classification : bool\n        If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem.\n        Used to determine the CV strategy.\n\n    cv : int, cross-validator\n        - (int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.\n        - (sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.\n            - max_depth (int): The maximum depth from any node to the root of the pipelines to be generated.\n\n    other_objective_functions : list, default=[]\n        A list of other objective functions to apply to the pipeline. The function takes a single parameter for the graphpipeline estimator and returns either a single score or a list of scores.\n\n    other_objective_functions_weights : list, default=[]\n        A list of weights to be applied to the other objective functions.\n\n    objective_function_names : list, default=None\n        A list of names to be applied to the objective functions. If None, will use the names of the objective functions.\n\n    bigger_is_better : bool, default=True\n        If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n\n    memory: Memory object or string, default=None\n        If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. This feature\n        is used to avoid computing the fit transformers within a pipeline if the parameters\n        and input data are identical with another fitted pipeline during optimization process.\n        - String 'auto':\n            TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.\n        - String path of a caching directory\n            TPOT uses memory caching with the provided directory and TPOT does NOT clean\n            the caching directory up upon shutdown. If the directory does not exist, TPOT will\n            create it.\n        - Memory object:\n            TPOT uses the instance of joblib.Memory for memory caching,\n            and TPOT does NOT clean the caching directory up upon shutdown.\n        - None:\n            TPOT does not use memory caching.              \n\n    categorical_features: list or None\n        Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n        - None : If None, TPOT will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing.\n        - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices\n\n    preprocessing : bool or BaseEstimator/Pipeline,\n        EXPERIMENTAL - will be changed in future versions\n        A pipeline that will be used to preprocess the data before CV. Note that the parameters for these steps are not optimized. Add them to the search space to be optimized.\n        - bool : If True, will use a default preprocessing pipeline which includes imputation followed by one hot encoding.\n        - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.\n\n    population_size : int, default=50\n        Size of the population\n\n    initial_population_size : int, default=None\n        Size of the initial population. If None, population_size will be used.\n\n    population_scaling : int, default=0.5\n        Scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n\n    generations_until_end_population : int, default=1\n        Number of generations until the population size reaches population_size\n\n    generations : int, default=50\n        Number of generations to run\n\n    max_time_mins : float, default=float(\"inf\")\n        Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n\n    max_eval_time_mins : float, default=5\n        Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n\n    validation_strategy : str, default='none'\n        EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT may overfit the cross validation score. A second validation set can be used to select the final pipeline.\n        - 'auto' : Automatically determine the validation strategy based on the dataset shape.\n        - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets.\n        - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets.\n        - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.\n\n    validation_fraction : float, default=0.2\n      EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.\n\n    disable_label_encoder : bool, default=False\n        If True, TPOT will check if the target needs to be relabeled to be sequential ints from 0 to N. This is necessary for XGBoost compatibility. If the labels need to be encoded, TPOT will use sklearn.preprocessing.LabelEncoder to encode the labels. The encoder can be accessed via the self.label_encoder_ attribute.\n        If False, no additional label encoders will be used.\n\n    early_stop : int, default=None\n        Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered. In general a value of around 5-20 is good.\n\n    scorers_early_stop_tol :\n        -list of floats\n            list of tolerances for each scorer. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n            If an index of the list is None, that item will not be used for early stopping\n        -int\n            If an int is given, it will be used as the tolerance for all objectives\n\n    other_objectives_early_stop_tol :\n        -list of floats\n            list of tolerances for each of the other objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n            If an index of the list is None, that item will not be used for early stopping\n        -int\n            If an int is given, it will be used as the tolerance for all objectives\n\n    threshold_evaluation_pruning : list [start, end], default=None\n        starting and ending percentile to use as a threshold for the evaluation early stopping.\n        Values between 0 and 100.\n\n    threshold_evaluation_scaling : float [0,inf), default=0.5\n        A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n        Must be greater than zero. Higher numbers will move the threshold to the end faster.\n\n    selection_evaluation_pruning : list, default=None\n        A lower and upper percent of the population size to select each round of CV.\n        Values between 0 and 1.\n\n    selection_evaluation_scaling : float, default=0.5\n        A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n        Must be greater than zero. Higher numbers will move the threshold to the end faster.\n\n    min_history_threshold : int, default=0\n        The minimum number of previous scores needed before using threshold early stopping.\n\n    survival_percentage : float, default=1\n        Percentage of the population size to utilize for mutation and crossover at the beginning of the generation. The rest are discarded. Individuals are selected with the selector passed into survival_selector. The value of this parameter must be between 0 and 1, inclusive.\n        For example, if the population size is 100 and the survival percentage is .5, 50 individuals will be selected with NSGA2 from the existing population. These will be used for mutation and crossover to generate the next 100 individuals for the next generation. The remainder are discarded from the live population. In the next generation, there will now be the 50 parents + the 100 individuals for a total of 150. Surivival percentage is based of the population size parameter and not the existing population size (current population size when using successive halving). Therefore, in the next generation we will still select 50 individuals from the currently existing 150.\n\n    crossover_probability : float, default=.2\n        Probability of generating a new individual by crossover between two individuals.\n\n    mutate_probability : float, default=.7\n        Probability of generating a new individual by crossover between one individuals.\n\n    mutate_then_crossover_probability : float, default=.05\n        Probability of generating a new individual by mutating two individuals followed by crossover.\n\n    crossover_then_mutate_probability : float, default=.05\n        Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.\n\n    survival_selector : function, default=survival_select_NSGA2\n        Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes.\n        Used to selected population_size * survival_percentage individuals at the start of each generation to use for mutation and crossover.\n\n    parent_selector : function, default=parent_select_NSGA2\n        Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.\n\n    budget_range : list [start, end], default=None\n        A starting and ending budget to use for the budget scaling.\n\n    budget_scaling float : [0,1], default=0.5\n        A scaling factor to use when determining how fast we move the budget from the start to end budget.\n\n    generations_until_end_budget : int, default=1\n        The number of generations to run before reaching the max budget.\n\n    stepwise_steps : int, default=1\n        The number of staircase steps to take when scaling the budget and population size.\n\n    n_jobs : int, default=1\n        Number of processes to run in parallel.\n\n    memory_limit : str, default=None\n        Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n\n    client : dask.distributed.Client, default=None\n        A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.\n\n    processes : bool, default=True\n        If True, will use multiprocessing to parallelize the optimization process. If False, will use threading.\n        True seems to perform better. However, False is required for interactive debugging.\n\n    warm_start : bool, default=False\n        If True, will use the continue the evolutionary algorithm from the last generation of the previous run.\n\n    periodic_checkpoint_folder : str, default=None\n        Folder to save the population to periodically. If None, no periodic saving will be done.\n        If provided, training will resume from this checkpoint.\n\n    callback : tpot.CallBackInterface, default=None\n        Callback object. Not implemented\n\n    verbose : int, default=1\n        How much information to print during the optimization process. Higher values include the information from lower values.\n        0. nothing\n        1. progress bar\n\n        3. best individual\n        4. warnings\n        &gt;=5. full warnings trace\n        6. evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_mins in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)\n\n    scatter : bool, default=True\n        If True, will scatter the data to the dask workers. If False, will not scatter the data. This can be useful for debugging.\n\n    random_state : int, None, default=None\n        A seed for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes\n\n        - int\n            Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'\n        - None\n            Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS\n\n    Attributes\n    ----------\n\n    fitted_pipeline_ : GraphPipeline\n        A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.\n\n    evaluated_individuals : A pandas data frame containing data for all evaluated individuals in the run.\n        Columns:\n        - *objective functions : The first few columns correspond to the passed in scorers and objective functions\n        - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population.\n        - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population.\n        - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator.\n        - Generation : The generation the pipeline first appeared.\n        - Pareto_Front\t: The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.\n                        To save on computational time, the best frontier is updated iteratively each generation.\n                        The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.\n                        All other pipelines are set to NaN.\n        - Instance\t: The unfitted GraphPipeline BaseEstimator.\n        - *validation objective functions : Objective function scores evaluated on the validation set.\n        - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.\n\n    pareto_front : The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.\n    '''\n\n    # sklearn BaseEstimator must have a corresponding attribute for each parameter.\n    # These should not be modified once set.\n\n    self.scorers = scorers\n    self.scorers_weights = scorers_weights\n    self.classification = classification\n    self.cv = cv\n    self.other_objective_functions = other_objective_functions\n    self.other_objective_functions_weights = other_objective_functions_weights\n    self.objective_function_names = objective_function_names\n    self.bigger_is_better = bigger_is_better\n\n    self.search_space = search_space\n\n    self.export_graphpipeline = export_graphpipeline\n    self.memory = memory\n\n    self.categorical_features = categorical_features\n\n    self.preprocessing = preprocessing\n    self.validation_strategy = validation_strategy\n    self.validation_fraction = validation_fraction\n    self.disable_label_encoder = disable_label_encoder\n    self.population_size = population_size\n    self.initial_population_size = initial_population_size\n    self.population_scaling = population_scaling\n    self.generations_until_end_population = generations_until_end_population\n    self.generations = generations\n    self.early_stop = early_stop\n    self.scorers_early_stop_tol = scorers_early_stop_tol\n    self.other_objectives_early_stop_tol = other_objectives_early_stop_tol\n    self.max_time_mins = max_time_mins\n    self.max_eval_time_mins = max_eval_time_mins\n    self.n_jobs= n_jobs\n    self.memory_limit = memory_limit\n    self.client = client\n    self.survival_percentage = survival_percentage\n    self.crossover_probability = crossover_probability\n    self.mutate_probability = mutate_probability\n    self.mutate_then_crossover_probability= mutate_then_crossover_probability\n    self.crossover_then_mutate_probability= crossover_then_mutate_probability\n    self.survival_selector=survival_selector\n    self.parent_selector=parent_selector\n    self.budget_range = budget_range\n    self.budget_scaling = budget_scaling\n    self.generations_until_end_budget = generations_until_end_budget\n    self.stepwise_steps = stepwise_steps\n    self.threshold_evaluation_pruning =threshold_evaluation_pruning\n    self.threshold_evaluation_scaling =  threshold_evaluation_scaling\n    self.min_history_threshold = min_history_threshold\n    self.selection_evaluation_pruning = selection_evaluation_pruning\n    self.selection_evaluation_scaling =  selection_evaluation_scaling\n    self.warm_start = warm_start\n    self.verbose = verbose\n    self.periodic_checkpoint_folder = periodic_checkpoint_folder\n    self.callback = callback\n    self.processes = processes\n\n\n    self.scatter = scatter\n\n\n    timer_set = self.max_time_mins != float(\"inf\") and self.max_time_mins is not None\n    if self.generations is not None and timer_set:\n        warnings.warn(\"Both generations and max_time_mins are set. TPOT will terminate when the first condition is met.\")\n\n    # create random number generator based on rngseed\n    self.rng = np.random.default_rng(random_state)\n    # save random state passed to us for other functions that use random_state\n    self.random_state = random_state\n\n    #Initialize other used params\n\n\n    if self.initial_population_size is None:\n        self._initial_population_size = self.population_size\n    else:\n        self._initial_population_size = self.initial_population_size\n\n    if isinstance(self.scorers, str):\n        self._scorers = [self.scorers]\n\n    elif callable(self.scorers):\n        self._scorers = [self.scorers]\n    else:\n        self._scorers = self.scorers\n\n    self._scorers = [sklearn.metrics.get_scorer(scoring) for scoring in self._scorers]\n    self._scorers_early_stop_tol = self.scorers_early_stop_tol\n\n    self._evolver = tpot.evolvers.BaseEvolver\n\n    self.objective_function_weights = [*scorers_weights, *other_objective_functions_weights]\n\n\n    if self.objective_function_names is None:\n        obj_names = [f.__name__ for f in other_objective_functions]\n    else:\n        obj_names = self.objective_function_names\n    self.objective_names = [f._score_func.__name__ if hasattr(f,\"_score_func\") else f.__name__ for f in self._scorers] + obj_names\n\n\n    if not isinstance(self.other_objectives_early_stop_tol, list):\n        self._other_objectives_early_stop_tol = [self.other_objectives_early_stop_tol for _ in range(len(self.other_objective_functions))]\n    else:\n        self._other_objectives_early_stop_tol = self.other_objectives_early_stop_tol\n\n    if not isinstance(self._scorers_early_stop_tol, list):\n        self._scorers_early_stop_tol = [self._scorers_early_stop_tol for _ in range(len(self._scorers))]\n    else:\n        self._scorers_early_stop_tol = self._scorers_early_stop_tol\n\n    self.early_stop_tol = [*self._scorers_early_stop_tol, *self._other_objectives_early_stop_tol]\n\n    self._evolver_instance = None\n    self.evaluated_individuals = None\n\n\n    self.label_encoder_ = None\n\n\n    set_dask_settings()\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/estimator/#tpot.tpot_estimator.estimator.apply_make_pipeline","title":"<code>apply_make_pipeline(ind, preprocessing_pipeline=None, export_graphpipeline=False, **pipeline_kwargs)</code>","text":"<p>Helper function to create a column of sklearn pipelines from the tpot individual class.</p> <p>Parameters:</p> Name Type Description Default <code>ind</code> <p>The individual to convert to a pipeline.</p> required <code>preprocessing_pipeline</code> <p>The preprocessing pipeline to include before the individual's pipeline.</p> <code>None</code> <code>export_graphpipeline</code> <p>Force the pipeline to be exported as a graph pipeline. Flattens all nested pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.</p> <code>False</code> <code>pipeline_kwargs</code> <p>Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>sklearn estimator</code> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def apply_make_pipeline(ind, preprocessing_pipeline=None, export_graphpipeline=False, **pipeline_kwargs):\n    \"\"\"\n    Helper function to create a column of sklearn pipelines from the tpot individual class.\n\n    Parameters\n    ----------\n    ind: tpot.SklearnIndividual\n        The individual to convert to a pipeline.\n    preprocessing_pipeline: sklearn.pipeline.Pipeline, optional\n        The preprocessing pipeline to include before the individual's pipeline.\n    export_graphpipeline: bool, default=False\n        Force the pipeline to be exported as a graph pipeline. Flattens all nested pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.\n    pipeline_kwargs: dict\n        Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.\n\n    Returns\n    -------\n    sklearn estimator\n    \"\"\"\n\n    try:\n\n        if export_graphpipeline:\n            est = ind.export_flattened_graphpipeline(**pipeline_kwargs)\n        else:\n            est = ind.export_pipeline(**pipeline_kwargs)\n\n\n        if preprocessing_pipeline is None:\n            return est\n        else:\n            return sklearn.pipeline.make_pipeline(sklearn.base.clone(preprocessing_pipeline), est)\n    except:\n        return None\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/estimator/#tpot.tpot_estimator.estimator.check_empty_values","title":"<code>check_empty_values(data)</code>","text":"<p>Checks for empty values in a dataset.</p> <p>Args:     data (numpy.ndarray or pandas.DataFrame): The dataset to check.</p> <p>Returns:     bool: True if the dataset contains empty values, False otherwise.</p> Source code in <code>tpot/tpot_estimator/estimator.py</code> <pre><code>def check_empty_values(data):\n    \"\"\"\n    Checks for empty values in a dataset.\n\n    Args:\n        data (numpy.ndarray or pandas.DataFrame): The dataset to check.\n\n    Returns:\n        bool: True if the dataset contains empty values, False otherwise.\n    \"\"\"\n    if isinstance(data, pd.DataFrame):\n        return data.isnull().values.any()\n    elif isinstance(data, np.ndarray):\n        return np.isnan(data).any()\n    else:\n        raise ValueError(\"Unsupported data type\")\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/estimator/#tpot.tpot_estimator.estimator.check_if_y_is_encoded","title":"<code>check_if_y_is_encoded(y)</code>","text":"<p>Checks if the target y is composed of sequential ints from 0 to N. XGBoost requires the target to be encoded in this way.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <p>The target vector.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the target is encoded as sequential ints from 0 to N, False otherwise</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def check_if_y_is_encoded(y):\n    '''\n    Checks if the target y is composed of sequential ints from 0 to N.\n    XGBoost requires the target to be encoded in this way.\n\n    Parameters\n    ----------\n    y: np.ndarray\n        The target vector.\n\n    Returns\n    -------\n    bool\n        True if the target is encoded as sequential ints from 0 to N, False otherwise\n    '''\n    y = sorted(set(y))\n    return all(i == j for i, j in enumerate(y))\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/estimator/#tpot.tpot_estimator.estimator.convert_parents_tuples_to_integers","title":"<code>convert_parents_tuples_to_integers(row, object_to_int)</code>","text":"<p>Helper function to convert the parent rows into integers representing the index of the parent in the population.</p> <p>Original pandas dataframe using a custom index for the parents. This function converts the custom index to an integer index for easier manipulation by end users.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <p>The row to convert.</p> required <code>object_to_int</code> <p>A dictionary mapping the object to an integer index.</p> required Returns <p>tuple     The row with the custom index converted to an integer index.</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def convert_parents_tuples_to_integers(row, object_to_int):\n    \"\"\"\n    Helper function to convert the parent rows into integers representing the index of the parent in the population.\n\n    Original pandas dataframe using a custom index for the parents. This function converts the custom index to an integer index for easier manipulation by end users.\n\n    Parameters\n    ----------\n    row: list, np.ndarray, tuple\n        The row to convert.\n    object_to_int: dict\n        A dictionary mapping the object to an integer index.\n\n    Returns \n    -------\n    tuple\n        The row with the custom index converted to an integer index.\n    \"\"\"\n    if type(row) == list or type(row) == np.ndarray or type(row) == tuple:\n        return tuple(object_to_int[obj] for obj in row)\n    else:\n        return np.nan\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/estimator/#tpot.tpot_estimator.estimator.cross_val_score_objective","title":"<code>cross_val_score_objective(estimator, X, y, scorers, cv, fold=None)</code>","text":"<p>Compute the cross validated scores for a estimator. Only fits the estimator once per fold, and loops over the scorers to evaluate the estimator.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <p>The estimator to fit and score.</p> required <code>X</code> <p>The feature matrix.</p> required <code>y</code> <p>The target vector.</p> required <code>scorers</code> <p>The scorers to use.  If a list, will loop over the scorers and return a list of scorers. If a single scorer, will return a single score.</p> required <code>cv</code> <p>The cross-validator to use. For example, sklearn.model_selection.KFold or sklearn.model_selection.StratifiedKFold.</p> required <code>fold</code> <p>The fold to return the scores for. If None, will return the mean of all the scores (per scorer). Default is None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>scores</code> <code>ndarray or float</code> <p>The scores for the estimator per scorer. If fold is None, will return the mean of all the scores (per scorer). Returns a list if multiple scorers are used, otherwise returns a float for the single scorer.</p> Source code in <code>tpot/tpot_estimator/cross_val_utils.py</code> <pre><code>def cross_val_score_objective(estimator, X, y, scorers, cv, fold=None):\n    \"\"\"\n    Compute the cross validated scores for a estimator. Only fits the estimator once per fold, and loops over the scorers to evaluate the estimator.\n\n    Parameters\n    ----------\n    estimator: sklearn.base.BaseEstimator\n        The estimator to fit and score.\n    X: np.ndarray or pd.DataFrame\n        The feature matrix.\n    y: np.ndarray or pd.Series\n        The target vector.\n    scorers: list or scorer\n        The scorers to use. \n        If a list, will loop over the scorers and return a list of scorers.\n        If a single scorer, will return a single score.\n    cv: sklearn cross-validator\n        The cross-validator to use. For example, sklearn.model_selection.KFold or sklearn.model_selection.StratifiedKFold.\n    fold: int, optional\n        The fold to return the scores for. If None, will return the mean of all the scores (per scorer). Default is None.\n\n    Returns\n    -------\n    scores: np.ndarray or float\n        The scores for the estimator per scorer. If fold is None, will return the mean of all the scores (per scorer).\n        Returns a list if multiple scorers are used, otherwise returns a float for the single scorer.\n\n    \"\"\"\n\n    #check if scores is not iterable\n    if not isinstance(scorers, Iterable): \n        scorers = [scorers]\n    scores = []\n    if fold is None:\n        for train_index, test_index in cv.split(X, y):\n            this_fold_estimator = sklearn.base.clone(estimator)\n            if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n            else:\n                X_train, X_test = X[train_index], X[test_index]\n\n            if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n            else:\n                y_train, y_test = y[train_index], y[test_index]\n\n\n            start = time.time()\n            this_fold_estimator.fit(X_train,y_train)\n            duration = time.time() - start\n\n            this_fold_scores = [sklearn.metrics.get_scorer(scorer)(this_fold_estimator, X_test, y_test) for scorer in scorers] \n            scores.append(this_fold_scores)\n            del this_fold_estimator\n            del X_train\n            del X_test\n            del y_train\n            del y_test\n\n\n        return np.mean(scores,0)\n    else:\n        this_fold_estimator = sklearn.base.clone(estimator)\n        train_index, test_index = list(cv.split(X, y))[fold]\n        if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        else:\n            X_train, X_test = X[train_index], X[test_index]\n\n        if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n        else:\n            y_train, y_test = y[train_index], y[test_index]\n\n        start = time.time()\n        this_fold_estimator.fit(X_train,y_train)\n        duration = time.time() - start\n        this_fold_scores = [sklearn.metrics.get_scorer(scorer)(this_fold_estimator, X_test, y_test) for scorer in scorers] \n        return this_fold_scores\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/estimator/#tpot.tpot_estimator.estimator.objective_function_generator","title":"<code>objective_function_generator(pipeline, x, y, scorers, cv, other_objective_functions, step=None, budget=None, is_classification=True, export_graphpipeline=False, **pipeline_kwargs)</code>","text":"<p>Uses cross validation to evaluate the pipeline using the scorers, and concatenates results with scores from standalone other objective functions.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <p>The individual to evaluate.</p> required <code>x</code> <p>The feature matrix.</p> required <code>y</code> <p>The target vector.</p> required <code>scorers</code> <p>The scorers to use for cross validation.</p> required <code>cv</code> <p>The cross-validator to use. For example, sklearn.model_selection.KFold or sklearn.model_selection.StratifiedKFold. If an int, will use sklearn.model_selection.KFold with n_splits=cv.</p> required <code>other_objective_functions</code> <p>A list of standalone objective functions to evaluate the pipeline. With signature obj(pipeline) -&gt; float. or obj(pipeline) -&gt; np.ndarray These functions take in the unfitted estimator.</p> required <code>step</code> <p>The fold to return the scores for. If None, will return the mean of all the scores (per scorer). Default is None.</p> <code>None</code> <code>budget</code> <p>The budget to subsample the data. If None, will use the full dataset. Default is None. Will subsample budget*len(x) samples.</p> <code>None</code> <code>is_classification</code> <p>If True, will stratify the subsampling. Default is True.</p> <code>True</code> <code>export_graphpipeline</code> <p>Force the pipeline to be exported as a graph pipeline. Flattens all nested sklearn pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.</p> <code>False</code> <code>pipeline_kwargs</code> <p>Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The concatenated scores for the pipeline. The first len(scorers) elements are the cross validation scores, and the remaining elements are the standalone objective functions.</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def objective_function_generator(pipeline, x,y, scorers, cv, other_objective_functions, step=None, budget=None, is_classification=True, export_graphpipeline=False, **pipeline_kwargs):\n    \"\"\"\n    Uses cross validation to evaluate the pipeline using the scorers, and concatenates results with scores from standalone other objective functions.\n\n    Parameters\n    ----------\n    pipeline: tpot.SklearnIndividual\n        The individual to evaluate.\n    x: np.ndarray\n        The feature matrix.\n    y: np.ndarray\n        The target vector.\n    scorers: list\n        The scorers to use for cross validation. \n    cv: int, float, or sklearn cross-validator\n        The cross-validator to use. For example, sklearn.model_selection.KFold or sklearn.model_selection.StratifiedKFold.\n        If an int, will use sklearn.model_selection.KFold with n_splits=cv.\n    other_objective_functions: list\n        A list of standalone objective functions to evaluate the pipeline. With signature obj(pipeline) -&gt; float. or obj(pipeline) -&gt; np.ndarray\n        These functions take in the unfitted estimator.\n    step: int, optional\n        The fold to return the scores for. If None, will return the mean of all the scores (per scorer). Default is None.\n    budget: float, optional\n        The budget to subsample the data. If None, will use the full dataset. Default is None.\n        Will subsample budget*len(x) samples.\n    is_classification: bool, default=True\n        If True, will stratify the subsampling. Default is True.\n    export_graphpipeline: bool, default=False\n        Force the pipeline to be exported as a graph pipeline. Flattens all nested sklearn pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.\n    pipeline_kwargs: dict\n        Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.\n\n    Returns\n    -------\n    np.ndarray\n        The concatenated scores for the pipeline. The first len(scorers) elements are the cross validation scores, and the remaining elements are the standalone objective functions.\n\n    \"\"\"\n\n    if export_graphpipeline:\n        pipeline = pipeline.export_flattened_graphpipeline(**pipeline_kwargs)\n    else:\n        pipeline = pipeline.export_pipeline(**pipeline_kwargs)\n\n    if budget is not None and budget &lt; 1:\n        if is_classification:\n            x,y = sklearn.utils.resample(x,y, stratify=y, n_samples=int(budget*len(x)), replace=False, random_state=1)\n        else:\n            x,y = sklearn.utils.resample(x,y, n_samples=int(budget*len(x)), replace=False, random_state=1)\n\n        if isinstance(cv, int) or isinstance(cv, float):\n            n_splits = cv\n        else:\n            n_splits = cv.n_splits\n\n    if len(scorers) &gt; 0:\n        cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)\n    else:\n        cv_obj_scores = []\n\n    if other_objective_functions is not None and len(other_objective_functions) &gt;0:\n        other_scores = [obj(sklearn.base.clone(pipeline)) for obj in other_objective_functions]\n        #flatten\n        other_scores = np.array(other_scores).flatten().tolist()\n    else:\n        other_scores = []\n\n    return np.concatenate([cv_obj_scores,other_scores])\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/estimator/#tpot.tpot_estimator.estimator.remove_underrepresented_classes","title":"<code>remove_underrepresented_classes(x, y, min_count)</code>","text":"<p>Helper function to remove classes with less than min_count samples from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>The feature matrix.</p> required <code>y</code> <p>The target vector.</p> required <code>min_count</code> <p>The minimum number of samples to keep a class.</p> required <p>Returns:</p> Type Description <code>(ndarray, ndarray)</code> <p>The feature matrix and target vector with rows from classes with less than min_count samples removed.</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def remove_underrepresented_classes(x, y, min_count):\n    \"\"\"\n    Helper function to remove classes with less than min_count samples from the dataset.\n\n    Parameters\n    ----------\n    x: np.ndarray or pd.DataFrame\n        The feature matrix.\n    y: np.ndarray or pd.Series\n        The target vector.\n    min_count: int\n        The minimum number of samples to keep a class.\n\n    Returns\n    -------\n    np.ndarray, np.ndarray\n        The feature matrix and target vector with rows from classes with less than min_count samples removed.\n    \"\"\"\n    if isinstance(y, (np.ndarray, pd.Series)):\n        unique, counts = np.unique(y, return_counts=True)\n        if min(counts) &gt;= min_count:\n            return x, y\n        keep_classes = unique[counts &gt;= min_count]\n        mask = np.isin(y, keep_classes)\n        x = x[mask]\n        y = y[mask]\n    elif isinstance(y, pd.DataFrame):\n        counts = y.apply(pd.Series.value_counts)\n        if min(counts) &gt;= min_count:\n            return x, y\n        keep_classes = counts.index[counts &gt;= min_count].tolist()\n        mask = y.isin(keep_classes).all(axis=1)\n        x = x[mask]\n        y = y[mask]\n    else:\n        raise TypeError(\"y must be a numpy array or a pandas Series/DataFrame\")\n    return x, y\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/estimator/#tpot.tpot_estimator.estimator.val_objective_function_generator","title":"<code>val_objective_function_generator(pipeline, X_train, y_train, X_test, y_test, scorers, other_objective_functions, export_graphpipeline=False, **pipeline_kwargs)</code>","text":"<p>Trains a pipeline on a training set and evaluates it on a test set using the scorers and other objective functions.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <p>The individual to evaluate.</p> required <code>X_train</code> <p>The feature matrix of the training set.</p> required <code>y_train</code> <p>The target vector of the training set.</p> required <code>X_test</code> <p>The feature matrix of the test set.</p> required <code>y_test</code> <p>The target vector of the test set.</p> required <code>scorers</code> <p>The scorers to use for cross validation.</p> required <code>other_objective_functions</code> <p>A list of standalone objective functions to evaluate the pipeline. With signature obj(pipeline) -&gt; float. or obj(pipeline) -&gt; np.ndarray These functions take in the unfitted estimator.</p> required <code>export_graphpipeline</code> <p>Force the pipeline to be exported as a graph pipeline. Flattens all nested sklearn pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.</p> <code>False</code> <code>pipeline_kwargs</code> <p>Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The concatenated scores for the pipeline. The first len(scorers) elements are the cross validation scores, and the remaining elements are the standalone objective functions.</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def val_objective_function_generator(pipeline, X_train, y_train, X_test, y_test, scorers, other_objective_functions, export_graphpipeline=False, **pipeline_kwargs):\n    \"\"\"\n    Trains a pipeline on a training set and evaluates it on a test set using the scorers and other objective functions.\n\n    Parameters\n    ----------\n\n    pipeline: tpot.SklearnIndividual\n        The individual to evaluate.\n    X_train: np.ndarray\n        The feature matrix of the training set.\n    y_train: np.ndarray\n        The target vector of the training set.\n    X_test: np.ndarray\n        The feature matrix of the test set.\n    y_test: np.ndarray\n        The target vector of the test set.\n    scorers: list\n        The scorers to use for cross validation.\n    other_objective_functions: list\n        A list of standalone objective functions to evaluate the pipeline. With signature obj(pipeline) -&gt; float. or obj(pipeline) -&gt; np.ndarray\n        These functions take in the unfitted estimator.\n    export_graphpipeline: bool, default=False\n        Force the pipeline to be exported as a graph pipeline. Flattens all nested sklearn pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.\n    pipeline_kwargs: dict\n        Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.\n\n    Returns\n    -------\n    np.ndarray\n        The concatenated scores for the pipeline. The first len(scorers) elements are the cross validation scores, and the remaining elements are the standalone objective functions.\n\n\n    \"\"\"\n\n    #subsample the data\n    if export_graphpipeline:\n        pipeline = pipeline.export_flattened_graphpipeline(**pipeline_kwargs)\n    else:\n        pipeline = pipeline.export_pipeline(**pipeline_kwargs)\n\n    fitted_pipeline = sklearn.base.clone(pipeline)\n    fitted_pipeline.fit(X_train, y_train)\n\n    if len(scorers) &gt; 0:\n        scores =[sklearn.metrics.get_scorer(scorer)(fitted_pipeline, X_test, y_test) for scorer in scorers]\n\n    other_scores = []\n    if other_objective_functions is not None and len(other_objective_functions) &gt;0:\n        other_scores = [obj(sklearn.base.clone(pipeline)) for obj in other_objective_functions]\n\n    return np.concatenate([scores,other_scores])\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/estimator_utils/","title":"Estimator utils","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/tpot_estimator/estimator_utils/#tpot.tpot_estimator.estimator_utils.apply_make_pipeline","title":"<code>apply_make_pipeline(ind, preprocessing_pipeline=None, export_graphpipeline=False, **pipeline_kwargs)</code>","text":"<p>Helper function to create a column of sklearn pipelines from the tpot individual class.</p> <p>Parameters:</p> Name Type Description Default <code>ind</code> <p>The individual to convert to a pipeline.</p> required <code>preprocessing_pipeline</code> <p>The preprocessing pipeline to include before the individual's pipeline.</p> <code>None</code> <code>export_graphpipeline</code> <p>Force the pipeline to be exported as a graph pipeline. Flattens all nested pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.</p> <code>False</code> <code>pipeline_kwargs</code> <p>Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>sklearn estimator</code> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def apply_make_pipeline(ind, preprocessing_pipeline=None, export_graphpipeline=False, **pipeline_kwargs):\n    \"\"\"\n    Helper function to create a column of sklearn pipelines from the tpot individual class.\n\n    Parameters\n    ----------\n    ind: tpot.SklearnIndividual\n        The individual to convert to a pipeline.\n    preprocessing_pipeline: sklearn.pipeline.Pipeline, optional\n        The preprocessing pipeline to include before the individual's pipeline.\n    export_graphpipeline: bool, default=False\n        Force the pipeline to be exported as a graph pipeline. Flattens all nested pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.\n    pipeline_kwargs: dict\n        Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.\n\n    Returns\n    -------\n    sklearn estimator\n    \"\"\"\n\n    try:\n\n        if export_graphpipeline:\n            est = ind.export_flattened_graphpipeline(**pipeline_kwargs)\n        else:\n            est = ind.export_pipeline(**pipeline_kwargs)\n\n\n        if preprocessing_pipeline is None:\n            return est\n        else:\n            return sklearn.pipeline.make_pipeline(sklearn.base.clone(preprocessing_pipeline), est)\n    except:\n        return None\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/estimator_utils/#tpot.tpot_estimator.estimator_utils.check_if_y_is_encoded","title":"<code>check_if_y_is_encoded(y)</code>","text":"<p>Checks if the target y is composed of sequential ints from 0 to N. XGBoost requires the target to be encoded in this way.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <p>The target vector.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the target is encoded as sequential ints from 0 to N, False otherwise</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def check_if_y_is_encoded(y):\n    '''\n    Checks if the target y is composed of sequential ints from 0 to N.\n    XGBoost requires the target to be encoded in this way.\n\n    Parameters\n    ----------\n    y: np.ndarray\n        The target vector.\n\n    Returns\n    -------\n    bool\n        True if the target is encoded as sequential ints from 0 to N, False otherwise\n    '''\n    y = sorted(set(y))\n    return all(i == j for i, j in enumerate(y))\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/estimator_utils/#tpot.tpot_estimator.estimator_utils.convert_parents_tuples_to_integers","title":"<code>convert_parents_tuples_to_integers(row, object_to_int)</code>","text":"<p>Helper function to convert the parent rows into integers representing the index of the parent in the population.</p> <p>Original pandas dataframe using a custom index for the parents. This function converts the custom index to an integer index for easier manipulation by end users.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <p>The row to convert.</p> required <code>object_to_int</code> <p>A dictionary mapping the object to an integer index.</p> required Returns <p>tuple     The row with the custom index converted to an integer index.</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def convert_parents_tuples_to_integers(row, object_to_int):\n    \"\"\"\n    Helper function to convert the parent rows into integers representing the index of the parent in the population.\n\n    Original pandas dataframe using a custom index for the parents. This function converts the custom index to an integer index for easier manipulation by end users.\n\n    Parameters\n    ----------\n    row: list, np.ndarray, tuple\n        The row to convert.\n    object_to_int: dict\n        A dictionary mapping the object to an integer index.\n\n    Returns \n    -------\n    tuple\n        The row with the custom index converted to an integer index.\n    \"\"\"\n    if type(row) == list or type(row) == np.ndarray or type(row) == tuple:\n        return tuple(object_to_int[obj] for obj in row)\n    else:\n        return np.nan\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/estimator_utils/#tpot.tpot_estimator.estimator_utils.objective_function_generator","title":"<code>objective_function_generator(pipeline, x, y, scorers, cv, other_objective_functions, step=None, budget=None, is_classification=True, export_graphpipeline=False, **pipeline_kwargs)</code>","text":"<p>Uses cross validation to evaluate the pipeline using the scorers, and concatenates results with scores from standalone other objective functions.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <p>The individual to evaluate.</p> required <code>x</code> <p>The feature matrix.</p> required <code>y</code> <p>The target vector.</p> required <code>scorers</code> <p>The scorers to use for cross validation.</p> required <code>cv</code> <p>The cross-validator to use. For example, sklearn.model_selection.KFold or sklearn.model_selection.StratifiedKFold. If an int, will use sklearn.model_selection.KFold with n_splits=cv.</p> required <code>other_objective_functions</code> <p>A list of standalone objective functions to evaluate the pipeline. With signature obj(pipeline) -&gt; float. or obj(pipeline) -&gt; np.ndarray These functions take in the unfitted estimator.</p> required <code>step</code> <p>The fold to return the scores for. If None, will return the mean of all the scores (per scorer). Default is None.</p> <code>None</code> <code>budget</code> <p>The budget to subsample the data. If None, will use the full dataset. Default is None. Will subsample budget*len(x) samples.</p> <code>None</code> <code>is_classification</code> <p>If True, will stratify the subsampling. Default is True.</p> <code>True</code> <code>export_graphpipeline</code> <p>Force the pipeline to be exported as a graph pipeline. Flattens all nested sklearn pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.</p> <code>False</code> <code>pipeline_kwargs</code> <p>Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The concatenated scores for the pipeline. The first len(scorers) elements are the cross validation scores, and the remaining elements are the standalone objective functions.</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def objective_function_generator(pipeline, x,y, scorers, cv, other_objective_functions, step=None, budget=None, is_classification=True, export_graphpipeline=False, **pipeline_kwargs):\n    \"\"\"\n    Uses cross validation to evaluate the pipeline using the scorers, and concatenates results with scores from standalone other objective functions.\n\n    Parameters\n    ----------\n    pipeline: tpot.SklearnIndividual\n        The individual to evaluate.\n    x: np.ndarray\n        The feature matrix.\n    y: np.ndarray\n        The target vector.\n    scorers: list\n        The scorers to use for cross validation. \n    cv: int, float, or sklearn cross-validator\n        The cross-validator to use. For example, sklearn.model_selection.KFold or sklearn.model_selection.StratifiedKFold.\n        If an int, will use sklearn.model_selection.KFold with n_splits=cv.\n    other_objective_functions: list\n        A list of standalone objective functions to evaluate the pipeline. With signature obj(pipeline) -&gt; float. or obj(pipeline) -&gt; np.ndarray\n        These functions take in the unfitted estimator.\n    step: int, optional\n        The fold to return the scores for. If None, will return the mean of all the scores (per scorer). Default is None.\n    budget: float, optional\n        The budget to subsample the data. If None, will use the full dataset. Default is None.\n        Will subsample budget*len(x) samples.\n    is_classification: bool, default=True\n        If True, will stratify the subsampling. Default is True.\n    export_graphpipeline: bool, default=False\n        Force the pipeline to be exported as a graph pipeline. Flattens all nested sklearn pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.\n    pipeline_kwargs: dict\n        Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.\n\n    Returns\n    -------\n    np.ndarray\n        The concatenated scores for the pipeline. The first len(scorers) elements are the cross validation scores, and the remaining elements are the standalone objective functions.\n\n    \"\"\"\n\n    if export_graphpipeline:\n        pipeline = pipeline.export_flattened_graphpipeline(**pipeline_kwargs)\n    else:\n        pipeline = pipeline.export_pipeline(**pipeline_kwargs)\n\n    if budget is not None and budget &lt; 1:\n        if is_classification:\n            x,y = sklearn.utils.resample(x,y, stratify=y, n_samples=int(budget*len(x)), replace=False, random_state=1)\n        else:\n            x,y = sklearn.utils.resample(x,y, n_samples=int(budget*len(x)), replace=False, random_state=1)\n\n        if isinstance(cv, int) or isinstance(cv, float):\n            n_splits = cv\n        else:\n            n_splits = cv.n_splits\n\n    if len(scorers) &gt; 0:\n        cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)\n    else:\n        cv_obj_scores = []\n\n    if other_objective_functions is not None and len(other_objective_functions) &gt;0:\n        other_scores = [obj(sklearn.base.clone(pipeline)) for obj in other_objective_functions]\n        #flatten\n        other_scores = np.array(other_scores).flatten().tolist()\n    else:\n        other_scores = []\n\n    return np.concatenate([cv_obj_scores,other_scores])\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/estimator_utils/#tpot.tpot_estimator.estimator_utils.remove_underrepresented_classes","title":"<code>remove_underrepresented_classes(x, y, min_count)</code>","text":"<p>Helper function to remove classes with less than min_count samples from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>The feature matrix.</p> required <code>y</code> <p>The target vector.</p> required <code>min_count</code> <p>The minimum number of samples to keep a class.</p> required <p>Returns:</p> Type Description <code>(ndarray, ndarray)</code> <p>The feature matrix and target vector with rows from classes with less than min_count samples removed.</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def remove_underrepresented_classes(x, y, min_count):\n    \"\"\"\n    Helper function to remove classes with less than min_count samples from the dataset.\n\n    Parameters\n    ----------\n    x: np.ndarray or pd.DataFrame\n        The feature matrix.\n    y: np.ndarray or pd.Series\n        The target vector.\n    min_count: int\n        The minimum number of samples to keep a class.\n\n    Returns\n    -------\n    np.ndarray, np.ndarray\n        The feature matrix and target vector with rows from classes with less than min_count samples removed.\n    \"\"\"\n    if isinstance(y, (np.ndarray, pd.Series)):\n        unique, counts = np.unique(y, return_counts=True)\n        if min(counts) &gt;= min_count:\n            return x, y\n        keep_classes = unique[counts &gt;= min_count]\n        mask = np.isin(y, keep_classes)\n        x = x[mask]\n        y = y[mask]\n    elif isinstance(y, pd.DataFrame):\n        counts = y.apply(pd.Series.value_counts)\n        if min(counts) &gt;= min_count:\n            return x, y\n        keep_classes = counts.index[counts &gt;= min_count].tolist()\n        mask = y.isin(keep_classes).all(axis=1)\n        x = x[mask]\n        y = y[mask]\n    else:\n        raise TypeError(\"y must be a numpy array or a pandas Series/DataFrame\")\n    return x, y\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/estimator_utils/#tpot.tpot_estimator.estimator_utils.val_objective_function_generator","title":"<code>val_objective_function_generator(pipeline, X_train, y_train, X_test, y_test, scorers, other_objective_functions, export_graphpipeline=False, **pipeline_kwargs)</code>","text":"<p>Trains a pipeline on a training set and evaluates it on a test set using the scorers and other objective functions.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <p>The individual to evaluate.</p> required <code>X_train</code> <p>The feature matrix of the training set.</p> required <code>y_train</code> <p>The target vector of the training set.</p> required <code>X_test</code> <p>The feature matrix of the test set.</p> required <code>y_test</code> <p>The target vector of the test set.</p> required <code>scorers</code> <p>The scorers to use for cross validation.</p> required <code>other_objective_functions</code> <p>A list of standalone objective functions to evaluate the pipeline. With signature obj(pipeline) -&gt; float. or obj(pipeline) -&gt; np.ndarray These functions take in the unfitted estimator.</p> required <code>export_graphpipeline</code> <p>Force the pipeline to be exported as a graph pipeline. Flattens all nested sklearn pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.</p> <code>False</code> <code>pipeline_kwargs</code> <p>Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The concatenated scores for the pipeline. The first len(scorers) elements are the cross validation scores, and the remaining elements are the standalone objective functions.</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def val_objective_function_generator(pipeline, X_train, y_train, X_test, y_test, scorers, other_objective_functions, export_graphpipeline=False, **pipeline_kwargs):\n    \"\"\"\n    Trains a pipeline on a training set and evaluates it on a test set using the scorers and other objective functions.\n\n    Parameters\n    ----------\n\n    pipeline: tpot.SklearnIndividual\n        The individual to evaluate.\n    X_train: np.ndarray\n        The feature matrix of the training set.\n    y_train: np.ndarray\n        The target vector of the training set.\n    X_test: np.ndarray\n        The feature matrix of the test set.\n    y_test: np.ndarray\n        The target vector of the test set.\n    scorers: list\n        The scorers to use for cross validation.\n    other_objective_functions: list\n        A list of standalone objective functions to evaluate the pipeline. With signature obj(pipeline) -&gt; float. or obj(pipeline) -&gt; np.ndarray\n        These functions take in the unfitted estimator.\n    export_graphpipeline: bool, default=False\n        Force the pipeline to be exported as a graph pipeline. Flattens all nested sklearn pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.\n    pipeline_kwargs: dict\n        Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.\n\n    Returns\n    -------\n    np.ndarray\n        The concatenated scores for the pipeline. The first len(scorers) elements are the cross validation scores, and the remaining elements are the standalone objective functions.\n\n\n    \"\"\"\n\n    #subsample the data\n    if export_graphpipeline:\n        pipeline = pipeline.export_flattened_graphpipeline(**pipeline_kwargs)\n    else:\n        pipeline = pipeline.export_pipeline(**pipeline_kwargs)\n\n    fitted_pipeline = sklearn.base.clone(pipeline)\n    fitted_pipeline.fit(X_train, y_train)\n\n    if len(scorers) &gt; 0:\n        scores =[sklearn.metrics.get_scorer(scorer)(fitted_pipeline, X_test, y_test) for scorer in scorers]\n\n    other_scores = []\n    if other_objective_functions is not None and len(other_objective_functions) &gt;0:\n        other_scores = [obj(sklearn.base.clone(pipeline)) for obj in other_objective_functions]\n\n    return np.concatenate([scores,other_scores])\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/steady_state_estimator/","title":"Steady state estimator","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/tpot_estimator/steady_state_estimator/#tpot.tpot_estimator.steady_state_estimator.TPOTEstimatorSteadyState","title":"<code>TPOTEstimatorSteadyState</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> Source code in <code>tpot/tpot_estimator/steady_state_estimator.py</code> <pre><code>class TPOTEstimatorSteadyState(BaseEstimator):\n    def __init__(self,  \n                        search_space,\n                        scorers= [],\n                        scorers_weights = [],\n                        classification = False,\n                        cv = 10,\n                        other_objective_functions=[], #tpot.objectives.estimator_objective_functions.number_of_nodes_objective],\n                        other_objective_functions_weights = [],\n                        objective_function_names = None,\n                        bigger_is_better = True,\n\n\n                        export_graphpipeline = False,\n                        memory = None,\n\n                        categorical_features = None,\n                        subsets = None,\n                        preprocessing = False,\n                        validation_strategy = \"none\",\n                        validation_fraction = .2,\n                        disable_label_encoder = False,\n\n                        initial_population_size = 50,\n                        population_size = 50,\n                        max_evaluated_individuals = None,\n\n\n\n                        early_stop = None,\n                        early_stop_mins = None,\n                        scorers_early_stop_tol = 0.001,\n                        other_objectives_early_stop_tol = None,\n                        max_time_mins=None,\n                        max_eval_time_mins=10,\n                        n_jobs=1,\n                        memory_limit = None,\n                        client = None,\n\n                        crossover_probability=.2,\n                        mutate_probability=.7,\n                        mutate_then_crossover_probability=.05,\n                        crossover_then_mutate_probability=.05,\n                        survival_selector = survival_select_NSGA2,\n                        parent_selector = tournament_selection_dominated,\n                        budget_range = None,\n                        budget_scaling = .5,\n                        individuals_until_end_budget = 1,\n                        stepwise_steps = 5,\n\n                        warm_start = False,\n\n                        verbose = 0,\n                        periodic_checkpoint_folder = None,\n                        callback = None,\n                        processes = True,\n\n                        scatter = True,\n\n                        # random seed for random number generator (rng)\n                        random_state = None,\n\n                        optuna_optimize_pareto_front = False,\n                        optuna_optimize_pareto_front_trials = 100,\n                        optuna_optimize_pareto_front_timeout = 60*10,\n                        optuna_storage = \"sqlite:///optuna.db\",\n                        ):\n\n        '''\n        An sklearn baseestimator that uses genetic programming to optimize a pipeline.\n\n        Parameters\n        ----------\n\n        scorers : (list, scorer)\n            A scorer or list of scorers to be used in the cross-validation process.\n            see https://scikit-learn.org/stable/modules/model_evaluation.html\n\n        scorers_weights : list\n            A list of weights to be applied to the scorers during the optimization process.\n\n        classification : bool\n            If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem.\n            Used to determine the CV strategy.\n\n        cv : int, cross-validator\n            - (int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.\n            - (sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.\n\n        other_objective_functions : list, default=[]\n            A list of other objective functions to apply to the pipeline. The function takes a single parameter for the graphpipeline estimator and returns either a single score or a list of scores.\n\n        other_objective_functions_weights : list, default=[]\n            A list of weights to be applied to the other objective functions.\n\n        objective_function_names : list, default=None\n            A list of names to be applied to the objective functions. If None, will use the names of the objective functions.\n\n        bigger_is_better : bool, default=True\n            If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n\n\n        max_size : int, default=np.inf\n            The maximum number of nodes of the pipelines to be generated.\n\n        linear_pipeline : bool, default=False\n            If True, the pipelines generated will be linear. If False, the pipelines generated will be directed acyclic graphs.\n\n        root_config_dict : dict, default='auto'\n            The configuration dictionary to use for the root node of the model.\n            If 'auto', will use \"classifiers\" if classification=True, else \"regressors\".\n            - 'selectors' : A selection of sklearn Selector methods.\n            - 'classifiers' : A selection of sklearn Classifier methods.\n            - 'regressors' : A selection of sklearn Regressor methods.\n            - 'transformers' : A selection of sklearn Transformer methods.\n            - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.\n            - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.\n            - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.\n                                        Subsets are set with the subsets parameter.\n            - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.\n            - 'MDR' : Includes MDR.\n            - 'ContinuousMDR' : Includes ContinuousMDR.\n            - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.\n            - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.\n            - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.\n\n        inner_config_dict : dict, default=[\"selectors\", \"transformers\"]\n            The configuration dictionary to use for the inner nodes of the model generation.\n            Default [\"selectors\", \"transformers\"]\n            - 'selectors' : A selection of sklearn Selector methods.\n            - 'classifiers' : A selection of sklearn Classifier methods.\n            - 'regressors' : A selection of sklearn Regressor methods.\n            - 'transformers' : A selection of sklearn Transformer methods.\n            - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.\n            - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.\n            - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.\n                                        Subsets are set with the subsets parameter.\n            - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.\n            - 'MDR' : Includes MDR.\n            - 'ContinuousMDR' : Includes ContinuousMDR.\n            - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.\n            - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.\n            - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.\n            - None : If None and max_depth&gt;1, the root_config_dict will be used for the inner nodes as well.\n\n        leaf_config_dict : dict, default=None\n            The configuration dictionary to use for the leaf node of the model. If set, leaf nodes must be from this dictionary.\n            Otherwise leaf nodes will be generated from the root_config_dict.\n            Default None\n            - 'selectors' : A selection of sklearn Selector methods.\n            - 'classifiers' : A selection of sklearn Classifier methods.\n            - 'regressors' : A selection of sklearn Regressor methods.\n            - 'transformers' : A selection of sklearn Transformer methods.\n            - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.\n            - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.\n            - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.\n                                        Subsets are set with the subsets parameter.\n            - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.\n            - 'MDR' : Includes MDR.\n            - 'ContinuousMDR' : Includes ContinuousMDR.\n            - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.\n            - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.\n            - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.\n            - None : If None, a leaf will not be required (i.e. the pipeline can be a single root node). Leaf nodes will be generated from the inner_config_dict.\n\n        categorical_features: list or None\n            Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n            - None : If None, TPOT will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing.\n            - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices\n\n\n        memory: Memory object or string, default=None\n            If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. This feature\n            is used to avoid computing the fit transformers within a pipeline if the parameters\n            and input data are identical with another fitted pipeline during optimization process.\n            - String 'auto':\n                TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.\n            - String path of a caching directory\n                TPOT uses memory caching with the provided directory and TPOT does NOT clean\n                the caching directory up upon shutdown. If the directory does not exist, TPOT will\n                create it.\n            - Memory object:\n                TPOT uses the instance of joblib.Memory for memory caching,\n                and TPOT does NOT clean the caching directory up upon shutdown.\n            - None:\n                TPOT does not use memory caching.\n\n        preprocessing : bool or BaseEstimator/Pipeline,\n            EXPERIMENTAL\n            A pipeline that will be used to preprocess the data before CV.\n            - bool : If True, will use a default preprocessing pipeline.\n            - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.\n\n        validation_strategy : str, default='none'\n            EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT may overfit the cross validation score. A second validation set can be used to select the final pipeline.\n            - 'auto' : Automatically determine the validation strategy based on the dataset shape.\n            - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets.\n            - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets.\n            - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.\n\n        validation_fraction : float, default=0.2\n          EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.\n\n        disable_label_encoder : bool, default=False\n            If True, TPOT will check if the target needs to be relabeled to be sequential ints from 0 to N. This is necessary for XGBoost compatibility. If the labels need to be encoded, TPOT will use sklearn.preprocessing.LabelEncoder to encode the labels. The encoder can be accessed via the self.label_encoder_ attribute.\n            If False, no additional label encoders will be used.\n\n        population_size : int, default=50\n            Size of the population\n\n        initial_population_size : int, default=None\n            Size of the initial population. If None, population_size will be used.\n\n        population_scaling : int, default=0.5\n            Scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n\n        generations_until_end_population : int, default=1\n            Number of generations until the population size reaches population_size\n\n        generations : int, default=50\n            Number of generations to run\n\n        early_stop : int, default=None\n            Number of evaluated individuals without improvement before early stopping. Counted across all objectives independently. Triggered when all objectives have not improved by the given number of individuals.\n\n        early_stop_mins : float, default=None\n            Number of seconds without improvement before early stopping. All objectives must not have improved for the given number of seconds for this to be triggered.\n\n        scorers_early_stop_tol :\n            -list of floats\n                list of tolerances for each scorer. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n                If an index of the list is None, that item will not be used for early stopping\n            -int\n                If an int is given, it will be used as the tolerance for all objectives\n\n        other_objectives_early_stop_tol :\n            -list of floats\n                list of tolerances for each of the other objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n                If an index of the list is None, that item will not be used for early stopping\n            -int\n                If an int is given, it will be used as the tolerance for all objectives\n\n        max_time_mins : float, default=float(\"inf\")\n            Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n\n        max_eval_time_mins : float, default=60*5\n            Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n\n        n_jobs : int, default=1\n            Number of processes to run in parallel.\n\n        memory_limit : str, default=None\n            Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n\n        client : dask.distributed.Client, default=None\n            A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.\n\n        crossover_probability : float, default=.2\n            Probability of generating a new individual by crossover between two individuals.\n\n        mutate_probability : float, default=.7\n            Probability of generating a new individual by crossover between one individuals.\n\n        mutate_then_crossover_probability : float, default=.05\n            Probability of generating a new individual by mutating two individuals followed by crossover.\n\n        crossover_then_mutate_probability : float, default=.05\n            Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.\n\n        survival_selector : function, default=survival_select_NSGA2\n            Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes.\n            Used to selected population_size individuals at the start of each generation to use for mutation and crossover.\n\n        parent_selector : function, default=parent_select_NSGA2\n            Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.\n\n        budget_range : list [start, end], default=None\n            A starting and ending budget to use for the budget scaling.\n\n        budget_scaling float : [0,1], default=0.5\n            A scaling factor to use when determining how fast we move the budget from the start to end budget.\n\n        individuals_until_end_budget : int, default=1\n            The number of generations to run before reaching the max budget.\n\n        stepwise_steps : int, default=1\n            The number of staircase steps to take when scaling the budget and population size.\n\n        threshold_evaluation_pruning : list [start, end], default=None\n            starting and ending percentile to use as a threshold for the evaluation early stopping.\n            Values between 0 and 100.\n\n        threshold_evaluation_scaling : float [0,inf), default=0.5\n            A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n            Must be greater than zero. Higher numbers will move the threshold to the end faster.\n\n        min_history_threshold : int, default=0\n            The minimum number of previous scores needed before using threshold early stopping.\n\n        selection_evaluation_pruning : list, default=None\n            A lower and upper percent of the population size to select each round of CV.\n            Values between 0 and 1.\n\n        selection_evaluation_scaling : float, default=0.5\n            A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n            Must be greater than zero. Higher numbers will move the threshold to the end faster.\n\n        n_initial_optimizations : int, default=0\n            Number of individuals to optimize before starting the evolution.\n\n        optimization_cv : int\n           Number of folds to use for the optuna optimization's internal cross-validation.\n\n        max_optimize_time_seconds : float, default=60*5\n            Maximum time to run an optimization\n\n        optimization_steps : int, default=10\n            Number of steps per optimization\n\n        warm_start : bool, default=False\n            If True, will use the continue the evolutionary algorithm from the last generation of the previous run.\n\n\n        verbose : int, default=1\n            How much information to print during the optimization process. Higher values include the information from lower values.\n            0. nothing\n            1. progress bar\n\n            3. best individual\n            4. warnings\n            &gt;=5. full warnings trace\n\n        random_state : int, None, default=None\n            A seed for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes\n            - int\n                Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'\n            - None\n                Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS\n\n\n        periodic_checkpoint_folder : str, default=None\n            Folder to save the population to periodically. If None, no periodic saving will be done.\n            If provided, training will resume from this checkpoint.\n\n        callback : tpot.CallBackInterface, default=None\n            Callback object. Not implemented\n\n        processes : bool, default=True\n            If True, will use multiprocessing to parallelize the optimization process. If False, will use threading.\n            True seems to perform better. However, False is required for interactive debugging.\n\n        Attributes\n        ----------\n\n        fitted_pipeline_ : GraphPipeline\n            A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.\n\n        evaluated_individuals : A pandas data frame containing data for all evaluated individuals in the run.\n            Columns:\n            - *objective functions : The first few columns correspond to the passed in scorers and objective functions\n            - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population.\n            - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population.\n            - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator.\n            - Generation : The generation the pipeline first appeared.\n            - Pareto_Front\t: The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.\n                            To save on computational time, the best frontier is updated iteratively each generation.\n                            The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.\n                            All other pipelines are set to NaN.\n            - Instance\t: The unfitted GraphPipeline BaseEstimator.\n            - *validation objective functions : Objective function scores evaluated on the validation set.\n            - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.\n\n        pareto_front : The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.\n        '''\n\n        # sklearn BaseEstimator must have a corresponding attribute for each parameter.\n        # These should not be modified once set.\n\n        self.search_space = search_space\n        self.scorers = scorers\n        self.scorers_weights = scorers_weights\n        self.classification = classification\n        self.cv = cv\n        self.other_objective_functions = other_objective_functions\n        self.other_objective_functions_weights = other_objective_functions_weights\n        self.objective_function_names = objective_function_names\n        self.bigger_is_better = bigger_is_better\n\n        self.export_graphpipeline = export_graphpipeline\n        self.memory = memory\n\n        self.categorical_features = categorical_features\n        self.preprocessing = preprocessing\n        self.validation_strategy = validation_strategy\n        self.validation_fraction = validation_fraction\n        self.disable_label_encoder = disable_label_encoder\n        self.population_size = population_size\n        self.initial_population_size = initial_population_size\n\n        self.early_stop = early_stop\n        self.early_stop_mins = early_stop_mins\n        self.scorers_early_stop_tol = scorers_early_stop_tol\n        self.other_objectives_early_stop_tol = other_objectives_early_stop_tol\n        self.max_time_mins = max_time_mins\n        self.max_eval_time_mins = max_eval_time_mins\n        self.n_jobs= n_jobs\n        self.memory_limit = memory_limit\n        self.client = client\n\n        self.crossover_probability = crossover_probability\n        self.mutate_probability = mutate_probability\n        self.mutate_then_crossover_probability= mutate_then_crossover_probability\n        self.crossover_then_mutate_probability= crossover_then_mutate_probability\n        self.survival_selector=survival_selector\n        self.parent_selector=parent_selector\n        self.budget_range = budget_range\n        self.budget_scaling = budget_scaling\n        self.individuals_until_end_budget = individuals_until_end_budget\n        self.stepwise_steps = stepwise_steps\n\n        self.warm_start = warm_start\n\n        self.verbose = verbose\n        self.periodic_checkpoint_folder = periodic_checkpoint_folder\n        self.callback = callback\n        self.processes = processes\n\n\n        self.scatter = scatter\n\n        self.optuna_optimize_pareto_front = optuna_optimize_pareto_front\n        self.optuna_optimize_pareto_front_trials = optuna_optimize_pareto_front_trials\n        self.optuna_optimize_pareto_front_timeout = optuna_optimize_pareto_front_timeout\n        self.optuna_storage = optuna_storage\n\n        # create random number generator based on rngseed\n        self.rng = np.random.default_rng(random_state)\n        # save random state passed to us for other functions that use random_state\n        self.random_state = random_state\n\n        self.max_evaluated_individuals = max_evaluated_individuals\n\n        #Initialize other used params\n\n        if self.initial_population_size is None:\n            self._initial_population_size = self.population_size\n        else:\n            self._initial_population_size = self.initial_population_size\n\n        if isinstance(self.scorers, str):\n            self._scorers = [self.scorers]\n\n        elif callable(self.scorers):\n            self._scorers = [self.scorers]\n        else:\n            self._scorers = self.scorers\n\n        self._scorers = [sklearn.metrics.get_scorer(scoring) for scoring in self._scorers]\n        self._scorers_early_stop_tol = self.scorers_early_stop_tol\n\n        self._evolver = tpot.evolvers.SteadyStateEvolver\n\n\n\n        self.objective_function_weights = [*scorers_weights, *other_objective_functions_weights]\n\n\n        if self.objective_function_names is None:\n            obj_names = [f.__name__ for f in other_objective_functions]\n        else:\n            obj_names = self.objective_function_names\n        self.objective_names = [f._score_func.__name__ if hasattr(f,\"_score_func\") else f.__name__ for f in self._scorers] + obj_names\n\n\n        if not isinstance(self.other_objectives_early_stop_tol, list):\n            self._other_objectives_early_stop_tol = [self.other_objectives_early_stop_tol for _ in range(len(self.other_objective_functions))]\n        else:\n            self._other_objectives_early_stop_tol = self.other_objectives_early_stop_tol\n\n        if not isinstance(self._scorers_early_stop_tol, list):\n            self._scorers_early_stop_tol = [self._scorers_early_stop_tol for _ in range(len(self._scorers))]\n        else:\n            self._scorers_early_stop_tol = self._scorers_early_stop_tol\n\n        self.early_stop_tol = [*self._scorers_early_stop_tol, *self._other_objectives_early_stop_tol]\n\n        self._evolver_instance = None\n        self.evaluated_individuals = None\n\n        self.label_encoder_ = None\n\n        set_dask_settings()\n\n\n    def fit(self, X, y):\n        if self.client is not None: #If user passed in a client manually\n           _client = self.client\n        else:\n\n            if self.verbose &gt;= 4:\n                silence_logs = 30\n            elif self.verbose &gt;=5:\n                silence_logs = 40\n            else:\n                silence_logs = 50\n            cluster = LocalCluster(n_workers=self.n_jobs, #if no client is passed in and no global client exists, create our own\n                    threads_per_worker=1,\n                    processes=self.processes,\n                    silence_logs=silence_logs,\n                    memory_limit=self.memory_limit)\n            _client = Client(cluster)\n\n\n        if self.classification and not self.disable_label_encoder and not check_if_y_is_encoded(y):\n            warnings.warn(\"Labels are not encoded as ints from 0 to N. For compatibility with some classifiers such as sklearn, TPOT has encoded y with the sklearn LabelEncoder. When using pipelines outside the main TPOT estimator class, you can encode the labels with est.label_encoder_\")\n            self.label_encoder_ = LabelEncoder()\n            y = self.label_encoder_.fit_transform(y)\n\n        self.evaluated_individuals = None\n        #determine validation strategy\n        if self.validation_strategy == 'auto':\n            nrows = X.shape[0]\n            ncols = X.shape[1]\n\n            if nrows/ncols &lt; 20:\n                validation_strategy = 'reshuffled'\n            elif nrows/ncols &lt; 100:\n                validation_strategy = 'split'\n            else:\n                validation_strategy = 'none'\n        else:\n            validation_strategy = self.validation_strategy\n\n        if validation_strategy == 'split':\n            if self.classification:\n                X, X_val, y, y_val = train_test_split(X, y, test_size=self.validation_fraction, stratify=y, random_state=self.random_state)\n            else:\n                X, X_val, y, y_val = train_test_split(X, y, test_size=self.validation_fraction, random_state=self.random_state)\n\n\n        X_original = X\n        y_original = y\n        if isinstance(self.cv, int) or isinstance(self.cv, float):\n            n_folds = self.cv\n        else:\n            n_folds = self.cv.get_n_splits(X, y)\n\n        if self.classification:\n            X, y = remove_underrepresented_classes(X, y, n_folds)\n\n        if self.preprocessing:\n            #X = pd.DataFrame(X)\n\n            if not isinstance(self.preprocessing, bool) and isinstance(self.preprocessing, sklearn.base.BaseEstimator):\n                self._preprocessing_pipeline = self.preprocessing\n\n            #TODO: check if there are missing values in X before imputation. If not, don't include imputation in pipeline. Check if there are categorical columns. If not, don't include one hot encoding in pipeline\n            else: #if self.preprocessing is True or not a sklearn estimator\n\n                pipeline_steps = []\n\n                if self.categorical_features is not None: #if categorical features are specified, use those\n                    pipeline_steps.append((\"impute_categorical\", tpot.builtin_modules.ColumnSimpleImputer(self.categorical_features, strategy='most_frequent')))\n                    pipeline_steps.append((\"impute_numeric\", tpot.builtin_modules.ColumnSimpleImputer(\"numeric\", strategy='mean')))\n                    pipeline_steps.append((\"ColumnOneHotEncoder\", tpot.builtin_modules.ColumnOneHotEncoder(self.categorical_features, strategy='most_frequent')))\n\n                else:\n                    if isinstance(X, pd.DataFrame):\n                        categorical_columns = X.select_dtypes(include=['object']).columns\n                        if len(categorical_columns) &gt; 0:\n                            pipeline_steps.append((\"impute_categorical\", tpot.builtin_modules.ColumnSimpleImputer(\"categorical\", strategy='most_frequent')))\n                            pipeline_steps.append((\"impute_numeric\", tpot.builtin_modules.ColumnSimpleImputer(\"numeric\", strategy='mean')))\n                            pipeline_steps.append((\"ColumnOneHotEncoder\", tpot.builtin_modules.ColumnOneHotEncoder(\"categorical\", strategy='most_frequent')))\n                        else:\n                            pipeline_steps.append((\"impute_numeric\", tpot.builtin_modules.ColumnSimpleImputer(\"all\", strategy='mean')))\n                    else:\n                        pipeline_steps.append((\"impute_numeric\", tpot.builtin_modules.ColumnSimpleImputer(\"all\", strategy='mean')))\n\n                self._preprocessing_pipeline = sklearn.pipeline.Pipeline(pipeline_steps)\n\n            X = self._preprocessing_pipeline.fit_transform(X, y)\n\n        else:\n            self._preprocessing_pipeline = None\n\n        #_, y = sklearn.utils.check_X_y(X, y, y_numeric=True)\n\n        #Set up the configuation dictionaries and the search spaces\n\n        #check if self.cv is a number\n        if isinstance(self.cv, int) or isinstance(self.cv, float):\n            if self.classification:\n                self.cv_gen = sklearn.model_selection.StratifiedKFold(n_splits=self.cv, shuffle=True, random_state=self.random_state)\n            else:\n                self.cv_gen = sklearn.model_selection.KFold(n_splits=self.cv, shuffle=True, random_state=self.random_state)\n\n        else:\n            self.cv_gen = sklearn.model_selection.check_cv(self.cv, y, classifier=self.classification)\n\n\n        n_samples= int(math.floor(X.shape[0]/n_folds))\n        n_features=X.shape[1]\n\n        if isinstance(X, pd.DataFrame):\n            self.feature_names = X.columns\n        else:\n            self.feature_names = None\n\n\n\n\n        def objective_function(pipeline_individual,\n                                            X,\n                                            y,\n                                            is_classification=self.classification,\n                                            scorers= self._scorers,\n                                            cv=self.cv_gen,\n                                            other_objective_functions=self.other_objective_functions,\n                                            export_graphpipeline=self.export_graphpipeline,\n                                            memory=self.memory,\n                                            **kwargs):\n            return objective_function_generator(\n                pipeline_individual,\n                X,\n                y,\n                is_classification=is_classification,\n                scorers= scorers,\n                cv=cv,\n                other_objective_functions=other_objective_functions,\n                export_graphpipeline=export_graphpipeline,\n                memory=memory,\n                **kwargs,\n            )\n\n        def ind_generator(rng):\n            rng = np.random.default_rng(rng)\n            while True:\n                yield self.search_space.generate(rng)\n\n\n\n        if self.scatter:\n            X_future = _client.scatter(X)\n            y_future = _client.scatter(y)\n        else:\n            X_future = X\n            y_future = y\n\n        #If warm start and we have an evolver instance, use the existing one\n        if not(self.warm_start and self._evolver_instance is not None):\n            self._evolver_instance = self._evolver(   individual_generator=ind_generator(self.rng),\n                                            objective_functions= [objective_function],\n                                            objective_function_weights = self.objective_function_weights,\n                                            objective_names=self.objective_names,\n                                            bigger_is_better = self.bigger_is_better,\n                                            population_size= self.population_size,\n\n                                            initial_population_size = self._initial_population_size,\n                                            n_jobs=self.n_jobs,\n                                            verbose = self.verbose,\n                                            max_time_mins =      self.max_time_mins ,\n                                            max_eval_time_mins = self.max_eval_time_mins,\n\n\n\n                                            periodic_checkpoint_folder = self.periodic_checkpoint_folder,\n\n\n                                            early_stop_tol = self.early_stop_tol,\n                                            early_stop= self.early_stop,\n                                            early_stop_mins =  self.early_stop_mins,\n\n                                            budget_range = self.budget_range,\n                                            budget_scaling = self.budget_scaling,\n                                            individuals_until_end_budget = self.individuals_until_end_budget,\n\n\n                                            stepwise_steps = self.stepwise_steps,\n                                            client = _client,\n                                            objective_kwargs = {\"X\": X_future, \"y\": y_future},\n                                            survival_selector=self.survival_selector,\n                                            parent_selector=self.parent_selector,\n\n                                            crossover_probability = self.crossover_probability,\n                                            mutate_probability = self.mutate_probability,\n                                            mutate_then_crossover_probability= self.mutate_then_crossover_probability,\n                                            crossover_then_mutate_probability= self.crossover_then_mutate_probability,\n\n\n                                            max_evaluated_individuals = self.max_evaluated_individuals,\n\n                                            rng=self.rng,\n                                            )\n\n\n        self._evolver_instance.optimize()\n        #self._evolver_instance.population.update_pareto_fronts(self.objective_names, self.objective_function_weights)\n        self.make_evaluated_individuals()\n\n\n        if self.optuna_optimize_pareto_front:\n            pareto_front_inds = self.pareto_front['Individual'].values\n            all_graphs, all_scores = tpot.individual_representations.graph_pipeline_individual.simple_parallel_optuna(pareto_front_inds,  objective_function, self.objective_function_weights, _client, storage=self.optuna_storage, steps=self.optuna_optimize_pareto_front_trials, verbose=self.verbose, max_eval_time_mins=self.max_eval_time_mins, max_time_mins=self.optuna_optimize_pareto_front_timeout, **{\"X\": X, \"y\": y})\n            all_scores = tpot.utils.eval_utils.process_scores(all_scores, len(self.objective_function_weights))\n\n            if len(all_graphs) &gt; 0:\n                df = pd.DataFrame(np.column_stack((all_graphs, all_scores,np.repeat(\"Optuna\",len(all_graphs)))), columns=[\"Individual\"] + self.objective_names +[\"Parents\"])\n                for obj in self.objective_names:\n                    df[obj] = df[obj].apply(convert_to_float)\n\n                self.evaluated_individuals = pd.concat([self.evaluated_individuals, df], ignore_index=True)\n            else:\n                print(\"WARNING NO OPTUNA TRIALS COMPLETED\")\n\n        tpot.utils.get_pareto_frontier(self.evaluated_individuals, column_names=self.objective_names, weights=self.objective_function_weights)\n\n        if validation_strategy == 'reshuffled':\n            best_pareto_front_idx = list(self.pareto_front.index)\n            best_pareto_front = list(self.pareto_front.loc[best_pareto_front_idx]['Individual'])\n\n            #reshuffle rows\n            X, y = sklearn.utils.shuffle(X, y, random_state=self.random_state)\n\n            if self.scatter:\n                X_future = _client.scatter(X)\n                y_future = _client.scatter(y)\n            else:\n                X_future = X\n                y_future = y\n\n            val_objective_function_list = [lambda   ind,\n                                                    X,\n                                                    y,\n                                                    is_classification=self.classification,\n                                                    scorers= self._scorers,\n                                                    cv=self.cv_gen,\n                                                    other_objective_functions=self.other_objective_functions,\n                                                    export_graphpipeline=self.export_graphpipeline,\n                                                    memory=self.memory,\n\n                                                    **kwargs: objective_function_generator(\n                                                                                                ind,\n                                                                                                X,\n                                                                                                y,\n                                                                                                is_classification=is_classification,\n                                                                                                scorers= scorers,\n                                                                                                cv=cv,\n                                                                                                other_objective_functions=other_objective_functions,\n                                                                                                export_graphpipeline=export_graphpipeline,\n                                                                                                memory=memory,\n                                                                                                **kwargs,\n                                                                                                )]\n\n            objective_kwargs = {\"X\": X_future, \"y\": y_future}\n            val_scores, start_times, end_times, eval_errors = tpot.utils.eval_utils.parallel_eval_objective_list(best_pareto_front, val_objective_function_list, verbose=self.verbose, max_eval_time_mins=self.max_eval_time_mins, n_expected_columns=len(self.objective_names), client=_client, **objective_kwargs)\n\n            val_objective_names = ['validation_'+name for name in self.objective_names]\n            self.objective_names_for_selection = val_objective_names\n            self.evaluated_individuals.loc[best_pareto_front_idx,val_objective_names] = val_scores\n            self.evaluated_individuals.loc[best_pareto_front_idx,'validation_start_times'] = start_times\n            self.evaluated_individuals.loc[best_pareto_front_idx,'validation_end_times'] = end_times\n            self.evaluated_individuals.loc[best_pareto_front_idx,'validation_eval_errors'] = eval_errors\n\n            self.evaluated_individuals[\"Validation_Pareto_Front\"] = tpot.utils.get_pareto_frontier(self.evaluated_individuals, column_names=val_objective_names, weights=self.objective_function_weights)\n        elif validation_strategy == 'split':\n\n\n            if self.scatter:\n                X_future = _client.scatter(X)\n                y_future = _client.scatter(y)\n                X_val_future = _client.scatter(X_val)\n                y_val_future = _client.scatter(y_val)\n            else:\n                X_future = X\n                y_future = y\n                X_val_future = X_val\n                y_val_future = y_val\n\n            objective_kwargs = {\"X\": X_future, \"y\": y_future, \"X_val\" : X_val_future, \"y_val\":y_val_future }\n\n            best_pareto_front_idx = list(self.pareto_front.index)\n            best_pareto_front = list(self.pareto_front.loc[best_pareto_front_idx]['Individual'])\n            val_objective_function_list = [lambda   ind,\n                                                    X,\n                                                    y,\n                                                    X_val,\n                                                    y_val,\n                                                    scorers= self._scorers,\n                                                    other_objective_functions=self.other_objective_functions,\n                                                    export_graphpipeline=self.export_graphpipeline,\n                                                    memory=self.memory,\n                                                    **kwargs: val_objective_function_generator(\n                                                        ind,\n                                                        X,\n                                                        y,\n                                                        X_val,\n                                                        y_val,\n                                                        scorers= scorers,\n                                                        other_objective_functions=other_objective_functions,\n                                                        export_graphpipeline=export_graphpipeline,\n                                                        memory=memory,\n                                                        **kwargs,\n                                                        )]\n\n            val_scores, start_times, end_times, eval_errors = tpot.utils.eval_utils.parallel_eval_objective_list(best_pareto_front, val_objective_function_list, verbose=self.verbose, max_eval_time_mins=self.max_eval_time_mins, n_expected_columns=len(self.objective_names), client=_client, **objective_kwargs)\n\n\n\n            val_objective_names = ['validation_'+name for name in self.objective_names]\n            self.objective_names_for_selection = val_objective_names\n            self.evaluated_individuals.loc[best_pareto_front_idx,val_objective_names] = val_scores\n            self.evaluated_individuals.loc[best_pareto_front_idx,'validation_start_times'] = start_times\n            self.evaluated_individuals.loc[best_pareto_front_idx,'validation_end_times'] = end_times\n            self.evaluated_individuals.loc[best_pareto_front_idx,'validation_eval_errors'] = eval_errors\n\n            self.evaluated_individuals[\"Validation_Pareto_Front\"] = tpot.utils.get_pareto_frontier(self.evaluated_individuals, column_names=val_objective_names, weights=self.objective_function_weights)\n        else:\n            self.objective_names_for_selection = self.objective_names\n\n        val_scores = self.evaluated_individuals[self.evaluated_individuals[self.objective_names_for_selection].isin([\"TIMEOUT\",\"INVALID\"]).any(axis=1).ne(True)][self.objective_names_for_selection].astype(float)\n        weighted_scores = val_scores*self.objective_function_weights\n\n        if self.bigger_is_better:\n            best_indices = list(weighted_scores.sort_values(by=self.objective_names_for_selection, ascending=False).index)\n        else:\n            best_indices = list(weighted_scores.sort_values(by=self.objective_names_for_selection, ascending=True).index)\n\n        for best_idx in best_indices:\n\n            best_individual = self.evaluated_individuals.loc[best_idx]['Individual']\n            self.selected_best_score =  self.evaluated_individuals.loc[best_idx]\n\n\n            #TODO\n            #best_individual_pipeline = best_individual.export_pipeline(memory=self.memory, cross_val_predict_cv=self.cross_val_predict_cv)\n            if self.export_graphpipeline:\n                best_individual_pipeline = best_individual.export_flattened_graphpipeline(memory=self.memory)\n            else:\n                best_individual_pipeline = best_individual.export_pipeline(memory=self.memory)\n\n            if self.preprocessing:\n                self.fitted_pipeline_ = sklearn.pipeline.make_pipeline(sklearn.base.clone(self._preprocessing_pipeline), best_individual_pipeline )\n            else:\n                self.fitted_pipeline_ = best_individual_pipeline\n\n            try:\n                self.fitted_pipeline_.fit(X_original,y_original) #TODO use y_original as well?\n                break\n            except Exception as e:\n                if self.verbose &gt;= 4:\n                    warnings.warn(\"Final pipeline failed to fit. Rarely, the pipeline might work on the objective function but fail on the full dataset. Generally due to interactions with different features being selected or transformations having different properties. Trying next pipeline\")\n                    print(e)\n                continue\n\n\n        if self.client is None: #no client was passed in\n            #close cluster and client\n            # _client.close()\n            # cluster.close()\n            try:\n                _client.shutdown()\n                cluster.close()\n            #catch exception\n            except Exception as e:\n                print(\"Error shutting down client and cluster\")\n                Warning(e)\n\n        return self\n\n    def _estimator_has(attr):\n        '''Check if we can delegate a method to the underlying estimator.\n        First, we check the first fitted final estimator if available, otherwise we\n        check the unfitted final estimator.\n        '''\n        return  lambda self: (self.fitted_pipeline_ is not None and\n            hasattr(self.fitted_pipeline_, attr)\n        )\n\n\n\n\n\n\n    @available_if(_estimator_has('predict'))\n    def predict(self, X, **predict_params):\n        check_is_fitted(self)\n        #X = check_array(X)\n        preds = self.fitted_pipeline_.predict(X,**predict_params)\n        if self.classification and self.label_encoder_:\n            preds = self.label_encoder_.inverse_transform(preds)\n\n        return preds\n\n    @available_if(_estimator_has('predict_proba'))\n    def predict_proba(self, X, **predict_params):\n        check_is_fitted(self)\n        #X = check_array(X)\n        return self.fitted_pipeline_.predict_proba(X,**predict_params)\n\n    @available_if(_estimator_has('decision_function'))\n    def decision_function(self, X, **predict_params):\n        check_is_fitted(self)\n        #X = check_array(X)\n        return self.fitted_pipeline_.decision_function(X,**predict_params)\n\n    @available_if(_estimator_has('transform'))\n    def transform(self, X, **predict_params):\n        check_is_fitted(self)\n        #X = check_array(X)\n        return self.fitted_pipeline_.transform(X,**predict_params)\n\n    @property\n    def classes_(self):\n        \"\"\"The classes labels. Only exist if the last step is a classifier.\"\"\"\n\n        if self.label_encoder_:\n            return self.label_encoder_.classes_\n        else:\n            return self.fitted_pipeline_.classes_\n\n    @property\n    def _estimator_type(self):\n        return self.fitted_pipeline_._estimator_type\n\n    def make_evaluated_individuals(self):\n        #check if _evolver_instance exists\n        if self.evaluated_individuals is None:\n            self.evaluated_individuals  =  self._evolver_instance.population.evaluated_individuals.copy()\n            objects = list(self.evaluated_individuals.index)\n            object_to_int = dict(zip(objects, range(len(objects))))\n            self.evaluated_individuals = self.evaluated_individuals.set_index(self.evaluated_individuals.index.map(object_to_int))\n            self.evaluated_individuals['Parents'] = self.evaluated_individuals['Parents'].apply(lambda row: convert_parents_tuples_to_integers(row, object_to_int))\n\n            self.evaluated_individuals[\"Instance\"] = self.evaluated_individuals[\"Individual\"].apply(lambda ind: apply_make_pipeline(ind, preprocessing_pipeline=self._preprocessing_pipeline, export_graphpipeline=self.export_graphpipeline, memory=self.memory))\n\n        return self.evaluated_individuals\n\n    @property\n    def pareto_front(self):\n        #check if _evolver_instance exists\n        if self.evaluated_individuals is None:\n            return None\n        else:\n            if \"Pareto_Front\" not in self.evaluated_individuals:\n                return self.evaluated_individuals\n            else:\n                return self.evaluated_individuals[self.evaluated_individuals[\"Pareto_Front\"]==1]\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/steady_state_estimator/#tpot.tpot_estimator.steady_state_estimator.TPOTEstimatorSteadyState.classes_","title":"<code>classes_</code>  <code>property</code>","text":"<p>The classes labels. Only exist if the last step is a classifier.</p>"},{"location":"documentation/tpot/tpot_estimator/steady_state_estimator/#tpot.tpot_estimator.steady_state_estimator.TPOTEstimatorSteadyState.__init__","title":"<code>__init__(search_space, scorers=[], scorers_weights=[], classification=False, cv=10, other_objective_functions=[], other_objective_functions_weights=[], objective_function_names=None, bigger_is_better=True, export_graphpipeline=False, memory=None, categorical_features=None, subsets=None, preprocessing=False, validation_strategy='none', validation_fraction=0.2, disable_label_encoder=False, initial_population_size=50, population_size=50, max_evaluated_individuals=None, early_stop=None, early_stop_mins=None, scorers_early_stop_tol=0.001, other_objectives_early_stop_tol=None, max_time_mins=None, max_eval_time_mins=10, n_jobs=1, memory_limit=None, client=None, crossover_probability=0.2, mutate_probability=0.7, mutate_then_crossover_probability=0.05, crossover_then_mutate_probability=0.05, survival_selector=survival_select_NSGA2, parent_selector=tournament_selection_dominated, budget_range=None, budget_scaling=0.5, individuals_until_end_budget=1, stepwise_steps=5, warm_start=False, verbose=0, periodic_checkpoint_folder=None, callback=None, processes=True, scatter=True, random_state=None, optuna_optimize_pareto_front=False, optuna_optimize_pareto_front_trials=100, optuna_optimize_pareto_front_timeout=60 * 10, optuna_storage='sqlite:///optuna.db')</code>","text":"<p>An sklearn baseestimator that uses genetic programming to optimize a pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>scorers</code> <code>(list, scorer)</code> <p>A scorer or list of scorers to be used in the cross-validation process. see https://scikit-learn.org/stable/modules/model_evaluation.html</p> <code>[]</code> <code>scorers_weights</code> <code>list</code> <p>A list of weights to be applied to the scorers during the optimization process.</p> <code>[]</code> <code>classification</code> <code>bool</code> <p>If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem. Used to determine the CV strategy.</p> <code>False</code> <code>cv</code> <code>(int, cross - validator)</code> <ul> <li>(int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.</li> <li>(sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.</li> </ul> <code>10</code> <code>other_objective_functions</code> <code>list</code> <p>A list of other objective functions to apply to the pipeline. The function takes a single parameter for the graphpipeline estimator and returns either a single score or a list of scores.</p> <code>[]</code> <code>other_objective_functions_weights</code> <code>list</code> <p>A list of weights to be applied to the other objective functions.</p> <code>[]</code> <code>objective_function_names</code> <code>list</code> <p>A list of names to be applied to the objective functions. If None, will use the names of the objective functions.</p> <code>None</code> <code>bigger_is_better</code> <code>bool</code> <p>If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.</p> <code>True</code> <code>max_size</code> <code>int</code> <p>The maximum number of nodes of the pipelines to be generated.</p> <code>np.inf</code> <code>linear_pipeline</code> <code>bool</code> <p>If True, the pipelines generated will be linear. If False, the pipelines generated will be directed acyclic graphs.</p> <code>False</code> <code>root_config_dict</code> <code>dict</code> <p>The configuration dictionary to use for the root node of the model. If 'auto', will use \"classifiers\" if classification=True, else \"regressors\". - 'selectors' : A selection of sklearn Selector methods. - 'classifiers' : A selection of sklearn Classifier methods. - 'regressors' : A selection of sklearn Regressor methods. - 'transformers' : A selection of sklearn Transformer methods. - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators. - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes. - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.                             Subsets are set with the subsets parameter. - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF. - 'MDR' : Includes MDR. - 'ContinuousMDR' : Includes ContinuousMDR. - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL. - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL. - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.</p> <code>'auto'</code> <code>inner_config_dict</code> <code>dict</code> <p>The configuration dictionary to use for the inner nodes of the model generation. Default [\"selectors\", \"transformers\"] - 'selectors' : A selection of sklearn Selector methods. - 'classifiers' : A selection of sklearn Classifier methods. - 'regressors' : A selection of sklearn Regressor methods. - 'transformers' : A selection of sklearn Transformer methods. - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators. - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes. - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.                             Subsets are set with the subsets parameter. - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF. - 'MDR' : Includes MDR. - 'ContinuousMDR' : Includes ContinuousMDR. - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL. - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL. - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary. - None : If None and max_depth&gt;1, the root_config_dict will be used for the inner nodes as well.</p> <code>[\"selectors\", \"transformers\"]</code> <code>leaf_config_dict</code> <code>dict</code> <p>The configuration dictionary to use for the leaf node of the model. If set, leaf nodes must be from this dictionary. Otherwise leaf nodes will be generated from the root_config_dict. Default None - 'selectors' : A selection of sklearn Selector methods. - 'classifiers' : A selection of sklearn Classifier methods. - 'regressors' : A selection of sklearn Regressor methods. - 'transformers' : A selection of sklearn Transformer methods. - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators. - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes. - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.                             Subsets are set with the subsets parameter. - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF. - 'MDR' : Includes MDR. - 'ContinuousMDR' : Includes ContinuousMDR. - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL. - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL. - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary. - None : If None, a leaf will not be required (i.e. the pipeline can be a single root node). Leaf nodes will be generated from the inner_config_dict.</p> <code>None</code> <code>categorical_features</code> <p>Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False. - None : If None, TPOT will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing. - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices</p> <code>None</code> <code>memory</code> <p>If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. This feature is used to avoid computing the fit transformers within a pipeline if the parameters and input data are identical with another fitted pipeline during optimization process. - String 'auto':     TPOT uses memory caching with a temporary directory and cleans it up upon shutdown. - String path of a caching directory     TPOT uses memory caching with the provided directory and TPOT does NOT clean     the caching directory up upon shutdown. If the directory does not exist, TPOT will     create it. - Memory object:     TPOT uses the instance of joblib.Memory for memory caching,     and TPOT does NOT clean the caching directory up upon shutdown. - None:     TPOT does not use memory caching.</p> <code>None</code> <code>preprocessing</code> <code>(bool or BaseEstimator / Pipeline)</code> <p>EXPERIMENTAL A pipeline that will be used to preprocess the data before CV. - bool : If True, will use a default preprocessing pipeline. - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.</p> <code>False</code> <code>validation_strategy</code> <code>str</code> <p>EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT may overfit the cross validation score. A second validation set can be used to select the final pipeline. - 'auto' : Automatically determine the validation strategy based on the dataset shape. - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets. - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets. - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.</p> <code>'none'</code> <code>validation_fraction</code> <code>float</code> <p>EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.</p> <code>0.2</code> <code>disable_label_encoder</code> <code>bool</code> <p>If True, TPOT will check if the target needs to be relabeled to be sequential ints from 0 to N. This is necessary for XGBoost compatibility. If the labels need to be encoded, TPOT will use sklearn.preprocessing.LabelEncoder to encode the labels. The encoder can be accessed via the self.label_encoder_ attribute. If False, no additional label encoders will be used.</p> <code>False</code> <code>population_size</code> <code>int</code> <p>Size of the population</p> <code>50</code> <code>initial_population_size</code> <code>int</code> <p>Size of the initial population. If None, population_size will be used.</p> <code>None</code> <code>population_scaling</code> <code>int</code> <p>Scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.</p> <code>0.5</code> <code>generations_until_end_population</code> <code>int</code> <p>Number of generations until the population size reaches population_size</p> <code>1</code> <code>generations</code> <code>int</code> <p>Number of generations to run</p> <code>50</code> <code>early_stop</code> <code>int</code> <p>Number of evaluated individuals without improvement before early stopping. Counted across all objectives independently. Triggered when all objectives have not improved by the given number of individuals.</p> <code>None</code> <code>early_stop_mins</code> <code>float</code> <p>Number of seconds without improvement before early stopping. All objectives must not have improved for the given number of seconds for this to be triggered.</p> <code>None</code> <code>scorers_early_stop_tol</code> <p>-list of floats     list of tolerances for each scorer. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged     If an index of the list is None, that item will not be used for early stopping -int     If an int is given, it will be used as the tolerance for all objectives</p> <code>0.001</code> <code>other_objectives_early_stop_tol</code> <p>-list of floats     list of tolerances for each of the other objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged     If an index of the list is None, that item will not be used for early stopping -int     If an int is given, it will be used as the tolerance for all objectives</p> <code>None</code> <code>max_time_mins</code> <code>float</code> <p>Maximum time to run the optimization. If none or inf, will run until the end of the generations.</p> <code>float(\"inf\")</code> <code>max_eval_time_mins</code> <code>float</code> <p>Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.</p> <code>60*5</code> <code>n_jobs</code> <code>int</code> <p>Number of processes to run in parallel.</p> <code>1</code> <code>memory_limit</code> <code>str</code> <p>Memory limit for each job. See Dask LocalCluster documentation for more information.</p> <code>None</code> <code>client</code> <code>Client</code> <p>A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.</p> <code>None</code> <code>crossover_probability</code> <code>float</code> <p>Probability of generating a new individual by crossover between two individuals.</p> <code>.2</code> <code>mutate_probability</code> <code>float</code> <p>Probability of generating a new individual by crossover between one individuals.</p> <code>.7</code> <code>mutate_then_crossover_probability</code> <code>float</code> <p>Probability of generating a new individual by mutating two individuals followed by crossover.</p> <code>.05</code> <code>crossover_then_mutate_probability</code> <code>float</code> <p>Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.</p> <code>.05</code> <code>survival_selector</code> <code>function</code> <p>Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes. Used to selected population_size individuals at the start of each generation to use for mutation and crossover.</p> <code>survival_select_NSGA2</code> <code>parent_selector</code> <code>function</code> <p>Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.</p> <code>parent_select_NSGA2</code> <code>budget_range</code> <code>list[start, end]</code> <p>A starting and ending budget to use for the budget scaling.</p> <code>None</code> <code>budget_scaling</code> <p>A scaling factor to use when determining how fast we move the budget from the start to end budget.</p> <code>0.5</code> <code>individuals_until_end_budget</code> <code>int</code> <p>The number of generations to run before reaching the max budget.</p> <code>1</code> <code>stepwise_steps</code> <code>int</code> <p>The number of staircase steps to take when scaling the budget and population size.</p> <code>1</code> <code>threshold_evaluation_pruning</code> <code>list[start, end]</code> <p>starting and ending percentile to use as a threshold for the evaluation early stopping. Values between 0 and 100.</p> <code>None</code> <code>threshold_evaluation_scaling</code> <code>float [0,inf)</code> <p>A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile. Must be greater than zero. Higher numbers will move the threshold to the end faster.</p> <code>0.5</code> <code>min_history_threshold</code> <code>int</code> <p>The minimum number of previous scores needed before using threshold early stopping.</p> <code>0</code> <code>selection_evaluation_pruning</code> <code>list</code> <p>A lower and upper percent of the population size to select each round of CV. Values between 0 and 1.</p> <code>None</code> <code>selection_evaluation_scaling</code> <code>float</code> <p>A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile. Must be greater than zero. Higher numbers will move the threshold to the end faster.</p> <code>0.5</code> <code>n_initial_optimizations</code> <code>int</code> <p>Number of individuals to optimize before starting the evolution.</p> <code>0</code> <code>optimization_cv</code> <code>int</code> <p>Number of folds to use for the optuna optimization's internal cross-validation.</p> required <code>max_optimize_time_seconds</code> <code>float</code> <p>Maximum time to run an optimization</p> <code>60*5</code> <code>optimization_steps</code> <code>int</code> <p>Number of steps per optimization</p> <code>10</code> <code>warm_start</code> <code>bool</code> <p>If True, will use the continue the evolutionary algorithm from the last generation of the previous run.</p> <code>False</code> <code>verbose</code> <code>int</code> <p>How much information to print during the optimization process. Higher values include the information from lower values. 0. nothing 1. progress bar</p> <ol> <li>best individual</li> <li>warnings <p>=5. full warnings trace</p> </li> </ol> <code>1</code> <code>random_state</code> <code>(int, None)</code> <p>A seed for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes - int     Will be used to create and lock in Generator instance with 'numpy.random.default_rng()' - None     Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS</p> <code>None</code> <code>periodic_checkpoint_folder</code> <code>str</code> <p>Folder to save the population to periodically. If None, no periodic saving will be done. If provided, training will resume from this checkpoint.</p> <code>None</code> <code>callback</code> <code>CallBackInterface</code> <p>Callback object. Not implemented</p> <code>None</code> <code>processes</code> <code>bool</code> <p>If True, will use multiprocessing to parallelize the optimization process. If False, will use threading. True seems to perform better. However, False is required for interactive debugging.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>fitted_pipeline_</code> <code>GraphPipeline</code> <p>A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.</p> <code>evaluated_individuals</code> <code>A pandas data frame containing data for all evaluated individuals in the run.</code> <p>Columns: - objective functions : The first few columns correspond to the passed in scorers and objective functions - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population. - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population. - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator. - Generation : The generation the pipeline first appeared. - Pareto_Front      : The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.                 To save on computational time, the best frontier is updated iteratively each generation.                 The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.                 All other pipelines are set to NaN. - Instance  : The unfitted GraphPipeline BaseEstimator. - validation objective functions : Objective function scores evaluated on the validation set. - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.</p> <code>pareto_front</code> <code>The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.</code> Source code in <code>tpot/tpot_estimator/steady_state_estimator.py</code> <pre><code>def __init__(self,  \n                    search_space,\n                    scorers= [],\n                    scorers_weights = [],\n                    classification = False,\n                    cv = 10,\n                    other_objective_functions=[], #tpot.objectives.estimator_objective_functions.number_of_nodes_objective],\n                    other_objective_functions_weights = [],\n                    objective_function_names = None,\n                    bigger_is_better = True,\n\n\n                    export_graphpipeline = False,\n                    memory = None,\n\n                    categorical_features = None,\n                    subsets = None,\n                    preprocessing = False,\n                    validation_strategy = \"none\",\n                    validation_fraction = .2,\n                    disable_label_encoder = False,\n\n                    initial_population_size = 50,\n                    population_size = 50,\n                    max_evaluated_individuals = None,\n\n\n\n                    early_stop = None,\n                    early_stop_mins = None,\n                    scorers_early_stop_tol = 0.001,\n                    other_objectives_early_stop_tol = None,\n                    max_time_mins=None,\n                    max_eval_time_mins=10,\n                    n_jobs=1,\n                    memory_limit = None,\n                    client = None,\n\n                    crossover_probability=.2,\n                    mutate_probability=.7,\n                    mutate_then_crossover_probability=.05,\n                    crossover_then_mutate_probability=.05,\n                    survival_selector = survival_select_NSGA2,\n                    parent_selector = tournament_selection_dominated,\n                    budget_range = None,\n                    budget_scaling = .5,\n                    individuals_until_end_budget = 1,\n                    stepwise_steps = 5,\n\n                    warm_start = False,\n\n                    verbose = 0,\n                    periodic_checkpoint_folder = None,\n                    callback = None,\n                    processes = True,\n\n                    scatter = True,\n\n                    # random seed for random number generator (rng)\n                    random_state = None,\n\n                    optuna_optimize_pareto_front = False,\n                    optuna_optimize_pareto_front_trials = 100,\n                    optuna_optimize_pareto_front_timeout = 60*10,\n                    optuna_storage = \"sqlite:///optuna.db\",\n                    ):\n\n    '''\n    An sklearn baseestimator that uses genetic programming to optimize a pipeline.\n\n    Parameters\n    ----------\n\n    scorers : (list, scorer)\n        A scorer or list of scorers to be used in the cross-validation process.\n        see https://scikit-learn.org/stable/modules/model_evaluation.html\n\n    scorers_weights : list\n        A list of weights to be applied to the scorers during the optimization process.\n\n    classification : bool\n        If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem.\n        Used to determine the CV strategy.\n\n    cv : int, cross-validator\n        - (int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.\n        - (sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.\n\n    other_objective_functions : list, default=[]\n        A list of other objective functions to apply to the pipeline. The function takes a single parameter for the graphpipeline estimator and returns either a single score or a list of scores.\n\n    other_objective_functions_weights : list, default=[]\n        A list of weights to be applied to the other objective functions.\n\n    objective_function_names : list, default=None\n        A list of names to be applied to the objective functions. If None, will use the names of the objective functions.\n\n    bigger_is_better : bool, default=True\n        If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n\n\n    max_size : int, default=np.inf\n        The maximum number of nodes of the pipelines to be generated.\n\n    linear_pipeline : bool, default=False\n        If True, the pipelines generated will be linear. If False, the pipelines generated will be directed acyclic graphs.\n\n    root_config_dict : dict, default='auto'\n        The configuration dictionary to use for the root node of the model.\n        If 'auto', will use \"classifiers\" if classification=True, else \"regressors\".\n        - 'selectors' : A selection of sklearn Selector methods.\n        - 'classifiers' : A selection of sklearn Classifier methods.\n        - 'regressors' : A selection of sklearn Regressor methods.\n        - 'transformers' : A selection of sklearn Transformer methods.\n        - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.\n        - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.\n        - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.\n                                    Subsets are set with the subsets parameter.\n        - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.\n        - 'MDR' : Includes MDR.\n        - 'ContinuousMDR' : Includes ContinuousMDR.\n        - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.\n        - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.\n        - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.\n\n    inner_config_dict : dict, default=[\"selectors\", \"transformers\"]\n        The configuration dictionary to use for the inner nodes of the model generation.\n        Default [\"selectors\", \"transformers\"]\n        - 'selectors' : A selection of sklearn Selector methods.\n        - 'classifiers' : A selection of sklearn Classifier methods.\n        - 'regressors' : A selection of sklearn Regressor methods.\n        - 'transformers' : A selection of sklearn Transformer methods.\n        - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.\n        - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.\n        - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.\n                                    Subsets are set with the subsets parameter.\n        - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.\n        - 'MDR' : Includes MDR.\n        - 'ContinuousMDR' : Includes ContinuousMDR.\n        - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.\n        - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.\n        - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.\n        - None : If None and max_depth&gt;1, the root_config_dict will be used for the inner nodes as well.\n\n    leaf_config_dict : dict, default=None\n        The configuration dictionary to use for the leaf node of the model. If set, leaf nodes must be from this dictionary.\n        Otherwise leaf nodes will be generated from the root_config_dict.\n        Default None\n        - 'selectors' : A selection of sklearn Selector methods.\n        - 'classifiers' : A selection of sklearn Classifier methods.\n        - 'regressors' : A selection of sklearn Regressor methods.\n        - 'transformers' : A selection of sklearn Transformer methods.\n        - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.\n        - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.\n        - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.\n                                    Subsets are set with the subsets parameter.\n        - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.\n        - 'MDR' : Includes MDR.\n        - 'ContinuousMDR' : Includes ContinuousMDR.\n        - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.\n        - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.\n        - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.\n        - None : If None, a leaf will not be required (i.e. the pipeline can be a single root node). Leaf nodes will be generated from the inner_config_dict.\n\n    categorical_features: list or None\n        Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n        - None : If None, TPOT will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing.\n        - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices\n\n\n    memory: Memory object or string, default=None\n        If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. This feature\n        is used to avoid computing the fit transformers within a pipeline if the parameters\n        and input data are identical with another fitted pipeline during optimization process.\n        - String 'auto':\n            TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.\n        - String path of a caching directory\n            TPOT uses memory caching with the provided directory and TPOT does NOT clean\n            the caching directory up upon shutdown. If the directory does not exist, TPOT will\n            create it.\n        - Memory object:\n            TPOT uses the instance of joblib.Memory for memory caching,\n            and TPOT does NOT clean the caching directory up upon shutdown.\n        - None:\n            TPOT does not use memory caching.\n\n    preprocessing : bool or BaseEstimator/Pipeline,\n        EXPERIMENTAL\n        A pipeline that will be used to preprocess the data before CV.\n        - bool : If True, will use a default preprocessing pipeline.\n        - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.\n\n    validation_strategy : str, default='none'\n        EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT may overfit the cross validation score. A second validation set can be used to select the final pipeline.\n        - 'auto' : Automatically determine the validation strategy based on the dataset shape.\n        - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets.\n        - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets.\n        - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.\n\n    validation_fraction : float, default=0.2\n      EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.\n\n    disable_label_encoder : bool, default=False\n        If True, TPOT will check if the target needs to be relabeled to be sequential ints from 0 to N. This is necessary for XGBoost compatibility. If the labels need to be encoded, TPOT will use sklearn.preprocessing.LabelEncoder to encode the labels. The encoder can be accessed via the self.label_encoder_ attribute.\n        If False, no additional label encoders will be used.\n\n    population_size : int, default=50\n        Size of the population\n\n    initial_population_size : int, default=None\n        Size of the initial population. If None, population_size will be used.\n\n    population_scaling : int, default=0.5\n        Scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n\n    generations_until_end_population : int, default=1\n        Number of generations until the population size reaches population_size\n\n    generations : int, default=50\n        Number of generations to run\n\n    early_stop : int, default=None\n        Number of evaluated individuals without improvement before early stopping. Counted across all objectives independently. Triggered when all objectives have not improved by the given number of individuals.\n\n    early_stop_mins : float, default=None\n        Number of seconds without improvement before early stopping. All objectives must not have improved for the given number of seconds for this to be triggered.\n\n    scorers_early_stop_tol :\n        -list of floats\n            list of tolerances for each scorer. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n            If an index of the list is None, that item will not be used for early stopping\n        -int\n            If an int is given, it will be used as the tolerance for all objectives\n\n    other_objectives_early_stop_tol :\n        -list of floats\n            list of tolerances for each of the other objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n            If an index of the list is None, that item will not be used for early stopping\n        -int\n            If an int is given, it will be used as the tolerance for all objectives\n\n    max_time_mins : float, default=float(\"inf\")\n        Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n\n    max_eval_time_mins : float, default=60*5\n        Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n\n    n_jobs : int, default=1\n        Number of processes to run in parallel.\n\n    memory_limit : str, default=None\n        Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n\n    client : dask.distributed.Client, default=None\n        A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.\n\n    crossover_probability : float, default=.2\n        Probability of generating a new individual by crossover between two individuals.\n\n    mutate_probability : float, default=.7\n        Probability of generating a new individual by crossover between one individuals.\n\n    mutate_then_crossover_probability : float, default=.05\n        Probability of generating a new individual by mutating two individuals followed by crossover.\n\n    crossover_then_mutate_probability : float, default=.05\n        Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.\n\n    survival_selector : function, default=survival_select_NSGA2\n        Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes.\n        Used to selected population_size individuals at the start of each generation to use for mutation and crossover.\n\n    parent_selector : function, default=parent_select_NSGA2\n        Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.\n\n    budget_range : list [start, end], default=None\n        A starting and ending budget to use for the budget scaling.\n\n    budget_scaling float : [0,1], default=0.5\n        A scaling factor to use when determining how fast we move the budget from the start to end budget.\n\n    individuals_until_end_budget : int, default=1\n        The number of generations to run before reaching the max budget.\n\n    stepwise_steps : int, default=1\n        The number of staircase steps to take when scaling the budget and population size.\n\n    threshold_evaluation_pruning : list [start, end], default=None\n        starting and ending percentile to use as a threshold for the evaluation early stopping.\n        Values between 0 and 100.\n\n    threshold_evaluation_scaling : float [0,inf), default=0.5\n        A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n        Must be greater than zero. Higher numbers will move the threshold to the end faster.\n\n    min_history_threshold : int, default=0\n        The minimum number of previous scores needed before using threshold early stopping.\n\n    selection_evaluation_pruning : list, default=None\n        A lower and upper percent of the population size to select each round of CV.\n        Values between 0 and 1.\n\n    selection_evaluation_scaling : float, default=0.5\n        A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n        Must be greater than zero. Higher numbers will move the threshold to the end faster.\n\n    n_initial_optimizations : int, default=0\n        Number of individuals to optimize before starting the evolution.\n\n    optimization_cv : int\n       Number of folds to use for the optuna optimization's internal cross-validation.\n\n    max_optimize_time_seconds : float, default=60*5\n        Maximum time to run an optimization\n\n    optimization_steps : int, default=10\n        Number of steps per optimization\n\n    warm_start : bool, default=False\n        If True, will use the continue the evolutionary algorithm from the last generation of the previous run.\n\n\n    verbose : int, default=1\n        How much information to print during the optimization process. Higher values include the information from lower values.\n        0. nothing\n        1. progress bar\n\n        3. best individual\n        4. warnings\n        &gt;=5. full warnings trace\n\n    random_state : int, None, default=None\n        A seed for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes\n        - int\n            Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'\n        - None\n            Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS\n\n\n    periodic_checkpoint_folder : str, default=None\n        Folder to save the population to periodically. If None, no periodic saving will be done.\n        If provided, training will resume from this checkpoint.\n\n    callback : tpot.CallBackInterface, default=None\n        Callback object. Not implemented\n\n    processes : bool, default=True\n        If True, will use multiprocessing to parallelize the optimization process. If False, will use threading.\n        True seems to perform better. However, False is required for interactive debugging.\n\n    Attributes\n    ----------\n\n    fitted_pipeline_ : GraphPipeline\n        A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.\n\n    evaluated_individuals : A pandas data frame containing data for all evaluated individuals in the run.\n        Columns:\n        - *objective functions : The first few columns correspond to the passed in scorers and objective functions\n        - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population.\n        - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population.\n        - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator.\n        - Generation : The generation the pipeline first appeared.\n        - Pareto_Front\t: The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.\n                        To save on computational time, the best frontier is updated iteratively each generation.\n                        The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.\n                        All other pipelines are set to NaN.\n        - Instance\t: The unfitted GraphPipeline BaseEstimator.\n        - *validation objective functions : Objective function scores evaluated on the validation set.\n        - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.\n\n    pareto_front : The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.\n    '''\n\n    # sklearn BaseEstimator must have a corresponding attribute for each parameter.\n    # These should not be modified once set.\n\n    self.search_space = search_space\n    self.scorers = scorers\n    self.scorers_weights = scorers_weights\n    self.classification = classification\n    self.cv = cv\n    self.other_objective_functions = other_objective_functions\n    self.other_objective_functions_weights = other_objective_functions_weights\n    self.objective_function_names = objective_function_names\n    self.bigger_is_better = bigger_is_better\n\n    self.export_graphpipeline = export_graphpipeline\n    self.memory = memory\n\n    self.categorical_features = categorical_features\n    self.preprocessing = preprocessing\n    self.validation_strategy = validation_strategy\n    self.validation_fraction = validation_fraction\n    self.disable_label_encoder = disable_label_encoder\n    self.population_size = population_size\n    self.initial_population_size = initial_population_size\n\n    self.early_stop = early_stop\n    self.early_stop_mins = early_stop_mins\n    self.scorers_early_stop_tol = scorers_early_stop_tol\n    self.other_objectives_early_stop_tol = other_objectives_early_stop_tol\n    self.max_time_mins = max_time_mins\n    self.max_eval_time_mins = max_eval_time_mins\n    self.n_jobs= n_jobs\n    self.memory_limit = memory_limit\n    self.client = client\n\n    self.crossover_probability = crossover_probability\n    self.mutate_probability = mutate_probability\n    self.mutate_then_crossover_probability= mutate_then_crossover_probability\n    self.crossover_then_mutate_probability= crossover_then_mutate_probability\n    self.survival_selector=survival_selector\n    self.parent_selector=parent_selector\n    self.budget_range = budget_range\n    self.budget_scaling = budget_scaling\n    self.individuals_until_end_budget = individuals_until_end_budget\n    self.stepwise_steps = stepwise_steps\n\n    self.warm_start = warm_start\n\n    self.verbose = verbose\n    self.periodic_checkpoint_folder = periodic_checkpoint_folder\n    self.callback = callback\n    self.processes = processes\n\n\n    self.scatter = scatter\n\n    self.optuna_optimize_pareto_front = optuna_optimize_pareto_front\n    self.optuna_optimize_pareto_front_trials = optuna_optimize_pareto_front_trials\n    self.optuna_optimize_pareto_front_timeout = optuna_optimize_pareto_front_timeout\n    self.optuna_storage = optuna_storage\n\n    # create random number generator based on rngseed\n    self.rng = np.random.default_rng(random_state)\n    # save random state passed to us for other functions that use random_state\n    self.random_state = random_state\n\n    self.max_evaluated_individuals = max_evaluated_individuals\n\n    #Initialize other used params\n\n    if self.initial_population_size is None:\n        self._initial_population_size = self.population_size\n    else:\n        self._initial_population_size = self.initial_population_size\n\n    if isinstance(self.scorers, str):\n        self._scorers = [self.scorers]\n\n    elif callable(self.scorers):\n        self._scorers = [self.scorers]\n    else:\n        self._scorers = self.scorers\n\n    self._scorers = [sklearn.metrics.get_scorer(scoring) for scoring in self._scorers]\n    self._scorers_early_stop_tol = self.scorers_early_stop_tol\n\n    self._evolver = tpot.evolvers.SteadyStateEvolver\n\n\n\n    self.objective_function_weights = [*scorers_weights, *other_objective_functions_weights]\n\n\n    if self.objective_function_names is None:\n        obj_names = [f.__name__ for f in other_objective_functions]\n    else:\n        obj_names = self.objective_function_names\n    self.objective_names = [f._score_func.__name__ if hasattr(f,\"_score_func\") else f.__name__ for f in self._scorers] + obj_names\n\n\n    if not isinstance(self.other_objectives_early_stop_tol, list):\n        self._other_objectives_early_stop_tol = [self.other_objectives_early_stop_tol for _ in range(len(self.other_objective_functions))]\n    else:\n        self._other_objectives_early_stop_tol = self.other_objectives_early_stop_tol\n\n    if not isinstance(self._scorers_early_stop_tol, list):\n        self._scorers_early_stop_tol = [self._scorers_early_stop_tol for _ in range(len(self._scorers))]\n    else:\n        self._scorers_early_stop_tol = self._scorers_early_stop_tol\n\n    self.early_stop_tol = [*self._scorers_early_stop_tol, *self._other_objectives_early_stop_tol]\n\n    self._evolver_instance = None\n    self.evaluated_individuals = None\n\n    self.label_encoder_ = None\n\n    set_dask_settings()\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/steady_state_estimator/#tpot.tpot_estimator.steady_state_estimator.apply_make_pipeline","title":"<code>apply_make_pipeline(ind, preprocessing_pipeline=None, export_graphpipeline=False, **pipeline_kwargs)</code>","text":"<p>Helper function to create a column of sklearn pipelines from the tpot individual class.</p> <p>Parameters:</p> Name Type Description Default <code>ind</code> <p>The individual to convert to a pipeline.</p> required <code>preprocessing_pipeline</code> <p>The preprocessing pipeline to include before the individual's pipeline.</p> <code>None</code> <code>export_graphpipeline</code> <p>Force the pipeline to be exported as a graph pipeline. Flattens all nested pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.</p> <code>False</code> <code>pipeline_kwargs</code> <p>Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>sklearn estimator</code> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def apply_make_pipeline(ind, preprocessing_pipeline=None, export_graphpipeline=False, **pipeline_kwargs):\n    \"\"\"\n    Helper function to create a column of sklearn pipelines from the tpot individual class.\n\n    Parameters\n    ----------\n    ind: tpot.SklearnIndividual\n        The individual to convert to a pipeline.\n    preprocessing_pipeline: sklearn.pipeline.Pipeline, optional\n        The preprocessing pipeline to include before the individual's pipeline.\n    export_graphpipeline: bool, default=False\n        Force the pipeline to be exported as a graph pipeline. Flattens all nested pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.\n    pipeline_kwargs: dict\n        Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.\n\n    Returns\n    -------\n    sklearn estimator\n    \"\"\"\n\n    try:\n\n        if export_graphpipeline:\n            est = ind.export_flattened_graphpipeline(**pipeline_kwargs)\n        else:\n            est = ind.export_pipeline(**pipeline_kwargs)\n\n\n        if preprocessing_pipeline is None:\n            return est\n        else:\n            return sklearn.pipeline.make_pipeline(sklearn.base.clone(preprocessing_pipeline), est)\n    except:\n        return None\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/steady_state_estimator/#tpot.tpot_estimator.steady_state_estimator.check_if_y_is_encoded","title":"<code>check_if_y_is_encoded(y)</code>","text":"<p>Checks if the target y is composed of sequential ints from 0 to N. XGBoost requires the target to be encoded in this way.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <p>The target vector.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the target is encoded as sequential ints from 0 to N, False otherwise</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def check_if_y_is_encoded(y):\n    '''\n    Checks if the target y is composed of sequential ints from 0 to N.\n    XGBoost requires the target to be encoded in this way.\n\n    Parameters\n    ----------\n    y: np.ndarray\n        The target vector.\n\n    Returns\n    -------\n    bool\n        True if the target is encoded as sequential ints from 0 to N, False otherwise\n    '''\n    y = sorted(set(y))\n    return all(i == j for i, j in enumerate(y))\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/steady_state_estimator/#tpot.tpot_estimator.steady_state_estimator.convert_parents_tuples_to_integers","title":"<code>convert_parents_tuples_to_integers(row, object_to_int)</code>","text":"<p>Helper function to convert the parent rows into integers representing the index of the parent in the population.</p> <p>Original pandas dataframe using a custom index for the parents. This function converts the custom index to an integer index for easier manipulation by end users.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <p>The row to convert.</p> required <code>object_to_int</code> <p>A dictionary mapping the object to an integer index.</p> required Returns <p>tuple     The row with the custom index converted to an integer index.</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def convert_parents_tuples_to_integers(row, object_to_int):\n    \"\"\"\n    Helper function to convert the parent rows into integers representing the index of the parent in the population.\n\n    Original pandas dataframe using a custom index for the parents. This function converts the custom index to an integer index for easier manipulation by end users.\n\n    Parameters\n    ----------\n    row: list, np.ndarray, tuple\n        The row to convert.\n    object_to_int: dict\n        A dictionary mapping the object to an integer index.\n\n    Returns \n    -------\n    tuple\n        The row with the custom index converted to an integer index.\n    \"\"\"\n    if type(row) == list or type(row) == np.ndarray or type(row) == tuple:\n        return tuple(object_to_int[obj] for obj in row)\n    else:\n        return np.nan\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/steady_state_estimator/#tpot.tpot_estimator.steady_state_estimator.cross_val_score_objective","title":"<code>cross_val_score_objective(estimator, X, y, scorers, cv, fold=None)</code>","text":"<p>Compute the cross validated scores for a estimator. Only fits the estimator once per fold, and loops over the scorers to evaluate the estimator.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <p>The estimator to fit and score.</p> required <code>X</code> <p>The feature matrix.</p> required <code>y</code> <p>The target vector.</p> required <code>scorers</code> <p>The scorers to use.  If a list, will loop over the scorers and return a list of scorers. If a single scorer, will return a single score.</p> required <code>cv</code> <p>The cross-validator to use. For example, sklearn.model_selection.KFold or sklearn.model_selection.StratifiedKFold.</p> required <code>fold</code> <p>The fold to return the scores for. If None, will return the mean of all the scores (per scorer). Default is None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>scores</code> <code>ndarray or float</code> <p>The scores for the estimator per scorer. If fold is None, will return the mean of all the scores (per scorer). Returns a list if multiple scorers are used, otherwise returns a float for the single scorer.</p> Source code in <code>tpot/tpot_estimator/cross_val_utils.py</code> <pre><code>def cross_val_score_objective(estimator, X, y, scorers, cv, fold=None):\n    \"\"\"\n    Compute the cross validated scores for a estimator. Only fits the estimator once per fold, and loops over the scorers to evaluate the estimator.\n\n    Parameters\n    ----------\n    estimator: sklearn.base.BaseEstimator\n        The estimator to fit and score.\n    X: np.ndarray or pd.DataFrame\n        The feature matrix.\n    y: np.ndarray or pd.Series\n        The target vector.\n    scorers: list or scorer\n        The scorers to use. \n        If a list, will loop over the scorers and return a list of scorers.\n        If a single scorer, will return a single score.\n    cv: sklearn cross-validator\n        The cross-validator to use. For example, sklearn.model_selection.KFold or sklearn.model_selection.StratifiedKFold.\n    fold: int, optional\n        The fold to return the scores for. If None, will return the mean of all the scores (per scorer). Default is None.\n\n    Returns\n    -------\n    scores: np.ndarray or float\n        The scores for the estimator per scorer. If fold is None, will return the mean of all the scores (per scorer).\n        Returns a list if multiple scorers are used, otherwise returns a float for the single scorer.\n\n    \"\"\"\n\n    #check if scores is not iterable\n    if not isinstance(scorers, Iterable): \n        scorers = [scorers]\n    scores = []\n    if fold is None:\n        for train_index, test_index in cv.split(X, y):\n            this_fold_estimator = sklearn.base.clone(estimator)\n            if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n            else:\n                X_train, X_test = X[train_index], X[test_index]\n\n            if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n            else:\n                y_train, y_test = y[train_index], y[test_index]\n\n\n            start = time.time()\n            this_fold_estimator.fit(X_train,y_train)\n            duration = time.time() - start\n\n            this_fold_scores = [sklearn.metrics.get_scorer(scorer)(this_fold_estimator, X_test, y_test) for scorer in scorers] \n            scores.append(this_fold_scores)\n            del this_fold_estimator\n            del X_train\n            del X_test\n            del y_train\n            del y_test\n\n\n        return np.mean(scores,0)\n    else:\n        this_fold_estimator = sklearn.base.clone(estimator)\n        train_index, test_index = list(cv.split(X, y))[fold]\n        if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        else:\n            X_train, X_test = X[train_index], X[test_index]\n\n        if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n        else:\n            y_train, y_test = y[train_index], y[test_index]\n\n        start = time.time()\n        this_fold_estimator.fit(X_train,y_train)\n        duration = time.time() - start\n        this_fold_scores = [sklearn.metrics.get_scorer(scorer)(this_fold_estimator, X_test, y_test) for scorer in scorers] \n        return this_fold_scores\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/steady_state_estimator/#tpot.tpot_estimator.steady_state_estimator.objective_function_generator","title":"<code>objective_function_generator(pipeline, x, y, scorers, cv, other_objective_functions, step=None, budget=None, is_classification=True, export_graphpipeline=False, **pipeline_kwargs)</code>","text":"<p>Uses cross validation to evaluate the pipeline using the scorers, and concatenates results with scores from standalone other objective functions.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <p>The individual to evaluate.</p> required <code>x</code> <p>The feature matrix.</p> required <code>y</code> <p>The target vector.</p> required <code>scorers</code> <p>The scorers to use for cross validation.</p> required <code>cv</code> <p>The cross-validator to use. For example, sklearn.model_selection.KFold or sklearn.model_selection.StratifiedKFold. If an int, will use sklearn.model_selection.KFold with n_splits=cv.</p> required <code>other_objective_functions</code> <p>A list of standalone objective functions to evaluate the pipeline. With signature obj(pipeline) -&gt; float. or obj(pipeline) -&gt; np.ndarray These functions take in the unfitted estimator.</p> required <code>step</code> <p>The fold to return the scores for. If None, will return the mean of all the scores (per scorer). Default is None.</p> <code>None</code> <code>budget</code> <p>The budget to subsample the data. If None, will use the full dataset. Default is None. Will subsample budget*len(x) samples.</p> <code>None</code> <code>is_classification</code> <p>If True, will stratify the subsampling. Default is True.</p> <code>True</code> <code>export_graphpipeline</code> <p>Force the pipeline to be exported as a graph pipeline. Flattens all nested sklearn pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.</p> <code>False</code> <code>pipeline_kwargs</code> <p>Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The concatenated scores for the pipeline. The first len(scorers) elements are the cross validation scores, and the remaining elements are the standalone objective functions.</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def objective_function_generator(pipeline, x,y, scorers, cv, other_objective_functions, step=None, budget=None, is_classification=True, export_graphpipeline=False, **pipeline_kwargs):\n    \"\"\"\n    Uses cross validation to evaluate the pipeline using the scorers, and concatenates results with scores from standalone other objective functions.\n\n    Parameters\n    ----------\n    pipeline: tpot.SklearnIndividual\n        The individual to evaluate.\n    x: np.ndarray\n        The feature matrix.\n    y: np.ndarray\n        The target vector.\n    scorers: list\n        The scorers to use for cross validation. \n    cv: int, float, or sklearn cross-validator\n        The cross-validator to use. For example, sklearn.model_selection.KFold or sklearn.model_selection.StratifiedKFold.\n        If an int, will use sklearn.model_selection.KFold with n_splits=cv.\n    other_objective_functions: list\n        A list of standalone objective functions to evaluate the pipeline. With signature obj(pipeline) -&gt; float. or obj(pipeline) -&gt; np.ndarray\n        These functions take in the unfitted estimator.\n    step: int, optional\n        The fold to return the scores for. If None, will return the mean of all the scores (per scorer). Default is None.\n    budget: float, optional\n        The budget to subsample the data. If None, will use the full dataset. Default is None.\n        Will subsample budget*len(x) samples.\n    is_classification: bool, default=True\n        If True, will stratify the subsampling. Default is True.\n    export_graphpipeline: bool, default=False\n        Force the pipeline to be exported as a graph pipeline. Flattens all nested sklearn pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.\n    pipeline_kwargs: dict\n        Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.\n\n    Returns\n    -------\n    np.ndarray\n        The concatenated scores for the pipeline. The first len(scorers) elements are the cross validation scores, and the remaining elements are the standalone objective functions.\n\n    \"\"\"\n\n    if export_graphpipeline:\n        pipeline = pipeline.export_flattened_graphpipeline(**pipeline_kwargs)\n    else:\n        pipeline = pipeline.export_pipeline(**pipeline_kwargs)\n\n    if budget is not None and budget &lt; 1:\n        if is_classification:\n            x,y = sklearn.utils.resample(x,y, stratify=y, n_samples=int(budget*len(x)), replace=False, random_state=1)\n        else:\n            x,y = sklearn.utils.resample(x,y, n_samples=int(budget*len(x)), replace=False, random_state=1)\n\n        if isinstance(cv, int) or isinstance(cv, float):\n            n_splits = cv\n        else:\n            n_splits = cv.n_splits\n\n    if len(scorers) &gt; 0:\n        cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)\n    else:\n        cv_obj_scores = []\n\n    if other_objective_functions is not None and len(other_objective_functions) &gt;0:\n        other_scores = [obj(sklearn.base.clone(pipeline)) for obj in other_objective_functions]\n        #flatten\n        other_scores = np.array(other_scores).flatten().tolist()\n    else:\n        other_scores = []\n\n    return np.concatenate([cv_obj_scores,other_scores])\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/steady_state_estimator/#tpot.tpot_estimator.steady_state_estimator.remove_underrepresented_classes","title":"<code>remove_underrepresented_classes(x, y, min_count)</code>","text":"<p>Helper function to remove classes with less than min_count samples from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>The feature matrix.</p> required <code>y</code> <p>The target vector.</p> required <code>min_count</code> <p>The minimum number of samples to keep a class.</p> required <p>Returns:</p> Type Description <code>(ndarray, ndarray)</code> <p>The feature matrix and target vector with rows from classes with less than min_count samples removed.</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def remove_underrepresented_classes(x, y, min_count):\n    \"\"\"\n    Helper function to remove classes with less than min_count samples from the dataset.\n\n    Parameters\n    ----------\n    x: np.ndarray or pd.DataFrame\n        The feature matrix.\n    y: np.ndarray or pd.Series\n        The target vector.\n    min_count: int\n        The minimum number of samples to keep a class.\n\n    Returns\n    -------\n    np.ndarray, np.ndarray\n        The feature matrix and target vector with rows from classes with less than min_count samples removed.\n    \"\"\"\n    if isinstance(y, (np.ndarray, pd.Series)):\n        unique, counts = np.unique(y, return_counts=True)\n        if min(counts) &gt;= min_count:\n            return x, y\n        keep_classes = unique[counts &gt;= min_count]\n        mask = np.isin(y, keep_classes)\n        x = x[mask]\n        y = y[mask]\n    elif isinstance(y, pd.DataFrame):\n        counts = y.apply(pd.Series.value_counts)\n        if min(counts) &gt;= min_count:\n            return x, y\n        keep_classes = counts.index[counts &gt;= min_count].tolist()\n        mask = y.isin(keep_classes).all(axis=1)\n        x = x[mask]\n        y = y[mask]\n    else:\n        raise TypeError(\"y must be a numpy array or a pandas Series/DataFrame\")\n    return x, y\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/steady_state_estimator/#tpot.tpot_estimator.steady_state_estimator.val_objective_function_generator","title":"<code>val_objective_function_generator(pipeline, X_train, y_train, X_test, y_test, scorers, other_objective_functions, export_graphpipeline=False, **pipeline_kwargs)</code>","text":"<p>Trains a pipeline on a training set and evaluates it on a test set using the scorers and other objective functions.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <p>The individual to evaluate.</p> required <code>X_train</code> <p>The feature matrix of the training set.</p> required <code>y_train</code> <p>The target vector of the training set.</p> required <code>X_test</code> <p>The feature matrix of the test set.</p> required <code>y_test</code> <p>The target vector of the test set.</p> required <code>scorers</code> <p>The scorers to use for cross validation.</p> required <code>other_objective_functions</code> <p>A list of standalone objective functions to evaluate the pipeline. With signature obj(pipeline) -&gt; float. or obj(pipeline) -&gt; np.ndarray These functions take in the unfitted estimator.</p> required <code>export_graphpipeline</code> <p>Force the pipeline to be exported as a graph pipeline. Flattens all nested sklearn pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.</p> <code>False</code> <code>pipeline_kwargs</code> <p>Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The concatenated scores for the pipeline. The first len(scorers) elements are the cross validation scores, and the remaining elements are the standalone objective functions.</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def val_objective_function_generator(pipeline, X_train, y_train, X_test, y_test, scorers, other_objective_functions, export_graphpipeline=False, **pipeline_kwargs):\n    \"\"\"\n    Trains a pipeline on a training set and evaluates it on a test set using the scorers and other objective functions.\n\n    Parameters\n    ----------\n\n    pipeline: tpot.SklearnIndividual\n        The individual to evaluate.\n    X_train: np.ndarray\n        The feature matrix of the training set.\n    y_train: np.ndarray\n        The target vector of the training set.\n    X_test: np.ndarray\n        The feature matrix of the test set.\n    y_test: np.ndarray\n        The target vector of the test set.\n    scorers: list\n        The scorers to use for cross validation.\n    other_objective_functions: list\n        A list of standalone objective functions to evaluate the pipeline. With signature obj(pipeline) -&gt; float. or obj(pipeline) -&gt; np.ndarray\n        These functions take in the unfitted estimator.\n    export_graphpipeline: bool, default=False\n        Force the pipeline to be exported as a graph pipeline. Flattens all nested sklearn pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.\n    pipeline_kwargs: dict\n        Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.\n\n    Returns\n    -------\n    np.ndarray\n        The concatenated scores for the pipeline. The first len(scorers) elements are the cross validation scores, and the remaining elements are the standalone objective functions.\n\n\n    \"\"\"\n\n    #subsample the data\n    if export_graphpipeline:\n        pipeline = pipeline.export_flattened_graphpipeline(**pipeline_kwargs)\n    else:\n        pipeline = pipeline.export_pipeline(**pipeline_kwargs)\n\n    fitted_pipeline = sklearn.base.clone(pipeline)\n    fitted_pipeline.fit(X_train, y_train)\n\n    if len(scorers) &gt; 0:\n        scores =[sklearn.metrics.get_scorer(scorer)(fitted_pipeline, X_test, y_test) for scorer in scorers]\n\n    other_scores = []\n    if other_objective_functions is not None and len(other_objective_functions) &gt;0:\n        other_scores = [obj(sklearn.base.clone(pipeline)) for obj in other_objective_functions]\n\n    return np.concatenate([scores,other_scores])\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/templates/tpot_autoimputer/","title":"Tpot autoimputer","text":""},{"location":"documentation/tpot/tpot_estimator/templates/tpottemplates/","title":"Tpottemplates","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/tpot_estimator/templates/tpottemplates/#tpot.tpot_estimator.templates.tpottemplates.TPOTClassifier","title":"<code>TPOTClassifier</code>","text":"<p>               Bases: <code>TPOTEstimator</code></p> Source code in <code>tpot/tpot_estimator/templates/tpottemplates.py</code> <pre><code>class TPOTClassifier(TPOTEstimator):\n    def __init__(       self,\n                        search_space = \"linear\",\n                        scorers=['roc_auc_ovr'], \n                        scorers_weights=[1],\n                        cv = 10,\n                        other_objective_functions=[], #tpot.objectives.estimator_objective_functions.number_of_nodes_objective],\n                        other_objective_functions_weights = [],\n                        objective_function_names = None,\n                        bigger_is_better = True,\n                        categorical_features = None,\n                        memory = None,\n                        preprocessing = False,\n                        max_time_mins=60, \n                        max_eval_time_mins=10, \n                        n_jobs = 1,\n                        validation_strategy = \"none\",\n                        validation_fraction = .2, \n                        early_stop = None,\n                        warm_start = False,\n                        periodic_checkpoint_folder = None, \n                        verbose = 2,\n                        memory_limit = None,\n                        client = None,\n                        random_state=None,\n                        allow_inner_classifiers=None,\n                        **tpotestimator_kwargs,\n\n        ):\n        \"\"\"\n        An sklearn baseestimator that uses genetic programming to optimize a classification pipeline.\n        For more parameters, see the TPOTEstimator class.\n\n        Parameters\n        ----------\n\n        search_space : (String, tpot.search_spaces.SearchSpace)\n            - String : The default search space to use for the optimization.\n            | String     | Description      |\n            | :---        |    :----:   |\n            | linear  | A linear pipeline with the structure of \"Selector-&gt;(transformers+Passthrough)-&gt;(classifiers/regressors+Passthrough)-&gt;final classifier/regressor.\" For both the transformer and inner estimator layers, TPOT may choose one or more transformers/classifiers, or it may choose none. The inner classifier/regressor layer is optional. |\n            | linear-light | Same search space as linear, but without the inner classifier/regressor layer and with a reduced set of faster running estimators. |\n            | graph | TPOT will optimize a pipeline in the shape of a directed acyclic graph. The nodes of the graph can include selectors, scalers, transformers, or classifiers/regressors (inner classifiers/regressors can optionally be not included). This will return a custom GraphPipeline rather than an sklearn Pipeline. More details in Tutorial 6. |\n            | graph-light | Same as graph search space, but without the inner classifier/regressors and with a reduced set of faster running estimators. |\n            | mdr |TPOT will search over a series of feature selectors and Multifactor Dimensionality Reduction models to find a series of operators that maximize prediction accuracy. The TPOT MDR configuration is specialized for genome-wide association studies (GWAS), and is described in detail online here.\n\n            Note that TPOT MDR may be slow to run because the feature selection routines are computationally expensive, especially on large datasets. |\n            - SearchSpace : The search space to use for the optimization. This should be an instance of a SearchSpace.\n                The search space to use for the optimization. This should be an instance of a SearchSpace.\n                TPOT has groups of search spaces found in the following folders, tpot.search_spaces.nodes for the nodes in the pipeline and tpot.search_spaces.pipelines for the pipeline structure.\n\n        scorers : (list, scorer)\n            A scorer or list of scorers to be used in the cross-validation process.\n            see https://scikit-learn.org/stable/modules/model_evaluation.html\n\n        scorers_weights : list\n            A list of weights to be applied to the scorers during the optimization process.\n\n        classification : bool\n            If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem.\n            Used to determine the CV strategy.\n\n        cv : int, cross-validator\n            - (int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.\n            - (sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.\n                - max_depth (int): The maximum depth from any node to the root of the pipelines to be generated.\n\n        other_objective_functions : list, default=[]\n            A list of other objective functions to apply to the pipeline. The function takes a single parameter for the graphpipeline estimator and returns either a single score or a list of scores.\n\n        other_objective_functions_weights : list, default=[]\n            A list of weights to be applied to the other objective functions.\n\n        objective_function_names : list, default=None\n            A list of names to be applied to the objective functions. If None, will use the names of the objective functions.\n\n        bigger_is_better : bool, default=True\n            If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n\n        categorical_features : list or None\n            Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n\n        categorical_features: list or None\n            Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n            - None : If None, TPOT will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing.\n            - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices\n\n\n        memory: Memory object or string, default=None\n            If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. This feature\n            is used to avoid computing the fit transformers within a pipeline if the parameters\n            and input data are identical with another fitted pipeline during optimization process.\n            - String 'auto':\n                TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.\n            - String path of a caching directory\n                TPOT uses memory caching with the provided directory and TPOT does NOT clean\n                the caching directory up upon shutdown. If the directory does not exist, TPOT will\n                create it.\n            - Memory object:\n                TPOT uses the instance of joblib.Memory for memory caching,\n                and TPOT does NOT clean the caching directory up upon shutdown.\n            - None:\n                TPOT does not use memory caching.\n\n        preprocessing : bool or BaseEstimator/Pipeline,\n            EXPERIMENTAL\n            A pipeline that will be used to preprocess the data before CV. Note that the parameters for these steps are not optimized. Add them to the search space to be optimized.\n            - bool : If True, will use a default preprocessing pipeline which includes imputation followed by one hot encoding.\n            - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.\n\n        max_time_mins : float, default=float(\"inf\")\n            Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n\n        max_eval_time_mins : float, default=60*5\n            Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n\n\n        n_jobs : int, default=1\n            Number of processes to run in parallel.\n\n        validation_strategy : str, default='none'\n            EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT may overfit the cross validation score. A second validation set can be used to select the final pipeline.\n            - 'auto' : Automatically determine the validation strategy based on the dataset shape.\n            - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets.\n            - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets.\n            - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.\n\n        validation_fraction : float, default=0.2\n          EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.\n\n        early_stop : int, default=None\n            Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered. In general a value of around 5-20 is good.\n\n        warm_start : bool, default=False\n            If True, will use the continue the evolutionary algorithm from the last generation of the previous run.\n\n        periodic_checkpoint_folder : str, default=None\n            Folder to save the population to periodically. If None, no periodic saving will be done.\n            If provided, training will resume from this checkpoint.\n\n\n        verbose : int, default=1\n            How much information to print during the optimization process. Higher values include the information from lower values.\n            0. nothing\n            1. progress bar\n\n            3. best individual\n            4. warnings\n            &gt;=5. full warnings trace\n            6. evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_mins in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)\n\n\n        memory_limit : str, default=None\n            Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n\n        client : dask.distributed.Client, default=None\n            A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.\n\n        random_state : int, None, default=None\n            A seed for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes\n\n            - int\n                Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'\n            - None\n                Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS\n\n        allow_inner_classifiers : bool, default=True\n            If True, the search space will include ensembled classifiers. \n\n        Attributes\n        ----------\n\n        fitted_pipeline_ : GraphPipeline\n            A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.\n\n        evaluated_individuals : A pandas data frame containing data for all evaluated individuals in the run.\n            Columns:\n            - *objective functions : The first few columns correspond to the passed in scorers and objective functions\n            - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population.\n            - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population.\n            - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator.\n            - Generation : The generation the pipeline first appeared.\n            - Pareto_Front\t: The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.\n                            To save on computational time, the best frontier is updated iteratively each generation.\n                            The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.\n                            All other pipelines are set to NaN.\n            - Instance\t: The unfitted GraphPipeline BaseEstimator.\n            - *validation objective functions : Objective function scores evaluated on the validation set.\n            - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.\n\n        pareto_front : The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.\n        \"\"\"\n        self.search_space = search_space\n        self.scorers = scorers\n        self.scorers_weights = scorers_weights\n        self.cv = cv\n        self.other_objective_functions = other_objective_functions\n        self.other_objective_functions_weights = other_objective_functions_weights\n        self.objective_function_names = objective_function_names\n        self.bigger_is_better = bigger_is_better\n        self.categorical_features = categorical_features\n        self.memory = memory\n        self.preprocessing = preprocessing\n        self.max_time_mins = max_time_mins\n        self.max_eval_time_mins = max_eval_time_mins\n        self.n_jobs = n_jobs\n        self.validation_strategy = validation_strategy\n        self.validation_fraction = validation_fraction\n        self.early_stop = early_stop\n        self.warm_start = warm_start\n        self.periodic_checkpoint_folder = periodic_checkpoint_folder\n        self.verbose = verbose\n        self.memory_limit = memory_limit\n        self.client = client\n        self.random_state = random_state\n        self.tpotestimator_kwargs = tpotestimator_kwargs\n        self.allow_inner_classifiers = allow_inner_classifiers\n\n        self.initialized = False\n\n    def fit(self, X, y):\n\n        if not self.initialized:\n\n            get_search_space_params = {\"n_classes\": len(np.unique(y)), \n                                       \"n_samples\":len(y), \n                                       \"n_features\":X.shape[1], \n                                       \"random_state\":self.random_state}\n\n            search_space = get_template_search_spaces(self.search_space, classification=True, inner_predictors=self.allow_inner_classifiers, **get_search_space_params)\n\n\n            super(TPOTClassifier,self).__init__(\n                search_space=search_space,\n                scorers=self.scorers, \n                scorers_weights=self.scorers_weights,\n                cv = self.cv,\n                other_objective_functions=self.other_objective_functions, #tpot.objectives.estimator_objective_functions.number_of_nodes_objective],\n                other_objective_functions_weights = self.other_objective_functions_weights,\n                objective_function_names = self.objective_function_names,\n                bigger_is_better = self.bigger_is_better,\n                categorical_features = self.categorical_features,\n                memory = self.memory,\n                preprocessing = self.preprocessing,\n                max_time_mins=self.max_time_mins, \n                max_eval_time_mins=self.max_eval_time_mins, \n                n_jobs=self.n_jobs,\n                validation_strategy = self.validation_strategy,\n                validation_fraction = self.validation_fraction, \n                early_stop = self.early_stop,\n                warm_start = self.warm_start,\n                periodic_checkpoint_folder = self.periodic_checkpoint_folder, \n                verbose = self.verbose,\n                classification=True,\n                memory_limit = self.memory_limit,\n                client = self.client,\n                random_state=self.random_state,\n                **self.tpotestimator_kwargs)\n            self.initialized = True\n\n        return super().fit(X,y)\n\n\n    def predict(self, X, **predict_params):\n        check_is_fitted(self)\n        #X=check_array(X)\n        return self.fitted_pipeline_.predict(X,**predict_params)\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/templates/tpottemplates/#tpot.tpot_estimator.templates.tpottemplates.TPOTClassifier.__init__","title":"<code>__init__(search_space='linear', scorers=['roc_auc_ovr'], scorers_weights=[1], cv=10, other_objective_functions=[], other_objective_functions_weights=[], objective_function_names=None, bigger_is_better=True, categorical_features=None, memory=None, preprocessing=False, max_time_mins=60, max_eval_time_mins=10, n_jobs=1, validation_strategy='none', validation_fraction=0.2, early_stop=None, warm_start=False, periodic_checkpoint_folder=None, verbose=2, memory_limit=None, client=None, random_state=None, allow_inner_classifiers=None, **tpotestimator_kwargs)</code>","text":"<p>An sklearn baseestimator that uses genetic programming to optimize a classification pipeline. For more parameters, see the TPOTEstimator class.</p> <p>Parameters:</p> Name Type Description Default <code>search_space</code> <code>(String, SearchSpace)</code> <ul> <li>String : The default search space to use for the optimization. | String     | Description      | | :---        |    :----:   | | linear  | A linear pipeline with the structure of \"Selector-&gt;(transformers+Passthrough)-&gt;(classifiers/regressors+Passthrough)-&gt;final classifier/regressor.\" For both the transformer and inner estimator layers, TPOT may choose one or more transformers/classifiers, or it may choose none. The inner classifier/regressor layer is optional. | | linear-light | Same search space as linear, but without the inner classifier/regressor layer and with a reduced set of faster running estimators. | | graph | TPOT will optimize a pipeline in the shape of a directed acyclic graph. The nodes of the graph can include selectors, scalers, transformers, or classifiers/regressors (inner classifiers/regressors can optionally be not included). This will return a custom GraphPipeline rather than an sklearn Pipeline. More details in Tutorial 6. | | graph-light | Same as graph search space, but without the inner classifier/regressors and with a reduced set of faster running estimators. | | mdr |TPOT will search over a series of feature selectors and Multifactor Dimensionality Reduction models to find a series of operators that maximize prediction accuracy. The TPOT MDR configuration is specialized for genome-wide association studies (GWAS), and is described in detail online here.</li> </ul> <p>Note that TPOT MDR may be slow to run because the feature selection routines are computationally expensive, especially on large datasets. | - SearchSpace : The search space to use for the optimization. This should be an instance of a SearchSpace.     The search space to use for the optimization. This should be an instance of a SearchSpace.     TPOT has groups of search spaces found in the following folders, tpot.search_spaces.nodes for the nodes in the pipeline and tpot.search_spaces.pipelines for the pipeline structure.</p> <code>'linear'</code> <code>scorers</code> <code>(list, scorer)</code> <p>A scorer or list of scorers to be used in the cross-validation process. see https://scikit-learn.org/stable/modules/model_evaluation.html</p> <code>['roc_auc_ovr']</code> <code>scorers_weights</code> <code>list</code> <p>A list of weights to be applied to the scorers during the optimization process.</p> <code>[1]</code> <code>classification</code> <code>bool</code> <p>If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem. Used to determine the CV strategy.</p> required <code>cv</code> <code>(int, cross - validator)</code> <ul> <li>(int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.</li> <li>(sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.<ul> <li>max_depth (int): The maximum depth from any node to the root of the pipelines to be generated.</li> </ul> </li> </ul> <code>10</code> <code>other_objective_functions</code> <code>list</code> <p>A list of other objective functions to apply to the pipeline. The function takes a single parameter for the graphpipeline estimator and returns either a single score or a list of scores.</p> <code>[]</code> <code>other_objective_functions_weights</code> <code>list</code> <p>A list of weights to be applied to the other objective functions.</p> <code>[]</code> <code>objective_function_names</code> <code>list</code> <p>A list of names to be applied to the objective functions. If None, will use the names of the objective functions.</p> <code>None</code> <code>bigger_is_better</code> <code>bool</code> <p>If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.</p> <code>True</code> <code>categorical_features</code> <code>list or None</code> <p>Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.</p> <code>None</code> <code>categorical_features</code> <p>Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False. - None : If None, TPOT will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing. - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices</p> <code>None</code> <code>memory</code> <p>If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. This feature is used to avoid computing the fit transformers within a pipeline if the parameters and input data are identical with another fitted pipeline during optimization process. - String 'auto':     TPOT uses memory caching with a temporary directory and cleans it up upon shutdown. - String path of a caching directory     TPOT uses memory caching with the provided directory and TPOT does NOT clean     the caching directory up upon shutdown. If the directory does not exist, TPOT will     create it. - Memory object:     TPOT uses the instance of joblib.Memory for memory caching,     and TPOT does NOT clean the caching directory up upon shutdown. - None:     TPOT does not use memory caching.</p> <code>None</code> <code>preprocessing</code> <code>(bool or BaseEstimator / Pipeline)</code> <p>EXPERIMENTAL A pipeline that will be used to preprocess the data before CV. Note that the parameters for these steps are not optimized. Add them to the search space to be optimized. - bool : If True, will use a default preprocessing pipeline which includes imputation followed by one hot encoding. - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.</p> <code>False</code> <code>max_time_mins</code> <code>float</code> <p>Maximum time to run the optimization. If none or inf, will run until the end of the generations.</p> <code>float(\"inf\")</code> <code>max_eval_time_mins</code> <code>float</code> <p>Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.</p> <code>60*5</code> <code>n_jobs</code> <code>int</code> <p>Number of processes to run in parallel.</p> <code>1</code> <code>validation_strategy</code> <code>str</code> <p>EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT may overfit the cross validation score. A second validation set can be used to select the final pipeline. - 'auto' : Automatically determine the validation strategy based on the dataset shape. - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets. - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets. - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.</p> <code>'none'</code> <code>validation_fraction</code> <code>float</code> <p>EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.</p> <code>0.2</code> <code>early_stop</code> <code>int</code> <p>Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered. In general a value of around 5-20 is good.</p> <code>None</code> <code>warm_start</code> <code>bool</code> <p>If True, will use the continue the evolutionary algorithm from the last generation of the previous run.</p> <code>False</code> <code>periodic_checkpoint_folder</code> <code>str</code> <p>Folder to save the population to periodically. If None, no periodic saving will be done. If provided, training will resume from this checkpoint.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>How much information to print during the optimization process. Higher values include the information from lower values. 0. nothing 1. progress bar</p> <ol> <li>best individual</li> <li>warnings <p>=5. full warnings trace</p> </li> <li>evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_mins in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)</li> </ol> <code>1</code> <code>memory_limit</code> <code>str</code> <p>Memory limit for each job. See Dask LocalCluster documentation for more information.</p> <code>None</code> <code>client</code> <code>Client</code> <p>A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.</p> <code>None</code> <code>random_state</code> <code>(int, None)</code> <p>A seed for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes</p> <ul> <li>int     Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'</li> <li>None     Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS</li> </ul> <code>None</code> <code>allow_inner_classifiers</code> <code>bool</code> <p>If True, the search space will include ensembled classifiers.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>fitted_pipeline_</code> <code>GraphPipeline</code> <p>A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.</p> <code>evaluated_individuals</code> <code>A pandas data frame containing data for all evaluated individuals in the run.</code> <p>Columns: - objective functions : The first few columns correspond to the passed in scorers and objective functions - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population. - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population. - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator. - Generation : The generation the pipeline first appeared. - Pareto_Front      : The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.                 To save on computational time, the best frontier is updated iteratively each generation.                 The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.                 All other pipelines are set to NaN. - Instance  : The unfitted GraphPipeline BaseEstimator. - validation objective functions : Objective function scores evaluated on the validation set. - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.</p> <code>pareto_front</code> <code>The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.</code> Source code in <code>tpot/tpot_estimator/templates/tpottemplates.py</code> <pre><code>def __init__(       self,\n                    search_space = \"linear\",\n                    scorers=['roc_auc_ovr'], \n                    scorers_weights=[1],\n                    cv = 10,\n                    other_objective_functions=[], #tpot.objectives.estimator_objective_functions.number_of_nodes_objective],\n                    other_objective_functions_weights = [],\n                    objective_function_names = None,\n                    bigger_is_better = True,\n                    categorical_features = None,\n                    memory = None,\n                    preprocessing = False,\n                    max_time_mins=60, \n                    max_eval_time_mins=10, \n                    n_jobs = 1,\n                    validation_strategy = \"none\",\n                    validation_fraction = .2, \n                    early_stop = None,\n                    warm_start = False,\n                    periodic_checkpoint_folder = None, \n                    verbose = 2,\n                    memory_limit = None,\n                    client = None,\n                    random_state=None,\n                    allow_inner_classifiers=None,\n                    **tpotestimator_kwargs,\n\n    ):\n    \"\"\"\n    An sklearn baseestimator that uses genetic programming to optimize a classification pipeline.\n    For more parameters, see the TPOTEstimator class.\n\n    Parameters\n    ----------\n\n    search_space : (String, tpot.search_spaces.SearchSpace)\n        - String : The default search space to use for the optimization.\n        | String     | Description      |\n        | :---        |    :----:   |\n        | linear  | A linear pipeline with the structure of \"Selector-&gt;(transformers+Passthrough)-&gt;(classifiers/regressors+Passthrough)-&gt;final classifier/regressor.\" For both the transformer and inner estimator layers, TPOT may choose one or more transformers/classifiers, or it may choose none. The inner classifier/regressor layer is optional. |\n        | linear-light | Same search space as linear, but without the inner classifier/regressor layer and with a reduced set of faster running estimators. |\n        | graph | TPOT will optimize a pipeline in the shape of a directed acyclic graph. The nodes of the graph can include selectors, scalers, transformers, or classifiers/regressors (inner classifiers/regressors can optionally be not included). This will return a custom GraphPipeline rather than an sklearn Pipeline. More details in Tutorial 6. |\n        | graph-light | Same as graph search space, but without the inner classifier/regressors and with a reduced set of faster running estimators. |\n        | mdr |TPOT will search over a series of feature selectors and Multifactor Dimensionality Reduction models to find a series of operators that maximize prediction accuracy. The TPOT MDR configuration is specialized for genome-wide association studies (GWAS), and is described in detail online here.\n\n        Note that TPOT MDR may be slow to run because the feature selection routines are computationally expensive, especially on large datasets. |\n        - SearchSpace : The search space to use for the optimization. This should be an instance of a SearchSpace.\n            The search space to use for the optimization. This should be an instance of a SearchSpace.\n            TPOT has groups of search spaces found in the following folders, tpot.search_spaces.nodes for the nodes in the pipeline and tpot.search_spaces.pipelines for the pipeline structure.\n\n    scorers : (list, scorer)\n        A scorer or list of scorers to be used in the cross-validation process.\n        see https://scikit-learn.org/stable/modules/model_evaluation.html\n\n    scorers_weights : list\n        A list of weights to be applied to the scorers during the optimization process.\n\n    classification : bool\n        If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem.\n        Used to determine the CV strategy.\n\n    cv : int, cross-validator\n        - (int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.\n        - (sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.\n            - max_depth (int): The maximum depth from any node to the root of the pipelines to be generated.\n\n    other_objective_functions : list, default=[]\n        A list of other objective functions to apply to the pipeline. The function takes a single parameter for the graphpipeline estimator and returns either a single score or a list of scores.\n\n    other_objective_functions_weights : list, default=[]\n        A list of weights to be applied to the other objective functions.\n\n    objective_function_names : list, default=None\n        A list of names to be applied to the objective functions. If None, will use the names of the objective functions.\n\n    bigger_is_better : bool, default=True\n        If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n\n    categorical_features : list or None\n        Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n\n    categorical_features: list or None\n        Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n        - None : If None, TPOT will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing.\n        - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices\n\n\n    memory: Memory object or string, default=None\n        If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. This feature\n        is used to avoid computing the fit transformers within a pipeline if the parameters\n        and input data are identical with another fitted pipeline during optimization process.\n        - String 'auto':\n            TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.\n        - String path of a caching directory\n            TPOT uses memory caching with the provided directory and TPOT does NOT clean\n            the caching directory up upon shutdown. If the directory does not exist, TPOT will\n            create it.\n        - Memory object:\n            TPOT uses the instance of joblib.Memory for memory caching,\n            and TPOT does NOT clean the caching directory up upon shutdown.\n        - None:\n            TPOT does not use memory caching.\n\n    preprocessing : bool or BaseEstimator/Pipeline,\n        EXPERIMENTAL\n        A pipeline that will be used to preprocess the data before CV. Note that the parameters for these steps are not optimized. Add them to the search space to be optimized.\n        - bool : If True, will use a default preprocessing pipeline which includes imputation followed by one hot encoding.\n        - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.\n\n    max_time_mins : float, default=float(\"inf\")\n        Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n\n    max_eval_time_mins : float, default=60*5\n        Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n\n\n    n_jobs : int, default=1\n        Number of processes to run in parallel.\n\n    validation_strategy : str, default='none'\n        EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT may overfit the cross validation score. A second validation set can be used to select the final pipeline.\n        - 'auto' : Automatically determine the validation strategy based on the dataset shape.\n        - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets.\n        - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets.\n        - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.\n\n    validation_fraction : float, default=0.2\n      EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.\n\n    early_stop : int, default=None\n        Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered. In general a value of around 5-20 is good.\n\n    warm_start : bool, default=False\n        If True, will use the continue the evolutionary algorithm from the last generation of the previous run.\n\n    periodic_checkpoint_folder : str, default=None\n        Folder to save the population to periodically. If None, no periodic saving will be done.\n        If provided, training will resume from this checkpoint.\n\n\n    verbose : int, default=1\n        How much information to print during the optimization process. Higher values include the information from lower values.\n        0. nothing\n        1. progress bar\n\n        3. best individual\n        4. warnings\n        &gt;=5. full warnings trace\n        6. evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_mins in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)\n\n\n    memory_limit : str, default=None\n        Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n\n    client : dask.distributed.Client, default=None\n        A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.\n\n    random_state : int, None, default=None\n        A seed for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes\n\n        - int\n            Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'\n        - None\n            Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS\n\n    allow_inner_classifiers : bool, default=True\n        If True, the search space will include ensembled classifiers. \n\n    Attributes\n    ----------\n\n    fitted_pipeline_ : GraphPipeline\n        A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.\n\n    evaluated_individuals : A pandas data frame containing data for all evaluated individuals in the run.\n        Columns:\n        - *objective functions : The first few columns correspond to the passed in scorers and objective functions\n        - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population.\n        - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population.\n        - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator.\n        - Generation : The generation the pipeline first appeared.\n        - Pareto_Front\t: The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.\n                        To save on computational time, the best frontier is updated iteratively each generation.\n                        The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.\n                        All other pipelines are set to NaN.\n        - Instance\t: The unfitted GraphPipeline BaseEstimator.\n        - *validation objective functions : Objective function scores evaluated on the validation set.\n        - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.\n\n    pareto_front : The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.\n    \"\"\"\n    self.search_space = search_space\n    self.scorers = scorers\n    self.scorers_weights = scorers_weights\n    self.cv = cv\n    self.other_objective_functions = other_objective_functions\n    self.other_objective_functions_weights = other_objective_functions_weights\n    self.objective_function_names = objective_function_names\n    self.bigger_is_better = bigger_is_better\n    self.categorical_features = categorical_features\n    self.memory = memory\n    self.preprocessing = preprocessing\n    self.max_time_mins = max_time_mins\n    self.max_eval_time_mins = max_eval_time_mins\n    self.n_jobs = n_jobs\n    self.validation_strategy = validation_strategy\n    self.validation_fraction = validation_fraction\n    self.early_stop = early_stop\n    self.warm_start = warm_start\n    self.periodic_checkpoint_folder = periodic_checkpoint_folder\n    self.verbose = verbose\n    self.memory_limit = memory_limit\n    self.client = client\n    self.random_state = random_state\n    self.tpotestimator_kwargs = tpotestimator_kwargs\n    self.allow_inner_classifiers = allow_inner_classifiers\n\n    self.initialized = False\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/templates/tpottemplates/#tpot.tpot_estimator.templates.tpottemplates.TPOTRegressor","title":"<code>TPOTRegressor</code>","text":"<p>               Bases: <code>TPOTEstimator</code></p> Source code in <code>tpot/tpot_estimator/templates/tpottemplates.py</code> <pre><code>class TPOTRegressor(TPOTEstimator):\n    def __init__(       self,\n                        search_space = \"linear\",\n                        scorers=['neg_mean_squared_error'], \n                        scorers_weights=[1],\n                        cv = 10, #remove this and use a value based on dataset size?\n                        other_objective_functions=[], #tpot.objectives.estimator_objective_functions.number_of_nodes_objective],\n                        other_objective_functions_weights = [],\n                        objective_function_names = None,\n                        bigger_is_better = True,\n                        categorical_features = None,\n                        memory = None,\n                        preprocessing = False,\n                        max_time_mins=60, \n                        max_eval_time_mins=10, \n                        n_jobs = 1,\n                        validation_strategy = \"none\",\n                        validation_fraction = .2, \n                        early_stop = None,\n                        warm_start = False,\n                        periodic_checkpoint_folder = None, \n                        verbose = 2,\n                        memory_limit = None,\n                        client = None,\n                        random_state=None,\n                        allow_inner_regressors=None,\n                        **tpotestimator_kwargs,\n        ):\n        '''\n        An sklearn baseestimator that uses genetic programming to optimize a regression pipeline.\n        For more parameters, see the TPOTEstimator class.\n\n        Parameters\n        ----------\n\n        search_space : (String, tpot.search_spaces.SearchSpace)\n                        - String : The default search space to use for the optimization.\n            | String     | Description      |\n            | :---        |    :----:   |\n            | linear  | A linear pipeline with the structure of \"Selector-&gt;(transformers+Passthrough)-&gt;(classifiers/regressors+Passthrough)-&gt;final classifier/regressor.\" For both the transformer and inner estimator layers, TPOT may choose one or more transformers/classifiers, or it may choose none. The inner classifier/regressor layer is optional. |\n            | linear-light | Same search space as linear, but without the inner classifier/regressor layer and with a reduced set of faster running estimators. |\n            | graph | TPOT will optimize a pipeline in the shape of a directed acyclic graph. The nodes of the graph can include selectors, scalers, transformers, or classifiers/regressors (inner classifiers/regressors can optionally be not included). This will return a custom GraphPipeline rather than an sklearn Pipeline. More details in Tutorial 6. |\n            | graph-light | Same as graph search space, but without the inner classifier/regressors and with a reduced set of faster running estimators. |\n            | mdr |TPOT will search over a series of feature selectors and Multifactor Dimensionality Reduction models to find a series of operators that maximize prediction accuracy. The TPOT MDR configuration is specialized for genome-wide association studies (GWAS), and is described in detail online here.\n\n            Note that TPOT MDR may be slow to run because the feature selection routines are computationally expensive, especially on large datasets. |\n            - SearchSpace : The search space to use for the optimization. This should be an instance of a SearchSpace.\n                The search space to use for the optimization. This should be an instance of a SearchSpace.\n                TPOT has groups of search spaces found in the following folders, tpot.search_spaces.nodes for the nodes in the pipeline and tpot.search_spaces.pipelines for the pipeline structure.\n\n        scorers : (list, scorer)\n            A scorer or list of scorers to be used in the cross-validation process.\n            see https://scikit-learn.org/stable/modules/model_evaluation.html\n\n        scorers_weights : list\n            A list of weights to be applied to the scorers during the optimization process.\n\n        classification : bool\n            If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem.\n            Used to determine the CV strategy.\n\n        cv : int, cross-validator\n            - (int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.\n            - (sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.\n                - max_depth (int): The maximum depth from any node to the root of the pipelines to be generated.\n\n        other_objective_functions : list, default=[]\n            A list of other objective functions to apply to the pipeline. The function takes a single parameter for the graphpipeline estimator and returns either a single score or a list of scores.\n\n        other_objective_functions_weights : list, default=[]\n            A list of weights to be applied to the other objective functions.\n\n        objective_function_names : list, default=None\n            A list of names to be applied to the objective functions. If None, will use the names of the objective functions.\n\n        bigger_is_better : bool, default=True\n            If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n\n        categorical_features : list or None\n            Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n\n        categorical_features: list or None\n            Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n            - None : If None, TPOT will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing.\n            - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices\n\n\n        memory: Memory object or string, default=None\n            If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. This feature\n            is used to avoid computing the fit transformers within a pipeline if the parameters\n            and input data are identical with another fitted pipeline during optimization process.\n            - String 'auto':\n                TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.\n            - String path of a caching directory\n                TPOT uses memory caching with the provided directory and TPOT does NOT clean\n                the caching directory up upon shutdown. If the directory does not exist, TPOT will\n                create it.\n            - Memory object:\n                TPOT uses the instance of joblib.Memory for memory caching,\n                and TPOT does NOT clean the caching directory up upon shutdown.\n            - None:\n                TPOT does not use memory caching.\n\n        preprocessing : bool or BaseEstimator/Pipeline,\n            EXPERIMENTAL\n            A pipeline that will be used to preprocess the data before CV. Note that the parameters for these steps are not optimized. Add them to the search space to be optimized.\n            - bool : If True, will use a default preprocessing pipeline which includes imputation followed by one hot encoding.\n            - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.\n\n        max_time_mins : float, default=float(\"inf\")\n            Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n\n        max_eval_time_mins : float, default=60*5\n            Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n\n\n        n_jobs : int, default=1\n            Number of processes to run in parallel.\n\n        validation_strategy : str, default='none'\n            EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT may overfit the cross validation score. A second validation set can be used to select the final pipeline.\n            - 'auto' : Automatically determine the validation strategy based on the dataset shape.\n            - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets.\n            - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets.\n            - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.\n\n        validation_fraction : float, default=0.2\n          EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.\n\n        early_stop : int, default=None\n            Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered. In general a value of around 5-20 is good.\n\n        warm_start : bool, default=False\n            If True, will use the continue the evolutionary algorithm from the last generation of the previous run.\n\n        periodic_checkpoint_folder : str, default=None\n            Folder to save the population to periodically. If None, no periodic saving will be done.\n            If provided, training will resume from this checkpoint.\n\n\n        verbose : int, default=1\n            How much information to print during the optimization process. Higher values include the information from lower values.\n            0. nothing\n            1. progress bar\n\n            3. best individual\n            4. warnings\n            &gt;=5. full warnings trace\n            6. evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_mins in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)\n\n\n        memory_limit : str, default=None\n            Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n\n        client : dask.distributed.Client, default=None\n            A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.\n\n        random_state : int, None, default=None\n            A seed for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes\n\n            - int\n                Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'\n            - None\n                Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS\n\n        allow_inner_regressors : bool, default=True\n            If True, the search space will include ensembled regressors.\n\n        Attributes\n        ----------\n\n        fitted_pipeline_ : GraphPipeline\n            A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.\n\n        evaluated_individuals : A pandas data frame containing data for all evaluated individuals in the run.\n            Columns:\n            - *objective functions : The first few columns correspond to the passed in scorers and objective functions\n            - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population.\n            - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population.\n            - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator.\n            - Generation : The generation the pipeline first appeared.\n            - Pareto_Front\t: The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.\n                            To save on computational time, the best frontier is updated iteratively each generation.\n                            The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.\n                            All other pipelines are set to NaN.\n            - Instance\t: The unfitted GraphPipeline BaseEstimator.\n            - *validation objective functions : Objective function scores evaluated on the validation set.\n            - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.\n\n        pareto_front : The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.\n        '''\n\n        self.search_space = search_space\n        self.scorers = scorers\n        self.scorers_weights = scorers_weights\n        self.cv = cv\n        self.other_objective_functions = other_objective_functions\n        self.other_objective_functions_weights = other_objective_functions_weights\n        self.objective_function_names = objective_function_names\n        self.bigger_is_better = bigger_is_better\n        self.categorical_features = categorical_features\n        self.memory = memory\n        self.preprocessing = preprocessing\n        self.max_time_mins = max_time_mins\n        self.max_eval_time_mins = max_eval_time_mins\n        self.n_jobs = n_jobs\n        self.validation_strategy = validation_strategy\n        self.validation_fraction = validation_fraction\n        self.early_stop = early_stop\n        self.warm_start = warm_start\n        self.periodic_checkpoint_folder = periodic_checkpoint_folder\n        self.verbose = verbose\n        self.memory_limit = memory_limit\n        self.client = client\n        self.random_state = random_state\n        self.allow_inner_regressors = allow_inner_regressors\n        self.tpotestimator_kwargs = tpotestimator_kwargs\n\n        self.initialized = False\n\n\n    def fit(self, X, y):\n\n        if not self.initialized:\n            get_search_space_params = {\"n_classes\": None, \n                                        \"n_samples\":len(y), \n                                        \"n_features\":X.shape[1], \n                                        \"random_state\":self.random_state}\n\n            search_space = get_template_search_spaces(self.search_space, classification=False, inner_predictors=self.allow_inner_regressors, **get_search_space_params)\n\n            super(TPOTRegressor,self).__init__(\n                search_space=search_space,\n                scorers=self.scorers, \n                scorers_weights=self.scorers_weights,\n                cv=self.cv,\n                other_objective_functions=self.other_objective_functions, #tpot.objectives.estimator_objective_functions.number_of_nodes_objective],\n                other_objective_functions_weights = self.other_objective_functions_weights,\n                objective_function_names = self.objective_function_names,\n                bigger_is_better = self.bigger_is_better,\n                categorical_features = self.categorical_features,\n                memory = self.memory,\n                preprocessing = self.preprocessing,\n                max_time_mins=self.max_time_mins, \n                max_eval_time_mins=self.max_eval_time_mins, \n                n_jobs=self.n_jobs,\n                validation_strategy = self.validation_strategy,\n                validation_fraction = self.validation_fraction, \n                early_stop = self.early_stop,\n                warm_start = self.warm_start,\n                periodic_checkpoint_folder = self.periodic_checkpoint_folder, \n                verbose = self.verbose,\n                classification=False,\n                memory_limit = self.memory_limit,\n                client = self.client,\n                random_state=self.random_state,\n                **self.tpotestimator_kwargs)\n            self.initialized = True\n\n        return super().fit(X,y)\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/templates/tpottemplates/#tpot.tpot_estimator.templates.tpottemplates.TPOTRegressor.__init__","title":"<code>__init__(search_space='linear', scorers=['neg_mean_squared_error'], scorers_weights=[1], cv=10, other_objective_functions=[], other_objective_functions_weights=[], objective_function_names=None, bigger_is_better=True, categorical_features=None, memory=None, preprocessing=False, max_time_mins=60, max_eval_time_mins=10, n_jobs=1, validation_strategy='none', validation_fraction=0.2, early_stop=None, warm_start=False, periodic_checkpoint_folder=None, verbose=2, memory_limit=None, client=None, random_state=None, allow_inner_regressors=None, **tpotestimator_kwargs)</code>","text":"<p>An sklearn baseestimator that uses genetic programming to optimize a regression pipeline. For more parameters, see the TPOTEstimator class.</p> <p>Parameters:</p> Name Type Description Default <code>search_space</code> <code>(String, SearchSpace)</code> <pre><code>        - String : The default search space to use for the optimization.\n</code></pre> String Description linear A linear pipeline with the structure of \"Selector-&gt;(transformers+Passthrough)-&gt;(classifiers/regressors+Passthrough)-&gt;final classifier/regressor.\" For both the transformer and inner estimator layers, TPOT may choose one or more transformers/classifiers, or it may choose none. The inner classifier/regressor layer is optional. linear-light Same search space as linear, but without the inner classifier/regressor layer and with a reduced set of faster running estimators. graph TPOT will optimize a pipeline in the shape of a directed acyclic graph. The nodes of the graph can include selectors, scalers, transformers, or classifiers/regressors (inner classifiers/regressors can optionally be not included). This will return a custom GraphPipeline rather than an sklearn Pipeline. More details in Tutorial 6. graph-light Same as graph search space, but without the inner classifier/regressors and with a reduced set of faster running estimators. mdr TPOT will search over a series of feature selectors and Multifactor Dimensionality Reduction models to find a series of operators that maximize prediction accuracy. The TPOT MDR configuration is specialized for genome-wide association studies (GWAS), and is described in detail online here. <p>Note that TPOT MDR may be slow to run because the feature selection routines are computationally expensive, especially on large datasets. | - SearchSpace : The search space to use for the optimization. This should be an instance of a SearchSpace.     The search space to use for the optimization. This should be an instance of a SearchSpace.     TPOT has groups of search spaces found in the following folders, tpot.search_spaces.nodes for the nodes in the pipeline and tpot.search_spaces.pipelines for the pipeline structure.</p> <code>'linear'</code> <code>scorers</code> <code>(list, scorer)</code> <p>A scorer or list of scorers to be used in the cross-validation process. see https://scikit-learn.org/stable/modules/model_evaluation.html</p> <code>['neg_mean_squared_error']</code> <code>scorers_weights</code> <code>list</code> <p>A list of weights to be applied to the scorers during the optimization process.</p> <code>[1]</code> <code>classification</code> <code>bool</code> <p>If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem. Used to determine the CV strategy.</p> required <code>cv</code> <code>(int, cross - validator)</code> <ul> <li>(int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.</li> <li>(sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.<ul> <li>max_depth (int): The maximum depth from any node to the root of the pipelines to be generated.</li> </ul> </li> </ul> <code>10</code> <code>other_objective_functions</code> <code>list</code> <p>A list of other objective functions to apply to the pipeline. The function takes a single parameter for the graphpipeline estimator and returns either a single score or a list of scores.</p> <code>[]</code> <code>other_objective_functions_weights</code> <code>list</code> <p>A list of weights to be applied to the other objective functions.</p> <code>[]</code> <code>objective_function_names</code> <code>list</code> <p>A list of names to be applied to the objective functions. If None, will use the names of the objective functions.</p> <code>None</code> <code>bigger_is_better</code> <code>bool</code> <p>If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.</p> <code>True</code> <code>categorical_features</code> <code>list or None</code> <p>Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.</p> <code>None</code> <code>categorical_features</code> <p>Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False. - None : If None, TPOT will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing. - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices</p> <code>None</code> <code>memory</code> <p>If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. This feature is used to avoid computing the fit transformers within a pipeline if the parameters and input data are identical with another fitted pipeline during optimization process. - String 'auto':     TPOT uses memory caching with a temporary directory and cleans it up upon shutdown. - String path of a caching directory     TPOT uses memory caching with the provided directory and TPOT does NOT clean     the caching directory up upon shutdown. If the directory does not exist, TPOT will     create it. - Memory object:     TPOT uses the instance of joblib.Memory for memory caching,     and TPOT does NOT clean the caching directory up upon shutdown. - None:     TPOT does not use memory caching.</p> <code>None</code> <code>preprocessing</code> <code>(bool or BaseEstimator / Pipeline)</code> <p>EXPERIMENTAL A pipeline that will be used to preprocess the data before CV. Note that the parameters for these steps are not optimized. Add them to the search space to be optimized. - bool : If True, will use a default preprocessing pipeline which includes imputation followed by one hot encoding. - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.</p> <code>False</code> <code>max_time_mins</code> <code>float</code> <p>Maximum time to run the optimization. If none or inf, will run until the end of the generations.</p> <code>float(\"inf\")</code> <code>max_eval_time_mins</code> <code>float</code> <p>Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.</p> <code>60*5</code> <code>n_jobs</code> <code>int</code> <p>Number of processes to run in parallel.</p> <code>1</code> <code>validation_strategy</code> <code>str</code> <p>EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT may overfit the cross validation score. A second validation set can be used to select the final pipeline. - 'auto' : Automatically determine the validation strategy based on the dataset shape. - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets. - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets. - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.</p> <code>'none'</code> <code>validation_fraction</code> <code>float</code> <p>EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.</p> <code>0.2</code> <code>early_stop</code> <code>int</code> <p>Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered. In general a value of around 5-20 is good.</p> <code>None</code> <code>warm_start</code> <code>bool</code> <p>If True, will use the continue the evolutionary algorithm from the last generation of the previous run.</p> <code>False</code> <code>periodic_checkpoint_folder</code> <code>str</code> <p>Folder to save the population to periodically. If None, no periodic saving will be done. If provided, training will resume from this checkpoint.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>How much information to print during the optimization process. Higher values include the information from lower values. 0. nothing 1. progress bar</p> <ol> <li>best individual</li> <li>warnings <p>=5. full warnings trace</p> </li> <li>evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_mins in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)</li> </ol> <code>1</code> <code>memory_limit</code> <code>str</code> <p>Memory limit for each job. See Dask LocalCluster documentation for more information.</p> <code>None</code> <code>client</code> <code>Client</code> <p>A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.</p> <code>None</code> <code>random_state</code> <code>(int, None)</code> <p>A seed for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes</p> <ul> <li>int     Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'</li> <li>None     Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS</li> </ul> <code>None</code> <code>allow_inner_regressors</code> <code>bool</code> <p>If True, the search space will include ensembled regressors.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>fitted_pipeline_</code> <code>GraphPipeline</code> <p>A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.</p> <code>evaluated_individuals</code> <code>A pandas data frame containing data for all evaluated individuals in the run.</code> <p>Columns: - objective functions : The first few columns correspond to the passed in scorers and objective functions - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population. - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population. - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator. - Generation : The generation the pipeline first appeared. - Pareto_Front      : The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.                 To save on computational time, the best frontier is updated iteratively each generation.                 The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.                 All other pipelines are set to NaN. - Instance  : The unfitted GraphPipeline BaseEstimator. - validation objective functions : Objective function scores evaluated on the validation set. - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.</p> <code>pareto_front</code> <code>The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.</code> Source code in <code>tpot/tpot_estimator/templates/tpottemplates.py</code> <pre><code>def __init__(       self,\n                    search_space = \"linear\",\n                    scorers=['neg_mean_squared_error'], \n                    scorers_weights=[1],\n                    cv = 10, #remove this and use a value based on dataset size?\n                    other_objective_functions=[], #tpot.objectives.estimator_objective_functions.number_of_nodes_objective],\n                    other_objective_functions_weights = [],\n                    objective_function_names = None,\n                    bigger_is_better = True,\n                    categorical_features = None,\n                    memory = None,\n                    preprocessing = False,\n                    max_time_mins=60, \n                    max_eval_time_mins=10, \n                    n_jobs = 1,\n                    validation_strategy = \"none\",\n                    validation_fraction = .2, \n                    early_stop = None,\n                    warm_start = False,\n                    periodic_checkpoint_folder = None, \n                    verbose = 2,\n                    memory_limit = None,\n                    client = None,\n                    random_state=None,\n                    allow_inner_regressors=None,\n                    **tpotestimator_kwargs,\n    ):\n    '''\n    An sklearn baseestimator that uses genetic programming to optimize a regression pipeline.\n    For more parameters, see the TPOTEstimator class.\n\n    Parameters\n    ----------\n\n    search_space : (String, tpot.search_spaces.SearchSpace)\n                    - String : The default search space to use for the optimization.\n        | String     | Description      |\n        | :---        |    :----:   |\n        | linear  | A linear pipeline with the structure of \"Selector-&gt;(transformers+Passthrough)-&gt;(classifiers/regressors+Passthrough)-&gt;final classifier/regressor.\" For both the transformer and inner estimator layers, TPOT may choose one or more transformers/classifiers, or it may choose none. The inner classifier/regressor layer is optional. |\n        | linear-light | Same search space as linear, but without the inner classifier/regressor layer and with a reduced set of faster running estimators. |\n        | graph | TPOT will optimize a pipeline in the shape of a directed acyclic graph. The nodes of the graph can include selectors, scalers, transformers, or classifiers/regressors (inner classifiers/regressors can optionally be not included). This will return a custom GraphPipeline rather than an sklearn Pipeline. More details in Tutorial 6. |\n        | graph-light | Same as graph search space, but without the inner classifier/regressors and with a reduced set of faster running estimators. |\n        | mdr |TPOT will search over a series of feature selectors and Multifactor Dimensionality Reduction models to find a series of operators that maximize prediction accuracy. The TPOT MDR configuration is specialized for genome-wide association studies (GWAS), and is described in detail online here.\n\n        Note that TPOT MDR may be slow to run because the feature selection routines are computationally expensive, especially on large datasets. |\n        - SearchSpace : The search space to use for the optimization. This should be an instance of a SearchSpace.\n            The search space to use for the optimization. This should be an instance of a SearchSpace.\n            TPOT has groups of search spaces found in the following folders, tpot.search_spaces.nodes for the nodes in the pipeline and tpot.search_spaces.pipelines for the pipeline structure.\n\n    scorers : (list, scorer)\n        A scorer or list of scorers to be used in the cross-validation process.\n        see https://scikit-learn.org/stable/modules/model_evaluation.html\n\n    scorers_weights : list\n        A list of weights to be applied to the scorers during the optimization process.\n\n    classification : bool\n        If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem.\n        Used to determine the CV strategy.\n\n    cv : int, cross-validator\n        - (int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.\n        - (sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.\n            - max_depth (int): The maximum depth from any node to the root of the pipelines to be generated.\n\n    other_objective_functions : list, default=[]\n        A list of other objective functions to apply to the pipeline. The function takes a single parameter for the graphpipeline estimator and returns either a single score or a list of scores.\n\n    other_objective_functions_weights : list, default=[]\n        A list of weights to be applied to the other objective functions.\n\n    objective_function_names : list, default=None\n        A list of names to be applied to the objective functions. If None, will use the names of the objective functions.\n\n    bigger_is_better : bool, default=True\n        If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n\n    categorical_features : list or None\n        Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n\n    categorical_features: list or None\n        Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n        - None : If None, TPOT will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing.\n        - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices\n\n\n    memory: Memory object or string, default=None\n        If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. This feature\n        is used to avoid computing the fit transformers within a pipeline if the parameters\n        and input data are identical with another fitted pipeline during optimization process.\n        - String 'auto':\n            TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.\n        - String path of a caching directory\n            TPOT uses memory caching with the provided directory and TPOT does NOT clean\n            the caching directory up upon shutdown. If the directory does not exist, TPOT will\n            create it.\n        - Memory object:\n            TPOT uses the instance of joblib.Memory for memory caching,\n            and TPOT does NOT clean the caching directory up upon shutdown.\n        - None:\n            TPOT does not use memory caching.\n\n    preprocessing : bool or BaseEstimator/Pipeline,\n        EXPERIMENTAL\n        A pipeline that will be used to preprocess the data before CV. Note that the parameters for these steps are not optimized. Add them to the search space to be optimized.\n        - bool : If True, will use a default preprocessing pipeline which includes imputation followed by one hot encoding.\n        - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.\n\n    max_time_mins : float, default=float(\"inf\")\n        Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n\n    max_eval_time_mins : float, default=60*5\n        Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n\n\n    n_jobs : int, default=1\n        Number of processes to run in parallel.\n\n    validation_strategy : str, default='none'\n        EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT may overfit the cross validation score. A second validation set can be used to select the final pipeline.\n        - 'auto' : Automatically determine the validation strategy based on the dataset shape.\n        - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets.\n        - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets.\n        - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.\n\n    validation_fraction : float, default=0.2\n      EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.\n\n    early_stop : int, default=None\n        Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered. In general a value of around 5-20 is good.\n\n    warm_start : bool, default=False\n        If True, will use the continue the evolutionary algorithm from the last generation of the previous run.\n\n    periodic_checkpoint_folder : str, default=None\n        Folder to save the population to periodically. If None, no periodic saving will be done.\n        If provided, training will resume from this checkpoint.\n\n\n    verbose : int, default=1\n        How much information to print during the optimization process. Higher values include the information from lower values.\n        0. nothing\n        1. progress bar\n\n        3. best individual\n        4. warnings\n        &gt;=5. full warnings trace\n        6. evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_mins in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)\n\n\n    memory_limit : str, default=None\n        Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n\n    client : dask.distributed.Client, default=None\n        A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.\n\n    random_state : int, None, default=None\n        A seed for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes\n\n        - int\n            Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'\n        - None\n            Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS\n\n    allow_inner_regressors : bool, default=True\n        If True, the search space will include ensembled regressors.\n\n    Attributes\n    ----------\n\n    fitted_pipeline_ : GraphPipeline\n        A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.\n\n    evaluated_individuals : A pandas data frame containing data for all evaluated individuals in the run.\n        Columns:\n        - *objective functions : The first few columns correspond to the passed in scorers and objective functions\n        - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population.\n        - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population.\n        - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator.\n        - Generation : The generation the pipeline first appeared.\n        - Pareto_Front\t: The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.\n                        To save on computational time, the best frontier is updated iteratively each generation.\n                        The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.\n                        All other pipelines are set to NaN.\n        - Instance\t: The unfitted GraphPipeline BaseEstimator.\n        - *validation objective functions : Objective function scores evaluated on the validation set.\n        - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.\n\n    pareto_front : The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.\n    '''\n\n    self.search_space = search_space\n    self.scorers = scorers\n    self.scorers_weights = scorers_weights\n    self.cv = cv\n    self.other_objective_functions = other_objective_functions\n    self.other_objective_functions_weights = other_objective_functions_weights\n    self.objective_function_names = objective_function_names\n    self.bigger_is_better = bigger_is_better\n    self.categorical_features = categorical_features\n    self.memory = memory\n    self.preprocessing = preprocessing\n    self.max_time_mins = max_time_mins\n    self.max_eval_time_mins = max_eval_time_mins\n    self.n_jobs = n_jobs\n    self.validation_strategy = validation_strategy\n    self.validation_fraction = validation_fraction\n    self.early_stop = early_stop\n    self.warm_start = warm_start\n    self.periodic_checkpoint_folder = periodic_checkpoint_folder\n    self.verbose = verbose\n    self.memory_limit = memory_limit\n    self.client = client\n    self.random_state = random_state\n    self.allow_inner_regressors = allow_inner_regressors\n    self.tpotestimator_kwargs = tpotestimator_kwargs\n\n    self.initialized = False\n</code></pre>"},{"location":"documentation/tpot/utils/amltk_parser/","title":"Amltk parser","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/utils/amltk_parser/#tpot.utils.amltk_parser.tpot_parser","title":"<code>tpot_parser(node)</code>","text":"<p>Convert amltk pipeline search space into a tpot pipeline search space.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The node to convert.</p> required <p>Returns:</p> Type Description <code>SearchSpace</code> <p>The equivalent TPOT search space which can be optimized by TPOT.</p> Source code in <code>tpot/utils/amltk_parser.py</code> <pre><code>def tpot_parser(\n    node: Node,\n    ):\n    \"\"\"\n    Convert amltk pipeline search space into a tpot pipeline search space.\n\n    Parameters\n    ----------\n    node: amltk.pipeline.Node\n        The node to convert.\n\n    Returns\n    -------\n    tpot.search_spaces.base.SearchSpace\n        The equivalent TPOT search space which can be optimized by TPOT.\n    \"\"\"\n\n    if isinstance(node, Component):\n        return component_to_estimatornode(node)\n    elif isinstance(node, Sequential):\n        return sequential_to_sequentialpipeline(node)\n    elif isinstance(node, Choice):\n        return choice_to_choicepipeline(node)\n    elif isinstance(node, Fixed):\n        return fixed_to_estimatornode(node)\n    elif isinstance(node, Split):\n        return split_to_unionpipeline(node)\n    else:\n        raise ValueError(f\"Node type {type(node)} not supported\")\n</code></pre>"},{"location":"documentation/tpot/utils/eval_utils/","title":"Eval utils","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/utils/eval_utils/#tpot.utils.eval_utils.process_scores","title":"<code>process_scores(scores, n)</code>","text":"<p>Purpose: This function processes a list of scores to ensure that each score list has the same length, n. If a score list is shorter than n, the function fills the list with either \"TIMEOUT\" or \"INVALID\" values.</p> <p>Parameters:</p> <pre><code>scores: A list of score lists. Each score list represents a set of scores for a particular player or team. The score lists may have different lengths.\nn: An integer representing the desired length for each score list.\n</code></pre> <p>Returns:</p> <pre><code>The scores list, after processing.\n</code></pre> Source code in <code>tpot/utils/eval_utils.py</code> <pre><code>def process_scores(scores, n):\n    '''\n    Purpose: This function processes a list of scores to ensure that each score list has the same length, n. If a score list is shorter than n, the function fills the list with either \"TIMEOUT\" or \"INVALID\" values.\n\n    Parameters:\n\n        scores: A list of score lists. Each score list represents a set of scores for a particular player or team. The score lists may have different lengths.\n        n: An integer representing the desired length for each score list.\n\n    Returns:\n\n        The scores list, after processing.\n\n    '''\n    for i in range(len(scores)):\n        if len(scores[i]) &lt; n:\n            if \"TIMEOUT\" in scores[i]:\n                scores[i] = [\"TIMEOUT\" for j in range(n)]\n            else:\n                scores[i] = [\"INVALID\" for j in range(n)]\n    return scores\n</code></pre>"},{"location":"documentation/tpot/utils/utils/","title":"Utils","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/utils/utils/#tpot.utils.utils.is_pareto_efficient","title":"<code>is_pareto_efficient(scores, return_mask=True)</code>","text":"<p>Find the pareto-efficient points :param scores: An (n_points, n_scores) array :param return_mask: True to return a mask :return: An array of indices of pareto-efficient points.     If return_mask is True, this will be an (n_points, ) boolean array     Otherwise it will be a (n_efficient_points, ) integer array of indices.</p> Source code in <code>tpot/utils/utils.py</code> <pre><code>def is_pareto_efficient(scores, return_mask = True):\n    \"\"\"\n    Find the pareto-efficient points\n    :param scores: An (n_points, n_scores) array\n    :param return_mask: True to return a mask\n    :return: An array of indices of pareto-efficient points.\n        If return_mask is True, this will be an (n_points, ) boolean array\n        Otherwise it will be a (n_efficient_points, ) integer array of indices.\n    \"\"\"\n    is_efficient = np.arange(scores.shape[0])\n    n_points = scores.shape[0]\n    next_point_index = 0  # Next index in the is_efficient array to search for\n    while next_point_index&lt;len(scores):\n        nondominated_point_mask = np.any(scores&gt;scores[next_point_index], axis=1)\n        nondominated_point_mask[next_point_index] = True\n        is_efficient = is_efficient[nondominated_point_mask]  # Remove dominated points\n        scores = scores[nondominated_point_mask]\n        next_point_index = np.sum(nondominated_point_mask[:next_point_index])+1\n    if return_mask:\n        is_efficient_mask = np.zeros(n_points, dtype = bool)\n        is_efficient_mask[is_efficient] = True\n        return is_efficient_mask\n    else:\n        return is_efficient\n</code></pre>"}]}