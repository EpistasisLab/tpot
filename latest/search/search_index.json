{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#tpot","title":"TPOT","text":"<p>TPOT stands for Tree-based Pipeline Optimization Tool. TPOT is a Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming. Consider TPOT your Data Science Assistant.</p>"},{"location":"#contributors","title":"Contributors","text":"<p>TPOT recently went through a major refactoring. The package was rewritten from scratch to improve efficiency and performance, support new features, and fix numerous bugs. New features include genetic feature selection, a significantly expanded and more flexible method of defining search spaces, multi-objective optimization, a more modular framework allowing for easier customization of the evolutionary algorithm, and more. While in development, this new version was referred to as \"TPOT2\" but we have now merged what was once TPOT2 into the main TPOT package. You can learn more about this new version of TPOT in our GPTP paper titled \"TPOT2: A New Graph-Based Implementation of the Tree-Based Pipeline Optimization Tool for Automated Machine Learning.\"</p> <pre><code>Ribeiro, P. et al. (2024). TPOT2: A New Graph-Based Implementation of the Tree-Based Pipeline Optimization Tool for Automated Machine Learning. In: Winkler, S., Trujillo, L., Ofria, C., Hu, T. (eds) Genetic Programming Theory and Practice XX. Genetic and Evolutionary Computation. Springer, Singapore. https://doi.org/10.1007/978-981-99-8413-8_1\n</code></pre> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (Lead developer - https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)  </p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors  </p>"},{"location":"#license","title":"License","text":"<p>Please see the repository license for the licensing and usage information for TPOT. Generally, we have licensed TPOT to make it as widely usable as possible.</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>The documentation webpage can be found here.</p> <p>We also recommend looking at the Tutorials folder for jupyter notebooks with examples and guides.</p>"},{"location":"#installation","title":"Installation","text":"<p>TPOT requires a working installation of Python.</p>"},{"location":"#creating-a-conda-environment-optional","title":"Creating a conda environment (optional)","text":"<p>We recommend using conda environments for installing TPOT, though it would work equally well if manually installed without it.</p> <p>More information on making anaconda environments found here.</p> <pre><code>conda create --name tpotenv python=3.10\nconda activate tpotenv\n</code></pre>"},{"location":"#packages-used","title":"Packages Used","text":"<p>python version &gt;=3.10, &lt;3.13 numpy scipy scikit-learn update_checker tqdm stopit pandas joblib xgboost matplotlib traitlets lightgbm optuna jupyter networkx dask distributed dask-ml dask-jobqueue func_timeout configspace</p> <p>Many of the hyperparameter ranges used in our configspaces were adapted from either the original TPOT package or the AutoSklearn package. </p>"},{"location":"#note-for-m1-mac-or-other-arm-based-cpu-users","title":"Note for M1 Mac or other Arm-based CPU users","text":"<p>You need to install the lightgbm package directly from conda using the following command before installing TPOT. </p> <p>This is to ensure that you get the version that is compatible with your system.</p> <pre><code>conda install --yes -c conda-forge 'lightgbm&gt;=3.3.3'\n</code></pre>"},{"location":"#installing-extra-features-with-pip","title":"Installing Extra Features with pip","text":"<p>If you want to utilize the additional features provided by TPOT along with <code>scikit-learn</code> extensions, you can install them using <code>pip</code>. The command to install TPOT with these extra features is as follows:</p> <pre><code>pip install tpot[sklearnex]\n</code></pre> <p>Please note that while these extensions can speed up scikit-learn packages, there are some important considerations:</p> <p>These extensions may not be fully developed and tested on Arm-based CPUs, such as M1 Macs. You might encounter compatibility issues or reduced performance on such systems.</p> <p>We recommend using Python 3.9 when installing these extra features, as it provides better compatibility and stability.</p>"},{"location":"#developerlatest-branch-installation","title":"Developer/Latest Branch Installation","text":"<pre><code>pip install -e /path/to/tpotrepo\n</code></pre> <p>If you downloaded with git pull, then the repository folder will be named TPOT. (Note: this folder is the one that includes setup.py inside of it and not the folder of the same name inside it). If you downloaded as a zip, the folder may be called tpot-main. </p>"},{"location":"#usage","title":"Usage","text":"<p>See the Tutorials Folder for more instructions and examples.</p>"},{"location":"#best-practices","title":"Best Practices","text":""},{"location":"#1","title":"1","text":"<p>TPOT uses dask for parallel processing. When Python is parallelized, each module is imported within each processes. Therefore it is important to protect all code within a <code>if __name__ == \"__main__\"</code> when running TPOT from a script. This is not required when running TPOT from a notebook.</p> <p>For example:</p> <pre><code>#my_analysis.py\n\nimport tpot\nif __name__ == \"__main__\":\n    X, y = load_my_data()\n    est = tpot.TPOTClassifier()\n    est.fit(X,y)\n    #rest of analysis\n</code></pre>"},{"location":"#2","title":"2","text":"<p>When designing custom objective functions, avoid the use of global variables.</p> <p>Don't Do: <pre><code>global_X = [[1,2],[4,5]]\nglobal_y = [0,1]\ndef foo(est):\n    return my_scorer(est, X=global_X, y=global_y)\n</code></pre></p> <p>Instead use a partial</p> <pre><code>from functools import partial\n\ndef foo_scorer(est, X, y):\n    return my_scorer(est, X, y)\n\nif __name__=='__main__':\n    X = [[1,2],[4,5]]\n    y = [0,1]\n    final_scorer = partial(foo_scorer, X=X, y=y)\n</code></pre> <p>Similarly when using lambda functions.</p> <p>Dont Do:</p> <pre><code>def new_objective(est, a, b)\n    #definition\n\na = 100\nb = 20\nbad_function = lambda est :  new_objective(est=est, a=a, b=b)\n</code></pre> <p>Do: <pre><code>def new_objective(est, a, b)\n    #definition\n\na = 100\nb = 20\ngood_function = lambda est, a=a, b=b : new_objective(est=est, a=a, b=b)\n</code></pre></p>"},{"location":"#tips","title":"Tips","text":"<p>TPOT will not check if your data is correctly formatted. It will assume that you have passed in operators that can handle the type of data that was passed in. For instance, if you pass in a pandas dataframe with categorical features and missing data, then you should also include in your configuration operators that can handle those feautures of the data. Alternatively, if you pass in <code>preprocessing = True</code>, TPOT will impute missing values, one hot encode categorical features, then standardize the data. (Note that this is currently fitted and transformed on the entire training set before splitting for CV. Later there will be an option to apply per fold, and have the parameters be learnable.)</p> <p>Setting <code>verbose</code> to 5 can be helpful during debugging as it will print out the error generated by failing pipelines. </p>"},{"location":"#contributing-to-tpot","title":"Contributing to TPOT","text":"<p>We welcome you to check the existing issues for bugs or enhancements to work on. If you have an idea for an extension to TPOT, please file a new issue so we can discuss it.</p>"},{"location":"#citing-tpot","title":"Citing TPOT","text":"<p>If you use TPOT in a scientific publication, please consider citing at least one of the following papers:</p> <p>Trang T. Le, Weixuan Fu and Jason H. Moore (2020). Scaling tree-based automated machine learning to biomedical big data with a feature set selector. Bioinformatics.36(1): 250-256.</p> <p>BibTeX entry:</p> <pre><code>@article{le2020scaling,\n  title={Scaling tree-based automated machine learning to biomedical big data with a feature set selector},\n  author={Le, Trang T and Fu, Weixuan and Moore, Jason H},\n  journal={Bioinformatics},\n  volume={36},\n  number={1},\n  pages={250--256},\n  year={2020},\n  publisher={Oxford University Press}\n}\n</code></pre> <p>Randal S. Olson, Ryan J. Urbanowicz, Peter C. Andrews, Nicole A. Lavender, La Creis Kidd, and Jason H. Moore (2016). Automating biomedical data science through tree-based pipeline optimization. Applications of Evolutionary Computation, pages 123-137.</p> <p>BibTeX entry:</p> <pre><code>@inbook{Olson2016EvoBio,\n    author={Olson, Randal S. and Urbanowicz, Ryan J. and Andrews, Peter C. and Lavender, Nicole A. and Kidd, La Creis and Moore, Jason H.},\n    editor={Squillero, Giovanni and Burelli, Paolo},\n    chapter={Automating Biomedical Data Science Through Tree-Based Pipeline Optimization},\n    title={Applications of Evolutionary Computation: 19th European Conference, EvoApplications 2016, Porto, Portugal, March 30 -- April 1, 2016, Proceedings, Part I},\n    year={2016},\n    publisher={Springer International Publishing},\n    pages={123--137},\n    isbn={978-3-319-31204-0},\n    doi={10.1007/978-3-319-31204-0_9},\n    url={http://dx.doi.org/10.1007/978-3-319-31204-0_9}\n}\n</code></pre> <p>Randal S. Olson, Nathan Bartley, Ryan J. Urbanowicz, and Jason H. Moore (2016). Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science. Proceedings of GECCO 2016, pages 485-492.</p> <p>BibTeX entry:</p> <pre><code>@inproceedings{OlsonGECCO2016,\n    author = {Olson, Randal S. and Bartley, Nathan and Urbanowicz, Ryan J. and Moore, Jason H.},\n    title = {Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science},\n    booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference 2016},\n    series = {GECCO '16},\n    year = {2016},\n    isbn = {978-1-4503-4206-3},\n    location = {Denver, Colorado, USA},\n    pages = {485--492},\n    numpages = {8},\n    url = {http://doi.acm.org/10.1145/2908812.2908918},\n    doi = {10.1145/2908812.2908918},\n    acmid = {2908918},\n    publisher = {ACM},\n    address = {New York, NY, USA},\n}\n</code></pre>"},{"location":"#support-for-tpot","title":"Support for TPOT","text":"<p>TPOT was developed in the Artificial Intelligence Innovation (A2I) Lab at Cedars-Sinai with funding from the NIH under grants U01 AG066833 and R01 LM010098. We are incredibly grateful for the support of the NIH and the Cedars-Sinai during the development of this project.</p> <p>The TPOT logo was designed by Todd Newmuis, who generously donated his time to the project.</p>"},{"location":"cite/","title":"Citing TPOT","text":"<p>If you use TPOT in a scientific publication, please consider citing at least one of the following papers:</p> <p>Trang T. Le, Weixuan Fu and Jason H. Moore (2020). Scaling tree-based automated machine learning to biomedical big data with a feature set selector. Bioinformatics.36(1): 250-256.</p> <p>BibTeX entry:</p> <pre><code>@article{le2020scaling,\n  title={Scaling tree-based automated machine learning to biomedical big data with a feature set selector},\n  author={Le, Trang T and Fu, Weixuan and Moore, Jason H},\n  journal={Bioinformatics},\n  volume={36},\n  number={1},\n  pages={250--256},\n  year={2020},\n  publisher={Oxford University Press}\n}\n</code></pre> <p>Randal S. Olson, Ryan J. Urbanowicz, Peter C. Andrews, Nicole A. Lavender, La Creis Kidd, and Jason H. Moore (2016). Automating biomedical data science through tree-based pipeline optimization. Applications of Evolutionary Computation, pages 123-137.</p> <p>BibTeX entry:</p> <pre><code>@inbook{Olson2016EvoBio,\n    author={Olson, Randal S. and Urbanowicz, Ryan J. and Andrews, Peter C. and Lavender, Nicole A. and Kidd, La Creis and Moore, Jason H.},\n    editor={Squillero, Giovanni and Burelli, Paolo},\n    chapter={Automating Biomedical Data Science Through Tree-Based Pipeline Optimization},\n    title={Applications of Evolutionary Computation: 19th European Conference, EvoApplications 2016, Porto, Portugal, March 30 -- April 1, 2016, Proceedings, Part I},\n    year={2016},\n    publisher={Springer International Publishing},\n    pages={123--137},\n    isbn={978-3-319-31204-0},\n    doi={10.1007/978-3-319-31204-0_9},\n    url={http://dx.doi.org/10.1007/978-3-319-31204-0_9}\n}\n</code></pre> <p>Randal S. Olson, Nathan Bartley, Ryan J. Urbanowicz, and Jason H. Moore (2016). Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science. Proceedings of GECCO 2016, pages 485-492.</p> <p>BibTeX entry:</p> <pre><code>@inproceedings{OlsonGECCO2016,\n    author = {Olson, Randal S. and Bartley, Nathan and Urbanowicz, Ryan J. and Moore, Jason H.},\n    title = {Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science},\n    booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference 2016},\n    series = {GECCO '16},\n    year = {2016},\n    isbn = {978-1-4503-4206-3},\n    location = {Denver, Colorado, USA},\n    pages = {485--492},\n    numpages = {8},\n    url = {http://doi.acm.org/10.1145/2908812.2908918},\n    doi = {10.1145/2908812.2908918},\n    acmid = {2908918},\n    publisher = {ACM},\n    address = {New York, NY, USA},\n}\n</code></pre>"},{"location":"contribute/","title":"Contributing","text":"<p>We welcome you to check the existing issues for bugs or enhancements to work on. If you have an idea for an extension to TPOT, please file a new issue so we can discuss it.</p>"},{"location":"contribute/#contribution-guide","title":"Contribution Guide","text":"<p>We welcome you to check the existing issues for bugs or enhancements to work on. If you have an idea for an extension to TPOT, please file a new issue so we can discuss it.</p>"},{"location":"contribute/#project-layout","title":"Project layout","text":"<p>The latest stable release of TPOT is on the main branch, whereas the latest version of TPOT in development is on the development branch. Make sure you are looking at and working on the correct branch if you're looking to contribute code.</p> <p>In terms of directory structure:</p> <ul> <li>All of TPOT's code sources are in the <code>tpot</code> directory</li> <li>The documentation sources are in the <code>docs_sources</code> directory</li> <li>Images in the documentation are in the <code>images</code> directory</li> <li>Tutorials for TPOT are in the <code>tutorials</code> directory</li> <li>Unit tests for TPOT are in the <code>tests.py</code> file</li> </ul> <p>Make sure to familiarize yourself with the project layout before making any major contributions, and especially make sure to send all code changes to the <code>development</code> branch.</p>"},{"location":"contribute/#how-to-contribute","title":"How to contribute","text":"<p>The preferred way to contribute to TPOT is to fork the main repository on GitHub:</p> <ol> <li> <p>Fork the project repository:    click on the 'Fork' button near the top of the page. This creates    a copy of the code under your account on the GitHub server.</p> </li> <li> <p>Clone this copy to your local disk:</p> <pre><code>  $ git clone git@github.com:YourUsername/tpot.git\n  $ cd tpot\n</code></pre> </li> <li> <p>Create a branch to hold your changes:</p> <pre><code>  $ git checkout -b my-contribution\n</code></pre> </li> <li> <p>Make sure your local environment is setup correctly for development. Installation instructions are almost identical to the user instructions except that TPOT should not be installed. If you have TPOT installed on your computer then make sure you are using a virtual environment that does not have TPOT installed. Furthermore, you should make sure you have installed the <code>pytest</code> package into your development environment so that you can test changes locally.</p> <pre><code>  $ conda install pytest\n</code></pre> </li> <li> <p>Start making changes on your newly created branch, remembering to never work on the <code>main</code> branch! Work on this copy on your computer using Git to do the version control.</p> </li> <li> <p>Check your changes haven't broken any existing tests and pass all your new tests. Navigate the terminal into the <code>tpot/tpot/</code> folder and run the command <code>pytest</code> to start all tests. (note, you must have the <code>pytest</code> package installed within your dev environment for this to work):</p> <pre><code>  $ pytest\n</code></pre> </li> <li> <p>When you're done editing and local testing, run:</p> <pre><code>  $ git add modified_files\n  $ git commit\n</code></pre> </li> </ol> <p>to record your changes in Git, then push them to GitHub with:</p> <pre><code>      $ git push -u origin my-contribution\n</code></pre> <p>Finally, go to the web page of your fork of the TPOT repo, and click 'Pull Request' (PR) to send your changes to the maintainers for review. Make sure that you send your PR to the <code>dev</code> branch, as the <code>main</code> branch is reserved for the latest stable release. This will start the CI server to check all the project's unit tests run and send an email to the maintainers.</p> <p>(If any of the above seems like magic to you, then look up the Git documentation on the web.)</p>"},{"location":"contribute/#before-submitting-your-pull-request","title":"Before submitting your pull request","text":"<p>Before you submit a pull request for your contribution, please work through this checklist to make sure that you have done everything necessary so we can efficiently review and accept your changes.</p> <p>If your contribution changes TPOT in any way:</p> <ul> <li> <p>Update the documentation so all of your changes are reflected there.</p> </li> <li> <p>Update the README if anything there has changed.</p> </li> </ul> <p>If your contribution involves any code changes:</p> <ul> <li> <p>Update the project unit tests to test your code changes.</p> </li> <li> <p>Make sure that your code is properly commented with docstrings and comments explaining your rationale behind non-obvious coding practices.</p> </li> </ul> <p>If your contribution requires a new library dependency:</p> <ul> <li>Double-check that the new dependency is easy to install via <code>pip</code> or Anaconda. If the dependency requires a complicated installation, then we most likely won't merge your changes because we want to keep TPOT easy to install.</li> </ul>"},{"location":"contribute/#after-submitting-your-pull-request","title":"After submitting your pull request","text":"<p>After submitting your pull request, GitHub will automatically run unit tests on your changes and make sure that your updated code builds and runs. We also use services that automatically check code quality and test coverage.</p> <p>Check back shortly after submitting your pull request to make sure that your code passes these checks. If any of the checks come back with a red X, then do your best to address the errors.</p>"},{"location":"installation/","title":"Installation","text":"<p>TPOT requires a working installation of Python.</p>"},{"location":"installation/#creating-a-conda-environment-optional","title":"Creating a conda environment (optional)","text":"<p>We recommend using conda environments for installing TPOT, though it would work equally well if manually installed without it.</p> <p>More information on making anaconda environments found here.</p> <pre><code>conda create --name tpotenv python=3.12\nconda activate tpotenv\n</code></pre>"},{"location":"installation/#note-for-m1-mac-or-other-arm-based-cpu-users","title":"Note for M1 Mac or other Arm-based CPU users","text":"<p>You need to install the lightgbm package directly from conda using the following command before installing TPOT. </p> <p>This is to ensure that you get the version that is compatible with your system.</p> <pre><code>conda install --yes -c conda-forge 'lightgbm&gt;=3.3.3'\n</code></pre>"},{"location":"installation/#developerlatest-branch-installation","title":"Developer/Latest Branch Installation","text":"<pre><code>pip install -e /path/to/tpotrepo\n</code></pre> <p>If you downloaded with git pull, then the repository folder will be named TPOT. (Note: this folder is the one that includes setup.py inside of it and not the folder of the same name inside it). If you downloaded as a zip, the folder may be called tpot-main. </p>"},{"location":"related/","title":"Related","text":"<p>Other Automated Machine Learning (AutoML) tools and related projects:</p> Name Language License Description Auto-WEKA Java GPL-v3 Automated model selection and hyper-parameter tuning for Weka models. auto-sklearn Python BSD-3-Clause An automated machine learning toolkit and a drop-in replacement for a scikit-learn estimator. auto_ml Python MIT Automated machine learning for analytics &amp; production. Supports manual feature type declarations. H2O AutoML Java with Python, Scala &amp; R APIs and web GUI Apache 2.0 Automated: data prep, hyperparameter tuning, random grid search and stacked ensembles in a distributed ML platform. devol Python MIT Automated deep neural network design via genetic programming. MLBox Python BSD-3-Clause Accurate hyper-parameter optimization in high-dimensional space with support for distributed computing. Recipe C GPL-v3 Machine-learning pipeline optimization through genetic programming. Uses grammars to define pipeline structure. Xcessiv Python Apache 2.0 A web-based application for quick, scalable, and automated hyper-parameter tuning and stacked ensembling in Python. GAMA Python Apache 2.0 Machine-learning pipeline optimization through asynchronous evaluation based genetic programming.  PyMoo Python Apache 2.0 Multi-objective optimization in Python.  Karoo GP Python MIT A Python based genetic programming application suite with support for symbolic regression and classification.  MABE C++ See here A Python based genetic programming application suite with support for symbolic regression and classification.  SBBFramework Python BSD-2-Clause Python implementation of Symbiotic Bid-Based (SBB) framework for problem decomposition using Genetic Programming (GP).  Tiny GP Python GPL-v3 A minimalistic program implementing Koza-style (tree-based) genetic programming to solve a symbolic regression problem.  Baikal Python BSD-3-Clause A graph-based functional API for building complex scikit-learn pipelines.  skdag Python MIT A more flexible alternative to scikit-learn Pipelines.  d6tflow Python MIT A python library which makes building complex data science workflows easy, fast and intuitive."},{"location":"support/","title":"Support","text":"<p>TPOT was developed in the Artificial Intelligence Innovation (A2I) Lab at Cedars-Sinai with funding from the NIH under grants U01 AG066833 and R01 LM010098. We are incredibly grateful for the support of the NIH and the Cedars-Sinai during the development of this project.</p> <p>The TPOT logo was designed by Todd Newmuis, who generously donated his time to the project.</p>"},{"location":"using/","title":"Using TPOT","text":"<p>See the Tutorials Folder for more instructions and examples.</p>"},{"location":"using/#best-practices","title":"Best Practices","text":""},{"location":"using/#1","title":"1","text":"<p>TPOT uses dask for parallel processing. When Python is parallelized, each module is imported within each processes. Therefore it is important to protect all code within a <code>if __name__ == \"__main__\"</code> when running TPOT from a script. This is not required when running TPOT from a notebook.</p> <p>For example:</p> <pre><code>#my_analysis.py\n\nimport tpot\nif __name__ == \"__main__\":\n    X, y = load_my_data()\n    est = tpot.TPOTClassifier()\n    est.fit(X,y)\n    #rest of analysis\n</code></pre>"},{"location":"using/#2","title":"2","text":"<p>When designing custom objective functions, avoid the use of global variables.</p> <p>Don't Do: <pre><code>global_X = [[1,2],[4,5]]\nglobal_y = [0,1]\ndef foo(est):\n    return my_scorer(est, X=global_X, y=global_y)\n</code></pre></p> <p>Instead use a partial</p> <pre><code>from functools import partial\n\ndef foo_scorer(est, X, y):\n    return my_scorer(est, X, y)\n\nif __name__=='__main__':\n    X = [[1,2],[4,5]]\n    y = [0,1]\n    final_scorer = partial(foo_scorer, X=X, y=y)\n</code></pre> <p>Similarly when using lambda functions.</p> <p>Dont Do:</p> <pre><code>def new_objective(est, a, b)\n    #definition\n\na = 100\nb = 20\nbad_function = lambda est :  new_objective(est=est, a=a, b=b)\n</code></pre> <p>Do: <pre><code>def new_objective(est, a, b)\n    #definition\n\na = 100\nb = 20\ngood_function = lambda est, a=a, b=b : new_objective(est=est, a=a, b=b)\n</code></pre></p>"},{"location":"using/#tips","title":"Tips","text":"<p>TPOT will not check if your data is correctly formatted. It will assume that you have passed in operators that can handle the type of data that was passed in. For instance, if you pass in a pandas dataframe with categorical features and missing data, then you should also include in your configuration operators that can handle those feautures of the data. Alternatively, if you pass in <code>preprocessing = True</code>, TPOT will impute missing values, one hot encode categorical features, then standardize the data. (Note that this is currently fitted and transformed on the entire training set before splitting for CV. Later there will be an option to apply per fold, and have the parameters be learnable.)</p> <p>Setting <code>verbose</code> to 5 can be helpful during debugging as it will print out the error generated by failing pipelines. </p>"},{"location":"Tutorial/1_Using_TPOT/","title":"What to expect from AutoML software","text":"In\u00a0[1]: Copied! <pre>import tpot\nfrom tpot import TPOTClassifier\n</pre> import tpot from tpot import TPOTClassifier <pre>Matplotlib is building the font cache; this may take a moment.\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> <p>then create an instance of TPOT as follows:</p> In\u00a0[2]: Copied! <pre>classification_optimizer = TPOTClassifier()\n</pre> classification_optimizer = TPOTClassifier() <p>It's also possible to use TPOT for regression problems with the TPOTRegressor class. Other than the class name, a TPOTRegressor is used the same way as a TPOTClassifier. You can read more about the TPOTClassifier and TPOTRegressor classes in the API documentation.</p> In\u00a0[3]: Copied! <pre>from tpot import TPOTRegressor\nregression_optimizer = TPOTRegressor()\n</pre> from tpot import TPOTRegressor regression_optimizer = TPOTRegressor() <p>Fitting a TPOT model works exactly like any other sklearn estimator. Some example code with custom TPOT parameters might look like:</p> In\u00a0[4]: Copied! <pre>import sklearn\nimport sklearn.datasets\nimport sklearn.metrics\nimport tpot\n\nclassification_optimizer = TPOTClassifier(search_space=\"linear-light\", max_time_mins=30/60, n_jobs=30, cv=5)\n\nX, y = sklearn.datasets.load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1, test_size=0.2)\n\nclassification_optimizer.fit(X_train, y_train)\n\nauroc_score = sklearn.metrics.roc_auc_score(y_test, classification_optimizer.predict_proba(X_test)[:,1])\nprint(\"auroc_score: \", auroc_score)\n</pre> import sklearn import sklearn.datasets import sklearn.metrics import tpot  classification_optimizer = TPOTClassifier(search_space=\"linear-light\", max_time_mins=30/60, n_jobs=30, cv=5)  X, y = sklearn.datasets.load_breast_cancer(return_X_y=True) X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1, test_size=0.2)  classification_optimizer.fit(X_train, y_train)  auroc_score = sklearn.metrics.roc_auc_score(y_test, classification_optimizer.predict_proba(X_test)[:,1]) print(\"auroc_score: \", auroc_score) <pre>Generation: : 5it [00:32,  6.57s/it]\n</pre> <pre>auroc_score:  0.9950396825396826\n</pre> In\u00a0[5]: Copied! <pre>import tpot\nfrom tpot.search_spaces.pipelines import SequentialPipeline\nfrom tpot.config import get_search_space\n\nstc_search_space = SequentialPipeline([\n    get_search_space(\"selectors\"),\n    get_search_space(\"all_transformers\"),\n    get_search_space(\"classifiers\"),\n])\n\nest = tpot.TPOTEstimator(\n    search_space = stc_search_space,\n    scorers=[\"roc_auc_ovr\", tpot.objectives.complexity_scorer],\n    scorers_weights=[1.0, -1.0],\n    classification = True,\n    cv = 5,\n    max_eval_time_mins = 10,\n    early_stop = 2,\n    verbose = 2,\n    n_jobs=4,\n)\n</pre> import tpot from tpot.search_spaces.pipelines import SequentialPipeline from tpot.config import get_search_space  stc_search_space = SequentialPipeline([     get_search_space(\"selectors\"),     get_search_space(\"all_transformers\"),     get_search_space(\"classifiers\"), ])  est = tpot.TPOTEstimator(     search_space = stc_search_space,     scorers=[\"roc_auc_ovr\", tpot.objectives.complexity_scorer],     scorers_weights=[1.0, -1.0],     classification = True,     cv = 5,     max_eval_time_mins = 10,     early_stop = 2,     verbose = 2,     n_jobs=4, )  <p>Using a built in method</p> In\u00a0[6]: Copied! <pre>est = tpot.TPOTEstimator(\n    search_space = \"linear\",\n    scorers=[\"roc_auc_ovr\", tpot.objectives.complexity_scorer],\n    scorers_weights=[1.0, -1.0],\n    classification = True,\n    cv = 5,\n    max_eval_time_mins = 10,\n    early_stop = 2,\n    verbose = 2,\n    n_jobs=4,\n)\n</pre> est = tpot.TPOTEstimator(     search_space = \"linear\",     scorers=[\"roc_auc_ovr\", tpot.objectives.complexity_scorer],     scorers_weights=[1.0, -1.0],     classification = True,     cv = 5,     max_eval_time_mins = 10,     early_stop = 2,     verbose = 2,     n_jobs=4, ) <p>The specific hyperparameter ranges used by TPOT can be found in files in the tpot/config folder. The template search spaces listed above are defined in tpot/config/template_search_spaces.py. Search spaces for individual models can be acquired in the tpot/config/get_configspace.py file (<code>tpot.config.get_search_space</code>). More details on customizing search spaces can be found in Tutorial 2.</p> <pre><code>`tpot.config.template_search_spaces.get_template_search_spaces`\nReturns a search space which can be optimized by TPOT.\n\nParameters\n----------\nsearch_space: str or SearchSpace\n    The default search space to use. If a string, it should be one of the following:\n        - 'linear': A search space for linear pipelines\n        - 'linear-light': A search space for linear pipelines with a smaller, faster search space\n        - 'graph': A search space for graph pipelines\n        - 'graph-light': A search space for graph pipelines with a smaller, faster search space\n        - 'mdr': A search space for MDR pipelines\n    If a SearchSpace object, it should be a valid search space object for TPOT.\n\nclassification: bool, default=True\n    Whether the problem is a classification problem or a regression problem.\n\ninner_predictors: bool, default=None\n    Whether to include additional classifiers/regressors before the final classifier/regressor (allowing for ensembles). \n    Defaults to False for 'linear-light' and 'graph-light' search spaces, and True otherwise. (Not used for 'mdr' search space)\n\ncross_val_predict_cv: int, default=None\n    The number of folds to use for cross_val_predict. \n    Defaults to 0 for 'linear-light' and 'graph-light' search spaces, and 5 otherwise. (Not used for 'mdr' search space)\n\nget_search_space_params: dict\n    Additional parameters to pass to the get_search_space function.</code></pre> In\u00a0[7]: Copied! <pre>linear_with_cross_val_predict_sp = tpot.config.template_search_spaces.get_template_search_spaces(search_space=\"linear\", classification=True, inner_predictors=True, cross_val_predict_cv=5)\nclassification_optimizer = TPOTClassifier(search_space=linear_with_cross_val_predict_sp, max_time_mins=30/60, n_jobs=30, cv=5)\n</pre> linear_with_cross_val_predict_sp = tpot.config.template_search_spaces.get_template_search_spaces(search_space=\"linear\", classification=True, inner_predictors=True, cross_val_predict_cv=5) classification_optimizer = TPOTClassifier(search_space=linear_with_cross_val_predict_sp, max_time_mins=30/60, n_jobs=30, cv=5) In\u00a0[8]: Copied! <pre>from dask.distributed import Client, LocalCluster\nimport tpot\nimport sklearn\nimport sklearn.datasets\nimport numpy as np\n\nif __name__==\"__main__\":\n    scorer = sklearn.metrics.get_scorer('roc_auc_ovo')\n    X, y = sklearn.datasets.load_digits(return_X_y=True)\n    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)\n\n\n    est = tpot.TPOTClassifier(n_jobs=4, max_time_mins=3, verbose=2, early_stop=3)\n    est.fit(X_train, y_train)\n\n\n    print(scorer(est, X_test, y_test))\n</pre> from dask.distributed import Client, LocalCluster import tpot import sklearn import sklearn.datasets import numpy as np  if __name__==\"__main__\":     scorer = sklearn.metrics.get_scorer('roc_auc_ovo')     X, y = sklearn.datasets.load_digits(return_X_y=True)     X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)       est = tpot.TPOTClassifier(n_jobs=4, max_time_mins=3, verbose=2, early_stop=3)     est.fit(X_train, y_train)       print(scorer(est, X_test, y_test)) <pre>Generation: : 1it [03:13, 193.20s/it]\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 1 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 2 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 3 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 4 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 5 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 6 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 7 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 8 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 9 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 10 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 11 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 12 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 13 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 14 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 15 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 16 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 17 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 18 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 19 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 20 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 21 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 22 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 23 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 24 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 25 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 26 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 27 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 28 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 29 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 30 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 31 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 32 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 33 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 34 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 35 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 36 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 37 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 38 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 39 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 40 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 41 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 42 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 43 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 44 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 45 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 46 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 47 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 48 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 49 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 50 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 51 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 52 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 53 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 54 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 55 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 56 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 57 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 58 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 59 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n</pre> <pre>0.999621947852182\n</pre> In\u00a0[9]: Copied! <pre>from dask.distributed import Client, LocalCluster\nimport tpot\nimport sklearn\nimport sklearn.datasets\nimport numpy as np\n\nimport tpot.objectives\n\n\nscorer = sklearn.metrics.get_scorer('roc_auc_ovr')\n\nX, y = sklearn.datasets.load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)\n\n\nest = tpot.TPOTClassifier(\n    scorers=[scorer, tpot.objectives.complexity_scorer],\n    scorers_weights=[1.0, -1.0],\n\n    search_space=\"linear\",\n    n_jobs=4, \n    max_time_mins=60, \n    max_eval_time_mins=10,\n    early_stop=2,\n    verbose=2,)\nest.fit(X_train, y_train)\n\nprint(scorer(est, X_test, y_test))\n</pre> from dask.distributed import Client, LocalCluster import tpot import sklearn import sklearn.datasets import numpy as np  import tpot.objectives   scorer = sklearn.metrics.get_scorer('roc_auc_ovr')  X, y = sklearn.datasets.load_breast_cancer(return_X_y=True) X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)   est = tpot.TPOTClassifier(     scorers=[scorer, tpot.objectives.complexity_scorer],     scorers_weights=[1.0, -1.0],      search_space=\"linear\",     n_jobs=4,      max_time_mins=60,      max_eval_time_mins=10,     early_stop=2,     verbose=2,) est.fit(X_train, y_train)  print(scorer(est, X_test, y_test)) <pre>Generation: : 4it [02:34, 38.64s/it]\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n</pre> <pre>0.9978289188015632\n</pre> <p>You can access the best pipeline selected by TPOT with the <code>fitted_pipeline_</code> attribute. This is the pipeline with the highest cross validation score (on the first scorer, or first objective function if no scorer is provided.)</p> In\u00a0[10]: Copied! <pre>best_pipeline = est.fitted_pipeline_\nbest_pipeline\n</pre> best_pipeline = est.fitted_pipeline_ best_pipeline Out[10]: <pre>Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n                ('selectpercentile',\n                 SelectPercentile(percentile=68.60012151662)),\n                ('featureunion-1',\n                 FeatureUnion(transformer_list=[('skiptransformer',\n                                                 SkipTransformer()),\n                                                ('passthrough',\n                                                 Passthrough())])),\n                ('featureunion-2',\n                 FeatureUnion(transformer_list=[('skiptransformer',\n                                                 SkipTransformer()),\n                                                ('passthrough',\n                                                 Passthrough())])),\n                ('mlpclassifier',\n                 MLPClassifier(activation='identity', alpha=0.0023692590029,\n                               hidden_layer_sizes=[139, 139],\n                               learning_rate='invscaling',\n                               learning_rate_init=0.0004707733364,\n                               n_iter_no_change=32))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiFitted<pre>Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n                ('selectpercentile',\n                 SelectPercentile(percentile=68.60012151662)),\n                ('featureunion-1',\n                 FeatureUnion(transformer_list=[('skiptransformer',\n                                                 SkipTransformer()),\n                                                ('passthrough',\n                                                 Passthrough())])),\n                ('featureunion-2',\n                 FeatureUnion(transformer_list=[('skiptransformer',\n                                                 SkipTransformer()),\n                                                ('passthrough',\n                                                 Passthrough())])),\n                ('mlpclassifier',\n                 MLPClassifier(activation='identity', alpha=0.0023692590029,\n                               hidden_layer_sizes=[139, 139],\n                               learning_rate='invscaling',\n                               learning_rate_init=0.0004707733364,\n                               n_iter_no_change=32))])</pre> \u00a0MinMaxScaler?Documentation for MinMaxScaler<pre>MinMaxScaler()</pre> \u00a0SelectPercentile?Documentation for SelectPercentile<pre>SelectPercentile(percentile=68.60012151662)</pre> \u00a0featureunion-1: FeatureUnion?Documentation for featureunion-1: FeatureUnion<pre>FeatureUnion(transformer_list=[('skiptransformer', SkipTransformer()),\n                               ('passthrough', Passthrough())])</pre> skiptransformerSkipTransformer<pre>SkipTransformer()</pre> passthroughPassthrough<pre>Passthrough()</pre> \u00a0featureunion-2: FeatureUnion?Documentation for featureunion-2: FeatureUnion<pre>FeatureUnion(transformer_list=[('skiptransformer', SkipTransformer()),\n                               ('passthrough', Passthrough())])</pre> skiptransformerSkipTransformer<pre>SkipTransformer()</pre> passthroughPassthrough<pre>Passthrough()</pre> \u00a0MLPClassifier?Documentation for MLPClassifier<pre>MLPClassifier(activation='identity', alpha=0.0023692590029,\n              hidden_layer_sizes=[139, 139], learning_rate='invscaling',\n              learning_rate_init=0.0004707733364, n_iter_no_change=32)</pre> In\u00a0[11]: Copied! <pre>best_pipeline.predict(X_test)\n</pre> best_pipeline.predict(X_test) Out[11]: <pre>array([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,\n       0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,\n       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n       1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n       1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n       1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1])</pre> In\u00a0[13]: Copied! <pre>import dill as pickle\nwith open(\"best_pipeline.pkl\", \"wb\") as f:\n    pickle.dump(best_pipeline, f)\n\n#load the pipeline\nimport dill as pickle\nwith open(\"best_pipeline.pkl\", \"rb\") as f:\n    my_loaded_best_pipeline = pickle.load(f)\n</pre> import dill as pickle with open(\"best_pipeline.pkl\", \"wb\") as f:     pickle.dump(best_pipeline, f)  #load the pipeline import dill as pickle with open(\"best_pipeline.pkl\", \"rb\") as f:     my_loaded_best_pipeline = pickle.load(f) In\u00a0[14]: Copied! <pre>#get the score/objective column names generated by TPOT\nest.objective_names\n</pre> #get the score/objective column names generated by TPOT est.objective_names Out[14]: <pre>['roc_auc_score', 'complexity_scorer']</pre> In\u00a0[15]: Copied! <pre>df = est.evaluated_individuals\ndf\n</pre> df = est.evaluated_individuals df Out[15]: roc_auc_score complexity_scorer Parents Variation_Function Individual Generation Submitted Timestamp Completed Timestamp Eval Error Pareto_Front Instance 0 NaN NaN NaN NaN &lt;tpot.search_spaces.pipelines.sequential.Seque... 0.0 1.740178e+09 1.740178e+09 INVALID NaN (MaxAbsScaler(), RFE(estimator=ExtraTreesClass... 1 NaN NaN NaN NaN &lt;tpot.search_spaces.pipelines.sequential.Seque... 0.0 1.740178e+09 1.740179e+09 INVALID NaN (RobustScaler(quantile_range=(0.1386847479391,... 2 NaN NaN NaN NaN &lt;tpot.search_spaces.pipelines.sequential.Seque... 0.0 1.740178e+09 1.740178e+09 INVALID NaN (RobustScaler(quantile_range=(0.0087917518794,... 3 NaN NaN NaN NaN &lt;tpot.search_spaces.pipelines.sequential.Seque... 0.0 1.740178e+09 1.740178e+09 INVALID NaN (Passthrough(), Passthrough(), FeatureUnion(tr... 4 0.969262 241.2 NaN NaN &lt;tpot.search_spaces.pipelines.sequential.Seque... 0.0 1.740178e+09 1.740178e+09 None NaN (RobustScaler(quantile_range=(0.0359502923061,... ... ... ... ... ... ... ... ... ... ... ... ... 245 0.986280 44.0 (184, 184) ind_crossover &lt;tpot.search_spaces.pipelines.sequential.Seque... 4.0 1.740179e+09 1.740179e+09 None NaN (RobustScaler(quantile_range=(0.1428289713161,... 246 0.902845 9.0 (145, 148) ind_mutate , ind_mutate , ind_crossover &lt;tpot.search_spaces.pipelines.sequential.Seque... 4.0 1.740179e+09 1.740179e+09 None NaN (MinMaxScaler(), SelectFwe(alpha=0.00184795618... 247 0.992851 5301.0 (155, 133) ind_mutate , ind_mutate , ind_crossover &lt;tpot.search_spaces.pipelines.sequential.Seque... 4.0 1.740179e+09 1.740179e+09 None NaN (MaxAbsScaler(), SelectFwe(alpha=0.00212090942... 248 0.992349 7749.0 (152, 152) ind_mutate &lt;tpot.search_spaces.pipelines.sequential.Seque... 4.0 1.740179e+09 1.740179e+09 None NaN (MinMaxScaler(), SelectFromModel(estimator=Ext... 249 0.515242 9.0 (182, 182) ind_mutate &lt;tpot.search_spaces.pipelines.sequential.Seque... 4.0 1.740179e+09 1.740179e+09 None NaN (MaxAbsScaler(), VarianceThreshold(threshold=0... <p>250 rows \u00d7 11 columns</p> In\u00a0[18]: Copied! <pre>import seaborn as sns\nimport matplotlib.pyplot as plt\n#replace nans in pareto front with 0\nfig, ax = plt.subplots(figsize=(5,5))\nsns.scatterplot(df[df['Pareto_Front']!=1], x='roc_auc_score', y='complexity_scorer', label='other', ax=ax)\nsns.scatterplot(df[df['Pareto_Front']==1], x='roc_auc_score', y='complexity_scorer', label='Pareto Front', ax=ax)\nax.title.set_text('Performance of all pipelines')\n#log scale y\nax.set_yscale('log')\nplt.show()\n\n#replace nans in pareto front with 0\nfig, ax = plt.subplots(figsize=(10,5))\nsns.scatterplot(df[df['Pareto_Front']==1], x='roc_auc_score', y='complexity_scorer', label='Pareto Front', ax=ax)\nax.title.set_text('Performance of only the Pareto Front')\n#log scale y\n# ax.set_yscale('log')\nplt.show()\n</pre> import seaborn as sns import matplotlib.pyplot as plt #replace nans in pareto front with 0 fig, ax = plt.subplots(figsize=(5,5)) sns.scatterplot(df[df['Pareto_Front']!=1], x='roc_auc_score', y='complexity_scorer', label='other', ax=ax) sns.scatterplot(df[df['Pareto_Front']==1], x='roc_auc_score', y='complexity_scorer', label='Pareto Front', ax=ax) ax.title.set_text('Performance of all pipelines') #log scale y ax.set_yscale('log') plt.show()  #replace nans in pareto front with 0 fig, ax = plt.subplots(figsize=(10,5)) sns.scatterplot(df[df['Pareto_Front']==1], x='roc_auc_score', y='complexity_scorer', label='Pareto Front', ax=ax) ax.title.set_text('Performance of only the Pareto Front') #log scale y # ax.set_yscale('log') plt.show() In\u00a0[19]: Copied! <pre>#plot only the pareto front pipelines\nsorted_pareto_front = df[df['Pareto_Front']==1].sort_values('roc_auc_score', ascending=False)\nsorted_pareto_front\n</pre> #plot only the pareto front pipelines sorted_pareto_front = df[df['Pareto_Front']==1].sort_values('roc_auc_score', ascending=False) sorted_pareto_front Out[19]: roc_auc_score complexity_scorer Parents Variation_Function Individual Generation Submitted Timestamp Completed Timestamp Eval Error Pareto_Front Instance 51 0.996818 582.0 (13, 13) ind_mutate &lt;tpot.search_spaces.pipelines.sequential.Seque... 1.0 1.740179e+09 1.740179e+09 None 1.0 (MinMaxScaler(), SelectPercentile(percentile=6... 133 0.996239 31.0 (65, 65) ind_mutate &lt;tpot.search_spaces.pipelines.sequential.Seque... 2.0 1.740179e+09 1.740179e+09 None 1.0 (StandardScaler(), SelectFwe(alpha=0.002276474... 185 0.995843 30.9 (133, 133) ind_mutate &lt;tpot.search_spaces.pipelines.sequential.Seque... 3.0 1.740179e+09 1.740179e+09 None 1.0 (StandardScaler(), SelectFwe(alpha=0.000234016... 233 0.995115 30.7 (185, 185) ind_mutate &lt;tpot.search_spaces.pipelines.sequential.Seque... 4.0 1.740179e+09 1.740179e+09 None 1.0 (StandardScaler(), SelectFwe(alpha=0.000234016... 85 0.990894 26.0 (6, 23) ind_crossover &lt;tpot.search_spaces.pipelines.sequential.Seque... 1.0 1.740179e+09 1.740179e+09 None 1.0 (MaxAbsScaler(), SelectFwe(alpha=0.00114277554... 228 0.990081 19.0 (162, 162) ind_mutate &lt;tpot.search_spaces.pipelines.sequential.Seque... 4.0 1.740179e+09 1.740179e+09 None 1.0 (MaxAbsScaler(), VarianceThreshold(threshold=0... 215 0.988614 9.0 (162, 162) ind_mutate &lt;tpot.search_spaces.pipelines.sequential.Seque... 4.0 1.740179e+09 1.740179e+09 None 1.0 (MaxAbsScaler(), VarianceThreshold(threshold=0... 121 0.982524 7.0 (10, 10) ind_mutate &lt;tpot.search_spaces.pipelines.sequential.Seque... 2.0 1.740179e+09 1.740179e+09 None 1.0 (MaxAbsScaler(), SelectFwe(alpha=0.03019980124... <p>In some cases, you may want to select a slightly lower performing pipeline that is signficantly less complex.</p> In\u00a0[20]: Copied! <pre>#access the best performing pipeline with the lowest complexity\n\nbest_pipeline_lowest_complexity = sorted_pareto_front.iloc[-1]['Instance']\nbest_pipeline_lowest_complexity\n</pre> #access the best performing pipeline with the lowest complexity  best_pipeline_lowest_complexity = sorted_pareto_front.iloc[-1]['Instance'] best_pipeline_lowest_complexity Out[20]: <pre>Pipeline(steps=[('maxabsscaler', MaxAbsScaler()),\n                ('selectfwe', SelectFwe(alpha=0.0301998012478)),\n                ('featureunion-1',\n                 FeatureUnion(transformer_list=[('skiptransformer',\n                                                 SkipTransformer()),\n                                                ('passthrough',\n                                                 Passthrough())])),\n                ('featureunion-2',\n                 FeatureUnion(transformer_list=[('skiptransformer',\n                                                 SkipTransformer()),\n                                                ('passthrough',\n                                                 Passthrough())])),\n                ('kneighborsclassifier',\n                 KNeighborsClassifier(n_jobs=1, n_neighbors=2))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('maxabsscaler', MaxAbsScaler()),\n                ('selectfwe', SelectFwe(alpha=0.0301998012478)),\n                ('featureunion-1',\n                 FeatureUnion(transformer_list=[('skiptransformer',\n                                                 SkipTransformer()),\n                                                ('passthrough',\n                                                 Passthrough())])),\n                ('featureunion-2',\n                 FeatureUnion(transformer_list=[('skiptransformer',\n                                                 SkipTransformer()),\n                                                ('passthrough',\n                                                 Passthrough())])),\n                ('kneighborsclassifier',\n                 KNeighborsClassifier(n_jobs=1, n_neighbors=2))])</pre> \u00a0MaxAbsScaler?Documentation for MaxAbsScaler<pre>MaxAbsScaler()</pre> \u00a0SelectFwe?Documentation for SelectFwe<pre>SelectFwe(alpha=0.0301998012478)</pre> \u00a0featureunion-1: FeatureUnion?Documentation for featureunion-1: FeatureUnion<pre>FeatureUnion(transformer_list=[('skiptransformer', SkipTransformer()),\n                               ('passthrough', Passthrough())])</pre> skiptransformerSkipTransformer<pre>SkipTransformer()</pre> passthroughPassthrough<pre>Passthrough()</pre> \u00a0featureunion-2: FeatureUnion?Documentation for featureunion-2: FeatureUnion<pre>FeatureUnion(transformer_list=[('skiptransformer', SkipTransformer()),\n                               ('passthrough', Passthrough())])</pre> skiptransformerSkipTransformer<pre>SkipTransformer()</pre> passthroughPassthrough<pre>Passthrough()</pre> \u00a0KNeighborsClassifier?Documentation for KNeighborsClassifier<pre>KNeighborsClassifier(n_jobs=1, n_neighbors=2)</pre> In\u00a0[21]: Copied! <pre>#get columns where roc_auc_score is not NaN\nscores_and_times = df[df['roc_auc_score'].notna()][['roc_auc_score', 'Completed Timestamp']].sort_values('Completed Timestamp', ascending=True).to_numpy()\n\n#get best score at a given time\nbest_scores = np.maximum.accumulate(scores_and_times[:,0])\ntimes = scores_and_times[:,1]\ntimes = times - df['Submitted Timestamp'].min()\n\nfig, ax = plt.subplots(figsize=(10,5))\nax.plot(times, best_scores)\nax.set_xlabel('Time (seconds)')\nax.set_ylabel('Best Score')\nplt.show()\n</pre> #get columns where roc_auc_score is not NaN scores_and_times = df[df['roc_auc_score'].notna()][['roc_auc_score', 'Completed Timestamp']].sort_values('Completed Timestamp', ascending=True).to_numpy()  #get best score at a given time best_scores = np.maximum.accumulate(scores_and_times[:,0]) times = scores_and_times[:,1] times = times - df['Submitted Timestamp'].min()  fig, ax = plt.subplots(figsize=(10,5)) ax.plot(times, best_scores) ax.set_xlabel('Time (seconds)') ax.set_ylabel('Best Score') plt.show()  In\u00a0[22]: Copied! <pre>from tpot import TPOTClassifier\nfrom tempfile import mkdtemp\nfrom joblib import Memory\nfrom shutil import rmtree\n\n# Method 1, auto mode: TPOT uses memory caching with a temporary directory and cleans it up upon shutdown\nest = TPOTClassifier(memory='auto')\n\n# Method 2, with a custom directory for memory caching\nest = TPOTClassifier(memory='/to/your/path')\n\n# Method 3, with a Memory object\nmemory = Memory(location='./to/your/path', verbose=0)\nest = TPOTClassifier(memory=memory)\n</pre> from tpot import TPOTClassifier from tempfile import mkdtemp from joblib import Memory from shutil import rmtree  # Method 1, auto mode: TPOT uses memory caching with a temporary directory and cleans it up upon shutdown est = TPOTClassifier(memory='auto')  # Method 2, with a custom directory for memory caching est = TPOTClassifier(memory='/to/your/path')  # Method 3, with a Memory object memory = Memory(location='./to/your/path', verbose=0) est = TPOTClassifier(memory=memory)  <p>Note: TPOT does NOT clean up memory caches if users set a custom directory path or Memory object. We recommend that you clean up the memory caches when you don't need it anymore.</p> In\u00a0[23]: Copied! <pre>import tpot\nimport sklearn\nimport sklearn.datasets\n\n\ngraph_search_space = tpot.search_spaces.pipelines.GraphSearchPipeline(\n    root_search_space= tpot.config.get_search_space([\"KNeighborsClassifier\", \"LogisticRegression\", \"DecisionTreeClassifier\"]),\n    leaf_search_space = tpot.config.get_search_space(\"selectors\"), \n    inner_search_space = tpot.config.get_search_space([\"transformers\"]),\n    max_size = 10,\n)\n\nest = tpot.TPOTEstimatorSteadyState( \n                            search_space = graph_search_space,\n                            scorers=['roc_auc_ovr',tpot.objectives.complexity_scorer],\n                            scorers_weights=[1,-1],\n\n\n                            classification=True,\n\n                            max_eval_time_mins=15,\n                            max_time_mins=30,\n                            early_stop=10, #In TPOTEstimatorSteadyState, since there are no generations, early_stop is the number of pipelines to evaluate before stopping.\n                            n_jobs=30,\n                            verbose=2)\n\n\nscorer = sklearn.metrics.get_scorer('roc_auc_ovo')\nX, y = sklearn.datasets.load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)\nest.fit(X_train, y_train)\nprint(scorer(est, X_test, y_test))\n</pre> import tpot import sklearn import sklearn.datasets   graph_search_space = tpot.search_spaces.pipelines.GraphSearchPipeline(     root_search_space= tpot.config.get_search_space([\"KNeighborsClassifier\", \"LogisticRegression\", \"DecisionTreeClassifier\"]),     leaf_search_space = tpot.config.get_search_space(\"selectors\"),      inner_search_space = tpot.config.get_search_space([\"transformers\"]),     max_size = 10, )  est = tpot.TPOTEstimatorSteadyState(                              search_space = graph_search_space,                             scorers=['roc_auc_ovr',tpot.objectives.complexity_scorer],                             scorers_weights=[1,-1],                               classification=True,                              max_eval_time_mins=15,                             max_time_mins=30,                             early_stop=10, #In TPOTEstimatorSteadyState, since there are no generations, early_stop is the number of pipelines to evaluate before stopping.                             n_jobs=30,                             verbose=2)   scorer = sklearn.metrics.get_scorer('roc_auc_ovo') X, y = sklearn.datasets.load_breast_cancer(return_X_y=True) X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25) est.fit(X_train, y_train) print(scorer(est, X_test, y_test)) <pre>Evaluations: : 119it [00:37,  3.21it/s]\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:2785: UserWarning: n_quantiles (688) is greater than the total number of samples (426). n_quantiles is set to n_samples.\n  warnings.warn(\n</pre> <pre>0.9816225907664725\n</pre> In\u00a0[24]: Copied! <pre>fitted_pipeline = est.fitted_pipeline_ # access best pipeline directly\nfitted_pipeline.plot()\n</pre> fitted_pipeline = est.fitted_pipeline_ # access best pipeline directly fitted_pipeline.plot() In\u00a0[25]: Copied! <pre>#view the summary of all evaluated individuals as a pandas dataframe\nest.evaluated_individuals.head()\n</pre> #view the summary of all evaluated individuals as a pandas dataframe est.evaluated_individuals.head() Out[25]: roc_auc_score complexity_scorer Parents Variation_Function Individual Submitted Timestamp Completed Timestamp Eval Error Pareto_Front Instance 0 0.841954 95.0 NaN NaN &lt;tpot.search_spaces.pipelines.graph.GraphPipel... 1.740179e+09 1.740179e+09 None NaN [('DecisionTreeClassifier_1', 'SelectPercentil... 1 0.967781 89.0 NaN NaN &lt;tpot.search_spaces.pipelines.graph.GraphPipel... 1.740179e+09 1.740179e+09 None NaN [('DecisionTreeClassifier_1', 'SelectFwe_1'), ... 2 0.972412 22.0 NaN NaN &lt;tpot.search_spaces.pipelines.graph.GraphPipel... 1.740179e+09 1.740179e+09 None NaN [('KNeighborsClassifier_1', 'ColumnOneHotEncod... 3 0.975926 54.0 NaN NaN &lt;tpot.search_spaces.pipelines.graph.GraphPipel... 1.740179e+09 1.740179e+09 None NaN [('KNeighborsClassifier_1', 'SelectFwe_1')] 4 0.964352 84.0 NaN NaN &lt;tpot.search_spaces.pipelines.graph.GraphPipel... 1.740179e+09 1.740179e+09 None NaN [('DecisionTreeClassifier_1', 'ZeroCount_1'), ... In\u00a0[26]: Copied! <pre>import tpot\nimport sklearn\nimport sklearn.datasets\n\nest = tpot.TPOTEstimator(  \n                            search_space = graph_search_space,\n                            max_time_mins=10,\n                            scorers=['roc_auc_ovr'], #scorers can be a list of strings or a list of scorers. These get evaluated during cross validation. \n                            scorers_weights=[1],\n                            classification=True,\n                            n_jobs=1, \n                            early_stop=5, #how many generations with no improvement to stop after\n                            \n                            #List of other objective functions. All objective functions take in an untrained GraphPipeline and return a score or a list of scores\n                            other_objective_functions= [ ],\n                            \n                            #List of weights for the other objective functions. Must be the same length as other_objective_functions. By default, bigger is better is set to True. \n                            other_objective_functions_weights=[],\n                            verbose=2)\n\nscorer = sklearn.metrics.get_scorer('roc_auc_ovo')\nX, y = sklearn.datasets.load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)\nest.fit(X_train, y_train)\nprint(scorer(est, X_test, y_test))\n</pre> import tpot import sklearn import sklearn.datasets  est = tpot.TPOTEstimator(                               search_space = graph_search_space,                             max_time_mins=10,                             scorers=['roc_auc_ovr'], #scorers can be a list of strings or a list of scorers. These get evaluated during cross validation.                              scorers_weights=[1],                             classification=True,                             n_jobs=1,                              early_stop=5, #how many generations with no improvement to stop after                                                          #List of other objective functions. All objective functions take in an untrained GraphPipeline and return a score or a list of scores                             other_objective_functions= [ ],                                                          #List of weights for the other objective functions. Must be the same length as other_objective_functions. By default, bigger is better is set to True.                              other_objective_functions_weights=[],                             verbose=2)  scorer = sklearn.metrics.get_scorer('roc_auc_ovo') X, y = sklearn.datasets.load_breast_cancer(return_X_y=True) X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25) est.fit(X_train, y_train) print(scorer(est, X_test, y_test)) <pre>Generation: : 5it [10:06, 121.38s/it]\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n</pre> <pre>0.996046608406159\n</pre> <p>Regression Example</p> In\u00a0[27]: Copied! <pre>import tpot\nimport sklearn\nimport sklearn.metrics\nimport sklearn.datasets\n\nscorer = sklearn.metrics.get_scorer('neg_mean_squared_error')\nX, y = sklearn.datasets.load_diabetes(return_X_y=True)\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)\n\nest = tpot.tpot_estimator.templates.TPOTRegressor(n_jobs=4, max_time_mins=30, verbose=2, cv=5, early_stop=5)\nest.fit(X_train, y_train)\n\nprint(scorer(est, X_test, y_test))\n</pre> import tpot import sklearn import sklearn.metrics import sklearn.datasets  scorer = sklearn.metrics.get_scorer('neg_mean_squared_error') X, y = sklearn.datasets.load_diabetes(return_X_y=True) X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)  est = tpot.tpot_estimator.templates.TPOTRegressor(n_jobs=4, max_time_mins=30, verbose=2, cv=5, early_stop=5) est.fit(X_train, y_train)  print(scorer(est, X_test, y_test)) <pre>Generation: : 24it [18:15, 45.63s/it]\n</pre> <pre>-2968.0005982574958\n</pre>"},{"location":"Tutorial/1_Using_TPOT/#what-to-expect-from-automl-software","title":"What to expect from AutoML software\u00b6","text":"<p>Automated machine learning (AutoML) takes a higher-level approach to machine learning than most practitioners are used to, so we've gathered a handful of guidelines on what to expect when running AutoML software such as TPOT.</p>"},{"location":"Tutorial/1_Using_TPOT/#automl-algorithms-arent-intended-to-run-for-only-a-few-minutes","title":"AUTOML ALGORITHMS AREN'T INTENDED TO RUN FOR ONLY A FEW MINUTES\u00b6","text":"<p>Of course, you can run TPOT for only a few minutes, and it will find a reasonably good pipeline for your dataset. However, if you don't run TPOT for long enough, it may not find the best possible pipeline for your dataset. It may not even find any suitable pipeline at all, in which case a RuntimeError('A pipeline has not yet been optimized. Please call fit() first.') will be raised. Often it is worthwhile to run multiple instances of TPOT in parallel for a long time (hours to days) to allow TPOT to thoroughly search the pipeline space for your dataset.</p>"},{"location":"Tutorial/1_Using_TPOT/#automl-algorithms-can-take-a-long-time-to-finish-their-search","title":"AUTOML ALGORITHMS CAN TAKE A LONG TIME TO FINISH THEIR SEARCH\u00b6","text":"<p>AutoML algorithms aren't as simple as fitting one model on the dataset; they consider multiple machine learning algorithms (random forests, linear models, SVMs, etc.) in a pipeline with multiple preprocessing steps (missing value imputation, scaling, PCA, feature selection, etc.), the hyperparameters for all of the models and preprocessing steps, and multiple ways to ensemble or stack the algorithms within the pipeline.</p> <p>As such, TPOT will take a while to run on larger datasets, but it's important to realize why. With the default TPOT settings (100 generations with 100 population size), TPOT will evaluate 10,000 pipeline configurations before finishing. To put this number into context, think about a grid search of 10,000 hyperparameter combinations for a machine learning algorithm and how long that grid search will take. That is 10,000 model configurations to evaluate with 10-fold cross-validation, which means that roughly 100,000 models are fit and evaluated on the training data in one grid search. That's a time-consuming procedure, even for simpler models like decision trees.</p> <p>Typical TPOT runs will take hours to days to finish (unless it's a small dataset), but you can always interrupt the run partway through and see the best results so far. TPOT also provides a warm_start and a periodic_checkpoint_folder parameter that lets you restart a TPOT run from where it left off.</p>"},{"location":"Tutorial/1_Using_TPOT/#automl-algorithms-can-recommend-different-solutions-for-the-same-dataset","title":"AUTOML ALGORITHMS CAN RECOMMEND DIFFERENT SOLUTIONS FOR THE SAME DATASET\u00b6","text":"<p>If you're working with a reasonably complex dataset or run TPOT for a short amount of time, different TPOT runs may result in different pipeline recommendations. TPOT's optimization algorithm is stochastic, which means that it uses randomness (in part) to search the possible pipeline space. When two TPOT runs recommend different pipelines, this means that the TPOT runs didn't converge due to lack of time or that multiple pipelines perform more-or-less the same on your dataset.</p> <p>This is actually an advantage over fixed grid search techniques: TPOT is meant to be an assistant that gives you ideas on how to solve a particular machine learning problem by exploring pipeline configurations that you might have never considered, then leaves the fine-tuning to more constrained parameter tuning techniques such as grid search or bayesian optimization.</p>"},{"location":"Tutorial/1_Using_TPOT/#tpot-with-code","title":"TPOT with code\u00b6","text":"<p>We've designed the TPOT interface to be as similar as possible to scikit-learn.</p> <p>TPOT can be imported just like any regular Python module. To import TPOT, type:</p>"},{"location":"Tutorial/1_Using_TPOT/#scorers-objective-functions-and-multi-objective-optimization","title":"Scorers, Objective Functions, and multi objective optimization.\u00b6","text":"<p>There are two ways of passing objectives into TPOT.</p> <ol> <li><p><code>scorers</code>: Scorers are functions that have the signature (estimator, X_test, y_test) and take in estimators that are expected to be fitted to training data. These can be produced with the sklearn.metrics.make_scorer function. This function is used to evaluate the test folds during cross validation (defined in the <code>cv</code> parameter). These are passed into TPOT via the scorers parameter. This can take in the scorer itself or the string corresponding to a scoring function (as listed here). TPOT also supports passing in a list of several scorers for multi-objective optimization. For each fold of CV, TPOT only fits the estimator once, then evaluates all provided scorers in a loop.</p> </li> <li><p><code>other_objective_functions</code> : Other objective functions in TPOT have the signature (estimator) and returns a float or list of floats. These get passed a single unfitted estimator once, outside of cross validation. The user may choose to fit the pipeline within this objective function as well.</p> </li> </ol> <p>Each scorer and objective function must be accompanied by a list of weights corresponding to the list of objectives, these are <code>scorers_weights</code> and <code>other_objective_function_weights</code>, respectively. By default, TPOT maximizes objective functions (this can be changed by <code>bigger_is_better=False</code>). Positive weights means that TPOT will seek to maximize that objective, and negative weights correspond to minimization. For most selectors (and the default), only the sign matters. The scale of the weight may matter if using a custom selection function for the optimization algorithm. A zero weight means that the score will not have an impact on the selection algorithm.</p> <p>Here is an example of using two scorers</p> <pre><code>scorers=['roc_auc_ovr',tpot.objectives.complexity_scorer],\nscorers_weights=[1,-1],</code></pre> <p>Here is an example with a scorer and a secondary objective function</p> <pre><code>scorers=['roc_auc_ovr'],\nscorers_weights=[1],\nother_objective_functions=[tpot.objectives.number_of_leaves_objective],\nother_objective_functions_weights=[-1],</code></pre> <p>TPOT will always automatically name the scorers based on the function name for the columns in the final results dataframe. TPOT will use the function name as the column name for <code>other_objective_functions</code>. However, if you would like to specify custom column names, you can set the <code>objective_function_names</code> to be a list of names (str) for each value returned by the function in <code>other_objective_functions</code>. This can be useful if your additional functions return more than one value per function.</p> <p>It is possible to have either the scorer or other_objective_function to return multiple values. In that case, just make sure that the <code>scorers_weights</code> and <code>other_objective_function_weights</code> are the same length as the number of returned scores.</p> <p>TPOT comes with a few additional built in objective functions you can use. The first table are objectives applied to fitted pipelines, and thus are passee into the <code>scorers</code> parameter. The second table are objective functions for the <code>other_objective_functions</code> param.</p> <p>Scorers:</p> Function Description tpot.objectives.complexity_scorer Estimates the number of learned parameters across all classifiers and regressors in the pipelines. Additionally, currently transformers add 1 point and selectors add 0 points (since they don't affect the complexity of the \"final\" predictive pipeline.) <p>Other Objective Functions.</p> Function Description tpot.objectives.average_path_length Computes the average shortest path from all nodes to the root/final estimator (only supported for GraphPipeline) tpot.objectives.number_of_leaves_objective Calculates the number of leaves (input nodes) in a GraphPipeline tpot.objectives.number_of_nodes_objective Calculates the number of nodes in a pipeline (whether it is an scikit-learn Pipeline, GraphPipeline, Feature Union, or the previous nested within each other)"},{"location":"Tutorial/1_Using_TPOT/#measuring-model-complexity","title":"Measuring Model Complexity\u00b6","text":"<p>When running TPOT, including a secondary objective that measures model complexity can sometimes be beneficial. More complex models can yield higher performance, but this comes at the cost of interpretability. Simpler models may be more interpretable but often have lower predictive performance. Sometimes, however, vast increases in complexity only marginally improve predictive performance. There may be other simpler and more interpretable pipelines with marginal performance decreases that could be acceptable for the increased interpretability. However, these pipelines are often missed when optimizing purely for performance. By including both performance and complexity as objective functions, TPOT will attempt to optimize the best pipeline for all complexity levels simultaneously. After optimization, the user will be able to see the complexity vs performance tradeoff and decide which pipeline best suits their needs.</p> <p>Two methods of measuring complexity to consider would be <code>tpot.objectives.number_of_nodes_objective</code> or <code>tpot.objectives.complexity_scorer</code>. The number of nodes objective simply calculates the number of steps within a pipeline. This is a simple metric, however it does not differentiate between the complexity of different model types. For example, a simple LogisticRegression counts the same as the much more complex XGBoost. The complexity scorer tries to estimate the number of learned parameters included in the classifiers and regressors of the pipeline. It is challenging and potentially subjective how to exactly quantify and compare complexity between different classes of models. However, this function provides a reasonable heuristic for the evolutionary algorithm that at least separates out qualitatively more or less complex algorithms from one another. While it may be hard to compare the relative complexities of LogisticRegression and XGBoost exactly, for example, both will always be on opposite ends of the complexity values returned by this function. This allows for pareto fronts with LogisticRegression on one side, and XGBoost on the other.</p> <p>An example of this analysis is demonstrated in a following section.</p>"},{"location":"Tutorial/1_Using_TPOT/#built-in-configurations","title":"Built In Configurations\u00b6","text":"<p>TPOT can be used to optimize hyperparameters, select models, and optimize pipelines of models including determining the sequence of steps. Tutorial 2 goes into more detail on how to customize search spaces with custom hyperparameter ranges, model types, and possible pipeline configurations. TPOT also comes with a handful of default operators and parameter configurations that we believe work well for optimizing machine learning pipelines. Below is a list of the current built-in configurations that come with TPOT. These can be passed in as strings to the <code>search space</code> parameter of any of the TPOT estimators.</p> String Description linear A linear pipeline with the structure of \"Selector-&gt;(transformers+Passthrough)-&gt;(classifiers/regressors+Passthrough)-&gt;final classifier/regressor.\" For both the transformer and inner estimator layers, TPOT may choose one or more transformers/classifiers, or it may choose none. The inner classifier/regressor layer is optional. linear-light Same search space as linear, but without the inner classifier/regressor layer and with a reduced set of faster running estimators. graph TPOT will optimize a pipeline in the shape of a directed acyclic graph. The nodes of the graph can include selectors, scalers, transformers, or classifiers/regressors (inner classifiers/regressors can optionally be not included). This will return a custom GraphPipeline rather than an sklearn Pipeline. More details in Tutorial 6. graph-light Same as graph search space, but without the inner classifier/regressors and with a reduced set of faster running estimators. <p>| mdr |TPOT will search over a series of feature selectors and Multifactor Dimensionality Reduction models to find a series of operators that maximize prediction accuracy. The TPOT MDR configuration is specialized for genome-wide association studies (GWAS), and is described in detail online here.</p> <p>Note that TPOT MDR may be slow to run because the feature selection routines are computationally expensive, especially on large datasets. |</p> <p>The <code>linear</code> and <code>graph</code> configurations by default allow for additional stacked classifiers/regressors within the pipeline in addition to the final classifier/regressor. If you would like to disable this, you can manually get the search space without inner classifier/regressors through the function <code>tpot.config.template_search_spaces.get_template_search_spaces</code> with <code>inner_predictios=False</code>. You can pass the resulting search space into the <code>search space</code> param.</p>"},{"location":"Tutorial/1_Using_TPOT/#cross_val_predict_cv","title":"cross_val_predict_cv\u00b6","text":"<p>Additionally, utilizing <code>cross_val_predict_cv</code> may increase performance when training models with inner classifiers/regressors. If this parameter is set, during model training any classifiers or regressors that is not the final predictor will use <code>sklearn.model_selection.cross_val_predict</code> to pass out of sample predictions into the following steps of the model. The model will still be fit to the full data which will be used for predictions after training. Training downstream models on out of sample predictions can often prevent overfitting and increase performance. The reason is that this gives downstream models a estimate of how upstream models compare on unseen data. Otherwise, if an upsteam model heavily overfits the data, downsteam models may simply learn to blindly trust the seemingly well-predicting model, propagating the over-fitting through to the end result.</p> <p>The downside is that cross_val_predict_cv is significantly more computationally demanding, and may not be necessary for your given dataset.</p>"},{"location":"Tutorial/1_Using_TPOT/#terminating-optimization-early-stopping","title":"Terminating Optimization (Early Stopping)\u00b6","text":"<p>Note that we use a short time duration for a quick example, but in practice, you may need to run TPOT for a longer duration. By default, TPOT sets a time limit of 1 hour with a max limit of 5 minutes per pipeline. In practice, you may want to increase these values.</p> <p>There are three methods of terminating a TPOT run and ending the optimization process. TPOT will terminate as soon as one of the conditions is met.</p> <ul> <li><code>max_time_mins</code> : (Default, 60 minutes) After this many minutes, TPOT will terminate and return the best pipeline it found so far.</li> <li><code>early_stop</code> : The number of generations without seeing an improvement in performance, after which TPOT terminates. Generally, a value of around 5 to 20 is sufficient to be reasonably sure that performance has converged.</li> <li><code>generations</code>: The total number of generations of the evolutionary algorithm to run.</li> </ul> <p>By default, TPOT will run until the time limit is up, with no generation or early stop limits.</p>"},{"location":"Tutorial/1_Using_TPOT/#best-practices-and-tips","title":"Best Practices and tips:\u00b6","text":"<ul> <li>When running tpot from an .py script, it is important to protect code with <code>if __name__==\"__main__\":</code> . This is because of how TPOT handles parallelization with Python and Dask.</li> </ul>"},{"location":"Tutorial/1_Using_TPOT/#example-analysis-and-the-estimator-class","title":"Example analysis and the Estimator class\u00b6","text":"<p>Here we use a toy example dataset included in scikit-learn. We will use the <code>light</code> configuration and the <code>complexity_scorer</code> to estimate complexity.</p> <p>Note, for this toy example, we set a relatively short run time. In practice, we would recommend running TPOT for a longer duration with an <code>early_stop</code> value of around 5 to 20 (more details below).</p>"},{"location":"Tutorial/1_Using_TPOT/#saving-the-pipeline","title":"Saving the Pipeline\u00b6","text":"<p>We recommend using dill or pickle to save the instance of the fitted_pipeline_. Note that we do not recommend pickling the TPOT object itself.</p>"},{"location":"Tutorial/1_Using_TPOT/#the-evaluated_individuals-dataframe-further-analysis-of-results","title":"The evaluated_individuals Dataframe - Further analysis of results\u00b6","text":"<p>The <code>evaluated_individuals</code> attribute of the tpot estimator object is a Pandas Dataframe containing information about a run. Each row corresponds to an individual pipeline explored by tpot. The dataframe contains the following columns:</p> Column Description &lt;n objective function columns&gt; The first set of columns will correspond to each objective function. These can either be automatically named by TPOT, or passed in by the user. Parents This contains a tuple that contains the indexes of the 'parents' of the current pipeline. For example, (29, 42) means that the pipelines in indexes 29 and 42 were utilized to generate that pipeline. Variation_Function The function applied to the parents to generate the new pipeline Individual The individual class that represents a specific pipeline and hyperparameter configuration. This class also contains functions for mutation and crossover. To get the sklearn estimator/pipeline object from the individual you can call the <code>export_pipeline()</code> function. (as in, <code>pipe = ind.export_pipeline()</code>) Generation The generation where the individual was created. (Note that the higher performing pipelines from previous generations may still be present in the current \"population\" of a given generation if selected.) Submitted Timestamp Timestamp, in seconds, at which the pipeline was sent to be evaluated. This is the output of time.time(), which is \"Return the time in seconds since the epoch as a floating-point number. \" Completed Timestamp Timestamp at which the pipeline evaluation completed in the same units as Submitted Timestamp Pareto_Front If you have multiple parameters, this column is True if the pipeline performance fall on the pareto front line. This is the set of pipelines with scores that are strictly better than pipelines not on the line, but not strictly better than one another. Instance This contains the unfitted pipeline evaluated for this row. (This is the pipeline returned by calling the export_pipeline() function of the individual class)"},{"location":"Tutorial/1_Using_TPOT/#lets-plot-the-performances-of-the-different-pipelines-including-the-pareto-front","title":"Let's plot the performances of the different pipelines, including the Pareto front\u00b6","text":"<p>Plotting the performance of multiple objectives in a scatterplot is a helpful way to visualize the tradeoff between model complexity and predictive performance. This is best visualized when plotting the Pareto front pipelines, which present the best-performing pipeline along the spectrum of complexity. Generally, higher complexity models may yield higher performance but be more difficult to interpret.</p>"},{"location":"Tutorial/1_Using_TPOT/#plot-performance-over-time-continuing-a-run-from-where-it-left-off","title":"Plot performance over time + Continuing a run from where it left off\u00b6","text":"<p>Plotting performance over time is a good way to assess whether or not the TPOT model has converged. If performance asymptotes over time, there may not be much more performance to be gained by running for a longer period. If the plot looks like it is still improving, it may be worth running TPOT for a longer duration.</p> <p>In this case, we can see that performance is near optimal and has slowed, so more time is likely unnecessary.</p>"},{"location":"Tutorial/1_Using_TPOT/#checkpointing","title":"Checkpointing\u00b6","text":"<p>There are two ways to resume TPOT.</p> <ul> <li>If the <code>warm_start</code> parameter is set to True, subsequent calls to <code>fit</code> will continue training where it left off (The conventional scikit-learn default is to retrain from scratch on subsequent calls to fit).</li> <li>If <code>periodic_checkpoint_folder</code> is set, TPOT will periodically save its current state to disk. If TPOT is interrupted (job canceled, PC shut off, crashes), you can resume training from where it left off. The checkpoint folder stores a data frame of all evaluated pipelines. This data frame can be loaded and inspected to help diagnose problems when debugging.</li> </ul> <p>Note: TPOT does not clean up the checkpoint files. If the <code>periodic_checkpoint_folder</code> parameter is set, training from the last saved point will always continue, even if the input data has changed. A common issue is forgetting to change this folder between experiments and TPOT continuing training from pipelines optimized for another dataset. If you intend to start a run from scratch, you must either remove the parameter, supply an empty folder, or delete the original checkpoint folder.</p>"},{"location":"Tutorial/1_Using_TPOT/#common-parameters","title":"Common parameters\u00b6","text":"<p>Here is a subset of the most common parameters to customize and what they do. See the docs for <code>TPOTEstimator</code> or <code>TPOTEstimatorSteadyState</code> full documentation of all parameters.</p> Parameter Type Description scorers list, scorer List of scorers for cross-validation; see scorers_weights list Weights applied to scorers during optimization classification bool Problem type: True for classification, False for regression cv int, cross-validator Cross-validation strategy: int for folds or custom cross-validator max_depth int Maximum pipeline depth other_objective_functions list Additional objective functions; default: [average_path_length_objective] other_objective_functions_weights list Weights for additional objective functions; default: [-1] objective_function_names list Names for objective functions; default: None (uses function names) bigger_is_better bool Optimization direction: True for maximize, False for minimize generations int Number of optimization generations; default: 50 max_time_mins float Maximum optimization time (minutes); default: infinite max_eval_time_mins float Maximum evaluation time per individual (minutes); default: 300 n_jobs int Number of parallel processes; default: 1 memory_limit str Memory limit per job; default: \"4GB\" verbose int Optimization process verbosity: 0 (none), 1 (progress), 3 (best individual), 4 (warnings), 5+ (full warnings) memory str, memory object If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. periodic_checkpoint_folder str Folder to save the population to periodically. If None, no periodic saving will be done. If provided, training will resume from this checkpoint. <pre><code>    </code></pre>"},{"location":"Tutorial/1_Using_TPOT/#preventing-overfitting","title":"Preventing Overfitting\u00b6","text":"<p>On small datasets, it is not impossible for TPOT to overfit the cross-validation score itself. This can lead to lower-than-expected performance on held-out datasets. TPOT will always return the model with the highest CV score as its final fitted_pipeline. However, if the highest performing model, as evaluated by cross-validation, actually was just overfit to the CV score, it may actually be worse performing compared to other models on the Pareto front. * Using a secondary complexity objective and evaluating the entire pareto front may be beneficial. In some cases a lower performing pipeline with lower complexity can actually perform better on held out sets. These can either be evaluated and compared on a held out validation set, or sometimes, if very data limited, simply using a different seed of splitting the CV folds can work as well. * TPOT can do this automatically. The <code>validation_strategy</code> parameter can be set to re-test the final pareto front on either a held-out validation set (percent of data set by <code>validation_fraction</code>) or a different seed for splitting the CV folds. These can be selected by setting <code>validation_strategy</code> to \"split\" or \"reshuffled\", respectively. * Increasing the number of folds of cross-validation can mitigate this. * Nested cross-validation can also be used to estimate the performance of the TPOT optimization algorithm itself. * Removing more complex methods from the search space can reduce the chances of overfitting</p>"},{"location":"Tutorial/1_Using_TPOT/#tips-and-tricks-for-speeding-up-tpot","title":"Tips and tricks for speeding up TPOT\u00b6","text":"<p>TPOT can be a computationally demanding algorithm as it fits thousands of complex machine learning pipelines on potentially large datasets. There are several strategies available for improving run time by reducing the compute needed.</p> <p>There are three main strategies implemented in TPOT to reduce redundant work and/or prevent wasting compute on poorly performing pipelines.</p> <ol> <li>TPOT pipelines will often have the exact same components doing the exact same computation (e.g. the first steps of the pipeline remain the same and only the parameters of the final classifier changed.) In these cases, that The first strategy is to simply cache these repeat computations so that they only happen once. More info in the next subsection.</li> <li>Successive Halving. This idea was first tested with TPOT by Parmentier et al. in \"TPOT-SH: a Faster Optimization Algorithm to Solve the AutoML Problem on Large Datasets\". The algorithm operates in two stages. Initially, it trains early generations using a small data subset and a large population size. Later generations then evaluate a smaller set of promising pipelines on larger, or even full, data portions. This approach rapidly identifies top-performing pipeline configurations through initial rough evaluations, followed by more comprehensive assessments. More information on this strategy in Tutorial 8.</li> <li>Most often, we will be evaluating pipelines using cross validation. However, we can often tell within the first few folds whether or not the pipeline is going have a reasonable change of outperforming the previous best pipelines. For example, if the best score so far is .92 AUROC and the average score of the first five folds of our current pipeline is only around .61, we can be reasonably confident that the next five folds are unlikely to this pipeline ahead of the others. We can save a significant amount of compute by not computing the rest of the folds. There are two strategies that TPOT can use to accomplish this (More information on these strategies in Tutorial 8).<ol> <li>Threshold Pruning: Pipelines must achieve a score above a predefined percentile threshold (based on previous pipeline scores) to proceed in each cross-validation (CV) fold.</li> <li>Selection Pruning: Within each population, only the top N% of pipelines (ranked by performance in the previous CV fold) are selected to evaluate in the next fold.\"</li> </ol> </li> </ol>"},{"location":"Tutorial/1_Using_TPOT/#pipeline-caching-in-tpot-joblibmemory","title":"Pipeline caching in TPOT (joblib.Memory)\u00b6","text":"<p>With the memory parameter, pipelines can cache the results of each transformer after fitting them. This feature is used to avoid repeated computation by transformers within a pipeline if the parameters and input data are identical to another fitted pipeline during the optimization process. TPOT allows users to specify a custom directory path or joblib.Memory in case they want to re-use the memory cache in future TPOT runs (or a warm_start run).</p> <p>There are three methods for enabling memory caching in TPOT:</p>"},{"location":"Tutorial/1_Using_TPOT/","title":"\u00b6","text":""},{"location":"Tutorial/1_Using_TPOT/#advanced-parallelization-hpc-and-multi-node-training","title":"Advanced Parallelization (HPC and multi-node training)\u00b6","text":"<p>See Tutorial 7 for more details on parallelization with Dask, including information of using multiple nodes.</p>"},{"location":"Tutorial/1_Using_TPOT/#faq-and-debugging","title":"FAQ and Debugging\u00b6","text":"<p>If you are experiencing issues with TPOT, here are some common issues and how to address them.</p> <ul> <li>Performance is lower than expected. What can I do?<ul> <li>TPOT may have to be run for a longer duration, increase <code>max_time_mins</code>, <code>early_stop</code>, or <code>generations</code>.</li> <li>Individual pipelines may need more time to complete fitting; increase <code>max_eval_time_seconds.</code></li> <li>The configuration may not include the optimal model types or hyperparameter ranges, explore other included templates, or customize your own search space (see Tutorial 2!)</li> <li>Check that <code>periodic_checkpoint_folder</code> is set correctly. A common issue is forgetting to change this folder between experiments and TPOT continuing training from pipelines optimized for another dataset.</li> </ul> </li> <li>TPOT is too slow! It is running forever and never terminating<ul> <li>Check that at least one of the three termination conditions is set to a reasonable level. These are <code>max_time_mins</code>, <code>early_stop</code>, or <code>generations</code>. Additionally, check that <code>max_eval_time_seconds</code> gives enough time for most models to train without being overly long. (Some estimators may take an unreasonably long time to fit; this parameter is intended to prevent them from slowing everything to a halt. In my experience, SVC and SVR tend to be the culprits, so removing them from the search space may also improve run time).</li> <li>Set the <code>memory</code> parameter to allow TPOT to prevent repeated work when using either scikit-learn pipelines or TPOT GraphPipelines.</li> <li>Increase n_jobs to use more processes/CPU power. See Tutorial 7 for advanced Dask usage, including parallelizing across multiple nodes on an HPC.</li> <li>Use feature selection, either the build in configuration of sklearn methods (see Tutorial 2), or genetic feature selection (see Tutorials 3 and 5 for two different strategies).</li> <li>Use successive halving to reduce computational load (See tutorial 8).</li> </ul> </li> <li>Many pipelines in the evaluated_individuals data frame have crashed or turned up invalid!<ul> <li>This is normal and is expected behavior for TPOT. In some cases, TPOT may attempt an invalid hyperparameter combination, resulting in the pipeline not working. Other times, the pipeline configuration itself may be invalid. For example, a selector may not select any features due to its hyperparameter. Another common example is <code>MultinomialNB</code> throwing an error because it expects positive values, but a prior transformation yielded a negative value.</li> <li>If you used custom search spaces, you can use <code>ConfigSpace</code> conditionals to prevent invalid hyperparameters (this may still occur due to how TPOT uses crossover).</li> <li>Setting <code>verbose=5</code> will print out the full error message for all failed pipelines. This can be useful for debugging whether or not there is something misconfigured in your pipeline, custom search space modules, or something else.</li> </ul> </li> <li>TPOT is crashing due to memory issues<ul> <li>Set the <code>memory_limit</code> parameter so that n_jobs*memorylimit is less than the available RAM on your machine, plus some wiggle room. This should prevent crashing due to memory concerns.</li> <li>Using feature selection may also improve memory usage, as described above.</li> <li>Remove modules that create high RAM usage (e.g. multiple PolynomialFeatures or one with high degree).</li> </ul> </li> <li>Why are my TPOT runs not reproducible when random_state is set?<ul> <li>Check that <code>periodic_checkpoint_folder</code> is set correctly. If this is set to a non-empty folder, TPOT will continue training from the checkpoint rather than start a new run from scratch. For TPOT runs to be reproducible, they have to have the same starting points.</li> <li>If using custom search spaces, pass in a fixed <code>random_state</code> value into the configspace of the scikit-learn modules that utilize them. TPOT does not check whether estimators do or do not take in a random state value (See Tutorial 2).</li> <li>If using the pre-built search spaces provided by TPOT, make sure to pass in <code>random_state</code> to <code>tpot.config.get_configspace</code> or <code>tpot.config.template_search_spaces.get_template_search_spaces</code>. This ensures all estimators that support it get a fixed random_state value. (See Tutorial 2).</li> <li>If using custom Node and Pipeline types, ensure all random decisions utilize the rng parameter passed into the mutation/crossover functions.</li> <li>If <code>max_eval_time_mins</code> is set, TPOT will terminate pipelines that exceed this time limit. If the pipeline evaluation happens to be very similar to the time limit, small random fluctuations in CPU allocation may cause a given pipeline to be evaluated in one run but not another. This slightly different result would throw off the random number generator throughout the rest of the run. Setting <code>max_eval_time_mins</code> to None or a higher value may prevent this edge case.</li> <li>If using <code>TPOTEstimatorSteadyState</code> with <code>n_jobs</code>&gt;1, it is also possible that random fluctuations in CPU allocation slightly change the order in which pipelines are evaluated, which will affect the downstream results. <code>TPOTEstimatorSteadyState</code> is more reliably reproducible when <code>n_jobs=1</code> (This is not an issue for the default <code>TPOTEstimator</code>, <code>TPOTClassifier</code>, <code>TPOTRegressor</code> as they used a batched generational approach where execution order does not impact results).</li> </ul> </li> <li>TPOT is not using all the CPU cores I expected, given my <code>n_jobs</code> setting.<ul> <li>The default TPOT algorithm uses a generational approach. This means the TPOT will need to evaluate <code>population_size</code> (default 50) pipelines before starting the next batch. At the end of each generation, TPOT may leave threads unused while it waits for the last few pipelines to finish evaluating. Some estimators or pipelines can be significantly slower to evaluate than others. This can be addressed in a few ways:<ul> <li>Decrease <code>max_eval_time_mins</code> to cut long-running pipeline evaluations early.</li> <li>Remove estimators or hyperparameter configurations that are prone to very slow convergence (which is very often <code>SVC</code> or <code>SVR</code>).</li> <li>Alternatively, <code>TPOTEstimatorSteadyState</code> uses a slightly different backend for the evolutionary algorithm that does not utilize the generational approach. Instead, new pipelines are generated and evaluated as soon as the previous one finishes. With this estimator, all cores should be utilized at all times.</li> <li>Sometimes, setting n_jobs to a multiple of the number of threads can help minimize the chances of threads being idle while waiting for others to finish</li> </ul> </li> </ul> </li> </ul>"},{"location":"Tutorial/1_Using_TPOT/#more-options","title":"More Options\u00b6","text":"<p><code>tpot.TPOTClassifier</code> and <code>tpot.TPOTRegressor</code> have a simplified set of hyperparameters with default values set for classification and regression problems. Currently, both of these use the standard evolutionary algorithm in the <code>tpot.TPOTEstimator</code> class. If you want more control, you can look into either the <code>tpot.TPOTEstimator</code> or <code>tpot.TPOTEstimatorSteadyState</code> class.</p> <p>There are two evolutionary algorithms built into TPOT, which corresponds to two different estimator classes.</p> <ol> <li><p>The <code>tpot.TPOTEstimator</code> uses a standard evolutionary algorithm that evaluates exactly population_size individuals each generation. This is similar to the algorithm in TPOT1. The next generation does not start until the previous is completely finished evaluating. This leads to underutilized CPU time as the cores are waiting for the last individuals to finish training, but may preserve diversity in the population.</p> </li> <li><p>The <code>tpot.TPOTEstimatorSteadyState</code> differs in that it will generate and evaluate the next individual as soon as an individual finishes the evaluation. The number of individuals being evaluated is determined by the n_jobs parameter. There is no longer a concept of generations. The population_size parameter now refers to the size of the list of evaluated parents. When an individual is evaluated, the selection method updates the list of parents. This allows more efficient utilization when using multiple cores.</p> </li> </ol>"},{"location":"Tutorial/1_Using_TPOT/#tpottpotestimatorsteadystate","title":"tpot.TPOTEstimatorSteadyState\u00b6","text":""},{"location":"Tutorial/1_Using_TPOT/#tpottpotestimator","title":"tpot.TPOTEstimator\u00b6","text":""},{"location":"Tutorial/2_Search_Spaces/","title":"Intro","text":"In\u00a0[1]: Copied! <pre>from ConfigSpace import ConfigurationSpace\nfrom ConfigSpace import ConfigurationSpace, Integer, Float, Categorical, Normal\nfrom sklearn.ensemble import RandomForestClassifier\nimport tpot\nimport numpy as np\nimport sklearn\nimport sklearn.datasets\n\nrf_configspace = ConfigurationSpace(\n    space = {\n            'n_estimators': 128, #as recommended by Oshiro et al. (2012\n            'max_features': Float(\"max_features\", bounds=(0.01,1), log=True), #log scale like autosklearn?\n            'criterion': Categorical(\"criterion\", ['gini', 'entropy']),\n            'min_samples_split': Integer(\"min_samples_split\", bounds=(2, 20)),\n            'min_samples_leaf': Integer(\"min_samples_leaf\", bounds=(1, 20)),\n            'bootstrap': Categorical(\"bootstrap\", [True, False]),\n            #random_state = 1, # If you want results to be reproducible, you can set a fixed random_state.\n        }\n)\n\nhyperparameters = dict(rf_configspace.sample_configuration())\nprint(\"sampled hyperparameters\")\nprint(hyperparameters)\n\nrf = RandomForestClassifier(**hyperparameters)\nrf\n</pre> from ConfigSpace import ConfigurationSpace from ConfigSpace import ConfigurationSpace, Integer, Float, Categorical, Normal from sklearn.ensemble import RandomForestClassifier import tpot import numpy as np import sklearn import sklearn.datasets  rf_configspace = ConfigurationSpace(     space = {             'n_estimators': 128, #as recommended by Oshiro et al. (2012             'max_features': Float(\"max_features\", bounds=(0.01,1), log=True), #log scale like autosklearn?             'criterion': Categorical(\"criterion\", ['gini', 'entropy']),             'min_samples_split': Integer(\"min_samples_split\", bounds=(2, 20)),             'min_samples_leaf': Integer(\"min_samples_leaf\", bounds=(1, 20)),             'bootstrap': Categorical(\"bootstrap\", [True, False]),             #random_state = 1, # If you want results to be reproducible, you can set a fixed random_state.         } )  hyperparameters = dict(rf_configspace.sample_configuration()) print(\"sampled hyperparameters\") print(hyperparameters)  rf = RandomForestClassifier(**hyperparameters) rf <pre>sampled hyperparameters\n{'bootstrap': True, 'criterion': 'gini', 'max_features': 0.8874647037836, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 128}\n</pre> <pre>/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> Out[1]: <pre>RandomForestClassifier(max_features=0.8874647037836, min_samples_leaf=2,\n                       min_samples_split=5, n_estimators=128)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0RandomForestClassifier?Documentation for RandomForestClassifieriNot fitted<pre>RandomForestClassifier(max_features=0.8874647037836, min_samples_leaf=2,\n                       min_samples_split=5, n_estimators=128)</pre> <p>More simply:</p> In\u00a0[2]: Copied! <pre>rf_configspace = ConfigurationSpace(\n    space = {\n            'n_estimators': 128, #as recommended by Oshiro et al. (2012\n            'max_features':(0.01,1), #not log scaled\n            'criterion': ['gini', 'entropy'],\n            'min_samples_split': (2, 20),\n            'min_samples_leaf': (1, 20),\n            'bootstrap': [True, False],\n            #random_state = 1, # If you want results to be reproducible, you can set a fixed random_state.\n        }\n)\n\nhyperparameters = dict(rf_configspace.sample_configuration())\nprint(\"sampled hyperparameters\")\nprint(hyperparameters)\n\nrf = RandomForestClassifier(**hyperparameters)\nrf\n</pre> rf_configspace = ConfigurationSpace(     space = {             'n_estimators': 128, #as recommended by Oshiro et al. (2012             'max_features':(0.01,1), #not log scaled             'criterion': ['gini', 'entropy'],             'min_samples_split': (2, 20),             'min_samples_leaf': (1, 20),             'bootstrap': [True, False],             #random_state = 1, # If you want results to be reproducible, you can set a fixed random_state.         } )  hyperparameters = dict(rf_configspace.sample_configuration()) print(\"sampled hyperparameters\") print(hyperparameters)  rf = RandomForestClassifier(**hyperparameters) rf <pre>sampled hyperparameters\n{'bootstrap': False, 'criterion': 'entropy', 'max_features': 0.8418685817308, 'min_samples_leaf': 5, 'min_samples_split': 2, 'n_estimators': 128}\n</pre> Out[2]: <pre>RandomForestClassifier(bootstrap=False, criterion='entropy',\n                       max_features=0.8418685817308, min_samples_leaf=5,\n                       n_estimators=128)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0RandomForestClassifier?Documentation for RandomForestClassifieriNot fitted<pre>RandomForestClassifier(bootstrap=False, criterion='entropy',\n                       max_features=0.8418685817308, min_samples_leaf=5,\n                       n_estimators=128)</pre> In\u00a0[3]: Copied! <pre>import tpot\nfrom ConfigSpace import ConfigurationSpace\nfrom ConfigSpace import ConfigurationSpace, Integer, Float, Categorical, Normal\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn_configspace = ConfigurationSpace(\n    space = {\n\n        'n_neighbors': Integer(\"n_neighbors\", bounds=(1, 10)),\n        'weights': Categorical(\"weights\", ['uniform', 'distance']),\n        'p': Integer(\"p\", bounds=(1, 3)),\n        'metric': Categorical(\"metric\", ['euclidean', 'minkowski']),\n        'n_jobs': 1,\n    }\n)\n\n\nknn_node = tpot.search_spaces.nodes.EstimatorNode(\n    method = KNeighborsClassifier,\n    space = knn_configspace,\n)\n</pre> import tpot from ConfigSpace import ConfigurationSpace from ConfigSpace import ConfigurationSpace, Integer, Float, Categorical, Normal from sklearn.neighbors import KNeighborsClassifier  knn_configspace = ConfigurationSpace(     space = {          'n_neighbors': Integer(\"n_neighbors\", bounds=(1, 10)),         'weights': Categorical(\"weights\", ['uniform', 'distance']),         'p': Integer(\"p\", bounds=(1, 3)),         'metric': Categorical(\"metric\", ['euclidean', 'minkowski']),         'n_jobs': 1,     } )   knn_node = tpot.search_spaces.nodes.EstimatorNode(     method = KNeighborsClassifier,     space = knn_configspace, ) <p>You can sample generate an individual with the generate() function. This individual samples from the search space as well as provides mutation and crossover functions to modify the current sample.</p> In\u00a0[4]: Copied! <pre>knn_individual = knn_node.generate()\nknn_individual\n</pre> knn_individual = knn_node.generate() knn_individual Out[4]: <pre>&lt;tpot.search_spaces.nodes.estimator_node.EstimatorNodeIndividual at 0x103cb5a80&gt;</pre> In\u00a0[5]: Copied! <pre>print(\"sampled hyperparameters\")\nprint(knn_individual.hyperparameters)\n</pre> print(\"sampled hyperparameters\") print(knn_individual.hyperparameters) <pre>sampled hyperparameters\n{'metric': 'minkowski', 'n_jobs': 1, 'n_neighbors': 4, 'p': 1, 'weights': 'uniform'}\n</pre> <p>All Individual objects have mutation and crossover operators that TPOT uses to optimize the pipelines.</p> In\u00a0[6]: Copied! <pre>knn_individual.mutate() # mutate the individual\nprint(\"mutated hyperparameters\")\nprint(knn_individual.hyperparameters)\n</pre> knn_individual.mutate() # mutate the individual print(\"mutated hyperparameters\") print(knn_individual.hyperparameters) <pre>mutated hyperparameters\n{'metric': 'minkowski', 'n_jobs': 1, 'n_neighbors': 6, 'p': 2, 'weights': 'distance'}\n</pre> <p>In TPOT, crossover only modifies the individual calling the crossover function, the second individual remains the same</p> In\u00a0[7]: Copied! <pre>knn_individual1 = knn_node.generate()\nknn_individual2 = knn_node.generate()\n\nprint(\"original hyperparameters for individual 1\")\nprint(knn_individual1.hyperparameters)\n\nprint(\"original hyperparameters for individual 2\")\nprint(knn_individual2.hyperparameters)\n\nprint()\n\nknn_individual1.crossover(knn_individual2) # crossover the individuals\nprint(\"post crossover hyperparameters for individual 1\")\nprint(knn_individual1.hyperparameters)\nprint(\"post crossover hyperparameters for individual 2\")\nprint(knn_individual2.hyperparameters)\n</pre> knn_individual1 = knn_node.generate() knn_individual2 = knn_node.generate()  print(\"original hyperparameters for individual 1\") print(knn_individual1.hyperparameters)  print(\"original hyperparameters for individual 2\") print(knn_individual2.hyperparameters)  print()  knn_individual1.crossover(knn_individual2) # crossover the individuals print(\"post crossover hyperparameters for individual 1\") print(knn_individual1.hyperparameters) print(\"post crossover hyperparameters for individual 2\") print(knn_individual2.hyperparameters)   <pre>original hyperparameters for individual 1\n{'metric': 'euclidean', 'n_jobs': 1, 'n_neighbors': 8, 'p': 3, 'weights': 'uniform'}\noriginal hyperparameters for individual 2\n{'metric': 'minkowski', 'n_jobs': 1, 'n_neighbors': 3, 'p': 2, 'weights': 'distance'}\n\npost crossover hyperparameters for individual 1\n{'metric': 'minkowski', 'n_jobs': 1, 'n_neighbors': 3, 'p': 2, 'weights': 'distance'}\npost crossover hyperparameters for individual 2\n{'metric': 'minkowski', 'n_jobs': 1, 'n_neighbors': 3, 'p': 2, 'weights': 'distance'}\n</pre> <p>All search spaces have an export_pipeline function that returns an sklearn <code>BaseEstimator</code></p> In\u00a0[8]: Copied! <pre>est = knn_individual1.export_pipeline()\nest\n</pre> est = knn_individual1.export_pipeline() est Out[8]: <pre>KNeighborsClassifier(n_jobs=1, n_neighbors=3, weights='distance')</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0KNeighborsClassifier?Documentation for KNeighborsClassifieriNot fitted<pre>KNeighborsClassifier(n_jobs=1, n_neighbors=3, weights='distance')</pre> <p>If a dictionary of parameters is passed instead of of a ConfigSpace object, then the hyperparameters will always be fixed and not learned.</p> In\u00a0[9]: Copied! <pre>import tpot\nfrom ConfigSpace import ConfigurationSpace\nfrom ConfigSpace import ConfigurationSpace, Integer, Float, Categorical, Normal\nfrom sklearn.neighbors import KNeighborsClassifier\n\nspace = {\n\n    'n_neighbors':10,\n}\n\nknn_node = tpot.search_spaces.nodes.EstimatorNode(\n    method = KNeighborsClassifier,\n    space = space,\n)\n\nknn_node.generate().export_pipeline()\n</pre> import tpot from ConfigSpace import ConfigurationSpace from ConfigSpace import ConfigurationSpace, Integer, Float, Categorical, Normal from sklearn.neighbors import KNeighborsClassifier  space = {      'n_neighbors':10, }  knn_node = tpot.search_spaces.nodes.EstimatorNode(     method = KNeighborsClassifier,     space = space, )  knn_node.generate().export_pipeline() Out[9]: <pre>KNeighborsClassifier(n_neighbors=10)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0KNeighborsClassifier?Documentation for KNeighborsClassifieriNot fitted<pre>KNeighborsClassifier(n_neighbors=10)</pre> In\u00a0[10]: Copied! <pre>import tpot\nfrom ConfigSpace import ConfigurationSpace\nfrom ConfigSpace import ConfigurationSpace, Integer, Float, Categorical, Normal\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\nknn_configspace = ConfigurationSpace(\n    space = {\n\n        'n_neighbors': Integer(\"n_neighbors\", bounds=(1, 10)),\n        'weights': Categorical(\"weights\", ['uniform', 'distance']),\n        'p': Integer(\"p\", bounds=(1, 3)),\n        'metric': Categorical(\"metric\", ['euclidean', 'minkowski']),\n        'n_jobs': 1,\n    }\n)\n\nlr_configspace = ConfigurationSpace(\n        space = {\n            'solver': Categorical(\"solver\", ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']),\n            'penalty': Categorical(\"penalty\", ['l1', 'l2']),\n            'dual': Categorical(\"dual\", [True, False]),\n            'C': Float(\"C\", bounds=(1e-4, 1e4), log=True),\n            'class_weight': Categorical(\"class_weight\", ['balanced']),\n            'n_jobs': 1,\n            'max_iter': 1000,\n        }\n    )\n\ndt_configspace = ConfigurationSpace(\n        space = {\n            'criterion': Categorical(\"criterion\", ['gini', 'entropy']),\n            'max_depth': Integer(\"max_depth\", bounds=(1, 11)),\n            'min_samples_split': Integer(\"min_samples_split\", bounds=(2, 21)),\n            'min_samples_leaf': Integer(\"min_samples_leaf\", bounds=(1, 21)),\n            'max_features': Categorical(\"max_features\", ['sqrt', 'log2']),\n            'min_weight_fraction_leaf': 0.0,\n        }\n    )\n\nknn_node = tpot.search_spaces.nodes.EstimatorNode(\n    method = KNeighborsClassifier,\n    space = knn_configspace,\n)\n\nlr_node = tpot.search_spaces.nodes.EstimatorNode(\n    method = LogisticRegression,\n    space = lr_configspace,\n)\n\ndt_node = tpot.search_spaces.nodes.EstimatorNode(\n    method = DecisionTreeClassifier,\n    space = dt_configspace,\n)\n\nclassifier_node = tpot.search_spaces.pipelines.ChoicePipeline(\n    search_spaces=[\n        knn_node,\n        lr_node,\n        dt_node,\n    ]\n)\n\n\ntpot.search_spaces.pipelines.ChoicePipeline(\n    search_spaces = [\n        tpot.search_spaces.nodes.EstimatorNode(\n            method = KNeighborsClassifier,\n            space = knn_configspace,\n            ),\n        tpot.search_spaces.nodes.EstimatorNode(\n            method = LogisticRegression,\n            space = lr_configspace,\n        ),\n        tpot.search_spaces.nodes.EstimatorNode(\n            method = DecisionTreeClassifier,\n            space = dt_configspace,\n        ),\n    ]\n)\n</pre> import tpot from ConfigSpace import ConfigurationSpace from ConfigSpace import ConfigurationSpace, Integer, Float, Categorical, Normal from sklearn.neighbors import KNeighborsClassifier from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier  knn_configspace = ConfigurationSpace(     space = {          'n_neighbors': Integer(\"n_neighbors\", bounds=(1, 10)),         'weights': Categorical(\"weights\", ['uniform', 'distance']),         'p': Integer(\"p\", bounds=(1, 3)),         'metric': Categorical(\"metric\", ['euclidean', 'minkowski']),         'n_jobs': 1,     } )  lr_configspace = ConfigurationSpace(         space = {             'solver': Categorical(\"solver\", ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']),             'penalty': Categorical(\"penalty\", ['l1', 'l2']),             'dual': Categorical(\"dual\", [True, False]),             'C': Float(\"C\", bounds=(1e-4, 1e4), log=True),             'class_weight': Categorical(\"class_weight\", ['balanced']),             'n_jobs': 1,             'max_iter': 1000,         }     )  dt_configspace = ConfigurationSpace(         space = {             'criterion': Categorical(\"criterion\", ['gini', 'entropy']),             'max_depth': Integer(\"max_depth\", bounds=(1, 11)),             'min_samples_split': Integer(\"min_samples_split\", bounds=(2, 21)),             'min_samples_leaf': Integer(\"min_samples_leaf\", bounds=(1, 21)),             'max_features': Categorical(\"max_features\", ['sqrt', 'log2']),             'min_weight_fraction_leaf': 0.0,         }     )  knn_node = tpot.search_spaces.nodes.EstimatorNode(     method = KNeighborsClassifier,     space = knn_configspace, )  lr_node = tpot.search_spaces.nodes.EstimatorNode(     method = LogisticRegression,     space = lr_configspace, )  dt_node = tpot.search_spaces.nodes.EstimatorNode(     method = DecisionTreeClassifier,     space = dt_configspace, )  classifier_node = tpot.search_spaces.pipelines.ChoicePipeline(     search_spaces=[         knn_node,         lr_node,         dt_node,     ] )   tpot.search_spaces.pipelines.ChoicePipeline(     search_spaces = [         tpot.search_spaces.nodes.EstimatorNode(             method = KNeighborsClassifier,             space = knn_configspace,             ),         tpot.search_spaces.nodes.EstimatorNode(             method = LogisticRegression,             space = lr_configspace,         ),         tpot.search_spaces.nodes.EstimatorNode(             method = DecisionTreeClassifier,             space = dt_configspace,         ),     ] ) Out[10]: <pre>&lt;tpot.search_spaces.pipelines.choice.ChoicePipeline at 0x32f769780&gt;</pre> <p>Search space objects provided by pipeline search spaces work the same as with node search spaces. Note that crossover only works when both individuals have sampled the same method.</p> In\u00a0[11]: Copied! <pre>classifier_individual = classifier_node.generate()\n\nprint(\"sampled pipeline\")\nclassifier_individual.export_pipeline()\n</pre> classifier_individual = classifier_node.generate()  print(\"sampled pipeline\") classifier_individual.export_pipeline() <pre>sampled pipeline\n</pre> Out[11]: <pre>KNeighborsClassifier(n_jobs=1, n_neighbors=3)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0KNeighborsClassifier?Documentation for KNeighborsClassifieriNot fitted<pre>KNeighborsClassifier(n_jobs=1, n_neighbors=3)</pre> In\u00a0[12]: Copied! <pre>print(\"mutated pipeline\")\nclassifier_individual.mutate()\nclassifier_individual.export_pipeline()\n</pre> print(\"mutated pipeline\") classifier_individual.mutate() classifier_individual.export_pipeline() <pre>mutated pipeline\n</pre> Out[12]: <pre>KNeighborsClassifier(metric='euclidean', n_jobs=1, n_neighbors=9)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0KNeighborsClassifier?Documentation for KNeighborsClassifieriNot fitted<pre>KNeighborsClassifier(metric='euclidean', n_jobs=1, n_neighbors=9)</pre> <p>TPOT also comes with predefined hyperparameter search spaces. The current search spaces were adapted from a combination of the original TPOT package as well as the search spaces used in AutoSklearn. The helper function <code>tpot.config.get_search_space</code> takes in a string or a list of strings, and returns either a EstimatorNode or a ChoicePipeline (including all methods in the list), respectively.</p> String Corresponding Method SGDClassifier &lt;class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'&gt; RandomForestClassifier &lt;class 'sklearn.ensemble._forest.RandomForestClassifier'&gt; ExtraTreesClassifier &lt;class 'sklearn.ensemble._forest.ExtraTreesClassifier'&gt; GradientBoostingClassifier &lt;class 'sklearn.ensemble._gb.GradientBoostingClassifier'&gt; MLPClassifier &lt;class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'&gt; DecisionTreeClassifier &lt;class 'sklearn.tree._classes.DecisionTreeClassifier'&gt; XGBClassifier &lt;class 'xgboost.sklearn.XGBClassifier'&gt; KNeighborsClassifier &lt;class 'sklearn.neighbors._classification.KNeighborsClassifier'&gt; SVC &lt;class 'sklearn.svm._classes.SVC'&gt; LogisticRegression &lt;class 'sklearn.linear_model._logistic.LogisticRegression'&gt; LGBMClassifier &lt;class 'lightgbm.sklearn.LGBMClassifier'&gt; LinearSVC &lt;class 'sklearn.svm._classes.LinearSVC'&gt; GaussianNB &lt;class 'sklearn.naive_bayes.GaussianNB'&gt; BernoulliNB &lt;class 'sklearn.naive_bayes.BernoulliNB'&gt; MultinomialNB &lt;class 'sklearn.naive_bayes.MultinomialNB'&gt; ExtraTreesRegressor &lt;class 'sklearn.ensemble._forest.ExtraTreesRegressor'&gt; RandomForestRegressor &lt;class 'sklearn.ensemble._forest.RandomForestRegressor'&gt; GradientBoostingRegressor &lt;class 'sklearn.ensemble._gb.GradientBoostingRegressor'&gt; BaggingRegressor &lt;class 'sklearn.ensemble._bagging.BaggingRegressor'&gt; DecisionTreeRegressor &lt;class 'sklearn.tree._classes.DecisionTreeRegressor'&gt; KNeighborsRegressor &lt;class 'sklearn.neighbors._regression.KNeighborsRegressor'&gt; XGBRegressor &lt;class 'xgboost.sklearn.XGBRegressor'&gt; ZeroCount &lt;class 'tpot.builtin_modules.zero_count.ZeroCount'&gt; ColumnOneHotEncoder &lt;class 'tpot.builtin_modules.column_one_hot_encoder.ColumnOneHotEncoder'&gt; Binarizer &lt;class 'sklearn.preprocessing._data.Binarizer'&gt; FastICA &lt;class 'sklearn.decomposition._fastica.FastICA'&gt; FeatureAgglomeration &lt;class 'sklearn.cluster._agglomerative.FeatureAgglomeration'&gt; MaxAbsScaler &lt;class 'sklearn.preprocessing._data.MaxAbsScaler'&gt; MinMaxScaler &lt;class 'sklearn.preprocessing._data.MinMaxScaler'&gt; Normalizer &lt;class 'sklearn.preprocessing._data.Normalizer'&gt; Nystroem &lt;class 'sklearn.kernel_approximation.Nystroem'&gt; PCA &lt;class 'sklearn.decomposition._pca.PCA'&gt; PolynomialFeatures &lt;class 'sklearn.preprocessing._polynomial.PolynomialFeatures'&gt; RBFSampler &lt;class 'sklearn.kernel_approximation.RBFSampler'&gt; RobustScaler &lt;class 'sklearn.preprocessing._data.RobustScaler'&gt; StandardScaler &lt;class 'sklearn.preprocessing._data.StandardScaler'&gt; SelectFwe &lt;class 'sklearn.feature_selection._univariate_selection.SelectFwe'&gt; SelectPercentile &lt;class 'sklearn.feature_selection._univariate_selection.SelectPercentile'&gt; VarianceThreshold &lt;class 'sklearn.feature_selection._variance_threshold.VarianceThreshold'&gt; SGDRegressor &lt;class 'sklearn.linear_model._stochastic_gradient.SGDRegressor'&gt; Ridge &lt;class 'sklearn.linear_model._ridge.Ridge'&gt; Lasso &lt;class 'sklearn.linear_model._coordinate_descent.Lasso'&gt; ElasticNet &lt;class 'sklearn.linear_model._coordinate_descent.ElasticNet'&gt; Lars &lt;class 'sklearn.linear_model._least_angle.Lars'&gt; LassoLars &lt;class 'sklearn.linear_model._least_angle.LassoLars'&gt; LassoLarsCV &lt;class 'sklearn.linear_model._least_angle.LassoLarsCV'&gt; RidgeCV &lt;class 'sklearn.linear_model._ridge.RidgeCV'&gt; SVR &lt;class 'sklearn.svm._classes.SVR'&gt; LinearSVR &lt;class 'sklearn.svm._classes.LinearSVR'&gt; AdaBoostRegressor &lt;class 'sklearn.ensemble._weight_boosting.AdaBoostRegressor'&gt; ElasticNetCV &lt;class 'sklearn.linear_model._coordinate_descent.ElasticNetCV'&gt; AdaBoostClassifier &lt;class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'&gt; MLPRegressor &lt;class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'&gt; GaussianProcessRegressor &lt;class 'sklearn.gaussian_process._gpr.GaussianProcessRegressor'&gt; HistGradientBoostingClassifier &lt;class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'&gt; HistGradientBoostingRegressor &lt;class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'&gt; AddTransformer &lt;class 'tpot.builtin_modules.arithmetictransformer.AddTransformer'&gt; mul_neg_1_Transformer &lt;class 'tpot.builtin_modules.arithmetictransformer.mul_neg_1_Transformer'&gt; MulTransformer &lt;class 'tpot.builtin_modules.arithmetictransformer.MulTransformer'&gt; SafeReciprocalTransformer &lt;class 'tpot.builtin_modules.arithmetictransformer.SafeReciprocalTransformer'&gt; EQTransformer &lt;class 'tpot.builtin_modules.arithmetictransformer.EQTransformer'&gt; NETransformer &lt;class 'tpot.builtin_modules.arithmetictransformer.NETransformer'&gt; GETransformer &lt;class 'tpot.builtin_modules.arithmetictransformer.GETransformer'&gt; GTTransformer &lt;class 'tpot.builtin_modules.arithmetictransformer.GTTransformer'&gt; LETransformer &lt;class 'tpot.builtin_modules.arithmetictransformer.LETransformer'&gt; LTTransformer &lt;class 'tpot.builtin_modules.arithmetictransformer.LTTransformer'&gt; MinTransformer &lt;class 'tpot.builtin_modules.arithmetictransformer.MinTransformer'&gt; MaxTransformer &lt;class 'tpot.builtin_modules.arithmetictransformer.MaxTransformer'&gt; ZeroTransformer &lt;class 'tpot.builtin_modules.arithmetictransformer.ZeroTransformer'&gt; OneTransformer &lt;class 'tpot.builtin_modules.arithmetictransformer.OneTransformer'&gt; NTransformer &lt;class 'tpot.builtin_modules.arithmetictransformer.NTransformer'&gt; PowerTransformer &lt;class 'sklearn.preprocessing._data.PowerTransformer'&gt; QuantileTransformer &lt;class 'sklearn.preprocessing._data.QuantileTransformer'&gt; ARDRegression &lt;class 'sklearn.linear_model._bayes.ARDRegression'&gt; QuadraticDiscriminantAnalysis &lt;class 'sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis'&gt; PassiveAggressiveClassifier &lt;class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier'&gt; LinearDiscriminantAnalysis &lt;class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'&gt; DominantEncoder &lt;class 'tpot.builtin_modules.genetic_encoders.DominantEncoder'&gt; RecessiveEncoder &lt;class 'tpot.builtin_modules.genetic_encoders.RecessiveEncoder'&gt; HeterosisEncoder &lt;class 'tpot.builtin_modules.genetic_encoders.HeterosisEncoder'&gt; UnderDominanceEncoder &lt;class 'tpot.builtin_modules.genetic_encoders.UnderDominanceEncoder'&gt; OverDominanceEncoder &lt;class 'tpot.builtin_modules.genetic_encoders.OverDominanceEncoder'&gt; GaussianProcessClassifier &lt;class 'sklearn.gaussian_process._gpc.GaussianProcessClassifier'&gt; BaggingClassifier &lt;class 'sklearn.ensemble._bagging.BaggingClassifier'&gt; LGBMRegressor &lt;class 'lightgbm.sklearn.LGBMRegressor'&gt; Passthrough &lt;class 'tpot.builtin_modules.passthrough.Passthrough'&gt; SkipTransformer &lt;class 'tpot.builtin_modules.passthrough.SkipTransformer'&gt; PassKBinsDiscretizer &lt;class 'tpot.builtin_modules.passkbinsdiscretizer.PassKBinsDiscretizer'&gt; SimpleImputer &lt;class 'sklearn.impute._base.SimpleImputer'&gt; IterativeImputer &lt;class 'sklearn.impute._iterative.IterativeImputer'&gt; KNNImputer &lt;class 'sklearn.impute._knn.KNNImputer'&gt; MDR &lt;class 'mdr.mdr.MDR'&gt; ContinuousMDR &lt;class 'mdr.continuous_mdr.ContinuousMDR'&gt; ReliefF &lt;class 'skrebate.relieff.ReliefF'&gt; SURF &lt;class 'skrebate.surf.SURF'&gt; SURFstar &lt;class 'skrebate.surfstar.SURFstar'&gt; MultiSURF &lt;class 'skrebate.multisurf.MultiSURF'&gt; LinearRegression_sklearnex &lt;class 'sklearnex.linear_model.linear.LinearRegression'&gt; Ridge_sklearnex &lt;class 'daal4py.sklearn.linear_model._ridge.Ridge'&gt; Lasso_sklearnex &lt;class 'daal4py.sklearn.linear_model._coordinate_descent.Lasso'&gt; ElasticNet_sklearnex &lt;class 'daal4py.sklearn.linear_model._coordinate_descent.ElasticNet'&gt; SVR_sklearnex &lt;class 'sklearnex.svm.svr.SVR'&gt; NuSVR_sklearnex &lt;class 'sklearnex.svm.nusvr.NuSVR'&gt; RandomForestRegressor_sklearnex &lt;class 'sklearnex.ensemble._forest.RandomForestRegressor'&gt; KNeighborsRegressor_sklearnex &lt;class 'sklearnex.neighbors.knn_regression.KNeighborsRegressor'&gt; RandomForestClassifier_sklearnex &lt;class 'sklearnex.ensemble._forest.RandomForestClassifier'&gt; KNeighborsClassifier_sklearnex &lt;class 'sklearnex.neighbors.knn_classification.KNeighborsClassifier'&gt; SVC_sklearnex &lt;class 'sklearnex.svm.svc.SVC'&gt; NuSVC_sklearnex &lt;class 'sklearnex.svm.nusvc.NuSVC'&gt; LogisticRegression_sklearnex &lt;class 'sklearnex.linear_model.logistic_regression.LogisticRegression'&gt; <p>Some methods require a wrapped estimator. To account for both regression and classification, these have been grouped separately with their own special strings.</p> Wrapper Special String Notes RFE_classification FRE with learned ExtraTreesClassifier RFE_regression RFE with learned ExtraTreesRegressor SelectFromModel_classification SelectFromModel with learned ExtraTreesClassifier SelectFromModel_regression SelectFromModel with learned ExtraTreesRegressor IterativeImputer_learned_estimators IterativeImputer with learned ExtraTreesRegressor <p>There are also special strings that include a predefined lists of methods. These will return a ChoicePipeline of the included methods.</p> List Special String Included methods \"selectors\" [\"SelectFwe\", \"SelectPercentile\", \"VarianceThreshold\",] \"selectors_classification\" [\"SelectFwe\", \"SelectPercentile\", \"VarianceThreshold\", \"RFE_classification\", \"SelectFromModel_classification\"] \"selectors_regression\" [\"SelectFwe\", \"SelectPercentile\", \"VarianceThreshold\", \"RFE_regression\", \"SelectFromModel_regression\"] \"classifiers\" [\"LGBMClassifier\", \"BaggingClassifier\", 'AdaBoostClassifier', 'BernoulliNB', 'DecisionTreeClassifier', 'ExtraTreesClassifier', 'GaussianNB', 'HistGradientBoostingClassifier', 'KNeighborsClassifier','LinearDiscriminantAnalysis', 'LogisticRegression', \"LinearSVC\", \"SVC\", 'MLPClassifier', 'MultinomialNB',  \"QuadraticDiscriminantAnalysis\", 'RandomForestClassifier', 'SGDClassifier', 'XGBClassifier'] \"regressors\" [\"LGBMRegressor\", 'AdaBoostRegressor', \"ARDRegression\", 'DecisionTreeRegressor', 'ExtraTreesRegressor', 'HistGradientBoostingRegressor', 'KNeighborsRegressor',  'LinearSVR', \"MLPRegressor\", 'RandomForestRegressor', 'SGDRegressor', 'SVR', 'XGBRegressor'] \"transformers\" [\"PassKBinsDiscretizer\", \"Binarizer\", \"PCA\", \"ZeroCount\", \"ColumnOneHotEncoder\", \"FastICA\", \"FeatureAgglomeration\", \"Nystroem\", \"RBFSampler\", \"QuantileTransformer\", \"PowerTransformer\"] \"scalers\" [\"MinMaxScaler\", \"RobustScaler\", \"StandardScaler\", \"MaxAbsScaler\", \"Normalizer\", ] \"all_transformers\" [\"transformers\", \"scalers\"] \"arithmatic\" [\"AddTransformer\", \"mul_neg_1_Transformer\", \"MulTransformer\", \"SafeReciprocalTransformer\", \"EQTransformer\", \"NETransformer\", \"GETransformer\", \"GTTransformer\", \"LETransformer\", \"LTTransformer\", \"MinTransformer\", \"MaxTransformer\"] \"imputers\" [\"SimpleImputer\", \"IterativeImputer\", \"KNNImputer\"] \"skrebate\" [\"ReliefF\", \"SURF\", \"SURFstar\", \"MultiSURF\"] \"genetic_encoders\" [\"DominantEncoder\", \"RecessiveEncoder\", \"HeterosisEncoder\", \"UnderDominanceEncoder\", \"OverDominanceEncoder\"] \"classifiers_sklearnex\" [\"RandomForestClassifier_sklearnex\", \"LogisticRegression_sklearnex\", \"KNeighborsClassifier_sklearnex\", \"SVC_sklearnex\",\"NuSVC_sklearnex\"] \"regressors_sklearnex\" [\"LinearRegression_sklearnex\", \"Ridge_sklearnex\", \"Lasso_sklearnex\", \"ElasticNet_sklearnex\", \"SVR_sklearnex\", \"NuSVR_sklearnex\", \"RandomForestRegressor_sklearnex\", \"KNeighborsRegressor_sklearnex\"] \"genetic encoders\" [\"DominantEncoder\", \"RecessiveEncoder\", \"HeterosisEncoder\", \"UnderDominanceEncoder\", \"OverDominanceEncoder\"] <p>Here are some examples of how to get search spaces using the <code>get_search_space</code> function.</p> In\u00a0[13]: Copied! <pre>#same pipeline search space as before.\nclassifier_choice = tpot.config.get_search_space([\"KNeighborsClassifier\", \"LogisticRegression\", \"DecisionTreeClassifier\"])\n\nprint(\"sampled pipeline 1\")\nclassifier_choice.generate().export_pipeline()\n</pre> #same pipeline search space as before. classifier_choice = tpot.config.get_search_space([\"KNeighborsClassifier\", \"LogisticRegression\", \"DecisionTreeClassifier\"])  print(\"sampled pipeline 1\") classifier_choice.generate().export_pipeline() <pre>sampled pipeline 1\n</pre> Out[13]: <pre>KNeighborsClassifier(n_jobs=1, n_neighbors=15, p=1, weights='distance')</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0KNeighborsClassifier?Documentation for KNeighborsClassifieriNot fitted<pre>KNeighborsClassifier(n_jobs=1, n_neighbors=15, p=1, weights='distance')</pre> In\u00a0[14]: Copied! <pre>print(\"sampled pipeline 2\")\nclassifier_choice.generate().export_pipeline()\n</pre> print(\"sampled pipeline 2\") classifier_choice.generate().export_pipeline() <pre>sampled pipeline 2\n</pre> Out[14]: <pre>LogisticRegression(C=5.9018435257131, max_iter=1000, n_jobs=1, solver='saga')</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0LogisticRegression?Documentation for LogisticRegressioniNot fitted<pre>LogisticRegression(C=5.9018435257131, max_iter=1000, n_jobs=1, solver='saga')</pre> In\u00a0[15]: Copied! <pre>#search space for all classifiers\nclassifier_choice = tpot.config.get_search_space(\"classifiers\")\n\nprint(\"sampled pipeline 1\")\nclassifier_choice.generate().export_pipeline()\n</pre> #search space for all classifiers classifier_choice = tpot.config.get_search_space(\"classifiers\")  print(\"sampled pipeline 1\") classifier_choice.generate().export_pipeline() <pre>sampled pipeline 1\n</pre> Out[15]: <pre>SGDClassifier(alpha=0.0007786971309, class_weight='balanced',\n              eta0=0.0209976430718, l1_ratio=0.8571538017043,\n              learning_rate='constant', loss='modified_huber', n_jobs=1,\n              penalty='elasticnet')</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0SGDClassifier?Documentation for SGDClassifieriNot fitted<pre>SGDClassifier(alpha=0.0007786971309, class_weight='balanced',\n              eta0=0.0209976430718, l1_ratio=0.8571538017043,\n              learning_rate='constant', loss='modified_huber', n_jobs=1,\n              penalty='elasticnet')</pre> In\u00a0[16]: Copied! <pre>print(\"sampled pipeline 2\")\nclassifier_choice.generate().export_pipeline()\n</pre> print(\"sampled pipeline 2\") classifier_choice.generate().export_pipeline() <pre>sampled pipeline 2\n</pre> Out[16]: <pre>BernoulliNB(alpha=0.0667141454883, fit_prior=False)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0BernoulliNB?Documentation for BernoulliNBiNot fitted<pre>BernoulliNB(alpha=0.0667141454883, fit_prior=False)</pre> In\u00a0[17]: Copied! <pre>reproducible_random_forest = tpot.config.get_search_space(\"RandomForestClassifier\", random_state=1)\nreproducible_random_forest.generate().export_pipeline()\n</pre> reproducible_random_forest = tpot.config.get_search_space(\"RandomForestClassifier\", random_state=1) reproducible_random_forest.generate().export_pipeline() Out[17]: <pre>RandomForestClassifier(bootstrap=False, max_features=0.0234127070363,\n                       min_samples_leaf=3, min_samples_split=8,\n                       n_estimators=128, n_jobs=1, random_state=1)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0RandomForestClassifier?Documentation for RandomForestClassifieriNot fitted<pre>RandomForestClassifier(bootstrap=False, max_features=0.0234127070363,\n                       min_samples_leaf=3, min_samples_split=8,\n                       n_estimators=128, n_jobs=1, random_state=1)</pre> In\u00a0[18]: Copied! <pre>selector_choicepipeline = tpot.config.get_search_space(\"VarianceThreshold\")\ntransformer_choicepipeline =  tpot.config.get_search_space(\"PCA\")\nclassifier_choicepipeline = tpot.config.get_search_space(\"LogisticRegression\")\n\nstc_pipeline = tpot.search_spaces.pipelines.SequentialPipeline([\n    selector_choicepipeline,\n    transformer_choicepipeline,\n    classifier_choicepipeline,\n])\n\nprint(\"sampled pipeline\")\nstc_pipeline.generate().export_pipeline()\n</pre> selector_choicepipeline = tpot.config.get_search_space(\"VarianceThreshold\") transformer_choicepipeline =  tpot.config.get_search_space(\"PCA\") classifier_choicepipeline = tpot.config.get_search_space(\"LogisticRegression\")  stc_pipeline = tpot.search_spaces.pipelines.SequentialPipeline([     selector_choicepipeline,     transformer_choicepipeline,     classifier_choicepipeline, ])  print(\"sampled pipeline\") stc_pipeline.generate().export_pipeline() <pre>sampled pipeline\n</pre> Out[18]: <pre>Pipeline(steps=[('variancethreshold',\n                 VarianceThreshold(threshold=0.00023551581)),\n                ('pca', PCA(n_components=0.9764631370244)),\n                ('logisticregression',\n                 LogisticRegression(C=1.9396611393109, max_iter=1000, n_jobs=1,\n                                    penalty='l1', solver='saga'))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('variancethreshold',\n                 VarianceThreshold(threshold=0.00023551581)),\n                ('pca', PCA(n_components=0.9764631370244)),\n                ('logisticregression',\n                 LogisticRegression(C=1.9396611393109, max_iter=1000, n_jobs=1,\n                                    penalty='l1', solver='saga'))])</pre> \u00a0VarianceThreshold?Documentation for VarianceThreshold<pre>VarianceThreshold(threshold=0.00023551581)</pre> \u00a0PCA?Documentation for PCA<pre>PCA(n_components=0.9764631370244)</pre> \u00a0LogisticRegression?Documentation for LogisticRegression<pre>LogisticRegression(C=1.9396611393109, max_iter=1000, n_jobs=1, penalty='l1',\n                   solver='saga')</pre> <p>Here is an example of the form Selector-Transformer-Classifier.</p> <p>Note that each step in the sequence is a ChoicePipeline this time. Here, the SequentialPipeline can sample from search provided search space in order.</p> In\u00a0[19]: Copied! <pre>selector_choicepipeline = tpot.config.get_search_space(\"selectors\")\ntransformer_choicepipeline =  tpot.config.get_search_space(\"transformers\")\nclassifier_choicepipeline = tpot.config.get_search_space(\"classifiers\")\n\nstc_pipeline = tpot.search_spaces.pipelines.SequentialPipeline([\n    selector_choicepipeline,\n    transformer_choicepipeline,\n    classifier_choicepipeline,\n])\n\nprint(\"sampled pipeline\")\nstc_pipeline.generate().export_pipeline()\n</pre> selector_choicepipeline = tpot.config.get_search_space(\"selectors\") transformer_choicepipeline =  tpot.config.get_search_space(\"transformers\") classifier_choicepipeline = tpot.config.get_search_space(\"classifiers\")  stc_pipeline = tpot.search_spaces.pipelines.SequentialPipeline([     selector_choicepipeline,     transformer_choicepipeline,     classifier_choicepipeline, ])  print(\"sampled pipeline\") stc_pipeline.generate().export_pipeline() <pre>sampled pipeline\n</pre> Out[19]: <pre>Pipeline(steps=[('variancethreshold',\n                 VarianceThreshold(threshold=0.0004317798946)),\n                ('kbinsdiscretizer',\n                 KBinsDiscretizer(encode='onehot-dense', n_bins=77)),\n                ('lgbmclassifier',\n                 LGBMClassifier(boosting_type='dart', max_depth=5,\n                                n_estimators=76, n_jobs=1, num_leaves=192,\n                                verbose=-1))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('variancethreshold',\n                 VarianceThreshold(threshold=0.0004317798946)),\n                ('kbinsdiscretizer',\n                 KBinsDiscretizer(encode='onehot-dense', n_bins=77)),\n                ('lgbmclassifier',\n                 LGBMClassifier(boosting_type='dart', max_depth=5,\n                                n_estimators=76, n_jobs=1, num_leaves=192,\n                                verbose=-1))])</pre> \u00a0VarianceThreshold?Documentation for VarianceThreshold<pre>VarianceThreshold(threshold=0.0004317798946)</pre> \u00a0KBinsDiscretizer?Documentation for KBinsDiscretizer<pre>KBinsDiscretizer(encode='onehot-dense', n_bins=77)</pre> LGBMClassifier<pre>LGBMClassifier(boosting_type='dart', max_depth=5, n_estimators=76, n_jobs=1,\n               num_leaves=192, verbose=-1)</pre> In\u00a0[20]: Copied! <pre>print(\"sampled pipeline\")\nstc_pipeline.generate().export_pipeline()\n</pre> print(\"sampled pipeline\") stc_pipeline.generate().export_pipeline() <pre>sampled pipeline\n</pre> Out[20]: <pre>Pipeline(steps=[('selectpercentile',\n                 SelectPercentile(percentile=4.5788544361168)),\n                ('columnonehotencoder', ColumnOneHotEncoder()),\n                ('decisiontreeclassifier',\n                 DecisionTreeClassifier(criterion='entropy', max_depth=10,\n                                        min_samples_split=13))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('selectpercentile',\n                 SelectPercentile(percentile=4.5788544361168)),\n                ('columnonehotencoder', ColumnOneHotEncoder()),\n                ('decisiontreeclassifier',\n                 DecisionTreeClassifier(criterion='entropy', max_depth=10,\n                                        min_samples_split=13))])</pre> \u00a0SelectPercentile?Documentation for SelectPercentile<pre>SelectPercentile(percentile=4.5788544361168)</pre> ColumnOneHotEncoder<pre>ColumnOneHotEncoder()</pre> \u00a0DecisionTreeClassifier?Documentation for DecisionTreeClassifier<pre>DecisionTreeClassifier(criterion='entropy', max_depth=10, min_samples_split=13)</pre> In\u00a0[21]: Copied! <pre>import tpot.config\n\n\nlinear_feature_engineering = tpot.search_spaces.pipelines.DynamicLinearPipeline(search_space = tpot.config.get_search_space([\"all_transformers\",\"selectors_classification\"]), max_length=10)\nprint(\"sampled pipeline\")\nlinear_feature_engineering.generate().export_pipeline()\n</pre> import tpot.config   linear_feature_engineering = tpot.search_spaces.pipelines.DynamicLinearPipeline(search_space = tpot.config.get_search_space([\"all_transformers\",\"selectors_classification\"]), max_length=10) print(\"sampled pipeline\") linear_feature_engineering.generate().export_pipeline() <pre>sampled pipeline\n</pre> Out[21]: <pre>Pipeline(steps=[('pca-1', PCA(n_components=0.6376571946485)),\n                ('pca-2', PCA(n_components=0.7836827180307)),\n                ('quantiletransformer',\n                 QuantileTransformer(n_quantiles=334,\n                                     output_distribution='normal'))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('pca-1', PCA(n_components=0.6376571946485)),\n                ('pca-2', PCA(n_components=0.7836827180307)),\n                ('quantiletransformer',\n                 QuantileTransformer(n_quantiles=334,\n                                     output_distribution='normal'))])</pre> \u00a0PCA?Documentation for PCA<pre>PCA(n_components=0.6376571946485)</pre> \u00a0PCA?Documentation for PCA<pre>PCA(n_components=0.7836827180307)</pre> \u00a0QuantileTransformer?Documentation for QuantileTransformer<pre>QuantileTransformer(n_quantiles=334, output_distribution='normal')</pre> In\u00a0[22]: Copied! <pre>print(\"sampled pipeline\")\nlinear_feature_engineering.generate().export_pipeline()\n</pre> print(\"sampled pipeline\") linear_feature_engineering.generate().export_pipeline() <pre>sampled pipeline\n</pre> Out[22]: <pre>Pipeline(steps=[('selectfwe', SelectFwe(alpha=0.0004164619371)),\n                ('binarizer', Binarizer(threshold=0.2392693027442)),\n                ('rbfsampler',\n                 RBFSampler(gamma=0.3669672326084, n_components=35))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('selectfwe', SelectFwe(alpha=0.0004164619371)),\n                ('binarizer', Binarizer(threshold=0.2392693027442)),\n                ('rbfsampler',\n                 RBFSampler(gamma=0.3669672326084, n_components=35))])</pre> \u00a0SelectFwe?Documentation for SelectFwe<pre>SelectFwe(alpha=0.0004164619371)</pre> \u00a0Binarizer?Documentation for Binarizer<pre>Binarizer(threshold=0.2392693027442)</pre> \u00a0RBFSampler?Documentation for RBFSampler<pre>RBFSampler(gamma=0.3669672326084, n_components=35)</pre> In\u00a0[23]: Copied! <pre>full_search_space = tpot.search_spaces.pipelines.SequentialPipeline([\n    linear_feature_engineering,\n    tpot.config.get_search_space(\"classifiers\"),\n])\n\nprint(\"sampled pipeline\")\nfull_search_space.generate().export_pipeline()\n</pre> full_search_space = tpot.search_spaces.pipelines.SequentialPipeline([     linear_feature_engineering,     tpot.config.get_search_space(\"classifiers\"), ])  print(\"sampled pipeline\") full_search_space.generate().export_pipeline() <pre>sampled pipeline\n</pre> Out[23]: <pre>Pipeline(steps=[('pipeline',\n                 Pipeline(steps=[('binarizer',\n                                  Binarizer(threshold=0.2150677779496)),\n                                 ('maxabsscaler', MaxAbsScaler()),\n                                 ('columnonehotencoder',\n                                  ColumnOneHotEncoder())])),\n                ('gaussiannb', GaussianNB())])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('pipeline',\n                 Pipeline(steps=[('binarizer',\n                                  Binarizer(threshold=0.2150677779496)),\n                                 ('maxabsscaler', MaxAbsScaler()),\n                                 ('columnonehotencoder',\n                                  ColumnOneHotEncoder())])),\n                ('gaussiannb', GaussianNB())])</pre> \u00a0pipeline: Pipeline?Documentation for pipeline: Pipeline<pre>Pipeline(steps=[('binarizer', Binarizer(threshold=0.2150677779496)),\n                ('maxabsscaler', MaxAbsScaler()),\n                ('columnonehotencoder', ColumnOneHotEncoder())])</pre> \u00a0Binarizer?Documentation for Binarizer<pre>Binarizer(threshold=0.2150677779496)</pre> \u00a0MaxAbsScaler?Documentation for MaxAbsScaler<pre>MaxAbsScaler()</pre> ColumnOneHotEncoder<pre>ColumnOneHotEncoder()</pre> \u00a0GaussianNB?Documentation for GaussianNB<pre>GaussianNB()</pre> In\u00a0[24]: Copied! <pre>print(\"sampled pipeline\")\nfull_search_space.generate().export_pipeline()\n</pre> print(\"sampled pipeline\") full_search_space.generate().export_pipeline() <pre>sampled pipeline\n</pre> Out[24]: <pre>Pipeline(steps=[('pipeline',\n                 Pipeline(steps=[('zerocount', ZeroCount()),\n                                 ('selectfrommodel',\n                                  SelectFromModel(estimator=ExtraTreesClassifier(class_weight='balanced',\n                                                                                 max_features=0.1619832293406,\n                                                                                 min_samples_leaf=7,\n                                                                                 min_samples_split=7,\n                                                                                 n_jobs=1),\n                                                  threshold=0.6414209870839)),\n                                 ('variancethreshold',\n                                  VarianceThreshold(threshold=0.0113542845765))])),\n                ('multinomialnb', MultinomialNB(alpha=0.0815128367119))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('pipeline',\n                 Pipeline(steps=[('zerocount', ZeroCount()),\n                                 ('selectfrommodel',\n                                  SelectFromModel(estimator=ExtraTreesClassifier(class_weight='balanced',\n                                                                                 max_features=0.1619832293406,\n                                                                                 min_samples_leaf=7,\n                                                                                 min_samples_split=7,\n                                                                                 n_jobs=1),\n                                                  threshold=0.6414209870839)),\n                                 ('variancethreshold',\n                                  VarianceThreshold(threshold=0.0113542845765))])),\n                ('multinomialnb', MultinomialNB(alpha=0.0815128367119))])</pre> \u00a0pipeline: Pipeline?Documentation for pipeline: Pipeline<pre>Pipeline(steps=[('zerocount', ZeroCount()),\n                ('selectfrommodel',\n                 SelectFromModel(estimator=ExtraTreesClassifier(class_weight='balanced',\n                                                                max_features=0.1619832293406,\n                                                                min_samples_leaf=7,\n                                                                min_samples_split=7,\n                                                                n_jobs=1),\n                                 threshold=0.6414209870839)),\n                ('variancethreshold',\n                 VarianceThreshold(threshold=0.0113542845765))])</pre> ZeroCount<pre>ZeroCount()</pre> \u00a0selectfrommodel: SelectFromModel?Documentation for selectfrommodel: SelectFromModel<pre>SelectFromModel(estimator=ExtraTreesClassifier(class_weight='balanced',\n                                               max_features=0.1619832293406,\n                                               min_samples_leaf=7,\n                                               min_samples_split=7, n_jobs=1),\n                threshold=0.6414209870839)</pre> estimator: ExtraTreesClassifier<pre>ExtraTreesClassifier(class_weight='balanced', max_features=0.1619832293406,\n                     min_samples_leaf=7, min_samples_split=7, n_jobs=1)</pre> \u00a0ExtraTreesClassifier?Documentation for ExtraTreesClassifier<pre>ExtraTreesClassifier(class_weight='balanced', max_features=0.1619832293406,\n                     min_samples_leaf=7, min_samples_split=7, n_jobs=1)</pre> \u00a0VarianceThreshold?Documentation for VarianceThreshold<pre>VarianceThreshold(threshold=0.0113542845765)</pre> \u00a0MultinomialNB?Documentation for MultinomialNB<pre>MultinomialNB(alpha=0.0815128367119)</pre> In\u00a0[25]: Copied! <pre>transform_and_passthrough = tpot.search_spaces.pipelines.UnionPipeline([\n    tpot.config.get_search_space(\"transformers\"),\n    tpot.config.get_search_space(\"Passthrough\"),\n])\n\ntransform_and_passthrough.generate().export_pipeline()\n</pre> transform_and_passthrough = tpot.search_spaces.pipelines.UnionPipeline([     tpot.config.get_search_space(\"transformers\"),     tpot.config.get_search_space(\"Passthrough\"), ])  transform_and_passthrough.generate().export_pipeline() Out[25]: <pre>FeatureUnion(transformer_list=[('pca', PCA(n_components=0.7674007136568)),\n                               ('passthrough', Passthrough())])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0FeatureUnion?Documentation for FeatureUnioniNot fitted<pre>FeatureUnion(transformer_list=[('pca', PCA(n_components=0.7674007136568)),\n                               ('passthrough', Passthrough())])</pre> pca\u00a0PCA?Documentation for PCA<pre>PCA(n_components=0.7674007136568)</pre> passthroughPassthrough<pre>Passthrough()</pre> <p>UnionPipelines are an excellent tool to expand the capabilities of the linear search spaces.</p> In\u00a0[26]: Copied! <pre>stc_pipeline2 = tpot.search_spaces.pipelines.SequentialPipeline([\n    tpot.config.get_search_space(\"selectors\"),\n    transform_and_passthrough,\n    tpot.config.get_search_space(\"classifiers\"),\n])\n\nstc_pipeline2.generate().export_pipeline()\n</pre> stc_pipeline2 = tpot.search_spaces.pipelines.SequentialPipeline([     tpot.config.get_search_space(\"selectors\"),     transform_and_passthrough,     tpot.config.get_search_space(\"classifiers\"), ])  stc_pipeline2.generate().export_pipeline() Out[26]: <pre>Pipeline(steps=[('selectpercentile',\n                 SelectPercentile(percentile=29.1049436421441)),\n                ('featureunion',\n                 FeatureUnion(transformer_list=[('powertransformer',\n                                                 PowerTransformer()),\n                                                ('passthrough',\n                                                 Passthrough())])),\n                ('extratreesclassifier',\n                 ExtraTreesClassifier(max_features=0.8376611419015,\n                                      min_samples_leaf=9, min_samples_split=17,\n                                      n_jobs=1))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('selectpercentile',\n                 SelectPercentile(percentile=29.1049436421441)),\n                ('featureunion',\n                 FeatureUnion(transformer_list=[('powertransformer',\n                                                 PowerTransformer()),\n                                                ('passthrough',\n                                                 Passthrough())])),\n                ('extratreesclassifier',\n                 ExtraTreesClassifier(max_features=0.8376611419015,\n                                      min_samples_leaf=9, min_samples_split=17,\n                                      n_jobs=1))])</pre> \u00a0SelectPercentile?Documentation for SelectPercentile<pre>SelectPercentile(percentile=29.1049436421441)</pre> \u00a0featureunion: FeatureUnion?Documentation for featureunion: FeatureUnion<pre>FeatureUnion(transformer_list=[('powertransformer', PowerTransformer()),\n                               ('passthrough', Passthrough())])</pre> powertransformer\u00a0PowerTransformer?Documentation for PowerTransformer<pre>PowerTransformer()</pre> passthroughPassthrough<pre>Passthrough()</pre> \u00a0ExtraTreesClassifier?Documentation for ExtraTreesClassifier<pre>ExtraTreesClassifier(max_features=0.8376611419015, min_samples_leaf=9,\n                     min_samples_split=17, n_jobs=1)</pre> <p>Union pipelines can also be used to create \"branches\" if you are trying to create a tree-like search space. This can be particularly useful when paired with the FeatureSetSelector node (FSSNode) as each branch can learn different feature engineering for different subsets of the features, for example.</p> In\u00a0[27]: Copied! <pre>st_pipeline = tpot.search_spaces.pipelines.SequentialPipeline([\n    tpot.config.get_search_space(\"selectors\"),\n    tpot.config.get_search_space(\"transformers\"),\n])\n\nbranched_pipeline = tpot.search_spaces.pipelines.SequentialPipeline([\n    tpot.search_spaces.pipelines.UnionPipeline([\n        st_pipeline,\n        st_pipeline,\n    ]),\n    tpot.config.get_search_space(\"classifiers\"),\n])\n\nbranched_pipeline.generate().export_pipeline()\n</pre> st_pipeline = tpot.search_spaces.pipelines.SequentialPipeline([     tpot.config.get_search_space(\"selectors\"),     tpot.config.get_search_space(\"transformers\"), ])  branched_pipeline = tpot.search_spaces.pipelines.SequentialPipeline([     tpot.search_spaces.pipelines.UnionPipeline([         st_pipeline,         st_pipeline,     ]),     tpot.config.get_search_space(\"classifiers\"), ])  branched_pipeline.generate().export_pipeline() Out[27]: <pre>Pipeline(steps=[('featureunion',\n                 FeatureUnion(transformer_list=[('pipeline-1',\n                                                 Pipeline(steps=[('selectfwe',\n                                                                  SelectFwe(alpha=0.0080564930162)),\n                                                                 ('quantiletransformer',\n                                                                  QuantileTransformer(n_quantiles=450,\n                                                                                      output_distribution='normal'))])),\n                                                ('pipeline-2',\n                                                 Pipeline(steps=[('variancethreshold',\n                                                                  VarianceThreshold(threshold=0.155443085484)),\n                                                                 ('columnonehotencoder...\n                               feature_types=None, gamma=14.5866790094856,\n                               grow_policy=None, importance_type=None,\n                               interaction_constraints=None,\n                               learning_rate=0.2226908938347, max_bin=None,\n                               max_cat_threshold=None, max_cat_to_onehot=None,\n                               max_delta_step=None, max_depth=11,\n                               max_leaves=None, min_child_weight=3, missing=nan,\n                               monotone_constraints=None, multi_strategy=None,\n                               n_estimators=100, n_jobs=1, nthread=1,\n                               num_parallel_tree=None, ...))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('featureunion',\n                 FeatureUnion(transformer_list=[('pipeline-1',\n                                                 Pipeline(steps=[('selectfwe',\n                                                                  SelectFwe(alpha=0.0080564930162)),\n                                                                 ('quantiletransformer',\n                                                                  QuantileTransformer(n_quantiles=450,\n                                                                                      output_distribution='normal'))])),\n                                                ('pipeline-2',\n                                                 Pipeline(steps=[('variancethreshold',\n                                                                  VarianceThreshold(threshold=0.155443085484)),\n                                                                 ('columnonehotencoder...\n                               feature_types=None, gamma=14.5866790094856,\n                               grow_policy=None, importance_type=None,\n                               interaction_constraints=None,\n                               learning_rate=0.2226908938347, max_bin=None,\n                               max_cat_threshold=None, max_cat_to_onehot=None,\n                               max_delta_step=None, max_depth=11,\n                               max_leaves=None, min_child_weight=3, missing=nan,\n                               monotone_constraints=None, multi_strategy=None,\n                               n_estimators=100, n_jobs=1, nthread=1,\n                               num_parallel_tree=None, ...))])</pre> \u00a0featureunion: FeatureUnion?Documentation for featureunion: FeatureUnion<pre>FeatureUnion(transformer_list=[('pipeline-1',\n                                Pipeline(steps=[('selectfwe',\n                                                 SelectFwe(alpha=0.0080564930162)),\n                                                ('quantiletransformer',\n                                                 QuantileTransformer(n_quantiles=450,\n                                                                     output_distribution='normal'))])),\n                               ('pipeline-2',\n                                Pipeline(steps=[('variancethreshold',\n                                                 VarianceThreshold(threshold=0.155443085484)),\n                                                ('columnonehotencoder',\n                                                 ColumnOneHotEncoder())]))])</pre> pipeline-1\u00a0SelectFwe?Documentation for SelectFwe<pre>SelectFwe(alpha=0.0080564930162)</pre> \u00a0QuantileTransformer?Documentation for QuantileTransformer<pre>QuantileTransformer(n_quantiles=450, output_distribution='normal')</pre> pipeline-2\u00a0VarianceThreshold?Documentation for VarianceThreshold<pre>VarianceThreshold(threshold=0.155443085484)</pre> ColumnOneHotEncoder<pre>ColumnOneHotEncoder()</pre> XGBClassifier<pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=14.5866790094856, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.2226908938347,\n              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=11, max_leaves=None,\n              min_child_weight=3, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=100, n_jobs=1, nthread=1,\n              num_parallel_tree=None, ...)</pre> In\u00a0[28]: Copied! <pre>dynamic_transformers = tpot.search_spaces.pipelines.DynamicUnionPipeline(tpot.config.get_search_space(\"transformers\"), max_estimators=4)\ndynamic_transformers.generate().export_pipeline()\n</pre> dynamic_transformers = tpot.search_spaces.pipelines.DynamicUnionPipeline(tpot.config.get_search_space(\"transformers\"), max_estimators=4) dynamic_transformers.generate().export_pipeline() Out[28]: <pre>FeatureUnion(transformer_list=[('fastica', FastICA(n_components=4))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0FeatureUnion?Documentation for FeatureUnioniNot fitted<pre>FeatureUnion(transformer_list=[('fastica', FastICA(n_components=4))])</pre> fastica\u00a0FastICA?Documentation for FastICA<pre>FastICA(n_components=4)</pre> <p>One good strategy could be to pair this with Passthrough in a feature union so that you output all the transformations along with the original data.</p> In\u00a0[29]: Copied! <pre>dynamic_transformers_with_passthrough = tpot.search_spaces.pipelines.UnionPipeline([\n    dynamic_transformers,\n    tpot.config.get_search_space(\"Passthrough\")],\n    )\n\ndynamic_transformers_with_passthrough.generate().export_pipeline()\n</pre> dynamic_transformers_with_passthrough = tpot.search_spaces.pipelines.UnionPipeline([     dynamic_transformers,     tpot.config.get_search_space(\"Passthrough\")],     )  dynamic_transformers_with_passthrough.generate().export_pipeline() Out[29]: <pre>FeatureUnion(transformer_list=[('featureunion',\n                                FeatureUnion(transformer_list=[('pca',\n                                                                PCA(n_components=0.9386236966835)),\n                                                               ('zerocount',\n                                                                ZeroCount()),\n                                                               ('featureagglomeration',\n                                                                FeatureAgglomeration(n_clusters=94,\n                                                                                     pooling_func=&lt;function max at 0x1048f3470&gt;))])),\n                               ('passthrough', Passthrough())])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0FeatureUnion?Documentation for FeatureUnioniNot fitted<pre>FeatureUnion(transformer_list=[('featureunion',\n                                FeatureUnion(transformer_list=[('pca',\n                                                                PCA(n_components=0.9386236966835)),\n                                                               ('zerocount',\n                                                                ZeroCount()),\n                                                               ('featureagglomeration',\n                                                                FeatureAgglomeration(n_clusters=94,\n                                                                                     pooling_func=&lt;function max at 0x1048f3470&gt;))])),\n                               ('passthrough', Passthrough())])</pre> featureunionpca\u00a0PCA?Documentation for PCA<pre>PCA(n_components=0.9386236966835)</pre> zerocountZeroCount<pre>ZeroCount()</pre> featureagglomeration\u00a0FeatureAgglomeration?Documentation for FeatureAgglomeration<pre>FeatureAgglomeration(n_clusters=94, pooling_func=&lt;function max at 0x1048f3470&gt;)</pre> passthroughPassthrough<pre>Passthrough()</pre> In\u00a0[30]: Copied! <pre>stc_pipeline3 = tpot.search_spaces.pipelines.SequentialPipeline([\n    tpot.config.get_search_space(\"selectors\"),\n    dynamic_transformers_with_passthrough,\n    tpot.config.get_search_space(\"classifiers\"),\n])\n\nstc_pipeline3.generate().export_pipeline()\n</pre> stc_pipeline3 = tpot.search_spaces.pipelines.SequentialPipeline([     tpot.config.get_search_space(\"selectors\"),     dynamic_transformers_with_passthrough,     tpot.config.get_search_space(\"classifiers\"), ])  stc_pipeline3.generate().export_pipeline() Out[30]: <pre>Pipeline(steps=[('variancethreshold',\n                 VarianceThreshold(threshold=0.0003352949622)),\n                ('featureunion',\n                 FeatureUnion(transformer_list=[('featureunion',\n                                                 FeatureUnion(transformer_list=[('featureagglomeration',\n                                                                                 FeatureAgglomeration(linkage='complete',\n                                                                                                      metric='cosine',\n                                                                                                      n_clusters=25)),\n                                                                                ('columnordinalencoder',\n                                                                                 ColumnOrdinalEncoder())])),\n                                                ('passthrough',\n                                                 Passthrough())])),\n                ('mlpclassifier',\n                 MLPClassifier(activation='identity', alpha=0.000256185492,\n                               early_stopping=True,\n                               hidden_layer_sizes=[146, 146, 146],\n                               learning_rate='invscaling',\n                               learning_rate_init=0.0006442167601,\n                               n_iter_no_change=32))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('variancethreshold',\n                 VarianceThreshold(threshold=0.0003352949622)),\n                ('featureunion',\n                 FeatureUnion(transformer_list=[('featureunion',\n                                                 FeatureUnion(transformer_list=[('featureagglomeration',\n                                                                                 FeatureAgglomeration(linkage='complete',\n                                                                                                      metric='cosine',\n                                                                                                      n_clusters=25)),\n                                                                                ('columnordinalencoder',\n                                                                                 ColumnOrdinalEncoder())])),\n                                                ('passthrough',\n                                                 Passthrough())])),\n                ('mlpclassifier',\n                 MLPClassifier(activation='identity', alpha=0.000256185492,\n                               early_stopping=True,\n                               hidden_layer_sizes=[146, 146, 146],\n                               learning_rate='invscaling',\n                               learning_rate_init=0.0006442167601,\n                               n_iter_no_change=32))])</pre> \u00a0VarianceThreshold?Documentation for VarianceThreshold<pre>VarianceThreshold(threshold=0.0003352949622)</pre> \u00a0featureunion: FeatureUnion?Documentation for featureunion: FeatureUnion<pre>FeatureUnion(transformer_list=[('featureunion',\n                                FeatureUnion(transformer_list=[('featureagglomeration',\n                                                                FeatureAgglomeration(linkage='complete',\n                                                                                     metric='cosine',\n                                                                                     n_clusters=25)),\n                                                               ('columnordinalencoder',\n                                                                ColumnOrdinalEncoder())])),\n                               ('passthrough', Passthrough())])</pre> featureunionfeatureagglomeration\u00a0FeatureAgglomeration?Documentation for FeatureAgglomeration<pre>FeatureAgglomeration(linkage='complete', metric='cosine', n_clusters=25)</pre> columnordinalencoderColumnOrdinalEncoder<pre>ColumnOrdinalEncoder()</pre> passthroughPassthrough<pre>Passthrough()</pre> \u00a0MLPClassifier?Documentation for MLPClassifier<pre>MLPClassifier(activation='identity', alpha=0.000256185492, early_stopping=True,\n              hidden_layer_sizes=[146, 146, 146], learning_rate='invscaling',\n              learning_rate_init=0.0006442167601, n_iter_no_change=32)</pre> In\u00a0[31]: Copied! <pre>SelectFromModel_configspace_part = ConfigurationSpace(\n                                        space = {\n                                            'threshold': Float('threshold', bounds=(1e-4, 1.0), log=True),\n                                        }\n                                    )\n\nextratrees_estimator_node = tpot.config.get_search_space(\"ExtraTreesClassifier\") #this exports an ExtraTreesClassifier node\nextratrees_estimator_node.generate().export_pipeline()\n</pre> SelectFromModel_configspace_part = ConfigurationSpace(                                         space = {                                             'threshold': Float('threshold', bounds=(1e-4, 1.0), log=True),                                         }                                     )  extratrees_estimator_node = tpot.config.get_search_space(\"ExtraTreesClassifier\") #this exports an ExtraTreesClassifier node extratrees_estimator_node.generate().export_pipeline() Out[31]: <pre>ExtraTreesClassifier(class_weight='balanced', max_features=0.9851993193336,\n                     min_samples_leaf=5, min_samples_split=6, n_jobs=1)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0ExtraTreesClassifier?Documentation for ExtraTreesClassifieriNot fitted<pre>ExtraTreesClassifier(class_weight='balanced', max_features=0.9851993193336,\n                     min_samples_leaf=5, min_samples_split=6, n_jobs=1)</pre> In\u00a0[32]: Copied! <pre>from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\nselect_from_model_wrapper_searchspace = tpot.search_spaces.pipelines.WrapperPipeline(\n            method=SelectFromModel,\n            space = SelectFromModel_configspace_part,\n            estimator_search_space= extratrees_estimator_node,\n        )\n\nselect_from_model_wrapper_searchspace.generate().export_pipeline()\n</pre> from sklearn.ensemble import ExtraTreesClassifier from sklearn.feature_selection import SelectFromModel  select_from_model_wrapper_searchspace = tpot.search_spaces.pipelines.WrapperPipeline(             method=SelectFromModel,             space = SelectFromModel_configspace_part,             estimator_search_space= extratrees_estimator_node,         )  select_from_model_wrapper_searchspace.generate().export_pipeline() Out[32]: <pre>SelectFromModel(estimator=ExtraTreesClassifier(max_features=0.277440186742,\n                                               min_samples_leaf=9,\n                                               min_samples_split=17, n_jobs=1),\n                threshold=0.0032005860778)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0SelectFromModel?Documentation for SelectFromModeliNot fitted<pre>SelectFromModel(estimator=ExtraTreesClassifier(max_features=0.277440186742,\n                                               min_samples_leaf=9,\n                                               min_samples_split=17, n_jobs=1),\n                threshold=0.0032005860778)</pre> estimator: ExtraTreesClassifier<pre>ExtraTreesClassifier(max_features=0.277440186742, min_samples_leaf=9,\n                     min_samples_split=17, n_jobs=1)</pre> \u00a0ExtraTreesClassifier?Documentation for ExtraTreesClassifier<pre>ExtraTreesClassifier(max_features=0.277440186742, min_samples_leaf=9,\n                     min_samples_split=17, n_jobs=1)</pre> In\u00a0[33]: Copied! <pre>classifiers = tpot.config.get_search_space(\"classifiers\")\nwrapped_estimators = tpot.search_spaces.pipelines.WrapperPipeline(tpot.builtin_modules.EstimatorTransformer, {}, classifiers)\n\nest = wrapped_estimators.generate().export_pipeline() #returns an estimator with a transform function\nest\n</pre> classifiers = tpot.config.get_search_space(\"classifiers\") wrapped_estimators = tpot.search_spaces.pipelines.WrapperPipeline(tpot.builtin_modules.EstimatorTransformer, {}, classifiers)  est = wrapped_estimators.generate().export_pipeline() #returns an estimator with a transform function est Out[33]: <pre>EstimatorTransformer(estimator=MLPClassifier(alpha=0.000648285661,\n                                             hidden_layer_sizes=[380],\n                                             learning_rate='invscaling',\n                                             learning_rate_init=0.0008851810314,\n                                             n_iter_no_change=32))</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0EstimatorTransformeriNot fitted<pre>EstimatorTransformer(estimator=MLPClassifier(alpha=0.000648285661,\n                                             hidden_layer_sizes=[380],\n                                             learning_rate='invscaling',\n                                             learning_rate_init=0.0008851810314,\n                                             n_iter_no_change=32))</pre> estimator: MLPClassifier<pre>MLPClassifier(alpha=0.000648285661, hidden_layer_sizes=[380],\n              learning_rate='invscaling', learning_rate_init=0.0008851810314,\n              n_iter_no_change=32)</pre> \u00a0MLPClassifier?Documentation for MLPClassifier<pre>MLPClassifier(alpha=0.000648285661, hidden_layer_sizes=[380],\n              learning_rate='invscaling', learning_rate_init=0.0008851810314,\n              n_iter_no_change=32)</pre> In\u00a0[34]: Copied! <pre>import numpy as np\nX, y = np.random.rand(100, 10), np.random.randint(0, 2, 100)\n\nest.fit_transform(X, y)[0:5]\n</pre> import numpy as np X, y = np.random.rand(100, 10), np.random.randint(0, 2, 100)  est.fit_transform(X, y)[0:5] <pre>/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n</pre> Out[34]: <pre>array([[0.34363566, 0.65636434],\n       [0.14785295, 0.85214705],\n       [0.45816571, 0.54183429],\n       [0.81083741, 0.18916259],\n       [0.56944478, 0.43055522]])</pre> <p>you can manually set the settings for an estimator the same way you would do it for an EstimatorNode. Here's another example with cross_val_predict and method being used.</p> In\u00a0[35]: Copied! <pre>classifiers = tpot.config.get_search_space(\"classifiers\")\nwrapped_estimators_cv = tpot.search_spaces.pipelines.WrapperPipeline(tpot.builtin_modules.EstimatorTransformer, {'cross_val_predict_cv':10, 'method':'predict'}, classifiers)\nest = wrapped_estimators_cv.generate().export_pipeline() #returns an estimator with a transform function\nest.fit_transform(X, y)[0:5]\n</pre> classifiers = tpot.config.get_search_space(\"classifiers\") wrapped_estimators_cv = tpot.search_spaces.pipelines.WrapperPipeline(tpot.builtin_modules.EstimatorTransformer, {'cross_val_predict_cv':10, 'method':'predict'}, classifiers) est = wrapped_estimators_cv.generate().export_pipeline() #returns an estimator with a transform function est.fit_transform(X, y)[0:5] Out[35]: <pre>array([[0],\n       [0],\n       [0],\n       [0],\n       [0]])</pre> <p>These can now be used inside a linear pipeline. This is fairly similar to the default linear pipeline search space.</p> In\u00a0[36]: Copied! <pre>dynamic_wrapped_classifiers_with_passthrough = tpot.search_spaces.pipelines.UnionPipeline([\n    tpot.search_spaces.pipelines.DynamicUnionPipeline(wrapped_estimators_cv, max_estimators=4),\n    tpot.config.get_search_space(\"Passthrough\")\n    ])\n\nstc_pipeline4 = tpot.search_spaces.pipelines.SequentialPipeline([\n    tpot.config.get_search_space(\"scalers\"),\n    dynamic_transformers_with_passthrough,\n    dynamic_wrapped_classifiers_with_passthrough,\n    tpot.config.get_search_space(\"classifiers\"),\n])\n\nstc_pipeline4.generate().export_pipeline()\n</pre> dynamic_wrapped_classifiers_with_passthrough = tpot.search_spaces.pipelines.UnionPipeline([     tpot.search_spaces.pipelines.DynamicUnionPipeline(wrapped_estimators_cv, max_estimators=4),     tpot.config.get_search_space(\"Passthrough\")     ])  stc_pipeline4 = tpot.search_spaces.pipelines.SequentialPipeline([     tpot.config.get_search_space(\"scalers\"),     dynamic_transformers_with_passthrough,     dynamic_wrapped_classifiers_with_passthrough,     tpot.config.get_search_space(\"classifiers\"), ])  stc_pipeline4.generate().export_pipeline() Out[36]: <pre>Pipeline(steps=[('robustscaler',\n                 RobustScaler(quantile_range=(0.2632669052042,\n                                              0.892009308738))),\n                ('featureunion-1',\n                 FeatureUnion(transformer_list=[('featureunion',\n                                                 FeatureUnion(transformer_list=[('columnonehotencoder',\n                                                                                 ColumnOneHotEncoder()),\n                                                                                ('kbinsdiscretizer',\n                                                                                 KBinsDiscretizer(encode='onehot-dense',\n                                                                                                  n_bins=58,\n                                                                                                  strategy='kmeans'))])),\n                                                ('passthrough',\n                                                 Passth...\n                                                                                                      estimator=LogisticRegression(C=334.8557628287718,\n                                                                                                                                   max_iter=1000,\n                                                                                                                                   n_jobs=1,\n                                                                                                                                   solver='saga'),\n                                                                                                      method='predict')),\n                                                                                ('estimatortransformer-2',\n                                                                                 EstimatorTransformer(cross_val_predict_cv=10,\n                                                                                                      estimator=QuadraticDiscriminantAnalysis(reg_param=0.0011738914966),\n                                                                                                      method='predict'))])),\n                                                ('passthrough',\n                                                 Passthrough())])),\n                ('lineardiscriminantanalysis', LinearDiscriminantAnalysis())])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('robustscaler',\n                 RobustScaler(quantile_range=(0.2632669052042,\n                                              0.892009308738))),\n                ('featureunion-1',\n                 FeatureUnion(transformer_list=[('featureunion',\n                                                 FeatureUnion(transformer_list=[('columnonehotencoder',\n                                                                                 ColumnOneHotEncoder()),\n                                                                                ('kbinsdiscretizer',\n                                                                                 KBinsDiscretizer(encode='onehot-dense',\n                                                                                                  n_bins=58,\n                                                                                                  strategy='kmeans'))])),\n                                                ('passthrough',\n                                                 Passth...\n                                                                                                      estimator=LogisticRegression(C=334.8557628287718,\n                                                                                                                                   max_iter=1000,\n                                                                                                                                   n_jobs=1,\n                                                                                                                                   solver='saga'),\n                                                                                                      method='predict')),\n                                                                                ('estimatortransformer-2',\n                                                                                 EstimatorTransformer(cross_val_predict_cv=10,\n                                                                                                      estimator=QuadraticDiscriminantAnalysis(reg_param=0.0011738914966),\n                                                                                                      method='predict'))])),\n                                                ('passthrough',\n                                                 Passthrough())])),\n                ('lineardiscriminantanalysis', LinearDiscriminantAnalysis())])</pre> \u00a0RobustScaler?Documentation for RobustScaler<pre>RobustScaler(quantile_range=(0.2632669052042, 0.892009308738))</pre> \u00a0featureunion-1: FeatureUnion?Documentation for featureunion-1: FeatureUnion<pre>FeatureUnion(transformer_list=[('featureunion',\n                                FeatureUnion(transformer_list=[('columnonehotencoder',\n                                                                ColumnOneHotEncoder()),\n                                                               ('kbinsdiscretizer',\n                                                                KBinsDiscretizer(encode='onehot-dense',\n                                                                                 n_bins=58,\n                                                                                 strategy='kmeans'))])),\n                               ('passthrough', Passthrough())])</pre> featureunioncolumnonehotencoderColumnOneHotEncoder<pre>ColumnOneHotEncoder()</pre> kbinsdiscretizer\u00a0KBinsDiscretizer?Documentation for KBinsDiscretizer<pre>KBinsDiscretizer(encode='onehot-dense', n_bins=58, strategy='kmeans')</pre> passthroughPassthrough<pre>Passthrough()</pre> \u00a0featureunion-2: FeatureUnion?Documentation for featureunion-2: FeatureUnion<pre>FeatureUnion(transformer_list=[('featureunion',\n                                FeatureUnion(transformer_list=[('estimatortransformer-1',\n                                                                EstimatorTransformer(cross_val_predict_cv=10,\n                                                                                     estimator=LogisticRegression(C=334.8557628287718,\n                                                                                                                  max_iter=1000,\n                                                                                                                  n_jobs=1,\n                                                                                                                  solver='saga'),\n                                                                                     method='predict')),\n                                                               ('estimatortransformer-2',\n                                                                EstimatorTransformer(cross_val_predict_cv=10,\n                                                                                     estimator=QuadraticDiscriminantAnalysis(reg_param=0.0011738914966),\n                                                                                     method='predict'))])),\n                               ('passthrough', Passthrough())])</pre> featureunionestimatortransformer-1estimator: LogisticRegression<pre>LogisticRegression(C=334.8557628287718, max_iter=1000, n_jobs=1, solver='saga')</pre> \u00a0LogisticRegression?Documentation for LogisticRegression<pre>LogisticRegression(C=334.8557628287718, max_iter=1000, n_jobs=1, solver='saga')</pre> estimatortransformer-2estimator: QuadraticDiscriminantAnalysis<pre>QuadraticDiscriminantAnalysis(reg_param=0.0011738914966)</pre> \u00a0QuadraticDiscriminantAnalysis?Documentation for QuadraticDiscriminantAnalysis<pre>QuadraticDiscriminantAnalysis(reg_param=0.0011738914966)</pre> passthroughPassthrough<pre>Passthrough()</pre> \u00a0LinearDiscriminantAnalysis?Documentation for LinearDiscriminantAnalysis<pre>LinearDiscriminantAnalysis()</pre> In\u00a0[37]: Copied! <pre>graph_search_space = tpot.search_spaces.pipelines.GraphSearchPipeline(\n    root_search_space= tpot.config.get_search_space([\"KNeighborsClassifier\", \"LogisticRegression\", \"DecisionTreeClassifier\"]),\n    leaf_search_space = tpot.config.get_search_space(\"selectors\"), \n    inner_search_space = tpot.config.get_search_space([\"transformers\"]),\n    max_size = 10,\n)\n\nind = graph_search_space.generate()\n</pre> graph_search_space = tpot.search_spaces.pipelines.GraphSearchPipeline(     root_search_space= tpot.config.get_search_space([\"KNeighborsClassifier\", \"LogisticRegression\", \"DecisionTreeClassifier\"]),     leaf_search_space = tpot.config.get_search_space(\"selectors\"),      inner_search_space = tpot.config.get_search_space([\"transformers\"]),     max_size = 10, )  ind = graph_search_space.generate() In\u00a0[38]: Copied! <pre>est1 = ind.export_pipeline()\nest1.plot() #GraphPipelines have a helpful plotting function to visualize the pipeline\n</pre> est1 = ind.export_pipeline() est1.plot() #GraphPipelines have a helpful plotting function to visualize the pipeline <p>Lets add a few more mutations and plot the final pipeline to get a sense of the diversity of pipelines that can be generated with this search space</p> In\u00a0[39]: Copied! <pre>for i in range(0,50):\n    ind.mutate()\n    if i%5==0:\n        est = ind.export_pipeline()\n        est.plot()\n</pre> for i in range(0,50):     ind.mutate()     if i%5==0:         est = ind.export_pipeline()         est.plot() <p>TreePipelines work the same way as GraphPipelines, but they are limited to a tree structure. This is similar to the search space in the original TPOT.</p> <p>(This search space is still experimental and currently built off GraphSearchPipeline. It may be rewritten with its own code in the future.)</p> In\u00a0[40]: Copied! <pre>tree_search_space = tpot.search_spaces.pipelines.TreePipeline(\n    root_search_space= tpot.config.get_search_space([\"KNeighborsClassifier\", \"LogisticRegression\", \"DecisionTreeClassifier\"]),\n    leaf_search_space = tpot.config.get_search_space(\"selectors\"), \n    inner_search_space = tpot.config.get_search_space([\"transformers\"]),\n    max_size = 10,\n)\n\nind = graph_search_space.generate()\nexp = ind.export_pipeline()\nexp.plot()\n</pre> tree_search_space = tpot.search_spaces.pipelines.TreePipeline(     root_search_space= tpot.config.get_search_space([\"KNeighborsClassifier\", \"LogisticRegression\", \"DecisionTreeClassifier\"]),     leaf_search_space = tpot.config.get_search_space(\"selectors\"),      inner_search_space = tpot.config.get_search_space([\"transformers\"]),     max_size = 10, )  ind = graph_search_space.generate() exp = ind.export_pipeline() exp.plot() <p>In this example, the FeatureUnion layer will always have at least one transformer selected and will always have one passthrough</p> In\u00a0[41]: Copied! <pre>from tpot.search_spaces.pipelines import *\nfrom tpot.config import get_search_space\n\n#This FeatureUnion layer will always have at least one transformer selected and will always have one passthrough\ntransformers_with_passthrough = UnionPipeline([\n                        DynamicUnionPipeline(get_search_space([\"transformers\"])),\n                        get_search_space(\"Passthrough\")\n                        ]\n                    )\n\ntransformers_with_passthrough.generate().export_pipeline()\n</pre> from tpot.search_spaces.pipelines import * from tpot.config import get_search_space  #This FeatureUnion layer will always have at least one transformer selected and will always have one passthrough transformers_with_passthrough = UnionPipeline([                         DynamicUnionPipeline(get_search_space([\"transformers\"])),                         get_search_space(\"Passthrough\")                         ]                     )  transformers_with_passthrough.generate().export_pipeline() Out[41]: <pre>FeatureUnion(transformer_list=[('featureunion',\n                                FeatureUnion(transformer_list=[('kbinsdiscretizer',\n                                                                KBinsDiscretizer(encode='onehot-dense',\n                                                                                 n_bins=80,\n                                                                                 strategy='kmeans')),\n                                                               ('fastica',\n                                                                FastICA(algorithm='deflation',\n                                                                        n_components=91))])),\n                               ('passthrough', Passthrough())])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0FeatureUnion?Documentation for FeatureUnioniNot fitted<pre>FeatureUnion(transformer_list=[('featureunion',\n                                FeatureUnion(transformer_list=[('kbinsdiscretizer',\n                                                                KBinsDiscretizer(encode='onehot-dense',\n                                                                                 n_bins=80,\n                                                                                 strategy='kmeans')),\n                                                               ('fastica',\n                                                                FastICA(algorithm='deflation',\n                                                                        n_components=91))])),\n                               ('passthrough', Passthrough())])</pre> featureunionkbinsdiscretizer\u00a0KBinsDiscretizer?Documentation for KBinsDiscretizer<pre>KBinsDiscretizer(encode='onehot-dense', n_bins=80, strategy='kmeans')</pre> fastica\u00a0FastICA?Documentation for FastICA<pre>FastICA(algorithm='deflation', n_components=91)</pre> passthroughPassthrough<pre>Passthrough()</pre> <p>In this example, the FeatureUnion layer will always one passthrough. In addition, it may select one or more transformer, but it may skip transformers altogether and only include a Passthrough.</p> In\u00a0[42]: Copied! <pre>final_transformers_layer =UnionPipeline([\n                        ChoicePipeline([\n                            DynamicUnionPipeline(get_search_space([\"transformers\"])),\n                            get_search_space(\"SkipTransformer\"),\n                        ]),\n                        get_search_space(\"Passthrough\")\n                        ]\n                    )\n\nfinal_transformers_layer.generate().export_pipeline()\n</pre> final_transformers_layer =UnionPipeline([                         ChoicePipeline([                             DynamicUnionPipeline(get_search_space([\"transformers\"])),                             get_search_space(\"SkipTransformer\"),                         ]),                         get_search_space(\"Passthrough\")                         ]                     )  final_transformers_layer.generate().export_pipeline() Out[42]: <pre>FeatureUnion(transformer_list=[('skiptransformer', SkipTransformer()),\n                               ('passthrough', Passthrough())])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0FeatureUnion?Documentation for FeatureUnioniNot fitted<pre>FeatureUnion(transformer_list=[('skiptransformer', SkipTransformer()),\n                               ('passthrough', Passthrough())])</pre> skiptransformerSkipTransformer<pre>SkipTransformer()</pre> passthroughPassthrough<pre>Passthrough()</pre> In\u00a0[43]: Copied! <pre>inner_estimators_layer = UnionPipeline([\n                            ChoicePipeline([\n                                DynamicUnionPipeline(wrapped_estimators, max_estimators=4),\n                                get_search_space(\"SkipTransformer\"),\n                            ]),\n                            get_search_space(\"Passthrough\")]\n                        )\n\ninner_estimators_layer.generate().export_pipeline()\n</pre> inner_estimators_layer = UnionPipeline([                             ChoicePipeline([                                 DynamicUnionPipeline(wrapped_estimators, max_estimators=4),                                 get_search_space(\"SkipTransformer\"),                             ]),                             get_search_space(\"Passthrough\")]                         )  inner_estimators_layer.generate().export_pipeline() Out[43]: <pre>FeatureUnion(transformer_list=[('skiptransformer', SkipTransformer()),\n                               ('passthrough', Passthrough())])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0FeatureUnion?Documentation for FeatureUnioniNot fitted<pre>FeatureUnion(transformer_list=[('skiptransformer', SkipTransformer()),\n                               ('passthrough', Passthrough())])</pre> skiptransformerSkipTransformer<pre>SkipTransformer()</pre> passthroughPassthrough<pre>Passthrough()</pre> In\u00a0[44]: Copied! <pre>final_linear_pipeline = SequentialPipeline([\n                            get_search_space(\"scalers\"),\n                            final_transformers_layer,\n                            inner_estimators_layer,\n                            get_search_space(\"classifiers\"),\n                        ])\n\nfinal_linear_pipeline.generate().export_pipeline()\n</pre> final_linear_pipeline = SequentialPipeline([                             get_search_space(\"scalers\"),                             final_transformers_layer,                             inner_estimators_layer,                             get_search_space(\"classifiers\"),                         ])  final_linear_pipeline.generate().export_pipeline() Out[44]: <pre>Pipeline(steps=[('normalizer', Normalizer(norm='l1')),\n                ('featureunion-1',\n                 FeatureUnion(transformer_list=[('skiptransformer',\n                                                 SkipTransformer()),\n                                                ('passthrough',\n                                                 Passthrough())])),\n                ('featureunion-2',\n                 FeatureUnion(transformer_list=[('skiptransformer',\n                                                 SkipTransformer()),\n                                                ('passthrough',\n                                                 Passthrough())])),\n                ('bernoullinb',\n                 BernoulliNB(alpha=5.0573782838899, fit_prior=False))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('normalizer', Normalizer(norm='l1')),\n                ('featureunion-1',\n                 FeatureUnion(transformer_list=[('skiptransformer',\n                                                 SkipTransformer()),\n                                                ('passthrough',\n                                                 Passthrough())])),\n                ('featureunion-2',\n                 FeatureUnion(transformer_list=[('skiptransformer',\n                                                 SkipTransformer()),\n                                                ('passthrough',\n                                                 Passthrough())])),\n                ('bernoullinb',\n                 BernoulliNB(alpha=5.0573782838899, fit_prior=False))])</pre> \u00a0Normalizer?Documentation for Normalizer<pre>Normalizer(norm='l1')</pre> \u00a0featureunion-1: FeatureUnion?Documentation for featureunion-1: FeatureUnion<pre>FeatureUnion(transformer_list=[('skiptransformer', SkipTransformer()),\n                               ('passthrough', Passthrough())])</pre> skiptransformerSkipTransformer<pre>SkipTransformer()</pre> passthroughPassthrough<pre>Passthrough()</pre> \u00a0featureunion-2: FeatureUnion?Documentation for featureunion-2: FeatureUnion<pre>FeatureUnion(transformer_list=[('skiptransformer', SkipTransformer()),\n                               ('passthrough', Passthrough())])</pre> skiptransformerSkipTransformer<pre>SkipTransformer()</pre> passthroughPassthrough<pre>Passthrough()</pre> \u00a0BernoulliNB?Documentation for BernoulliNB<pre>BernoulliNB(alpha=5.0573782838899, fit_prior=False)</pre> In\u00a0[45]: Copied! <pre>linear_search_space = tpot.config.template_search_spaces.get_template_search_spaces(\"linear\", inner_predictors=True, cross_val_predict_cv=5)\nlinear_search_space.generate().export_pipeline()\n</pre> linear_search_space = tpot.config.template_search_spaces.get_template_search_spaces(\"linear\", inner_predictors=True, cross_val_predict_cv=5) linear_search_space.generate().export_pipeline() Out[45]: <pre>Pipeline(steps=[('standardscaler', StandardScaler()),\n                ('rfe',\n                 RFE(estimator=ExtraTreesClassifier(max_features=0.8009842720563,\n                                                    min_samples_leaf=4,\n                                                    min_samples_split=9,\n                                                    n_jobs=1),\n                     step=0.4315847507401)),\n                ('featureunion-1',\n                 FeatureUnion(transformer_list=[('skiptransformer',\n                                                 SkipTransformer()),\n                                                ('passthrough',\n                                                 Passthrough())])),\n                ('featureunion-2',\n                 FeatureUnion(tran...\n                                                                                                                                       max_features='sqrt',\n                                                                                                                                       min_samples_leaf=17,\n                                                                                                                                       min_samples_split=8))),\n                                                                                ('estimatortransformer-2',\n                                                                                 EstimatorTransformer(cross_val_predict_cv=5,\n                                                                                                      estimator=LGBMClassifier(max_depth=3,\n                                                                                                                               n_estimators=84,\n                                                                                                                               n_jobs=1,\n                                                                                                                               num_leaves=244,\n                                                                                                                               verbose=-1)))])),\n                                                ('passthrough',\n                                                 Passthrough())])),\n                ('lineardiscriminantanalysis',\n                 LinearDiscriminantAnalysis(shrinkage=0.369619691802,\n                                            solver='eigen'))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('standardscaler', StandardScaler()),\n                ('rfe',\n                 RFE(estimator=ExtraTreesClassifier(max_features=0.8009842720563,\n                                                    min_samples_leaf=4,\n                                                    min_samples_split=9,\n                                                    n_jobs=1),\n                     step=0.4315847507401)),\n                ('featureunion-1',\n                 FeatureUnion(transformer_list=[('skiptransformer',\n                                                 SkipTransformer()),\n                                                ('passthrough',\n                                                 Passthrough())])),\n                ('featureunion-2',\n                 FeatureUnion(tran...\n                                                                                                                                       max_features='sqrt',\n                                                                                                                                       min_samples_leaf=17,\n                                                                                                                                       min_samples_split=8))),\n                                                                                ('estimatortransformer-2',\n                                                                                 EstimatorTransformer(cross_val_predict_cv=5,\n                                                                                                      estimator=LGBMClassifier(max_depth=3,\n                                                                                                                               n_estimators=84,\n                                                                                                                               n_jobs=1,\n                                                                                                                               num_leaves=244,\n                                                                                                                               verbose=-1)))])),\n                                                ('passthrough',\n                                                 Passthrough())])),\n                ('lineardiscriminantanalysis',\n                 LinearDiscriminantAnalysis(shrinkage=0.369619691802,\n                                            solver='eigen'))])</pre> \u00a0StandardScaler?Documentation for StandardScaler<pre>StandardScaler()</pre> \u00a0rfe: RFE?Documentation for rfe: RFE<pre>RFE(estimator=ExtraTreesClassifier(max_features=0.8009842720563,\n                                   min_samples_leaf=4, min_samples_split=9,\n                                   n_jobs=1),\n    step=0.4315847507401)</pre> estimator: ExtraTreesClassifier<pre>ExtraTreesClassifier(max_features=0.8009842720563, min_samples_leaf=4,\n                     min_samples_split=9, n_jobs=1)</pre> \u00a0ExtraTreesClassifier?Documentation for ExtraTreesClassifier<pre>ExtraTreesClassifier(max_features=0.8009842720563, min_samples_leaf=4,\n                     min_samples_split=9, n_jobs=1)</pre> \u00a0featureunion-1: FeatureUnion?Documentation for featureunion-1: FeatureUnion<pre>FeatureUnion(transformer_list=[('skiptransformer', SkipTransformer()),\n                               ('passthrough', Passthrough())])</pre> skiptransformerSkipTransformer<pre>SkipTransformer()</pre> passthroughPassthrough<pre>Passthrough()</pre> \u00a0featureunion-2: FeatureUnion?Documentation for featureunion-2: FeatureUnion<pre>FeatureUnion(transformer_list=[('featureunion',\n                                FeatureUnion(transformer_list=[('estimatortransformer-1',\n                                                                EstimatorTransformer(cross_val_predict_cv=5,\n                                                                                     estimator=DecisionTreeClassifier(criterion='entropy',\n                                                                                                                      max_depth=1,\n                                                                                                                      max_features='sqrt',\n                                                                                                                      min_samples_leaf=17,\n                                                                                                                      min_samples_split=8))),\n                                                               ('estimatortransformer-2',\n                                                                EstimatorTransformer(cross_val_predict_cv=5,\n                                                                                     estimator=LGBMClassifier(max_depth=3,\n                                                                                                              n_estimators=84,\n                                                                                                              n_jobs=1,\n                                                                                                              num_leaves=244,\n                                                                                                              verbose=-1)))])),\n                               ('passthrough', Passthrough())])</pre> featureunionestimatortransformer-1estimator: DecisionTreeClassifier<pre>DecisionTreeClassifier(criterion='entropy', max_depth=1, max_features='sqrt',\n                       min_samples_leaf=17, min_samples_split=8)</pre> \u00a0DecisionTreeClassifier?Documentation for DecisionTreeClassifier<pre>DecisionTreeClassifier(criterion='entropy', max_depth=1, max_features='sqrt',\n                       min_samples_leaf=17, min_samples_split=8)</pre> estimatortransformer-2estimator: LGBMClassifier<pre>LGBMClassifier(max_depth=3, n_estimators=84, n_jobs=1, num_leaves=244,\n               verbose=-1)</pre> LGBMClassifier<pre>LGBMClassifier(max_depth=3, n_estimators=84, n_jobs=1, num_leaves=244,\n               verbose=-1)</pre> passthroughPassthrough<pre>Passthrough()</pre> \u00a0LinearDiscriminantAnalysis?Documentation for LinearDiscriminantAnalysis<pre>LinearDiscriminantAnalysis(shrinkage=0.369619691802, solver='eigen')</pre> In\u00a0[46]: Copied! <pre>linear_search_space = tpot.config.template_search_spaces.get_template_search_spaces(\"linear\", inner_predictors=True, cross_val_predict_cv=5)\nlinear_est = tpot.TPOTEstimator(\n                            search_space = linear_search_space,\n                            scorers=['roc_auc_ovr',tpot.objectives.complexity_scorer],\n                            scorers_weights=[1,-1],\n                            classification=True,\n                            verbose=1,\n                            )\n\n#alternatively, you can use the template search space to generate a pipeline\nlinear_est = tpot.TPOTEstimator(\n                            search_space = \"linear\",\n                            scorers=['roc_auc_ovr',tpot.objectives.complexity_scorer],\n                            scorers_weights=[1,-1],\n                            n_jobs=32,\n                            classification=True,\n                            verbose=1,\n                            )\n</pre> linear_search_space = tpot.config.template_search_spaces.get_template_search_spaces(\"linear\", inner_predictors=True, cross_val_predict_cv=5) linear_est = tpot.TPOTEstimator(                             search_space = linear_search_space,                             scorers=['roc_auc_ovr',tpot.objectives.complexity_scorer],                             scorers_weights=[1,-1],                             classification=True,                             verbose=1,                             )  #alternatively, you can use the template search space to generate a pipeline linear_est = tpot.TPOTEstimator(                             search_space = \"linear\",                             scorers=['roc_auc_ovr',tpot.objectives.complexity_scorer],                             scorers_weights=[1,-1],                             n_jobs=32,                             classification=True,                             verbose=1,                             ) In\u00a0[47]: Copied! <pre>all_search_spaces ={\n    \"classifiers_only\" : classifier_choice,\n    \"stc_pipeline\" : stc_pipeline,\n    \"stc_pipeline2\": stc_pipeline2,\n    \"stc_pipeline3\": stc_pipeline3,\n    \"stc_pipeline4\": stc_pipeline4,\n    \"final_linear_pipeline\": final_linear_pipeline,\n    \"graph_pipeline\": graph_search_space,\n}\n\nX, y = sklearn.datasets.load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.5)\n</pre> all_search_spaces ={     \"classifiers_only\" : classifier_choice,     \"stc_pipeline\" : stc_pipeline,     \"stc_pipeline2\": stc_pipeline2,     \"stc_pipeline3\": stc_pipeline3,     \"stc_pipeline4\": stc_pipeline4,     \"final_linear_pipeline\": final_linear_pipeline,     \"graph_pipeline\": graph_search_space, }  X, y = sklearn.datasets.load_breast_cancer(return_X_y=True) X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.5) In\u00a0[48]: Copied! <pre>selected_search_space = all_search_spaces[\"stc_pipeline\"] #change this to select a different search space\n\n\nest = tpot.TPOTEstimator(\n    scorers=[\"roc_auc_ovr\", tpot.objectives.complexity_scorer],\n    scorers_weights=[1.0, -1.0],\n    classification = True,\n    cv = 5,\n    search_space = selected_search_space,\n    max_time_mins=10,\n    max_eval_time_mins = 10,\n    early_stop = 2,\n    verbose = 2,\n    n_jobs=4,\n)\n\nest.fit(X_train, y_train)\n</pre> selected_search_space = all_search_spaces[\"stc_pipeline\"] #change this to select a different search space   est = tpot.TPOTEstimator(     scorers=[\"roc_auc_ovr\", tpot.objectives.complexity_scorer],     scorers_weights=[1.0, -1.0],     classification = True,     cv = 5,     search_space = selected_search_space,     max_time_mins=10,     max_eval_time_mins = 10,     early_stop = 2,     verbose = 2,     n_jobs=4, )  est.fit(X_train, y_train) <pre>Generation: : 5it [00:58, 11.77s/it]\n</pre> Out[48]: <pre>TPOTEstimator(classification=True, cv=5, early_stop=2, max_time_mins=10,\n              n_jobs=4,\n              scorers=['roc_auc_ovr',\n                       &lt;function complexity_scorer at 0x32f4e0550&gt;],\n              scorers_weights=[1.0, -1.0],\n              search_space=&lt;tpot.search_spaces.pipelines.sequential.SequentialPipeline object at 0x32f7692d0&gt;,\n              verbose=2)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0TPOTEstimatoriFitted<pre>TPOTEstimator(classification=True, cv=5, early_stop=2, max_time_mins=10,\n              n_jobs=4,\n              scorers=['roc_auc_ovr',\n                       &lt;function complexity_scorer at 0x32f4e0550&gt;],\n              scorers_weights=[1.0, -1.0],\n              search_space=&lt;tpot.search_spaces.pipelines.sequential.SequentialPipeline object at 0x32f7692d0&gt;,\n              verbose=2)</pre> In\u00a0[49]: Copied! <pre># score the model\nauroc_scorer = sklearn.metrics.get_scorer(\"roc_auc\")\nauroc_score = auroc_scorer(est, X_test, y_test)\n\nprint(\"auroc score\", auroc_score)\n</pre> # score the model auroc_scorer = sklearn.metrics.get_scorer(\"roc_auc\") auroc_score = auroc_scorer(est, X_test, y_test)  print(\"auroc score\", auroc_score) <pre>auroc score 0.9947351959966638\n</pre> In\u00a0[50]: Copied! <pre>#plot the best pipeline\nif isinstance(est.fitted_pipeline_, tpot.GraphPipeline):\n    est.fitted_pipeline_.plot()\n    \nest.fitted_pipeline_\n</pre> #plot the best pipeline if isinstance(est.fitted_pipeline_, tpot.GraphPipeline):     est.fitted_pipeline_.plot()      est.fitted_pipeline_ Out[50]: <pre>Pipeline(steps=[('selectfwe', SelectFwe(alpha=0.0001569023321)),\n                ('powertransformer', PowerTransformer()),\n                ('mlpclassifier',\n                 MLPClassifier(activation='identity', alpha=0.0008696190619,\n                               hidden_layer_sizes=[203, 203],\n                               learning_rate_init=0.0135276110446,\n                               n_iter_no_change=32))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiFitted<pre>Pipeline(steps=[('selectfwe', SelectFwe(alpha=0.0001569023321)),\n                ('powertransformer', PowerTransformer()),\n                ('mlpclassifier',\n                 MLPClassifier(activation='identity', alpha=0.0008696190619,\n                               hidden_layer_sizes=[203, 203],\n                               learning_rate_init=0.0135276110446,\n                               n_iter_no_change=32))])</pre> \u00a0SelectFwe?Documentation for SelectFwe<pre>SelectFwe(alpha=0.0001569023321)</pre> \u00a0PowerTransformer?Documentation for PowerTransformer<pre>PowerTransformer()</pre> \u00a0MLPClassifier?Documentation for MLPClassifier<pre>MLPClassifier(activation='identity', alpha=0.0008696190619,\n              hidden_layer_sizes=[203, 203], learning_rate_init=0.0135276110446,\n              n_iter_no_change=32)</pre> In\u00a0[51]: Copied! <pre>import sklearn\nimport sklearn.datasets\nimport numpy as np\nimport tpot\n\n#in practice, cross validation is likely better, but this simple example is fine for demonstration purposes\ndef rmse_obective(est, X, missing_add=.2, rng=1, fitted=False):\n    rng = np.random.default_rng(rng)\n    X_missing = X.copy()\n    missing_idx = rng.random(X.shape) &lt; missing_add\n    X_missing[missing_idx] = np.nan\n    \n    if not fitted:\n        est.fit(X_missing)\n    \n    X_filled = est.transform(X_missing)\n    return np.sqrt(np.mean((X_filled[missing_idx] - X[missing_idx])**2))\n\nfrom sklearn.impute import SimpleImputer\n\nX, y = sklearn.datasets.load_diabetes(return_X_y=True)\n\nimp = SimpleImputer(strategy=\"mean\")\n\nrmse_obective(imp, X)\n</pre> import sklearn import sklearn.datasets import numpy as np import tpot  #in practice, cross validation is likely better, but this simple example is fine for demonstration purposes def rmse_obective(est, X, missing_add=.2, rng=1, fitted=False):     rng = np.random.default_rng(rng)     X_missing = X.copy()     missing_idx = rng.random(X.shape) &lt; missing_add     X_missing[missing_idx] = np.nan          if not fitted:         est.fit(X_missing)          X_filled = est.transform(X_missing)     return np.sqrt(np.mean((X_filled[missing_idx] - X[missing_idx])**2))  from sklearn.impute import SimpleImputer  X, y = sklearn.datasets.load_diabetes(return_X_y=True)  imp = SimpleImputer(strategy=\"mean\")  rmse_obective(imp, X) Out[51]: <pre>0.04690299241236334</pre> In\u00a0[52]: Copied! <pre>import tpot.search_spaces\nfrom ConfigSpace import ConfigurationSpace, Integer, Float, Categorical, Normal\n\n#set up an imputation search space that includes simple imputer, knn imputer, and iterative imputer (with an optimized ExtraTreesRegressor)\n\nsimple_imputer = tpot.config.get_search_space(\"SimpleImputer\")\nknn_imputer = tpot.config.get_search_space(\"KNNImputer\")\n\nspace = ConfigurationSpace({ 'initial_strategy' : Categorical('initial_strategy', \n                                            ['mean', 'median', \n                                            'most_frequent', 'constant']),\n            'n_nearest_features' : Integer('n_nearest_features', \n                                        bounds=(1, X.shape[1])),\n            'imputation_order' : Categorical('imputation_order', \n                                            ['ascending', 'descending', \n                                            'roman', 'arabic', 'random']),\n})\n\n# This optimizes both the iterative imputer parameters and the ExtraTreesRegressor parameters\niterative_imputer_sp = tpot.search_spaces.pipelines.WrapperPipeline(\n    method = sklearn.impute.IterativeImputer,\n    space = space,\n    estimator_search_space = tpot.config.get_search_space(\"ExtraTreesRegressor\"),\n)\n#this is equivalent to\n# iterative_imputer_sp = tpot.config.get_search_space(\"IterativeImputer_learned_estimators\")\n\nimputation_search_space = tpot.search_spaces.pipelines.ChoicePipeline(\n    search_spaces = [simple_imputer, knn_imputer, iterative_imputer_sp],\n)\nimputation_search_space.generate().export_pipeline()\n</pre> import tpot.search_spaces from ConfigSpace import ConfigurationSpace, Integer, Float, Categorical, Normal  #set up an imputation search space that includes simple imputer, knn imputer, and iterative imputer (with an optimized ExtraTreesRegressor)  simple_imputer = tpot.config.get_search_space(\"SimpleImputer\") knn_imputer = tpot.config.get_search_space(\"KNNImputer\")  space = ConfigurationSpace({ 'initial_strategy' : Categorical('initial_strategy',                                              ['mean', 'median',                                              'most_frequent', 'constant']),             'n_nearest_features' : Integer('n_nearest_features',                                          bounds=(1, X.shape[1])),             'imputation_order' : Categorical('imputation_order',                                              ['ascending', 'descending',                                              'roman', 'arabic', 'random']), })  # This optimizes both the iterative imputer parameters and the ExtraTreesRegressor parameters iterative_imputer_sp = tpot.search_spaces.pipelines.WrapperPipeline(     method = sklearn.impute.IterativeImputer,     space = space,     estimator_search_space = tpot.config.get_search_space(\"ExtraTreesRegressor\"), ) #this is equivalent to # iterative_imputer_sp = tpot.config.get_search_space(\"IterativeImputer_learned_estimators\")  imputation_search_space = tpot.search_spaces.pipelines.ChoicePipeline(     search_spaces = [simple_imputer, knn_imputer, iterative_imputer_sp], ) imputation_search_space.generate().export_pipeline() Out[52]: <pre>SimpleImputer()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0SimpleImputer?Documentation for SimpleImputeriNot fitted<pre>SimpleImputer()</pre> In\u00a0[53]: Copied! <pre>from functools import partial\n\nfinal_objective = partial(rmse_obective, X=X, missing_add=.2)\n\nest = tpot.TPOTEstimator(\n    scorers = [],\n    scorers_weights = [],\n    other_objective_functions = [final_objective],\n    other_objective_functions_weights = [-1],\n    objective_function_names = [\"rmse\"],\n    classification = True,\n    search_space = imputation_search_space,\n    max_time_mins=10,\n    max_eval_time_mins = 60*5,\n    verbose = 3,\n    early_stop = 2,\n    n_jobs=20,\n)\n\nest.fit(X, y=y)\n</pre> from functools import partial  final_objective = partial(rmse_obective, X=X, missing_add=.2)  est = tpot.TPOTEstimator(     scorers = [],     scorers_weights = [],     other_objective_functions = [final_objective],     other_objective_functions_weights = [-1],     objective_function_names = [\"rmse\"],     classification = True,     search_space = imputation_search_space,     max_time_mins=10,     max_eval_time_mins = 60*5,     verbose = 3,     early_stop = 2,     n_jobs=20, )  est.fit(X, y=y) <pre>/Users/ketrong/Desktop/tpotvalidation/tpot/tpot/tpot_estimator/estimator.py:535: UserWarning: Labels are not encoded as ints from 0 to N. For compatibility with some classifiers such as sklearn, TPOT has encoded y with the sklearn LabelEncoder. When using pipelines outside the main TPOT estimator class, you can encode the labels with est.label_encoder_\n  warnings.warn(\"Labels are not encoded as ints from 0 to N. For compatibility with some classifiers such as sklearn, TPOT has encoded y with the sklearn LabelEncoder. When using pipelines outside the main TPOT estimator class, you can encode the labels with est.label_encoder_\")\nGeneration: : 1it [00:19, 19.42s/it]</pre> <pre>Generation:  1\nBest rmse score: 0.03494378757292814\n</pre> <pre>Generation: : 2it [00:35, 17.45s/it]</pre> <pre>Generation:  2\nBest rmse score: 0.03494378757292814\n</pre> <pre>Generation: : 3it [00:51, 16.76s/it]</pre> <pre>Generation:  3\nBest rmse score: 0.034787576318641794\n</pre> <pre>Generation: : 3it [01:10, 23.47s/it]</pre> <pre>Generation:  4\nBest rmse score: 0.034283600126080886\nEarly stop\n</pre> <pre>\n</pre> Out[53]: <pre>TPOTEstimator(classification=True, early_stop=2, max_eval_time_mins=300,\n              max_time_mins=10, n_jobs=20, objective_function_names=['rmse'],\n              other_objective_functions=[functools.partial(&lt;function rmse_obective at 0x33edfd480&gt;, X=array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n         0.01990749, -0.01764613],\n       [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n        -...\n        -0.04688253,  0.01549073],\n       [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n         0.04452873, -0.02593034],\n       [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n        -0.00422151,  0.00306441]]), missing_add=0.2)],\n              other_objective_functions_weights=[-1], scorers=[],\n              scorers_weights=[],\n              search_space=&lt;tpot.search_spaces.pipelines.choice.ChoicePipeline object at 0x36ff3e770&gt;,\n              verbose=3)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0TPOTEstimatoriFitted<pre>TPOTEstimator(classification=True, early_stop=2, max_eval_time_mins=300,\n              max_time_mins=10, n_jobs=20, objective_function_names=['rmse'],\n              other_objective_functions=[functools.partial(&lt;function rmse_obective at 0x33edfd480&gt;, X=array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n         0.01990749, -0.01764613],\n       [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n        -...\n        -0.04688253,  0.01549073],\n       [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n         0.04452873, -0.02593034],\n       [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n        -0.00422151,  0.00306441]]), missing_add=0.2)],\n              other_objective_functions_weights=[-1], scorers=[],\n              scorers_weights=[],\n              search_space=&lt;tpot.search_spaces.pipelines.choice.ChoicePipeline object at 0x36ff3e770&gt;,\n              verbose=3)</pre> In\u00a0[54]: Copied! <pre># score the model\nrmse_score = final_objective(est, fitted=True)\nprint(\"final rmse score\", rmse_score)\n</pre> # score the model rmse_score = final_objective(est, fitted=True) print(\"final rmse score\", rmse_score) <pre>final rmse score 0.02796745384428642\n</pre> In\u00a0[55]: Copied! <pre>est.fitted_pipeline_\n</pre> est.fitted_pipeline_ Out[55]: <pre>IterativeImputer(estimator=ExtraTreesRegressor(criterion='friedman_mse',\n                                               max_features=0.6404215718013,\n                                               min_samples_leaf=2,\n                                               min_samples_split=10, n_jobs=1),\n                 imputation_order='arabic', n_nearest_features=9)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0IterativeImputer?Documentation for IterativeImputeriFitted<pre>IterativeImputer(estimator=ExtraTreesRegressor(criterion='friedman_mse',\n                                               max_features=0.6404215718013,\n                                               min_samples_leaf=2,\n                                               min_samples_split=10, n_jobs=1),\n                 imputation_order='arabic', n_nearest_features=9)</pre> estimator: ExtraTreesRegressor<pre>ExtraTreesRegressor(criterion='friedman_mse', max_features=0.6404215718013,\n                    min_samples_leaf=2, min_samples_split=10, n_jobs=1)</pre> \u00a0ExtraTreesRegressor?Documentation for ExtraTreesRegressor<pre>ExtraTreesRegressor(criterion='friedman_mse', max_features=0.6404215718013,\n                    min_samples_leaf=2, min_samples_split=10, n_jobs=1)</pre> In\u00a0[56]: Copied! <pre>from tpot.search_spaces.pipelines import *\nfrom tpot.config import get_search_space\n\nselectors = get_search_space([\"selectors_classification\", \"Passthrough\"])\nestimators = get_search_space([\"classifiers\"])\n\n\n# this allows us to wrap the classifiers in the EstimatorTransformer\n# this is necessary so that classifiers can be used inside of sklearn pipelines\nwrapped_estimators = WrapperPipeline(tpot.builtin_modules.EstimatorTransformer, {}, estimators)\n\nscalers = get_search_space([\"scalers\",\"Passthrough\"])\n\ntransformers_layer =UnionPipeline([\n                        ChoicePipeline([\n                            DynamicUnionPipeline(get_search_space([\"transformers\"])),\n                            get_search_space(\"SkipTransformer\"),\n                        ]),\n                        get_search_space(\"Passthrough\")\n                        ]\n                    )\n\ninner_estimators_layer = UnionPipeline([\n                            ChoicePipeline([\n                                DynamicUnionPipeline(wrapped_estimators),\n                                get_search_space(\"SkipTransformer\"),\n                            ]),\n                            get_search_space(\"Passthrough\")]\n                        )\n\n\nsearch_space = SequentialPipeline(search_spaces=[\n                                        scalers,\n                                        selectors, \n                                        transformers_layer,\n                                        inner_estimators_layer,\n                                        estimators,\n                                        ])\n\nest = tpot.TPOTEstimator(\n    scorers = [\"roc_auc\"],\n    scorers_weights = [1],\n    classification = True,\n    cv = 5,\n    search_space = search_space,\n    max_time_mins=10,\n    max_eval_time_mins = 60*5,\n    verbose = 2,\n    n_jobs=20,\n)\n\nest.fit(X_train, y_train)\n</pre> from tpot.search_spaces.pipelines import * from tpot.config import get_search_space  selectors = get_search_space([\"selectors_classification\", \"Passthrough\"]) estimators = get_search_space([\"classifiers\"])   # this allows us to wrap the classifiers in the EstimatorTransformer # this is necessary so that classifiers can be used inside of sklearn pipelines wrapped_estimators = WrapperPipeline(tpot.builtin_modules.EstimatorTransformer, {}, estimators)  scalers = get_search_space([\"scalers\",\"Passthrough\"])  transformers_layer =UnionPipeline([                         ChoicePipeline([                             DynamicUnionPipeline(get_search_space([\"transformers\"])),                             get_search_space(\"SkipTransformer\"),                         ]),                         get_search_space(\"Passthrough\")                         ]                     )  inner_estimators_layer = UnionPipeline([                             ChoicePipeline([                                 DynamicUnionPipeline(wrapped_estimators),                                 get_search_space(\"SkipTransformer\"),                             ]),                             get_search_space(\"Passthrough\")]                         )   search_space = SequentialPipeline(search_spaces=[                                         scalers,                                         selectors,                                          transformers_layer,                                         inner_estimators_layer,                                         estimators,                                         ])  est = tpot.TPOTEstimator(     scorers = [\"roc_auc\"],     scorers_weights = [1],     classification = True,     cv = 5,     search_space = search_space,     max_time_mins=10,     max_eval_time_mins = 60*5,     verbose = 2,     n_jobs=20, )  est.fit(X_train, y_train) <pre>Generation: : 25it [10:00, 24.01s/it]\n</pre> Out[56]: <pre>TPOTEstimator(classification=True, cv=5, max_eval_time_mins=300,\n              max_time_mins=10, n_jobs=20, scorers=['roc_auc'],\n              scorers_weights=[1],\n              search_space=&lt;tpot.search_spaces.pipelines.sequential.SequentialPipeline object at 0x35798c880&gt;,\n              verbose=2)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0TPOTEstimatoriFitted<pre>TPOTEstimator(classification=True, cv=5, max_eval_time_mins=300,\n              max_time_mins=10, n_jobs=20, scorers=['roc_auc'],\n              scorers_weights=[1],\n              search_space=&lt;tpot.search_spaces.pipelines.sequential.SequentialPipeline object at 0x35798c880&gt;,\n              verbose=2)</pre> In\u00a0[57]: Copied! <pre>est.fitted_pipeline_\n</pre> est.fitted_pipeline_ Out[57]: <pre>Pipeline(steps=[('maxabsscaler', MaxAbsScaler()),\n                ('selectfwe', SelectFwe(alpha=0.0004883916878)),\n                ('featureunion-1',\n                 FeatureUnion(transformer_list=[('featureunion',\n                                                 FeatureUnion(transformer_list=[('powertransformer',\n                                                                                 PowerTransformer())])),\n                                                ('passthrough',\n                                                 Passthrough())])),\n                ('featureunion-2',\n                 FeatureUnion(transformer_list=[('featureunion',\n                                                 FeatureUnion(transformer_list=[('estimatortransformer',\n                                                                                 EstimatorTransformer(estimator=LinearDiscriminantAnalysis(shrinkage=0.5801392483719,\n                                                                                                                                           solver='lsqr')))])),\n                                                ('passthrough',\n                                                 Passthrough())])),\n                ('mlpclassifier',\n                 MLPClassifier(activation='identity', alpha=0.0310773820788,\n                               hidden_layer_sizes=[54, 54, 54],\n                               learning_rate_init=0.0017701050157,\n                               n_iter_no_change=32))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiFitted<pre>Pipeline(steps=[('maxabsscaler', MaxAbsScaler()),\n                ('selectfwe', SelectFwe(alpha=0.0004883916878)),\n                ('featureunion-1',\n                 FeatureUnion(transformer_list=[('featureunion',\n                                                 FeatureUnion(transformer_list=[('powertransformer',\n                                                                                 PowerTransformer())])),\n                                                ('passthrough',\n                                                 Passthrough())])),\n                ('featureunion-2',\n                 FeatureUnion(transformer_list=[('featureunion',\n                                                 FeatureUnion(transformer_list=[('estimatortransformer',\n                                                                                 EstimatorTransformer(estimator=LinearDiscriminantAnalysis(shrinkage=0.5801392483719,\n                                                                                                                                           solver='lsqr')))])),\n                                                ('passthrough',\n                                                 Passthrough())])),\n                ('mlpclassifier',\n                 MLPClassifier(activation='identity', alpha=0.0310773820788,\n                               hidden_layer_sizes=[54, 54, 54],\n                               learning_rate_init=0.0017701050157,\n                               n_iter_no_change=32))])</pre> \u00a0MaxAbsScaler?Documentation for MaxAbsScaler<pre>MaxAbsScaler()</pre> \u00a0SelectFwe?Documentation for SelectFwe<pre>SelectFwe(alpha=0.0004883916878)</pre> \u00a0featureunion-1: FeatureUnion?Documentation for featureunion-1: FeatureUnion<pre>FeatureUnion(transformer_list=[('featureunion',\n                                FeatureUnion(transformer_list=[('powertransformer',\n                                                                PowerTransformer())])),\n                               ('passthrough', Passthrough())])</pre> featureunionpowertransformer\u00a0PowerTransformer?Documentation for PowerTransformer<pre>PowerTransformer()</pre> passthroughPassthrough<pre>Passthrough()</pre> \u00a0featureunion-2: FeatureUnion?Documentation for featureunion-2: FeatureUnion<pre>FeatureUnion(transformer_list=[('featureunion',\n                                FeatureUnion(transformer_list=[('estimatortransformer',\n                                                                EstimatorTransformer(estimator=LinearDiscriminantAnalysis(shrinkage=0.5801392483719,\n                                                                                                                          solver='lsqr')))])),\n                               ('passthrough', Passthrough())])</pre> featureunionestimatortransformerestimator: LinearDiscriminantAnalysis<pre>LinearDiscriminantAnalysis(shrinkage=0.5801392483719, solver='lsqr')</pre> \u00a0LinearDiscriminantAnalysis?Documentation for LinearDiscriminantAnalysis<pre>LinearDiscriminantAnalysis(shrinkage=0.5801392483719, solver='lsqr')</pre> passthroughPassthrough<pre>Passthrough()</pre> \u00a0MLPClassifier?Documentation for MLPClassifier<pre>MLPClassifier(activation='identity', alpha=0.0310773820788,\n              hidden_layer_sizes=[54, 54, 54],\n              learning_rate_init=0.0017701050157, n_iter_no_change=32)</pre>"},{"location":"Tutorial/2_Search_Spaces/#intro","title":"Intro\u00b6","text":"<p>TPOT gives the user a lot of options for customizing the search space, from hyperparameter ranges to model selection to pipeline configuration. TPOT is able to select models, optimize their hyperparameters, and build a complex pipeline structure. Each level of detail has multiple customization options. This tutorial will first explore how to set up a hyperparameter search space for a single method. Next, we will describe how to set up simultaneous model selection and hyperparameter tuning. Finally, we will cover how to utilize these steps to configure a search space for a fixed pipeline of multiple steps, as well as having TPOT optimize the pipeline structure itself.</p>"},{"location":"Tutorial/2_Search_Spaces/#hyperparameter-search-spaces-with-configspace","title":"Hyperparameter Search Spaces with ConfigSpace\u00b6","text":"<p>Hyperparameter search spaces are defined using the ConfigSpace package found here. More information on how to set up a hyperparameter space can be found in their documentation here.</p> <p>TPOT uses <code>ConfigSpace.ConfigurationSpace</code> objects to define the hyperparameter search space for individual models. This object can be used to keep track of the desired hyperparameters as well as provide functions for random sampling from this space.</p> <p>In short, you can use the <code>Integer</code>, <code>Float</code>, and <code>Categorical</code> functions of <code>ConfigSpace</code> to define a range of values used for each param. Alternatively, a tuple with (min,max) ints or floats can be used to specify an int/float search space and a list can be used to specify a categorical search space. A fixed value can also be provided for parameters that are not tunned. The space parameter of <code>ConfigurationSpace</code> takes in a dictionary of param names to these ranges.</p> <p>Note: If you want reproducible results, you need to set a fixed random_state in the search space.</p> <p>Here is an example of a hyperparameter range for RandomForest</p>"},{"location":"Tutorial/2_Search_Spaces/#tpot-search-spaces","title":"TPOT Search spaces\u00b6","text":"<p>TPOT allows you to create hyperparameter search spaces for individual methods and pipeline structure search spaces. For example, TPOT can create linear pipelines, trees, or graphs.</p> <p>TPOT search spaces are found in the <code>search_spaces</code> module. There are two primary kinds of search spaces, node and pipeline. Node search spaces specify a single sklearn <code>BaseEstimator</code> search space. Pipeline search spaces define the possible structures for a group of node search spaces. These take in node search spaces and produce a pipeline using nodes from that search space. Since sklearn Pipelines are also <code>BaseEstimator</code>, pipeline search spaces are also technically node search spaces. This means that pipeline search spaces can take in other pipeline search spaces in order to define more complex structures. The primary differentiating factor between node and pipeline search spaces is that pipeline search spaces must take in another search space as input to feed its individual nodes. Therefore, all search spaces eventually end in a node search space at the lowest level. Note that parameters for pipeline search spaces can differ, some take in only a single search space, some take in a list, or some take in multiple defined parameters.</p>"},{"location":"Tutorial/2_Search_Spaces/#node-search-spaces","title":"node search spaces\u00b6","text":"Name Info EstimatorNode Takes in a ConfigSpace along with the class of the method. This node will optimize the hyperparameters for a single method. GeneticFeatureSelectorNode Uses evolution to optimize a set of features, exports a basic sklearn Selector that simply selects the features chosen by the node. FSSNode FSS stands for FeatureSetSelector. This node takes in a list of user-defined subsets of features and selects a single predefined subset. Note that TPOT will not create new subsets nor will it select multiple subsets per node. If using a linear pipeline, this node should be set as the first step. In linear pipelines it is recommended that you only use a small number of feature sets. I recommend exploring using FSSNodes in pipelines that allow TPOT to select more than one FSSNode at a time. For example, DynamicUnionPipeline and GraphPipeline are both excellent combos for FSSNode. Use FFSNode inside a DynamicUnionPipeline at the start of linear pipeline to explore optimal combinations of subsets in linear pipelines. Set the leaf_search_space of GraphSearchPipeline TPOT can use multiple feature sets in different ways, for example, with different transformers for different sets."},{"location":"Tutorial/2_Search_Spaces/#pipeline-search-spaces","title":"pipeline search spaces\u00b6","text":"<p>found in tpot2.search_spaces.pipelines</p> <p>WrapperPipeline -         This search space is for wrapping a sklearn estimator with a method that takes another estimator and hyperparameters as arguments. For example, this can be used with sklearn.ensemble.BaggingClassifier or sklearn.ensemble.AdaBoostClassifier.</p> Name Info ChoicePipeline Takes in a list of search spaces. Will select one node from the search space. SequentialPipeline Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index. DynamicLinearPipeline Takes in a single search space. Will produce a linear pipeline of variable length. Each step in the pipeline will be pulled from the search space provided. UnionPipeline Takes in a list of search spaces. The returned pipeline will include one estimator per search space joined in an sklearn FeatureUnion. Useful for having many steps in one layer. DynamicUnionPipeline Takes in a single search space. It will pull anywhere from 1 to max_estimators number of estimators from the search space and concatenate them in a FeatureUnion. TreePipeline Generates a pipeline of variable length. Pipeline will have a tree structure similar to TPOT1. GraphSearchPipeline Generates a directed acyclic graph of variable size. Search spaces for root, leaf, and inner nodes can be defined separately if desired. WrapperPipeline This search space is for wrapping a sklearn estimator with a method that takes another estimator and hyperparameters as arguments. For example, this can be used with sklearn.ensemble.BaggingClassifier or sklearn.ensemble.AdaBoostClassifier."},{"location":"Tutorial/2_Search_Spaces/#node-search-space-examples","title":"Node Search Space Examples\u00b6","text":"<p>Node search spaces represent the smallest unit of an sklearn pipeline. All node search spaces create and optimize a single node which exports a single estimator object. For example this could be a KNeighborsClassifier or a FeatureSetSelector.</p>"},{"location":"Tutorial/2_Search_Spaces/#estimatornode","title":"EstimatorNode\u00b6","text":"<p>The EstimatorNode represents the hyperparameter search space for a scikit-learn estimator.</p>"},{"location":"Tutorial/2_Search_Spaces/#fssnode-and-geneticfeatureselectornode","title":"FSSNode and GeneticFeatureSelectorNode\u00b6","text":"<p>Both of these are given their own tutorials. See Tutorial 3 for FFSNode and Tutorial 5 for GeneticFeatureSelectorNode</p>"},{"location":"Tutorial/2_Search_Spaces/#pipeline-search-space-examples","title":"Pipeline Search Space Examples\u00b6","text":"<p>Pipeline search spaces are used to define the structure and restrictions of the pipelines TPOT can search. Unlike Node search spaces, all pipeline search spaces take in other search spaces as inputs. Rather than sample hyperparameters, pipeline search spaces can select models from the input search spaces and organize them within a linear sklearn Pipeline or a TPOT GraphPipeline.</p>"},{"location":"Tutorial/2_Search_Spaces/#choicepipeline","title":"ChoicePipeline\u00b6","text":"<p>The simplest pipeline search space is the ChoicePipeline. This takes in a list of search spaces and simply selects and samples from one. In this example, we will construct a search space that takes in several options for a classifier. The resulting search space will then first select a model from KNeighborsClassifier, LogisticRegression or DecisionTreeClassifier, and then select the hyperparameters for the given model.</p>"},{"location":"Tutorial/2_Search_Spaces/#built-in-search-spaces-for-estimatornode-and-choicepipeline","title":"Built in search spaces for EstimatorNode and ChoicePipeline\u00b6","text":""},{"location":"Tutorial/2_Search_Spaces/#a-note-on-reproducibility","title":"A note on reproducibility\u00b6","text":"<p>Many sklearn estimators, like RandomForestClassifier, are stochastic and require a random_state parameter in order to have deterministic results. If you want TPOT runs to be reproducible, it is important that the estimators used by TPOT have a random state set. TPOT will not automatically set this value. This can either be set manually in each search space, or by passing in the random state to the <code>get_search_space</code> function. For example:</p>"},{"location":"Tutorial/2_Search_Spaces/#sequentialpipeline","title":"SequentialPipeline\u00b6","text":"<p>SequentialPipelines are of fixed length and sample from a predefined distribution for each step.</p>"},{"location":"Tutorial/2_Search_Spaces/#dynamiclinearpipeline","title":"DynamicLinearPipeline\u00b6","text":"<p>DynamicLinearPipeline takes in a single search space and randomly samples and places estimators in a list without a predefined sequence. DynamicLinearPipeline are most often used when paired with LinearPipeline. A common strategy is to use DynamicLinearPipeline to optimize a series of preprocessing or feature engineering steps, followed by a final classifier or regressor.</p>"},{"location":"Tutorial/2_Search_Spaces/#unionpipeline","title":"UnionPipeline\u00b6","text":"<p>Union pipelines can be useful when you want to either do multiple transformations in a single layer. Another common strategy is to do a union with a transformer and a passthrough for when you want to keep the original data in addition to the transformation.</p>"},{"location":"Tutorial/2_Search_Spaces/#dynamicunionpipeline","title":"DynamicUnionPipeline\u00b6","text":"<p>DynamicUnionPipeline works similarly as UnionPipeline. Whereas UnionPipeline is fixed length, with each index corresponding to the search space provided as a list, DynamicUnionPipeline takes in a single search space and will sample 1 or more estimators/pipelines and concatenate them with a FeatureUnion.</p> <p>Note that DynamicUnionPipeline will check for pipeline uniqueness, so it will never concatenate two completely identical pipelines. In other words, all steps within the feature union will be unique.</p> <p>This can be useful when you want multiple transformers (or in some cases, pipelines), but are not sure how many or which ones.</p>"},{"location":"Tutorial/2_Search_Spaces/#wrapperpipeline","title":"WrapperPipeline\u00b6","text":"<p>Some sklearn estimators take in other sklearn estimators as a parameter. The wrapper pipeline is used to tune both the original estimators hyperparameters simultaneously with the inner estimators hyperparameters. In fact, the inner estimator in WrapperPipeline can be any search space defined with any of the methods described in this Tutorial.</p> <p>The <code>get_search_space</code> will automatically create an inner search space for sklearn estimators that do use require an inner estimator. For example \"SelectFromModel_classification\" will return the following search space</p>"},{"location":"Tutorial/2_Search_Spaces/#wrapperpipeline-strategy-for-ensemblesinner-classifiers-and-regressors-estimatortransformer","title":"WrapperPipeline strategy for ensembles/inner classifiers and regressors (EstimatorTransformer)\u00b6","text":"<p>Sklearn Pipelines only allow classifiers/regressors as the final step. All other steps are expected to implement a transform function. We can get around this by wrapping it in another transformer class that returns the output of predict or predict_proba inside the transform() function.</p> <p>To wrap classifiers as transfomers, you can use the following class: <code>tpot.builtin_modules.EstimatorTransformer</code>. You can specify whether to pass the outputs of predict, predict_proba, or decision function with the <code>method</code> parameter.</p>"},{"location":"Tutorial/2_Search_Spaces/#cross_val_predict_cv","title":"cross_val_predict_cv\u00b6","text":"<p>An additional consideration is whether or not to use <code>cross_val_predict_cv</code>. If this parameter is set, during model training any classifiers or regressors that is not the final predictor will use <code>sklearn.model_selection.cross_val_predict</code> to pass out of sample predictions into the following steps of the model. The model will still be fit to the full data which will be used for predictions after training. Training downstream models on out of sample predictions can often prevent overfitting and increase performance. The reason is that this gives downstream models a estimate of how upstream models compare on unseen data. Otherwise, if an upsteam model heavily overfits the data, downsteam models may simply learn to blindly trust the seemingly well-predicting model, propagating the over-fitting through to the end result.</p> <p>The downside is that cross_val_predict_cv is significantly more computationally demanding, and may not be necessary for your given dataset.</p> <p>Note: This is not necessary for <code>GraphSearchPipeline</code> as the exported GraphPipeline estimator does have builtin support for inner/regressors. Instead of using a wrapper, you can set the <code>cross_val_predict_cv</code> param when initializing the <code>GraphSearchPipeline</code> object.</p>"},{"location":"Tutorial/2_Search_Spaces/#graphsearchpipeline","title":"GraphSearchPipeline\u00b6","text":"<p>The GraphSearchPipeline is a flexible search space without a prior restriction of pipeline structure. With GraphSearchPipeline, TPOT will create a pipeline in the shape of a directed acyclic graph. Throughout the optimization process, TPOT may add/remove nodes, add/remove edges, and performs model selection and hyperparameter tuning for each node.</p> <p>The primary parameters for the graph_search_space are the root_search_space, inner_search_space, and leaf_search_space.</p> Parameter Type Description root_search_space SklearnIndividualGenerator The search space for the root node of the graph. This node will be the final estimator in the pipeline. inner_search_space SklearnIndividualGenerator, optional The search space for the inner nodes of the graph. If not defined, there will be no inner nodes. leaf_search_space SklearnIndividualGenerator, optional The search space for the leaf nodes of the graph. If not defined, the leaf nodes will be drawn from the inner_search_space. crossover_same_depth bool, optional If True, crossover will only occur between nodes at the same depth in the graph. If False, crossover will occur between nodes at any depth. cross_val_predict_cv int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy used in inner classifiers or regressors. method str, optional The prediction method to use for the inner classifiers or regressors. If 'auto', it will try to use predict_proba, decision_function, or predict in that order. <p>This search space exports a <code>tpot.GraphPipeline</code>. This is similar to a scikit-learn Pipeline, but for directed acyclic graph pipelines. You can learn more about using this module in Tutorial 6.</p>"},{"location":"Tutorial/2_Search_Spaces/#treepipeline","title":"TreePipeline\u00b6","text":""},{"location":"Tutorial/2_Search_Spaces/#tips-and-tricks","title":"Tips and Tricks\u00b6","text":"<ul> <li>Two very helpful transformers to use with search spaces are <code>tpot.buildin_models.Passthrough</code> and <code>tpot.builtin_models.SkipTransformer</code>. Passthrough will simply pass through the exact inputs it receives into the next step. This is particularly useful inside UnionSearchSpace as it allows for both the transformed data as well as the original data to be passed into the next step. SkipTransformer will always return nothing. This is helpful when inside a union with Passthrough and an optional second method. For example, if you are unsure of whether or not you will need a transformer, you can have SkipTransformer be one option that will skip the transformation step if selected.</li> </ul>"},{"location":"Tutorial/2_Search_Spaces/#template-search-spaces","title":"Template Search Spaces\u00b6","text":"<p>As mentioned in Tutorial 1, TPOT has several buildin search spaces. Here is the same table:</p> String Description linear A linear pipeline with the structure of \"Selector-&gt;(transformers+Passthrough)-&gt;(classifiers/regressors+Passthrough)-&gt;final classifier/regressor.\" For both the transformer and inner estimator layers, TPOT may choose one or more transformers/classifiers, or it may choose none. The inner classifier/regressor layer is optional. linear-light Same search space as linear, but without the inner classifier/regressor layer and with a reduced set of faster running estimators. graph TPOT will optimize a pipeline in the shape of a directed acyclic graph. The nodes of the graph can include selectors, scalers, transformers, or classifiers/regressors (inner classifiers/regressors can optionally be not included). This will return a custom GraphPipeline rather than an sklearn Pipeline. More details in Tutorial 6. graph-light Same as graph search space, but without the inner classifier/regressors and with a reduced set of faster running estimators. mdr TPOT will search over a series of feature selectors and Multifactor Dimensionality Reduction models to find a series of operators that maximize prediction accuracy. The TPOT MDR configuration is specialized for genome-wide association studies (GWAS), and is described in detail online here. <p>Rather than create your own search space, you can simply pass the string into the <code>search_space</code> param. Alternatively, you can access tpot.config.<code>template_search_spaces.get_template_search_spaces</code> directly which offers a few more customizable options for each template including <code>cross_val_predict_cv</code> and whether or not stacked classifiers/regressors are allowed. Or you can copy the code and customize it manually!</p> <pre><code>`tpot.config.template_search_spaces.get_template_search_spaces`\nReturns a search space which can be optimized by TPOT.\n\nParameters\n----------\nsearch_space: str or SearchSpace\n    The default search space to use. If a string, it should be one of the following:\n        - 'linear': A search space for linear pipelines\n        - 'linear-light': A search space for linear pipelines with a smaller, faster search space\n        - 'graph': A search space for graph pipelines\n        - 'graph-light': A search space for graph pipelines with a smaller, faster search space\n        - 'mdr': A search space for MDR pipelines\n    If a SearchSpace object, it should be a valid search space object for TPOT.\n\nclassification: bool, default=True\n    Whether the problem is a classification problem or a regression problem.\n\ninner_predictors: bool, default=None\n    Whether to include additional classifiers/regressors before the final classifier/regressor (allowing for ensembles). \n    Defaults to False for 'linear-light' and 'graph-light' search spaces, and True otherwise. (Not used for 'mdr' search space)\n\ncross_val_predict_cv: int, default=None\n    The number of folds to use for cross_val_predict. \n    Defaults to 0 for 'linear-light' and 'graph-light' search spaces, and 5 otherwise. (Not used for 'mdr' search space)\n\nget_search_space_params: dict\n    Additional parameters to pass to the get_search_space function.</code></pre>"},{"location":"Tutorial/2_Search_Spaces/#optimize-search-space-with-tpotestimator","title":"Optimize Search Space with TPOTEstimator\u00b6","text":"<p>Once you have constructed a search space, you can use TPOTEstimator to optimize a pipeline within that space. Simply pass that search space into the <code>search_space</code> parameter. Here is a cell where you can select different search spaces that we created in this tutorial.</p>"},{"location":"Tutorial/2_Search_Spaces/#transformer-only-pipelines-imputation-optimization-example","title":"Transformer-only pipelines - imputation optimization example\u00b6","text":"<p>Pipelines don't necessarily need to end in a classifier or regressor. Transformer only pipelines are possible as long as you have a custom objective function to match.</p>"},{"location":"Tutorial/2_Search_Spaces/#combined-search-space-example","title":"Combined Search Space Example\u00b6","text":""},{"location":"Tutorial/3_Feature_Set_Selector/","title":"Genetic Feature Selection nodes in TPOT","text":"In\u00a0[1]: Copied! <pre>import tpot\nimport pandas as pd\nimport numpy as np\n#make a dataframe with columns a,b,c,d,e,f\n\n#numpy array where columns are 1,2,3,4,5,6\ndata = np.repeat([np.arange(6)],10,0)\n\ndf = pd.DataFrame(data,columns=['a','b','c','d','e','f'])\nfss = tpot.builtin_modules.FeatureSetSelector(name='test',sel_subset=['a','b','c'])\n\nprint(\"original DataFrame\")\nprint(df)\nprint(\"Transformed Data\")\nprint(fss.fit_transform(df))\n</pre> import tpot import pandas as pd import numpy as np #make a dataframe with columns a,b,c,d,e,f  #numpy array where columns are 1,2,3,4,5,6 data = np.repeat([np.arange(6)],10,0)  df = pd.DataFrame(data,columns=['a','b','c','d','e','f']) fss = tpot.builtin_modules.FeatureSetSelector(name='test',sel_subset=['a','b','c'])  print(\"original DataFrame\") print(df) print(\"Transformed Data\") print(fss.fit_transform(df)) <pre>original DataFrame\n   a  b  c  d  e  f\n0  0  1  2  3  4  5\n1  0  1  2  3  4  5\n2  0  1  2  3  4  5\n3  0  1  2  3  4  5\n4  0  1  2  3  4  5\n5  0  1  2  3  4  5\n6  0  1  2  3  4  5\n7  0  1  2  3  4  5\n8  0  1  2  3  4  5\n9  0  1  2  3  4  5\nTransformed Data\n[[0 1 2]\n [0 1 2]\n [0 1 2]\n [0 1 2]\n [0 1 2]\n [0 1 2]\n [0 1 2]\n [0 1 2]\n [0 1 2]\n [0 1 2]]\n</pre> <pre>/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[2]: Copied! <pre>import tpot\nimport sklearn.datasets\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\nimport pandas as pd\nimport tpot\nimport sklearn.datasets\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\nfrom tpot.search_spaces.nodes import *\nfrom tpot.search_spaces.pipelines import *\nfrom tpot.config import get_search_space\n\n\nX, y = sklearn.datasets.make_classification(n_samples=1000, n_features=6, n_informative=6, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)\nX = np.hstack([X, np.random.rand(X.shape[0],6)]) #add six uninformative features\nX = pd.DataFrame(X, columns=['a','b','c','d','e','f','g','h','i', 'j', 'k', 'l']) # a, b ,c the rest are uninformative\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)\n\nX.head()\n</pre> import tpot import sklearn.datasets from sklearn.linear_model import LogisticRegression import numpy as np import pandas as pd import tpot import sklearn.datasets from sklearn.linear_model import LogisticRegression import numpy as np from tpot.search_spaces.nodes import * from tpot.search_spaces.pipelines import * from tpot.config import get_search_space   X, y = sklearn.datasets.make_classification(n_samples=1000, n_features=6, n_informative=6, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None) X = np.hstack([X, np.random.rand(X.shape[0],6)]) #add six uninformative features X = pd.DataFrame(X, columns=['a','b','c','d','e','f','g','h','i', 'j', 'k', 'l']) # a, b ,c the rest are uninformative X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)  X.head() Out[2]: a b c d e f g h i j k l 0 2.315814 -3.427720 -1.314654 -1.508737 -0.300932 0.089448 0.327651 0.329022 0.857495 0.734238 0.257218 0.652350 1 -0.191001 -1.396922 0.149488 -1.730145 -0.394932 0.519712 0.807762 0.509823 0.876159 0.002806 0.449828 0.671350 2 0.661264 -0.981737 0.703879 0.730321 -2.750405 0.396581 0.380302 0.532604 0.877129 0.610919 0.780108 0.625689 3 1.445936 0.354237 0.779040 1.288014 2.397133 0.186324 0.544191 0.465419 0.588535 0.919575 0.513460 0.831546 4 -0.989027 -1.824787 -1.448234 1.546442 1.643775 0.167975 0.188238 0.024149 0.544878 0.834503 0.877869 0.278330 <p>Lets say that either based on prior knowledge or interest, we know that the features can be grouped as follows</p> In\u00a0[3]: Copied! <pre>subsets = { \"group_one\" :  ['a','b','c',],\n            \"group_two\" :  ['d','e','f'],\n            \"group_three\" :  ['g','h','i'],\n            \"group_four\" :  ['j','k','l'],\n            }\n</pre> subsets = { \"group_one\" :  ['a','b','c',],             \"group_two\" :  ['d','e','f'],             \"group_three\" :  ['g','h','i'],             \"group_four\" :  ['j','k','l'],             } <p>We can create an FSSNode that will select from this subset. Each node in a pipeline only selects one subset.</p> In\u00a0[4]: Copied! <pre>fss_search_space = FSSNode(subsets=subsets)\n</pre> fss_search_space = FSSNode(subsets=subsets) <p>If we randomly sample from this search space, we can see that we get a single selector that selects one of the predefined sets. In this case, it selects groups two, which includes ['d', 'e', 'f']. (A random seed was set in the generate function so that the same group would be selected when rerunning the notebook.)</p> In\u00a0[5]: Copied! <pre>fss_selector = fss_search_space.generate(rng=1).export_pipeline()\nfss_selector\n</pre> fss_selector = fss_search_space.generate(rng=1).export_pipeline() fss_selector Out[5]: <pre>FeatureSetSelector(name='group_two', sel_subset=['d', 'e', 'f'])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0FeatureSetSelectoriNot fitted<pre>FeatureSetSelector(name='group_two', sel_subset=['d', 'e', 'f'])</pre> In\u00a0[6]: Copied! <pre>fss_selector.set_output(transform=\"pandas\") #by default sklearn selectors return numpy arrays. this will make it return pandas dataframes\nfss_selector.fit(X_train)\nfss_selector.transform(X_train)\n</pre> fss_selector.set_output(transform=\"pandas\") #by default sklearn selectors return numpy arrays. this will make it return pandas dataframes fss_selector.fit(X_train) fss_selector.transform(X_train) Out[6]: d e f 162 1.315442 -1.039258 0.194516 168 -1.908995 -0.953551 -1.430472 214 0.181162 1.022858 -2.289700 895 2.825765 -1.205520 1.147791 154 -2.300481 1.023173 0.449162 ... ... ... ... 32 -1.793062 2.209649 -0.045031 829 -0.221409 1.688750 0.069356 176 0.141471 -1.880294 1.984397 124 -0.359952 1.141758 2.019301 35 0.171312 0.079332 0.178522 <p>750 rows \u00d7 3 columns</p> <p>Under the hood, mutation will randomly select another feature set and crossover will swap the feature sets selected by two individuals</p> In\u00a0[7]: Copied! <pre>ind1 = fss_search_space.generate(rng=1)\nind1.export_pipeline()\n</pre> ind1 = fss_search_space.generate(rng=1) ind1.export_pipeline() Out[7]: <pre>FeatureSetSelector(name='group_two', sel_subset=['d', 'e', 'f'])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0FeatureSetSelectoriNot fitted<pre>FeatureSetSelector(name='group_two', sel_subset=['d', 'e', 'f'])</pre> In\u00a0[8]: Copied! <pre>ind1.mutate()\nind1.export_pipeline()\n</pre> ind1.mutate() ind1.export_pipeline() Out[8]: <pre>FeatureSetSelector(name='group_four', sel_subset=['j', 'k', 'l'])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0FeatureSetSelectoriNot fitted<pre>FeatureSetSelector(name='group_four', sel_subset=['j', 'k', 'l'])</pre> <p>We can now use this when defining our pipelines. For this first example, we will construct a simple linear pipeline where the first step is a feature set selector, and the second is a classifier</p> In\u00a0[9]: Copied! <pre>classification_search_space = get_search_space([\"RandomForestClassifier\"])\nfss_and_classifier_search_space = SequentialPipeline([fss_search_space, classification_search_space])\n\n\nest = tpot.TPOTEstimator(generations=5, \n                            scorers=[\"roc_auc_ovr\", tpot.objectives.complexity_scorer],\n                            scorers_weights=[1.0, -1.0],\n                            n_jobs=32,\n                            classification=True,\n                            search_space = fss_and_classifier_search_space,\n                            verbose=1,\n                            )\n\n\nscorer = sklearn.metrics.get_scorer('roc_auc_ovr')\nest.fit(X_train, y_train)\nprint(scorer(est, X_test, y_test))\n</pre>  classification_search_space = get_search_space([\"RandomForestClassifier\"]) fss_and_classifier_search_space = SequentialPipeline([fss_search_space, classification_search_space])   est = tpot.TPOTEstimator(generations=5,                              scorers=[\"roc_auc_ovr\", tpot.objectives.complexity_scorer],                             scorers_weights=[1.0, -1.0],                             n_jobs=32,                             classification=True,                             search_space = fss_and_classifier_search_space,                             verbose=1,                             )   scorer = sklearn.metrics.get_scorer('roc_auc_ovr') est.fit(X_train, y_train) print(scorer(est, X_test, y_test)) <pre>/Users/ketrong/Desktop/tpotvalidation/tpot/tpot/tpot_estimator/estimator.py:456: UserWarning: Both generations and max_time_mins are set. TPOT will terminate when the first condition is met.\n  warnings.warn(\"Both generations and max_time_mins are set. TPOT will terminate when the first condition is met.\")\nGeneration: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:36&lt;00:00,  7.26s/it]\n</pre> <pre>0.926166142557652\n</pre> In\u00a0[10]: Copied! <pre>est.fitted_pipeline_\n</pre> est.fitted_pipeline_ Out[10]: <pre>Pipeline(steps=[('featuresetselector',\n                 FeatureSetSelector(name='group_one',\n                                    sel_subset=['a', 'b', 'c'])),\n                ('randomforestclassifier',\n                 RandomForestClassifier(max_features=0.30141491087,\n                                        min_samples_leaf=4,\n                                        min_samples_split=17, n_estimators=128,\n                                        n_jobs=1))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiFitted<pre>Pipeline(steps=[('featuresetselector',\n                 FeatureSetSelector(name='group_one',\n                                    sel_subset=['a', 'b', 'c'])),\n                ('randomforestclassifier',\n                 RandomForestClassifier(max_features=0.30141491087,\n                                        min_samples_leaf=4,\n                                        min_samples_split=17, n_estimators=128,\n                                        n_jobs=1))])</pre> FeatureSetSelector<pre>FeatureSetSelector(name='group_one', sel_subset=['a', 'b', 'c'])</pre> \u00a0RandomForestClassifier?Documentation for RandomForestClassifier<pre>RandomForestClassifier(max_features=0.30141491087, min_samples_leaf=4,\n                       min_samples_split=17, n_estimators=128, n_jobs=1)</pre> <p>With this setup TPOT is able to identify one of the subsets used, but the performance is not optimal. In this case we happen to know that multiple feature sets are required. If we want to include multiple features in our pipelines, we will have to modify our search space. There are three options for this.</p> <ol> <li>UnionPipeline - This allows you to have a fixed number of feature sets selected. If you use a UnionPipeline with two FSSNodes, you will always select two feature sets that are simply concatenated together.</li> <li>DynamicUnionPipeline - This space allows multiple FSSNodes to be selected. Unlike UnionPipeline you don't have to specify the number of selected sets, TPOT will identify the number of sets that are optimal. Additionally, with DynamicUnionPipeline, the same feature set cannot be selected twice. Note that while DynamicUnionPipeline can select multiple feature sets, it never mixes two feature sets together.</li> <li>GraphSearchPipeline - When set as the leave_search_space, GraphSearchPipeline can also select multiple FSSNodes which act as an input to the rest of the pipeline.</li> </ol> In\u00a0[11]: Copied! <pre>union_fss_space = UnionPipeline([fss_search_space, fss_search_space])\n</pre> union_fss_space = UnionPipeline([fss_search_space, fss_search_space]) In\u00a0[12]: Copied! <pre># this union search space will always select exactly two fss_search_space\nselector1 = union_fss_space.generate(rng=1).export_pipeline()\nselector1\n</pre> # this union search space will always select exactly two fss_search_space selector1 = union_fss_space.generate(rng=1).export_pipeline() selector1 Out[12]: <pre>FeatureUnion(transformer_list=[('featuresetselector-1',\n                                FeatureSetSelector(name='group_two',\n                                                   sel_subset=['d', 'e', 'f'])),\n                               ('featuresetselector-2',\n                                FeatureSetSelector(name='group_three',\n                                                   sel_subset=['g', 'h',\n                                                               'i']))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0FeatureUnion?Documentation for FeatureUnioniNot fitted<pre>FeatureUnion(transformer_list=[('featuresetselector-1',\n                                FeatureSetSelector(name='group_two',\n                                                   sel_subset=['d', 'e', 'f'])),\n                               ('featuresetselector-2',\n                                FeatureSetSelector(name='group_three',\n                                                   sel_subset=['g', 'h',\n                                                               'i']))])</pre> featuresetselector-1FeatureSetSelector<pre>FeatureSetSelector(name='group_two', sel_subset=['d', 'e', 'f'])</pre> featuresetselector-2FeatureSetSelector<pre>FeatureSetSelector(name='group_three', sel_subset=['g', 'h', 'i'])</pre> In\u00a0[13]: Copied! <pre>selector1.set_output(transform=\"pandas\") \nselector1.fit(X_train)\nselector1.transform(X_train)\n</pre> selector1.set_output(transform=\"pandas\")  selector1.fit(X_train) selector1.transform(X_train) Out[13]: d e f g h i 162 1.315442 -1.039258 0.194516 0.751175 0.411340 0.824754 168 -1.908995 -0.953551 -1.430472 0.072697 0.875766 0.953255 214 0.181162 1.022858 -2.289700 0.135222 0.395847 0.232638 895 2.825765 -1.205520 1.147791 0.925905 0.486645 0.710991 154 -2.300481 1.023173 0.449162 0.645161 0.131657 0.863514 ... ... ... ... ... ... ... 32 -1.793062 2.209649 -0.045031 0.502947 0.994603 0.280062 829 -0.221409 1.688750 0.069356 0.328066 0.102381 0.492280 176 0.141471 -1.880294 1.984397 0.365550 0.465859 0.974601 124 -0.359952 1.141758 2.019301 0.329380 0.718647 0.365507 35 0.171312 0.079332 0.178522 0.215759 0.546279 0.662928 <p>750 rows \u00d7 6 columns</p> In\u00a0[14]: Copied! <pre>dynamic_fss_space = DynamicUnionPipeline(fss_search_space)\ndynamic_fss_space.generate(rng=1).export_pipeline()\n</pre> dynamic_fss_space = DynamicUnionPipeline(fss_search_space) dynamic_fss_space.generate(rng=1).export_pipeline() Out[14]: <pre>FeatureUnion(transformer_list=[('featuresetselector',\n                                FeatureSetSelector(name='group_three',\n                                                   sel_subset=['g', 'h',\n                                                               'i']))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0FeatureUnion?Documentation for FeatureUnioniNot fitted<pre>FeatureUnion(transformer_list=[('featuresetselector',\n                                FeatureSetSelector(name='group_three',\n                                                   sel_subset=['g', 'h',\n                                                               'i']))])</pre> featuresetselectorFeatureSetSelector<pre>FeatureSetSelector(name='group_three', sel_subset=['g', 'h', 'i'])</pre> In\u00a0[15]: Copied! <pre>dynamic_fss_space.generate(rng=3).export_pipeline()\n</pre> dynamic_fss_space.generate(rng=3).export_pipeline() Out[15]: <pre>FeatureUnion(transformer_list=[('featuresetselector-1',\n                                FeatureSetSelector(name='group_one',\n                                                   sel_subset=['a', 'b', 'c'])),\n                               ('featuresetselector-2',\n                                FeatureSetSelector(name='group_four',\n                                                   sel_subset=['j', 'k',\n                                                               'l']))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0FeatureUnion?Documentation for FeatureUnioniNot fitted<pre>FeatureUnion(transformer_list=[('featuresetselector-1',\n                                FeatureSetSelector(name='group_one',\n                                                   sel_subset=['a', 'b', 'c'])),\n                               ('featuresetselector-2',\n                                FeatureSetSelector(name='group_four',\n                                                   sel_subset=['j', 'k',\n                                                               'l']))])</pre> featuresetselector-1FeatureSetSelector<pre>FeatureSetSelector(name='group_one', sel_subset=['a', 'b', 'c'])</pre> featuresetselector-2FeatureSetSelector<pre>FeatureSetSelector(name='group_four', sel_subset=['j', 'k', 'l'])</pre> In\u00a0[16]: Copied! <pre>graph_search_space = tpot.search_spaces.pipelines.GraphSearchPipeline(\n    leaf_search_space = fss_search_space,\n    inner_search_space = tpot.config.get_search_space([\"transformers\"]),\n    root_search_space= tpot.config.get_search_space([\"KNeighborsClassifier\", \"LogisticRegression\", \"DecisionTreeClassifier\"]),\n    max_size = 10,\n)\n\ngraph_search_space.generate(rng=4).export_pipeline().plot()\n</pre> graph_search_space = tpot.search_spaces.pipelines.GraphSearchPipeline(     leaf_search_space = fss_search_space,     inner_search_space = tpot.config.get_search_space([\"transformers\"]),     root_search_space= tpot.config.get_search_space([\"KNeighborsClassifier\", \"LogisticRegression\", \"DecisionTreeClassifier\"]),     max_size = 10, )  graph_search_space.generate(rng=4).export_pipeline().plot() In\u00a0[17]: Copied! <pre>import tpot\nimport sklearn.datasets\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n\n\nfinal_classification_search_space = SequentialPipeline([dynamic_fss_space, classification_search_space])\n\nest = tpot.TPOTEstimator(generations=5, \n                            scorers=[\"roc_auc_ovr\", tpot.objectives.complexity_scorer],\n                            scorers_weights=[1.0, -1.0],\n                            n_jobs=32,\n                            classification=True,\n                            search_space = final_classification_search_space,\n                            verbose=1,\n                            )\n\n\nscorer = sklearn.metrics.get_scorer('roc_auc_ovr')\n\nest.fit(X_train, y_train)\nprint(scorer(est, X_test, y_test))\n</pre> import tpot import sklearn.datasets from sklearn.linear_model import LogisticRegression import numpy as np   final_classification_search_space = SequentialPipeline([dynamic_fss_space, classification_search_space])  est = tpot.TPOTEstimator(generations=5,                              scorers=[\"roc_auc_ovr\", tpot.objectives.complexity_scorer],                             scorers_weights=[1.0, -1.0],                             n_jobs=32,                             classification=True,                             search_space = final_classification_search_space,                             verbose=1,                             )   scorer = sklearn.metrics.get_scorer('roc_auc_ovr')  est.fit(X_train, y_train) print(scorer(est, X_test, y_test)) <pre>/Users/ketrong/Desktop/tpotvalidation/tpot/tpot/tpot_estimator/estimator.py:456: UserWarning: Both generations and max_time_mins are set. TPOT will terminate when the first condition is met.\n  warnings.warn(\"Both generations and max_time_mins are set. TPOT will terminate when the first condition is met.\")\nGeneration: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:41&lt;00:00,  8.33s/it]\n</pre> <pre>0.9838836477987423\n</pre> <p>We can see that this pipeline performed slightly better and correctly identified group one and group two as the feature sets used in the generative equation.</p> In\u00a0[18]: Copied! <pre>est.fitted_pipeline_\n</pre> est.fitted_pipeline_ Out[18]: <pre>Pipeline(steps=[('featureunion',\n                 FeatureUnion(transformer_list=[('featuresetselector-1',\n                                                 FeatureSetSelector(name='group_two',\n                                                                    sel_subset=['d',\n                                                                                'e',\n                                                                                'f'])),\n                                                ('featuresetselector-2',\n                                                 FeatureSetSelector(name='group_one',\n                                                                    sel_subset=['a',\n                                                                                'b',\n                                                                                'c']))])),\n                ('randomforestclassifier',\n                 RandomForestClassifier(max_features=0.0530704381152,\n                                        min_samples_leaf=2, min_samples_split=5,\n                                        n_estimators=128, n_jobs=1))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiFitted<pre>Pipeline(steps=[('featureunion',\n                 FeatureUnion(transformer_list=[('featuresetselector-1',\n                                                 FeatureSetSelector(name='group_two',\n                                                                    sel_subset=['d',\n                                                                                'e',\n                                                                                'f'])),\n                                                ('featuresetselector-2',\n                                                 FeatureSetSelector(name='group_one',\n                                                                    sel_subset=['a',\n                                                                                'b',\n                                                                                'c']))])),\n                ('randomforestclassifier',\n                 RandomForestClassifier(max_features=0.0530704381152,\n                                        min_samples_leaf=2, min_samples_split=5,\n                                        n_estimators=128, n_jobs=1))])</pre> \u00a0featureunion: FeatureUnion?Documentation for featureunion: FeatureUnion<pre>FeatureUnion(transformer_list=[('featuresetselector-1',\n                                FeatureSetSelector(name='group_two',\n                                                   sel_subset=['d', 'e', 'f'])),\n                               ('featuresetselector-2',\n                                FeatureSetSelector(name='group_one',\n                                                   sel_subset=['a', 'b',\n                                                               'c']))])</pre> featuresetselector-1FeatureSetSelector<pre>FeatureSetSelector(name='group_two', sel_subset=['d', 'e', 'f'])</pre> featuresetselector-2FeatureSetSelector<pre>FeatureSetSelector(name='group_one', sel_subset=['a', 'b', 'c'])</pre> \u00a0RandomForestClassifier?Documentation for RandomForestClassifier<pre>RandomForestClassifier(max_features=0.0530704381152, min_samples_leaf=2,\n                       min_samples_split=5, n_estimators=128, n_jobs=1)</pre> In\u00a0[21]: Copied! <pre>linear_search_space = tpot.config.template_search_spaces.get_template_search_spaces(\"linear\", classification=True)\nfss_and_linear_search_space = SequentialPipeline([fss_search_space, linear_search_space])\n\n# est = tpot.TPOTEstimator(  \n#                            population_size=32,\n#                            generations=10, \n#                            scorers=[\"roc_auc_ovr\", tpot.objectives.complexity_scorer],\n#                            scorers_weights=[1.0, -1.0],\n#                            other_objective_functions=[number_of_selected_features],\n#                            other_objective_functions_weights = [-1],\n#                            objective_function_names = [\"Number of selected features\"],\n\n#                            n_jobs=32,\n#                            classification=True,\n#                            search_space = fss_and_linear_search_space,\n#                            verbose=2,\n#                             )\n\nfss_and_linear_search_space.generate(rng=1).export_pipeline()\n</pre> linear_search_space = tpot.config.template_search_spaces.get_template_search_spaces(\"linear\", classification=True) fss_and_linear_search_space = SequentialPipeline([fss_search_space, linear_search_space])  # est = tpot.TPOTEstimator(   #                            population_size=32, #                            generations=10,  #                            scorers=[\"roc_auc_ovr\", tpot.objectives.complexity_scorer], #                            scorers_weights=[1.0, -1.0], #                            other_objective_functions=[number_of_selected_features], #                            other_objective_functions_weights = [-1], #                            objective_function_names = [\"Number of selected features\"],  #                            n_jobs=32, #                            classification=True, #                            search_space = fss_and_linear_search_space, #                            verbose=2, #                             )  fss_and_linear_search_space.generate(rng=1).export_pipeline() Out[21]: <pre>Pipeline(steps=[('featuresetselector',\n                 FeatureSetSelector(name='group_two',\n                                    sel_subset=['d', 'e', 'f'])),\n                ('pipeline',\n                 Pipeline(steps=[('maxabsscaler', MaxAbsScaler()),\n                                 ('rfe',\n                                  RFE(estimator=ExtraTreesClassifier(max_features=0.0390676831531,\n                                                                     min_samples_leaf=8,\n                                                                     min_samples_split=14,\n                                                                     n_jobs=1),\n                                      step=0.753983388654)),\n                                 ('featureunion-1',\n                                  FeatureUnion(transformer_lis...\n                                  FeatureUnion(transformer_list=[('skiptransformer',\n                                                                  SkipTransformer()),\n                                                                 ('passthrough',\n                                                                  Passthrough())])),\n                                 ('histgradientboostingclassifier',\n                                  HistGradientBoostingClassifier(early_stopping=True,\n                                                                 l2_regularization=9.1304e-09,\n                                                                 learning_rate=0.0036310282582,\n                                                                 max_features=0.238877814721,\n                                                                 max_leaf_nodes=1696,\n                                                                 min_samples_leaf=59,\n                                                                 n_iter_no_change=14,\n                                                                 tol=0.0001,\n                                                                 validation_fraction=None))]))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('featuresetselector',\n                 FeatureSetSelector(name='group_two',\n                                    sel_subset=['d', 'e', 'f'])),\n                ('pipeline',\n                 Pipeline(steps=[('maxabsscaler', MaxAbsScaler()),\n                                 ('rfe',\n                                  RFE(estimator=ExtraTreesClassifier(max_features=0.0390676831531,\n                                                                     min_samples_leaf=8,\n                                                                     min_samples_split=14,\n                                                                     n_jobs=1),\n                                      step=0.753983388654)),\n                                 ('featureunion-1',\n                                  FeatureUnion(transformer_lis...\n                                  FeatureUnion(transformer_list=[('skiptransformer',\n                                                                  SkipTransformer()),\n                                                                 ('passthrough',\n                                                                  Passthrough())])),\n                                 ('histgradientboostingclassifier',\n                                  HistGradientBoostingClassifier(early_stopping=True,\n                                                                 l2_regularization=9.1304e-09,\n                                                                 learning_rate=0.0036310282582,\n                                                                 max_features=0.238877814721,\n                                                                 max_leaf_nodes=1696,\n                                                                 min_samples_leaf=59,\n                                                                 n_iter_no_change=14,\n                                                                 tol=0.0001,\n                                                                 validation_fraction=None))]))])</pre> FeatureSetSelector<pre>FeatureSetSelector(name='group_two', sel_subset=['d', 'e', 'f'])</pre> \u00a0pipeline: Pipeline?Documentation for pipeline: Pipeline<pre>Pipeline(steps=[('maxabsscaler', MaxAbsScaler()),\n                ('rfe',\n                 RFE(estimator=ExtraTreesClassifier(max_features=0.0390676831531,\n                                                    min_samples_leaf=8,\n                                                    min_samples_split=14,\n                                                    n_jobs=1),\n                     step=0.753983388654)),\n                ('featureunion-1',\n                 FeatureUnion(transformer_list=[('featureunion',\n                                                 FeatureUnion(transformer_list=[('columnordinalencoder',\n                                                                                 ColumnOrdinalEncoder()),\n                                                                                ('pca',\n                                                                                 PCA(n_co...\n                 FeatureUnion(transformer_list=[('skiptransformer',\n                                                 SkipTransformer()),\n                                                ('passthrough',\n                                                 Passthrough())])),\n                ('histgradientboostingclassifier',\n                 HistGradientBoostingClassifier(early_stopping=True,\n                                                l2_regularization=9.1304e-09,\n                                                learning_rate=0.0036310282582,\n                                                max_features=0.238877814721,\n                                                max_leaf_nodes=1696,\n                                                min_samples_leaf=59,\n                                                n_iter_no_change=14, tol=0.0001,\n                                                validation_fraction=None))])</pre> \u00a0MaxAbsScaler?Documentation for MaxAbsScaler<pre>MaxAbsScaler()</pre> \u00a0rfe: RFE?Documentation for rfe: RFE<pre>RFE(estimator=ExtraTreesClassifier(max_features=0.0390676831531,\n                                   min_samples_leaf=8, min_samples_split=14,\n                                   n_jobs=1),\n    step=0.753983388654)</pre> estimator: ExtraTreesClassifier<pre>ExtraTreesClassifier(max_features=0.0390676831531, min_samples_leaf=8,\n                     min_samples_split=14, n_jobs=1)</pre> \u00a0ExtraTreesClassifier?Documentation for ExtraTreesClassifier<pre>ExtraTreesClassifier(max_features=0.0390676831531, min_samples_leaf=8,\n                     min_samples_split=14, n_jobs=1)</pre> \u00a0featureunion-1: FeatureUnion?Documentation for featureunion-1: FeatureUnion<pre>FeatureUnion(transformer_list=[('featureunion',\n                                FeatureUnion(transformer_list=[('columnordinalencoder',\n                                                                ColumnOrdinalEncoder()),\n                                                               ('pca',\n                                                                PCA(n_components=0.9286371732844))])),\n                               ('passthrough', Passthrough())])</pre> featureunioncolumnordinalencoderColumnOrdinalEncoder<pre>ColumnOrdinalEncoder()</pre> pca\u00a0PCA?Documentation for PCA<pre>PCA(n_components=0.9286371732844)</pre> passthroughPassthrough<pre>Passthrough()</pre> \u00a0featureunion-2: FeatureUnion?Documentation for featureunion-2: FeatureUnion<pre>FeatureUnion(transformer_list=[('skiptransformer', SkipTransformer()),\n                               ('passthrough', Passthrough())])</pre> skiptransformerSkipTransformer<pre>SkipTransformer()</pre> passthroughPassthrough<pre>Passthrough()</pre> \u00a0HistGradientBoostingClassifier?Documentation for HistGradientBoostingClassifier<pre>HistGradientBoostingClassifier(early_stopping=True,\n                               l2_regularization=9.1304e-09,\n                               learning_rate=0.0036310282582,\n                               max_features=0.238877814721, max_leaf_nodes=1696,\n                               min_samples_leaf=59, n_iter_no_change=14,\n                               tol=0.0001, validation_fraction=None)</pre> In\u00a0[22]: Copied! <pre>dynamic_transformers = DynamicUnionPipeline(get_search_space(\"all_transformers\"), max_estimators=4)\ndynamic_transformers_with_passthrough = tpot.search_spaces.pipelines.UnionPipeline([\n    dynamic_transformers,\n    tpot.config.get_search_space(\"Passthrough\")],\n    )\nmulti_step_engineering = DynamicLinearPipeline(dynamic_transformers_with_passthrough, max_length=4)\nfss_engineering_search_space = SequentialPipeline([fss_search_space, multi_step_engineering])\nunion_fss_engineering_search_space = DynamicUnionPipeline(fss_engineering_search_space)\n\nfinal_fancy_search_space = SequentialPipeline([union_fss_engineering_search_space, classification_search_space])\n</pre> dynamic_transformers = DynamicUnionPipeline(get_search_space(\"all_transformers\"), max_estimators=4) dynamic_transformers_with_passthrough = tpot.search_spaces.pipelines.UnionPipeline([     dynamic_transformers,     tpot.config.get_search_space(\"Passthrough\")],     ) multi_step_engineering = DynamicLinearPipeline(dynamic_transformers_with_passthrough, max_length=4) fss_engineering_search_space = SequentialPipeline([fss_search_space, multi_step_engineering]) union_fss_engineering_search_space = DynamicUnionPipeline(fss_engineering_search_space)  final_fancy_search_space = SequentialPipeline([union_fss_engineering_search_space, classification_search_space]) In\u00a0[23]: Copied! <pre>final_fancy_search_space.generate(rng=3).export_pipeline()\n</pre> final_fancy_search_space.generate(rng=3).export_pipeline() Out[23]: <pre>Pipeline(steps=[('featureunion',\n                 FeatureUnion(transformer_list=[('pipeline-1',\n                                                 Pipeline(steps=[('featuresetselector',\n                                                                  FeatureSetSelector(name='group_one',\n                                                                                     sel_subset=['a',\n                                                                                                 'b',\n                                                                                                 'c'])),\n                                                                 ('pipeline',\n                                                                  Pipeline(steps=[('featureunion',\n                                                                                   FeatureUnion(transformer_list=[('featureunion',\n                                                                                                                   FeatureUnion(transformer_list=[('zerocount',\n                                                                                                                                                   ZeroCount())])),\n                                                                                                                  ('passthrough',\n                                                                                                                   Passth...\n                                                                                                                                                   KBinsDiscretizer(encode='onehot-dense',\n                                                                                                                                                                    n_bins=11)),\n                                                                                                                                                  ('rbfsampler',\n                                                                                                                                                   RBFSampler(gamma=0.0925899621466,\n                                                                                                                                                              n_components=17)),\n                                                                                                                                                  ('maxabsscaler',\n                                                                                                                                                   MaxAbsScaler())])),\n                                                                                                                  ('passthrough',\n                                                                                                                   Passthrough())]))]))]))])),\n                ('randomforestclassifier',\n                 RandomForestClassifier(bootstrap=False,\n                                        class_weight='balanced',\n                                        max_features=0.8205760841606,\n                                        min_samples_leaf=16,\n                                        min_samples_split=11, n_estimators=128,\n                                        n_jobs=1))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('featureunion',\n                 FeatureUnion(transformer_list=[('pipeline-1',\n                                                 Pipeline(steps=[('featuresetselector',\n                                                                  FeatureSetSelector(name='group_one',\n                                                                                     sel_subset=['a',\n                                                                                                 'b',\n                                                                                                 'c'])),\n                                                                 ('pipeline',\n                                                                  Pipeline(steps=[('featureunion',\n                                                                                   FeatureUnion(transformer_list=[('featureunion',\n                                                                                                                   FeatureUnion(transformer_list=[('zerocount',\n                                                                                                                                                   ZeroCount())])),\n                                                                                                                  ('passthrough',\n                                                                                                                   Passth...\n                                                                                                                                                   KBinsDiscretizer(encode='onehot-dense',\n                                                                                                                                                                    n_bins=11)),\n                                                                                                                                                  ('rbfsampler',\n                                                                                                                                                   RBFSampler(gamma=0.0925899621466,\n                                                                                                                                                              n_components=17)),\n                                                                                                                                                  ('maxabsscaler',\n                                                                                                                                                   MaxAbsScaler())])),\n                                                                                                                  ('passthrough',\n                                                                                                                   Passthrough())]))]))]))])),\n                ('randomforestclassifier',\n                 RandomForestClassifier(bootstrap=False,\n                                        class_weight='balanced',\n                                        max_features=0.8205760841606,\n                                        min_samples_leaf=16,\n                                        min_samples_split=11, n_estimators=128,\n                                        n_jobs=1))])</pre> \u00a0featureunion: FeatureUnion?Documentation for featureunion: FeatureUnion<pre>FeatureUnion(transformer_list=[('pipeline-1',\n                                Pipeline(steps=[('featuresetselector',\n                                                 FeatureSetSelector(name='group_one',\n                                                                    sel_subset=['a',\n                                                                                'b',\n                                                                                'c'])),\n                                                ('pipeline',\n                                                 Pipeline(steps=[('featureunion',\n                                                                  FeatureUnion(transformer_list=[('featureunion',\n                                                                                                  FeatureUnion(transformer_list=[('zerocount',\n                                                                                                                                  ZeroCount())])),\n                                                                                                 ('passthrough',\n                                                                                                  Passthrough())]))]))])),\n                               ('pipeline-2',...\n                                                                                                                                  PCA(n_components=0.9470333477868))])),\n                                                                                                 ('passthrough',\n                                                                                                  Passthrough())])),\n                                                                 ('featureunion-3',\n                                                                  FeatureUnion(transformer_list=[('featureunion',\n                                                                                                  FeatureUnion(transformer_list=[('kbinsdiscretizer',\n                                                                                                                                  KBinsDiscretizer(encode='onehot-dense',\n                                                                                                                                                   n_bins=11)),\n                                                                                                                                 ('rbfsampler',\n                                                                                                                                  RBFSampler(gamma=0.0925899621466,\n                                                                                                                                             n_components=17)),\n                                                                                                                                 ('maxabsscaler',\n                                                                                                                                  MaxAbsScaler())])),\n                                                                                                 ('passthrough',\n                                                                                                  Passthrough())]))]))]))])</pre> pipeline-1FeatureSetSelector<pre>FeatureSetSelector(name='group_one', sel_subset=['a', 'b', 'c'])</pre> \u00a0pipeline: Pipeline?Documentation for pipeline: Pipeline<pre>Pipeline(steps=[('featureunion',\n                 FeatureUnion(transformer_list=[('featureunion',\n                                                 FeatureUnion(transformer_list=[('zerocount',\n                                                                                 ZeroCount())])),\n                                                ('passthrough',\n                                                 Passthrough())]))])</pre> \u00a0featureunion: FeatureUnion?Documentation for featureunion: FeatureUnion<pre>FeatureUnion(transformer_list=[('featureunion',\n                                FeatureUnion(transformer_list=[('zerocount',\n                                                                ZeroCount())])),\n                               ('passthrough', Passthrough())])</pre> featureunionzerocountZeroCount<pre>ZeroCount()</pre> passthroughPassthrough<pre>Passthrough()</pre> pipeline-2FeatureSetSelector<pre>FeatureSetSelector(name='group_four', sel_subset=['j', 'k', 'l'])</pre> \u00a0pipeline: Pipeline?Documentation for pipeline: Pipeline<pre>Pipeline(steps=[('featureunion-1',\n                 FeatureUnion(transformer_list=[('featureunion',\n                                                 FeatureUnion(transformer_list=[('kbinsdiscretizer',\n                                                                                 KBinsDiscretizer(encode='onehot-dense',\n                                                                                                  n_bins=37,\n                                                                                                  strategy='kmeans')),\n                                                                                ('featureagglomeration',\n                                                                                 FeatureAgglomeration(n_clusters=31))])),\n                                                ('passthrough',\n                                                 Passthrough())])),\n                ('featureunion-2',\n                 FeatureUnion(transformer_list=[('f...\n                                                                                 PCA(n_components=0.9470333477868))])),\n                                                ('passthrough',\n                                                 Passthrough())])),\n                ('featureunion-3',\n                 FeatureUnion(transformer_list=[('featureunion',\n                                                 FeatureUnion(transformer_list=[('kbinsdiscretizer',\n                                                                                 KBinsDiscretizer(encode='onehot-dense',\n                                                                                                  n_bins=11)),\n                                                                                ('rbfsampler',\n                                                                                 RBFSampler(gamma=0.0925899621466,\n                                                                                            n_components=17)),\n                                                                                ('maxabsscaler',\n                                                                                 MaxAbsScaler())])),\n                                                ('passthrough',\n                                                 Passthrough())]))])</pre> \u00a0featureunion-1: FeatureUnion?Documentation for featureunion-1: FeatureUnion<pre>FeatureUnion(transformer_list=[('featureunion',\n                                FeatureUnion(transformer_list=[('kbinsdiscretizer',\n                                                                KBinsDiscretizer(encode='onehot-dense',\n                                                                                 n_bins=37,\n                                                                                 strategy='kmeans')),\n                                                               ('featureagglomeration',\n                                                                FeatureAgglomeration(n_clusters=31))])),\n                               ('passthrough', Passthrough())])</pre> featureunionkbinsdiscretizer\u00a0KBinsDiscretizer?Documentation for KBinsDiscretizer<pre>KBinsDiscretizer(encode='onehot-dense', n_bins=37, strategy='kmeans')</pre> featureagglomeration\u00a0FeatureAgglomeration?Documentation for FeatureAgglomeration<pre>FeatureAgglomeration(n_clusters=31)</pre> passthroughPassthrough<pre>Passthrough()</pre> \u00a0featureunion-2: FeatureUnion?Documentation for featureunion-2: FeatureUnion<pre>FeatureUnion(transformer_list=[('featureunion',\n                                FeatureUnion(transformer_list=[('quantiletransformer',\n                                                                QuantileTransformer(n_quantiles=840,\n                                                                                    output_distribution='normal')),\n                                                               ('pca',\n                                                                PCA(n_components=0.9470333477868))])),\n                               ('passthrough', Passthrough())])</pre> featureunionquantiletransformer\u00a0QuantileTransformer?Documentation for QuantileTransformer<pre>QuantileTransformer(n_quantiles=840, output_distribution='normal')</pre> pca\u00a0PCA?Documentation for PCA<pre>PCA(n_components=0.9470333477868)</pre> passthroughPassthrough<pre>Passthrough()</pre> \u00a0featureunion-3: FeatureUnion?Documentation for featureunion-3: FeatureUnion<pre>FeatureUnion(transformer_list=[('featureunion',\n                                FeatureUnion(transformer_list=[('kbinsdiscretizer',\n                                                                KBinsDiscretizer(encode='onehot-dense',\n                                                                                 n_bins=11)),\n                                                               ('rbfsampler',\n                                                                RBFSampler(gamma=0.0925899621466,\n                                                                           n_components=17)),\n                                                               ('maxabsscaler',\n                                                                MaxAbsScaler())])),\n                               ('passthrough', Passthrough())])</pre> featureunionkbinsdiscretizer\u00a0KBinsDiscretizer?Documentation for KBinsDiscretizer<pre>KBinsDiscretizer(encode='onehot-dense', n_bins=11)</pre> rbfsampler\u00a0RBFSampler?Documentation for RBFSampler<pre>RBFSampler(gamma=0.0925899621466, n_components=17)</pre> maxabsscaler\u00a0MaxAbsScaler?Documentation for MaxAbsScaler<pre>MaxAbsScaler()</pre> passthroughPassthrough<pre>Passthrough()</pre> \u00a0RandomForestClassifier?Documentation for RandomForestClassifier<pre>RandomForestClassifier(bootstrap=False, class_weight='balanced',\n                       max_features=0.8205760841606, min_samples_leaf=16,\n                       min_samples_split=11, n_estimators=128, n_jobs=1)</pre> In\u00a0[24]: Copied! <pre>import tpot\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nimport sklearn\n\nsubsets = { \"group_one\" :  ['a','b','c'],\n            \"group_two\" :  ['d','e','f'],\n            \"group_three\" :  ['g','h','i'],\n            }\n\nfss_search_space = tpot.search_spaces.nodes.FSSNode(subsets=subsets)\n\nselector = fss_search_space.generate(rng=1).export_pipeline()\nselector.set_output(transform=\"pandas\")\nselector.fit(X_train)\nselector.transform(X_train)\n</pre> import tpot import pandas as pd import numpy as np from sklearn.linear_model import LogisticRegression import sklearn  subsets = { \"group_one\" :  ['a','b','c'],             \"group_two\" :  ['d','e','f'],             \"group_three\" :  ['g','h','i'],             }  fss_search_space = tpot.search_spaces.nodes.FSSNode(subsets=subsets)  selector = fss_search_space.generate(rng=1).export_pipeline() selector.set_output(transform=\"pandas\") selector.fit(X_train) selector.transform(X_train) Out[24]: d e f 162 1.315442 -1.039258 0.194516 168 -1.908995 -0.953551 -1.430472 214 0.181162 1.022858 -2.289700 895 2.825765 -1.205520 1.147791 154 -2.300481 1.023173 0.449162 ... ... ... ... 32 -1.793062 2.209649 -0.045031 829 -0.221409 1.688750 0.069356 176 0.141471 -1.880294 1.984397 124 -0.359952 1.141758 2.019301 35 0.171312 0.079332 0.178522 <p>750 rows \u00d7 3 columns</p> In\u00a0[25]: Copied! <pre>import tpot\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nimport sklearn\n\nsubsets = [['a','b','c'],['d','e','f'],['g','h','i']]\n\nfss_search_space = tpot.search_spaces.nodes.FSSNode(subsets=subsets)\n\nselector = fss_search_space.generate(rng=1).export_pipeline()\nselector.set_output(transform=\"pandas\")\nselector.fit(X_train)\nselector.transform(X_train)\n</pre> import tpot import pandas as pd import numpy as np from sklearn.linear_model import LogisticRegression import sklearn  subsets = [['a','b','c'],['d','e','f'],['g','h','i']]  fss_search_space = tpot.search_spaces.nodes.FSSNode(subsets=subsets)  selector = fss_search_space.generate(rng=1).export_pipeline() selector.set_output(transform=\"pandas\") selector.fit(X_train) selector.transform(X_train) Out[25]: d e f 162 1.315442 -1.039258 0.194516 168 -1.908995 -0.953551 -1.430472 214 0.181162 1.022858 -2.289700 895 2.825765 -1.205520 1.147791 154 -2.300481 1.023173 0.449162 ... ... ... ... 32 -1.793062 2.209649 -0.045031 829 -0.221409 1.688750 0.069356 176 0.141471 -1.880294 1.984397 124 -0.359952 1.141758 2.019301 35 0.171312 0.079332 0.178522 <p>750 rows \u00d7 3 columns</p> In\u00a0[26]: Copied! <pre>import tpot\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nimport sklearn\n\nsubsets = 'simple_fss.csv'\n'''\n# simple_fss.csv\none,a,b,c\ntwo,d,e,f\nthree,g,h,i\n'''\n\nfss_search_space = tpot.search_spaces.nodes.FSSNode(subsets=subsets)\n\nselector = fss_search_space.generate(rng=1).export_pipeline()\nselector.set_output(transform=\"pandas\")\nselector.fit(X_train)\nselector.transform(X_train)\n</pre> import tpot import pandas as pd import numpy as np from sklearn.linear_model import LogisticRegression import sklearn  subsets = 'simple_fss.csv' ''' # simple_fss.csv one,a,b,c two,d,e,f three,g,h,i '''  fss_search_space = tpot.search_spaces.nodes.FSSNode(subsets=subsets)  selector = fss_search_space.generate(rng=1).export_pipeline() selector.set_output(transform=\"pandas\") selector.fit(X_train) selector.transform(X_train) Out[26]: d e f 162 1.315442 -1.039258 0.194516 168 -1.908995 -0.953551 -1.430472 214 0.181162 1.022858 -2.289700 895 2.825765 -1.205520 1.147791 154 -2.300481 1.023173 0.449162 ... ... ... ... 32 -1.793062 2.209649 -0.045031 829 -0.221409 1.688750 0.069356 176 0.141471 -1.880294 1.984397 124 -0.359952 1.141758 2.019301 35 0.171312 0.079332 0.178522 <p>750 rows \u00d7 3 columns</p> <p>All of the above is the same when using numpy data, but the column names are replaced int indexes.</p> In\u00a0[27]: Copied! <pre>import tpot\nimport sklearn.datasets\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\nimport pandas as pd\n\nn_features = 6\nX, y = sklearn.datasets.make_classification(n_samples=1000, n_features=n_features, n_informative=6, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)\nX = np.hstack([X, np.random.rand(X.shape[0],3)]) #add three uninformative features\n\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)\n\nprint(X)\n</pre> import tpot import sklearn.datasets from sklearn.linear_model import LogisticRegression import numpy as np import pandas as pd  n_features = 6 X, y = sklearn.datasets.make_classification(n_samples=1000, n_features=n_features, n_informative=6, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None) X = np.hstack([X, np.random.rand(X.shape[0],3)]) #add three uninformative features  X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)  print(X) <pre>[[-0.31748616  2.20805859 -2.21719911 ...  0.5595234   0.80605806\n   0.41484993]\n [ 2.8673731   1.45905176 -1.11516833 ...  0.74646156  0.95635356\n   0.03575697]\n [-1.64867116  2.14478724  2.31196119 ...  0.22969172  0.72447325\n   0.81842014]\n ...\n [ 1.17772695  0.7188885  -0.52548496 ...  0.99266968  0.95436462\n   0.57430922]\n [ 0.14052568  0.15042817 -0.86281564 ...  0.25379746  0.1818071\n   0.55993116]\n [ 1.37273916 -0.14898886 -0.89938251 ...  0.767549    0.66184827\n   0.49174333]]\n</pre> In\u00a0[28]: Copied! <pre>import tpot\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nimport sklearn\n\nsubsets = { \"group_one\" :  [0,1,2],\n            \"group_two\" :  [3,4,5],\n            \"group_three\" :  [6,7,8],\n            }\n\nfss_search_space = tpot.search_spaces.nodes.FSSNode(subsets=subsets)\nselector = fss_search_space.generate(rng=1).export_pipeline()\nselector.fit(X_train)\nselector.transform(X_train)\n</pre> import tpot import pandas as pd import numpy as np from sklearn.linear_model import LogisticRegression import sklearn  subsets = { \"group_one\" :  [0,1,2],             \"group_two\" :  [3,4,5],             \"group_three\" :  [6,7,8],             }  fss_search_space = tpot.search_spaces.nodes.FSSNode(subsets=subsets) selector = fss_search_space.generate(rng=1).export_pipeline() selector.fit(X_train) selector.transform(X_train) Out[28]: <pre>array([[-0.76235619, -1.97629642,  1.05447979],\n       [ 2.16944118, -1.55515714,  0.67925075],\n       [ 1.96557199,  0.13789923,  1.588271  ],\n       ...,\n       [ 0.78956322,  2.12535053,  0.63115798],\n       [-0.80184984, -0.40793866,  1.3880617 ],\n       [-1.38085267,  1.62568989, -1.42046795]])</pre>"},{"location":"Tutorial/3_Feature_Set_Selector/#genetic-feature-selection-nodes-in-tpot","title":"Genetic Feature Selection nodes in TPOT\u00b6","text":"<p>TPOT can use evolutionary algorithms to optimize feature selection simultaneously with pipeline optimization. It includes two node search spaces with different feature selection strategies: FSSNode and GeneticFeatureSelectorNode.</p> <ol> <li><p>FSSNode - (Feature Set Selector) This node is useful if you have a list of predefined feature sets you want to select from. Each FeatureSetSelector Node will select a single group of features to be passed to the next step in the pipeline. Note that FSSNode does not create its own subset of features and does not mix/match multiple predefined feature sets.</p> </li> <li><p>GeneticFeatureSelectorNode\u2014Whereas the FSSNode selects from a predefined list of subsets of features, this node uses evolutionary algorithms to optimize a novel subset of features from scratch. This is useful where there is no predefined grouping of features.</p> </li> </ol> <p>This tutorial focuses on FSSNode. See Tutorial 5 for more information on GeneticFeatureSelectorNode.</p> <p>It may also be beneficial to pair these search spaces with a secondary objective function to minimize complexity. That would encourage TPOT to try to produce the simplest pipeline with the fewest number of features.</p> <p>tpot.objectives.number_of_nodes_objective - This can be used as an other_objective_function that counts the number of nodes.</p> <p>tpot.objectives.complexity_scorer - This is a scorer that tries to count the total number of learned parameters (number of coefficients, number of nodes in decision trees, etc.).</p>"},{"location":"Tutorial/3_Feature_Set_Selector/#feature-set-selector","title":"Feature Set Selector\u00b6","text":"<p>The FeatureSetSelector is a subclass of sklearn.feature_selection.SelectorMixin that simply returns the manually specified columns. The parameter sel_subset specifies the name or index of the column that it selects. The transform function then simply indexes and returns the selected columns. You can also optionally name the group with the name parameter, though this is only for note keeping and does is not used by the class.</p> <pre><code>sel_subset: list or int\n    If X is a dataframe, items in sel_subset list must correspond to column names\n    If X is a numpy array, items in sel_subset list must correspond to column indexes\n    int: index of a single column</code></pre>"},{"location":"Tutorial/3_Feature_Set_Selector/#fssnode","title":"FSSNode\u00b6","text":"<p>The <code>FSSNode</code> is a node search space that simply selects one feature set from a list of feature sets. This works identically to the EstimatorNode, but provides a easier interface for defining the feature sets.</p> <p>Note that the FSS is only well defined when used as the first step in a pipeline. This is because downstream nodes will receive different transformations of the data such that the original indexes no longer correspond to the same columns in the transformed data.</p> <p>The <code>FSSNode</code> takes in a single parameter <code>subsets</code> which defines the groups of features. There are four ways of defining the subsets.</p> <pre><code>subsets : str or list, default=None\n        Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries. \n        Features are defined by column names if using a Pandas data frame, or ints corresponding to indexes if using numpy arrays.\n        - str : If a string, it is assumed to be a path to a csv file with the subsets. \n            The first column is assumed to be the name of the subset and the remaining columns are the features in the subset.\n        - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets (i.e a list of lists).\n        - dict : A dictionary where keys are the names of the subsets and the values are the list of features.\n        - int : If an int, it is assumed to be the number of subsets to generate. Each subset will contain one feature.\n        - None : If None, each column will be treated as a subset. One column will be selected per subset.</code></pre> <p>Lets say you want to have three groups of features, each with three columns each. The following examples are equivalent:</p>"},{"location":"Tutorial/3_Feature_Set_Selector/#str","title":"str\u00b6","text":"<p>sel_subsets=simple_fss.csv</p> <pre><code>\\# simple_fss.csv\ngroup_one, 1,2,3\ngroup_two, 4,5,6\ngroup_three, 7,8,9</code></pre>"},{"location":"Tutorial/3_Feature_Set_Selector/#dict","title":"dict\u00b6","text":"<p>sel_subsets = { \"group_one\" :  [1,2,3], \"group_two\" :  [4,5,6], \"group_three\" :  [7,8,9], }</p>"},{"location":"Tutorial/3_Feature_Set_Selector/#list","title":"list\u00b6","text":"<p>sel_subsets = [[1,2,3], [4,5,6], [7,8,9]]</p>"},{"location":"Tutorial/3_Feature_Set_Selector/#examples","title":"Examples\u00b6","text":"<p>For these examples, we create a dummy dataset where the first six columns are informative and the rest are uninformative.</p>"},{"location":"Tutorial/3_Feature_Set_Selector/#unionpipeline-fssnode-example","title":"UnionPipeline + FSSNode example\u00b6","text":""},{"location":"Tutorial/3_Feature_Set_Selector/#dynamicunionpipeline-fssnode-example","title":"DynamicUnionPipeline + FSSNode example\u00b6","text":"<p>The dynamic union pipeline may select a variable number of feature sets.</p>"},{"location":"Tutorial/3_Feature_Set_Selector/#graphsearchpipeline-fssnode-example","title":"GraphSearchPipeline + FSSNode example\u00b6","text":"<p>FSSNodes must be set as the leaf search space as they act as the inputs to the pipeline.</p> <p>Here is an example pipeline from this search space that utilizes two feature sets.</p>"},{"location":"Tutorial/3_Feature_Set_Selector/#optimize-with-tpot","title":"Optimize with TPOT\u00b6","text":"<p>For this example, we will optimize the DynamicUnion search space</p>"},{"location":"Tutorial/3_Feature_Set_Selector/#combining-with-existing-search-spaces","title":"Combining with existing search spaces\u00b6","text":"<p>As with all search spaces, FSSNode can be combined with any other search space.</p> <p>You can also pair this with the existing prebuilt templates, for example:</p>"},{"location":"Tutorial/3_Feature_Set_Selector/#getting-fancy","title":"Getting Fancy\u00b6","text":"<p>If you want to get fancy, you can combine more search spaces in order to set up unique preprocessing pipelines per feature set. Here's an example:</p>"},{"location":"Tutorial/3_Feature_Set_Selector/#other-examples","title":"Other examples\u00b6","text":""},{"location":"Tutorial/3_Feature_Set_Selector/#dictionary","title":"dictionary\u00b6","text":""},{"location":"Tutorial/3_Feature_Set_Selector/#list","title":"list\u00b6","text":""},{"location":"Tutorial/3_Feature_Set_Selector/#csv-file","title":"csv file\u00b6","text":"<p>note: watch for spaces in the csv file!</p>"},{"location":"Tutorial/4_Genetic_Feature_Selection/","title":"GeneticFeatureSelectorNode","text":"<p>For these examples, we create a dummy dataset where the first six columns are informative and the rest are uninformative.</p> In\u00a0[1]: Copied! <pre>import tpot\nfrom tpot.search_spaces.nodes import *\nfrom tpot.search_spaces.pipelines import *\nimport tpot\nimport sklearn.datasets\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\nimport pandas as pd\nimport tpot\nimport sklearn.datasets\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\nfrom tpot.search_spaces.nodes import *\nfrom tpot.search_spaces.pipelines import *\nfrom tpot.config import get_search_space\n\n\nX, y = sklearn.datasets.make_classification(n_samples=1000, n_features=6, n_informative=6, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)\nX = np.hstack([X, np.random.rand(X.shape[0],6)]) #add six uninformative features\nX = pd.DataFrame(X, columns=['a','b','c','d','e','f','g','h','i', 'j', 'k', 'l']) # a, b ,c the rest are uninformative\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)\n\nX.head()\n</pre> import tpot from tpot.search_spaces.nodes import * from tpot.search_spaces.pipelines import * import tpot import sklearn.datasets from sklearn.linear_model import LogisticRegression import numpy as np import pandas as pd import tpot import sklearn.datasets from sklearn.linear_model import LogisticRegression import numpy as np from tpot.search_spaces.nodes import * from tpot.search_spaces.pipelines import * from tpot.config import get_search_space   X, y = sklearn.datasets.make_classification(n_samples=1000, n_features=6, n_informative=6, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None) X = np.hstack([X, np.random.rand(X.shape[0],6)]) #add six uninformative features X = pd.DataFrame(X, columns=['a','b','c','d','e','f','g','h','i', 'j', 'k', 'l']) # a, b ,c the rest are uninformative X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)  X.head() <pre>/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> Out[1]: a b c d e f g h i j k l 0 0.431031 1.889841 0.403235 0.130347 1.245051 -3.356587 0.254612 0.477369 0.145158 0.633607 0.200373 0.037735 1 0.016308 -1.035908 -1.625176 1.803391 0.442258 -0.844052 0.141507 0.024659 0.769976 0.658990 0.971987 0.570931 2 3.769469 0.209185 -1.303033 4.077509 2.935603 1.243487 0.088988 0.377935 0.019007 0.923725 0.760895 0.316752 3 -2.583292 0.172831 -1.531697 -0.078774 1.656190 0.475652 0.741539 0.179612 0.993759 0.624101 0.290679 0.946652 4 -0.833504 3.209340 -0.928798 0.345765 1.599057 0.242801 0.359656 0.697036 0.643063 0.198362 0.725530 0.974992 In\u00a0[2]: Copied! <pre>gfs_sp = GeneticFeatureSelectorNode(n_features=X.shape[1])\n</pre> gfs_sp = GeneticFeatureSelectorNode(n_features=X.shape[1]) <p>Each GeneticFeatureSelectorNode will select a new subset of features</p> In\u00a0[3]: Copied! <pre>selector = gfs_sp.generate().export_pipeline()\nselector.set_output(transform=\"pandas\") #by default sklearn selectors return numpy arrays. this will make it return pandas dataframes\nselector.fit(X_train, y_train)\nselector.transform(X_train)\n</pre> selector = gfs_sp.generate().export_pipeline() selector.set_output(transform=\"pandas\") #by default sklearn selectors return numpy arrays. this will make it return pandas dataframes selector.fit(X_train, y_train) selector.transform(X_train) Out[3]: b j 89 0.067735 0.839366 897 -0.175982 0.050951 824 -0.503185 0.826335 305 2.775297 0.877498 774 3.143969 0.429360 ... ... ... 310 1.402502 0.506769 333 2.384090 0.047125 259 5.262763 0.500726 30 1.107717 0.768569 757 3.606505 0.557151 <p>750 rows \u00d7 2 columns</p> In\u00a0[4]: Copied! <pre>selector = gfs_sp.generate().export_pipeline()\nselector.set_output(transform=\"pandas\") #by default sklearn selectors return numpy arrays. this will make it return pandas dataframes\nselector.fit(X_train, y_train)\nselector.transform(X_train)\n</pre> selector = gfs_sp.generate().export_pipeline() selector.set_output(transform=\"pandas\") #by default sklearn selectors return numpy arrays. this will make it return pandas dataframes selector.fit(X_train, y_train) selector.transform(X_train) Out[4]: k 89 0.179639 897 0.430166 824 0.354605 305 0.949369 774 0.499857 ... ... 310 0.624468 333 0.995309 259 0.138835 30 0.548930 757 0.643055 <p>750 rows \u00d7 1 columns</p> <p>Mutation and crossover can add or remove subsets from the learned feature set.</p> In\u00a0[5]: Copied! <pre>selector_ind = gfs_sp.generate()\nselector = selector_ind.export_pipeline()\nselected_features = X.columns[selector.mask]\n\nprint(\"selected features: \", selected_features)\n</pre> selector_ind = gfs_sp.generate() selector = selector_ind.export_pipeline() selected_features = X.columns[selector.mask]  print(\"selected features: \", selected_features) <pre>selected features:  Index(['a', 'j'], dtype='object')\n</pre> In\u00a0[6]: Copied! <pre>selector_ind.mutate()\nselector = selector_ind.export_pipeline()\nselected_features = X.columns[selector.mask]\nprint(\"selected features: \", selected_features)\n</pre> selector_ind.mutate() selector = selector_ind.export_pipeline() selected_features = X.columns[selector.mask] print(\"selected features: \", selected_features) <pre>selected features:  Index(['a', 'h', 'j'], dtype='object')\n</pre> In\u00a0[7]: Copied! <pre>import tpot\nimport sklearn.datasets\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\nfrom tpot.search_spaces.nodes import *\nfrom tpot.search_spaces.pipelines import *\n\ngfs_sp = GeneticFeatureSelectorNode(n_features=X.shape[1])\nclassifiers_sp = get_search_space('RandomForestClassifier')\nfinal_classification_search_space = SequentialPipeline([gfs_sp, classifiers_sp])\n\nest = tpot.TPOTEstimator(  population_size=32,\n                            generations=10, \n                            scorers=[\"roc_auc_ovr\", tpot.objectives.complexity_scorer],\n                            scorers_weights=[1.0, -1.0],\n                            n_jobs=32,\n                            classification=True,\n                            search_space = final_classification_search_space,\n                            verbose=1,\n                            )\n\n\nscorer = sklearn.metrics.get_scorer('roc_auc_ovo')\n\nest.fit(X_train, y_train)\nprint(scorer(est, X_test, y_test))\n</pre> import tpot import sklearn.datasets from sklearn.linear_model import LogisticRegression import numpy as np from tpot.search_spaces.nodes import * from tpot.search_spaces.pipelines import *  gfs_sp = GeneticFeatureSelectorNode(n_features=X.shape[1]) classifiers_sp = get_search_space('RandomForestClassifier') final_classification_search_space = SequentialPipeline([gfs_sp, classifiers_sp])  est = tpot.TPOTEstimator(  population_size=32,                             generations=10,                              scorers=[\"roc_auc_ovr\", tpot.objectives.complexity_scorer],                             scorers_weights=[1.0, -1.0],                             n_jobs=32,                             classification=True,                             search_space = final_classification_search_space,                             verbose=1,                             )   scorer = sklearn.metrics.get_scorer('roc_auc_ovo')  est.fit(X_train, y_train) print(scorer(est, X_test, y_test)) <pre>/Users/ketrong/Desktop/tpotvalidation/tpot/tpot/tpot_estimator/estimator.py:456: UserWarning: Both generations and max_time_mins are set. TPOT will terminate when the first condition is met.\n  warnings.warn(\"Both generations and max_time_mins are set. TPOT will terminate when the first condition is met.\")\nGeneration: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:53&lt;00:00,  5.33s/it]\n</pre> <pre>0.9458645653148825\n</pre> In\u00a0[8]: Copied! <pre>est.fitted_pipeline_\n</pre> est.fitted_pipeline_ Out[8]: <pre>Pipeline(steps=[('maskselector',\n                 MaskSelector(mask=array([ True,  True,  True,  True,  True,  True, False, False,  True,\n       False,  True,  True]))),\n                ('randomforestclassifier',\n                 RandomForestClassifier(class_weight='balanced',\n                                        criterion='entropy',\n                                        max_features=0.487196536075,\n                                        min_samples_leaf=5, min_samples_split=3,\n                                        n_estimators=128, n_jobs=1))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiFitted<pre>Pipeline(steps=[('maskselector',\n                 MaskSelector(mask=array([ True,  True,  True,  True,  True,  True, False, False,  True,\n       False,  True,  True]))),\n                ('randomforestclassifier',\n                 RandomForestClassifier(class_weight='balanced',\n                                        criterion='entropy',\n                                        max_features=0.487196536075,\n                                        min_samples_leaf=5, min_samples_split=3,\n                                        n_estimators=128, n_jobs=1))])</pre> MaskSelector<pre>MaskSelector(mask=array([ True,  True,  True,  True,  True,  True, False, False,  True,\n       False,  True,  True]))</pre> \u00a0RandomForestClassifier?Documentation for RandomForestClassifier<pre>RandomForestClassifier(class_weight='balanced', criterion='entropy',\n                       max_features=0.487196536075, min_samples_leaf=5,\n                       min_samples_split=3, n_estimators=128, n_jobs=1)</pre> In\u00a0[9]: Copied! <pre>selected_features = X.columns[est.fitted_pipeline_.steps[0][1].mask]\nprint(\"selected features: \", selected_features)\n</pre> selected_features = X.columns[est.fitted_pipeline_.steps[0][1].mask] print(\"selected features: \", selected_features) <pre>selected features:  Index(['a', 'b', 'c', 'd', 'e', 'f', 'i', 'k', 'l'], dtype='object')\n</pre> In\u00a0[10]: Copied! <pre>def number_of_selected_features(est):\n   return sum(est.steps[0][1].mask)\n\ngfs_sp = GeneticFeatureSelectorNode(n_features=X.shape[1])\nclassifiers_sp = get_search_space('RandomForestClassifier')\nfinal_classification_search_space = SequentialPipeline([gfs_sp, classifiers_sp])\n\nest = tpot.TPOTEstimator(  \n                           population_size=32,\n                           generations=10, \n                           scorers=[\"roc_auc_ovr\", tpot.objectives.complexity_scorer],\n                           scorers_weights=[1.0, -1.0],\n                           other_objective_functions=[number_of_selected_features],\n                           other_objective_functions_weights = [-1],\n                           objective_function_names = [\"Number of selected features\"],\n\n                           n_jobs=32,\n                           classification=True,\n                           search_space = final_classification_search_space,\n                           verbose=2,\n                            )\n\nscorer = sklearn.metrics.get_scorer('roc_auc_ovo')\n\nest.fit(X_train, y_train)\nprint(scorer(est, X_test, y_test))\n</pre> def number_of_selected_features(est):    return sum(est.steps[0][1].mask)  gfs_sp = GeneticFeatureSelectorNode(n_features=X.shape[1]) classifiers_sp = get_search_space('RandomForestClassifier') final_classification_search_space = SequentialPipeline([gfs_sp, classifiers_sp])  est = tpot.TPOTEstimator(                              population_size=32,                            generations=10,                             scorers=[\"roc_auc_ovr\", tpot.objectives.complexity_scorer],                            scorers_weights=[1.0, -1.0],                            other_objective_functions=[number_of_selected_features],                            other_objective_functions_weights = [-1],                            objective_function_names = [\"Number of selected features\"],                             n_jobs=32,                            classification=True,                            search_space = final_classification_search_space,                            verbose=2,                             )  scorer = sklearn.metrics.get_scorer('roc_auc_ovo')  est.fit(X_train, y_train) print(scorer(est, X_test, y_test)) <pre>/Users/ketrong/Desktop/tpotvalidation/tpot/tpot/tpot_estimator/estimator.py:456: UserWarning: Both generations and max_time_mins are set. TPOT will terminate when the first condition is met.\n  warnings.warn(\"Both generations and max_time_mins are set. TPOT will terminate when the first condition is met.\")\nGeneration: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:47&lt;00:00,  4.73s/it]\n</pre> <pre>0.9414440386956244\n</pre> In\u00a0[11]: Copied! <pre>selected_features = X.columns[est.fitted_pipeline_.steps[0][1].mask]\nprint(\"selected features: \", selected_features)\n</pre> selected_features = X.columns[est.fitted_pipeline_.steps[0][1].mask] print(\"selected features: \", selected_features) <pre>selected features:  Index(['b', 'c', 'd', 'e', 'f', 'g'], dtype='object')\n</pre> In\u00a0[12]: Copied! <pre>import seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = est.evaluated_individuals\ncol1 = \"Number of selected features\"\ncol2 = \"roc_auc_score\"\n\n# Multiple orange dots show because the pareto front in this case is actually 3D along the auroc score, number of features, and complexity.\n\n#replace nans in pareto front with 0\nfig, ax = plt.subplots(figsize=(5,5))\nsns.scatterplot(df[df['Pareto_Front']!=1], x=col1, y=col2, label='other', ax=ax)\nsns.scatterplot(df[df['Pareto_Front']==1], x=col1, y=col2, label='Pareto Front', ax=ax)\nax.title.set_text('Performance of all pipelines')\n#log scale y\nax.set_yscale('log')\nplt.show()\n\n#replace nans in pareto front with 0\nfig, ax = plt.subplots(figsize=(10,5))\nsns.scatterplot(df[df['Pareto_Front']==1], x=col1, y=col2, label='Pareto Front', ax=ax)\nax.title.set_text('Performance of only the Pareto Front')\n#log scale y\n# ax.set_yscale('log')\nplt.show()\n</pre> import seaborn as sns import matplotlib.pyplot as plt  df = est.evaluated_individuals col1 = \"Number of selected features\" col2 = \"roc_auc_score\"  # Multiple orange dots show because the pareto front in this case is actually 3D along the auroc score, number of features, and complexity.  #replace nans in pareto front with 0 fig, ax = plt.subplots(figsize=(5,5)) sns.scatterplot(df[df['Pareto_Front']!=1], x=col1, y=col2, label='other', ax=ax) sns.scatterplot(df[df['Pareto_Front']==1], x=col1, y=col2, label='Pareto Front', ax=ax) ax.title.set_text('Performance of all pipelines') #log scale y ax.set_yscale('log') plt.show()  #replace nans in pareto front with 0 fig, ax = plt.subplots(figsize=(10,5)) sns.scatterplot(df[df['Pareto_Front']==1], x=col1, y=col2, label='Pareto Front', ax=ax) ax.title.set_text('Performance of only the Pareto Front') #log scale y # ax.set_yscale('log') plt.show() In\u00a0[13]: Copied! <pre>linear_search_space = tpot.config.template_search_spaces.get_template_search_spaces(\"linear\", classification=True)\ngfs_and_linear_search_space = SequentialPipeline([gfs_sp, linear_search_space])\n\n# est = tpot.TPOTEstimator(  \n#                            population_size=32,\n#                            generations=10, \n#                            scorers=[\"roc_auc_ovr\", tpot.objectives.complexity_scorer],\n#                            scorers_weights=[1.0, -1.0],\n#                            other_objective_functions=[number_of_selected_features],\n#                            other_objective_functions_weights = [-1],\n#                            objective_function_names = [\"Number of selected features\"],\n\n#                            n_jobs=32,\n#                            classification=True,\n#                            search_space = gfs_and_linear_search_space,\n#                            verbose=2,\n#                             )\n\ngfs_and_linear_search_space.generate(rng=1).export_pipeline()\n</pre> linear_search_space = tpot.config.template_search_spaces.get_template_search_spaces(\"linear\", classification=True) gfs_and_linear_search_space = SequentialPipeline([gfs_sp, linear_search_space])  # est = tpot.TPOTEstimator(   #                            population_size=32, #                            generations=10,  #                            scorers=[\"roc_auc_ovr\", tpot.objectives.complexity_scorer], #                            scorers_weights=[1.0, -1.0], #                            other_objective_functions=[number_of_selected_features], #                            other_objective_functions_weights = [-1], #                            objective_function_names = [\"Number of selected features\"],  #                            n_jobs=32, #                            classification=True, #                            search_space = gfs_and_linear_search_space, #                            verbose=2, #                             )  gfs_and_linear_search_space.generate(rng=1).export_pipeline() Out[13]: <pre>Pipeline(steps=[('maskselector',\n                 MaskSelector(mask=array([False, False,  True, False, False, False, False, False, False,\n        True, False, False]))),\n                ('pipeline',\n                 Pipeline(steps=[('normalizer', Normalizer(norm='l1')),\n                                 ('selectpercentile',\n                                  SelectPercentile(percentile=74.2561844719571)),\n                                 ('featureunion-1',\n                                  FeatureUnion(transformer_list=[('featureunion',\n                                                                  FeatureUnion(transformer_list=[('binarizer',\n                                                                                                  Binarizer(threshold=0.0935770250992))])),\n                                                                 ('passthrough',\n                                                                  Passthrough())])),\n                                 ('featureunion-2',\n                                  FeatureUnion(transformer_list=[('skiptransformer',\n                                                                  SkipTransformer()),\n                                                                 ('passthrough',\n                                                                  Passthrough())])),\n                                 ('adaboostclassifier',\n                                  AdaBoostClassifier(algorithm='SAMME',\n                                                     learning_rate=0.9665397922726,\n                                                     n_estimators=320))]))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('maskselector',\n                 MaskSelector(mask=array([False, False,  True, False, False, False, False, False, False,\n        True, False, False]))),\n                ('pipeline',\n                 Pipeline(steps=[('normalizer', Normalizer(norm='l1')),\n                                 ('selectpercentile',\n                                  SelectPercentile(percentile=74.2561844719571)),\n                                 ('featureunion-1',\n                                  FeatureUnion(transformer_list=[('featureunion',\n                                                                  FeatureUnion(transformer_list=[('binarizer',\n                                                                                                  Binarizer(threshold=0.0935770250992))])),\n                                                                 ('passthrough',\n                                                                  Passthrough())])),\n                                 ('featureunion-2',\n                                  FeatureUnion(transformer_list=[('skiptransformer',\n                                                                  SkipTransformer()),\n                                                                 ('passthrough',\n                                                                  Passthrough())])),\n                                 ('adaboostclassifier',\n                                  AdaBoostClassifier(algorithm='SAMME',\n                                                     learning_rate=0.9665397922726,\n                                                     n_estimators=320))]))])</pre> MaskSelector<pre>MaskSelector(mask=array([False, False,  True, False, False, False, False, False, False,\n        True, False, False]))</pre> \u00a0pipeline: Pipeline?Documentation for pipeline: Pipeline<pre>Pipeline(steps=[('normalizer', Normalizer(norm='l1')),\n                ('selectpercentile',\n                 SelectPercentile(percentile=74.2561844719571)),\n                ('featureunion-1',\n                 FeatureUnion(transformer_list=[('featureunion',\n                                                 FeatureUnion(transformer_list=[('binarizer',\n                                                                                 Binarizer(threshold=0.0935770250992))])),\n                                                ('passthrough',\n                                                 Passthrough())])),\n                ('featureunion-2',\n                 FeatureUnion(transformer_list=[('skiptransformer',\n                                                 SkipTransformer()),\n                                                ('passthrough',\n                                                 Passthrough())])),\n                ('adaboostclassifier',\n                 AdaBoostClassifier(algorithm='SAMME',\n                                    learning_rate=0.9665397922726,\n                                    n_estimators=320))])</pre> \u00a0Normalizer?Documentation for Normalizer<pre>Normalizer(norm='l1')</pre> \u00a0SelectPercentile?Documentation for SelectPercentile<pre>SelectPercentile(percentile=74.2561844719571)</pre> \u00a0featureunion-1: FeatureUnion?Documentation for featureunion-1: FeatureUnion<pre>FeatureUnion(transformer_list=[('featureunion',\n                                FeatureUnion(transformer_list=[('binarizer',\n                                                                Binarizer(threshold=0.0935770250992))])),\n                               ('passthrough', Passthrough())])</pre> featureunionbinarizer\u00a0Binarizer?Documentation for Binarizer<pre>Binarizer(threshold=0.0935770250992)</pre> passthroughPassthrough<pre>Passthrough()</pre> \u00a0featureunion-2: FeatureUnion?Documentation for featureunion-2: FeatureUnion<pre>FeatureUnion(transformer_list=[('skiptransformer', SkipTransformer()),\n                               ('passthrough', Passthrough())])</pre> skiptransformerSkipTransformer<pre>SkipTransformer()</pre> passthroughPassthrough<pre>Passthrough()</pre> \u00a0AdaBoostClassifier?Documentation for AdaBoostClassifier<pre>AdaBoostClassifier(algorithm='SAMME', learning_rate=0.9665397922726,\n                   n_estimators=320)</pre> In\u00a0[14]: Copied! <pre>dynamic_transformers = DynamicUnionPipeline(get_search_space(\"all_transformers\"), max_estimators=4)\ndynamic_transformers_with_passthrough = tpot.search_spaces.pipelines.UnionPipeline([\n    dynamic_transformers,\n    tpot.config.get_search_space(\"Passthrough\")],\n    )\nmulti_step_engineering = DynamicLinearPipeline(dynamic_transformers_with_passthrough, max_length=4)\ngfs_engineering_search_space = SequentialPipeline([gfs_sp, multi_step_engineering])\nunion_fss_engineering_search_space = DynamicUnionPipeline(gfs_engineering_search_space)\nclassification_search_space = get_search_space('classifiers')\n\nfinal_fancy_search_space = SequentialPipeline([union_fss_engineering_search_space, classification_search_space])\n\nfinal_fancy_search_space.generate(rng=1).export_pipeline()\n</pre> dynamic_transformers = DynamicUnionPipeline(get_search_space(\"all_transformers\"), max_estimators=4) dynamic_transformers_with_passthrough = tpot.search_spaces.pipelines.UnionPipeline([     dynamic_transformers,     tpot.config.get_search_space(\"Passthrough\")],     ) multi_step_engineering = DynamicLinearPipeline(dynamic_transformers_with_passthrough, max_length=4) gfs_engineering_search_space = SequentialPipeline([gfs_sp, multi_step_engineering]) union_fss_engineering_search_space = DynamicUnionPipeline(gfs_engineering_search_space) classification_search_space = get_search_space('classifiers')  final_fancy_search_space = SequentialPipeline([union_fss_engineering_search_space, classification_search_space])  final_fancy_search_space.generate(rng=1).export_pipeline() Out[14]: <pre>Pipeline(steps=[('featureunion',\n                 FeatureUnion(transformer_list=[('pipeline',\n                                                 Pipeline(steps=[('maskselector',\n                                                                  MaskSelector(mask=array([False,  True, False, False, False, False, False, False,  True,\n       False, False, False]))),\n                                                                 ('pipeline',\n                                                                  Pipeline(steps=[('featureunion-1',\n                                                                                   FeatureUnion(transformer_list=[('featureunion',\n                                                                                                                   FeatureUnion(transformer_list=[('robustscaler',\n                                                                                                                                                   Robu...\n                                                                                   FeatureUnion(transformer_list=[('featureunion',\n                                                                                                                   FeatureUnion(transformer_list=[('normalizer',\n                                                                                                                                                   Normalizer(norm='l1')),\n                                                                                                                                                  ('nystroem',\n                                                                                                                                                   Nystroem(gamma=0.5186832611359,\n                                                                                                                                                            kernel='polynomial',\n                                                                                                                                                            n_components=3))])),\n                                                                                                                  ('passthrough',\n                                                                                                                   Passthrough())]))]))]))])),\n                ('sgdclassifier',\n                 SGDClassifier(alpha=0.0024802032445, eta0=0.2824117602653,\n                               l1_ratio=0.281711265998, loss='modified_huber',\n                               n_jobs=1, penalty='elasticnet'))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('featureunion',\n                 FeatureUnion(transformer_list=[('pipeline',\n                                                 Pipeline(steps=[('maskselector',\n                                                                  MaskSelector(mask=array([False,  True, False, False, False, False, False, False,  True,\n       False, False, False]))),\n                                                                 ('pipeline',\n                                                                  Pipeline(steps=[('featureunion-1',\n                                                                                   FeatureUnion(transformer_list=[('featureunion',\n                                                                                                                   FeatureUnion(transformer_list=[('robustscaler',\n                                                                                                                                                   Robu...\n                                                                                   FeatureUnion(transformer_list=[('featureunion',\n                                                                                                                   FeatureUnion(transformer_list=[('normalizer',\n                                                                                                                                                   Normalizer(norm='l1')),\n                                                                                                                                                  ('nystroem',\n                                                                                                                                                   Nystroem(gamma=0.5186832611359,\n                                                                                                                                                            kernel='polynomial',\n                                                                                                                                                            n_components=3))])),\n                                                                                                                  ('passthrough',\n                                                                                                                   Passthrough())]))]))]))])),\n                ('sgdclassifier',\n                 SGDClassifier(alpha=0.0024802032445, eta0=0.2824117602653,\n                               l1_ratio=0.281711265998, loss='modified_huber',\n                               n_jobs=1, penalty='elasticnet'))])</pre> \u00a0featureunion: FeatureUnion?Documentation for featureunion: FeatureUnion<pre>FeatureUnion(transformer_list=[('pipeline',\n                                Pipeline(steps=[('maskselector',\n                                                 MaskSelector(mask=array([False,  True, False, False, False, False, False, False,  True,\n       False, False, False]))),\n                                                ('pipeline',\n                                                 Pipeline(steps=[('featureunion-1',\n                                                                  FeatureUnion(transformer_list=[('featureunion',\n                                                                                                  FeatureUnion(transformer_list=[('robustscaler',\n                                                                                                                                  RobustScaler(quantile_range=(0.18740...\n                                                                                                                                  FeatureAgglomeration(linkage='complete',\n                                                                                                                                                       metric='l2',\n                                                                                                                                                       n_clusters=28))])),\n                                                                                                 ('passthrough',\n                                                                                                  Passthrough())])),\n                                                                 ('featureunion-2',\n                                                                  FeatureUnion(transformer_list=[('featureunion',\n                                                                                                  FeatureUnion(transformer_list=[('normalizer',\n                                                                                                                                  Normalizer(norm='l1')),\n                                                                                                                                 ('nystroem',\n                                                                                                                                  Nystroem(gamma=0.5186832611359,\n                                                                                                                                           kernel='polynomial',\n                                                                                                                                           n_components=3))])),\n                                                                                                 ('passthrough',\n                                                                                                  Passthrough())]))]))]))])</pre> pipelineMaskSelector<pre>MaskSelector(mask=array([False,  True, False, False, False, False, False, False,  True,\n       False, False, False]))</pre> \u00a0pipeline: Pipeline?Documentation for pipeline: Pipeline<pre>Pipeline(steps=[('featureunion-1',\n                 FeatureUnion(transformer_list=[('featureunion',\n                                                 FeatureUnion(transformer_list=[('robustscaler',\n                                                                                 RobustScaler(quantile_range=(0.1874078711948,\n                                                                                                              0.7642865555088))),\n                                                                                ('featureagglomeration',\n                                                                                 FeatureAgglomeration(linkage='complete',\n                                                                                                      metric='l2',\n                                                                                                      n_clusters=28))])),\n                                                ('passthrough',\n                                                 Passthrough())])),\n                ('featureunion-2',\n                 FeatureUnion(transformer_list=[('featureunion',\n                                                 FeatureUnion(transformer_list=[('normalizer',\n                                                                                 Normalizer(norm='l1')),\n                                                                                ('nystroem',\n                                                                                 Nystroem(gamma=0.5186832611359,\n                                                                                          kernel='polynomial',\n                                                                                          n_components=3))])),\n                                                ('passthrough',\n                                                 Passthrough())]))])</pre> \u00a0featureunion-1: FeatureUnion?Documentation for featureunion-1: FeatureUnion<pre>FeatureUnion(transformer_list=[('featureunion',\n                                FeatureUnion(transformer_list=[('robustscaler',\n                                                                RobustScaler(quantile_range=(0.1874078711948,\n                                                                                             0.7642865555088))),\n                                                               ('featureagglomeration',\n                                                                FeatureAgglomeration(linkage='complete',\n                                                                                     metric='l2',\n                                                                                     n_clusters=28))])),\n                               ('passthrough', Passthrough())])</pre> featureunionrobustscaler\u00a0RobustScaler?Documentation for RobustScaler<pre>RobustScaler(quantile_range=(0.1874078711948, 0.7642865555088))</pre> featureagglomeration\u00a0FeatureAgglomeration?Documentation for FeatureAgglomeration<pre>FeatureAgglomeration(linkage='complete', metric='l2', n_clusters=28)</pre> passthroughPassthrough<pre>Passthrough()</pre> \u00a0featureunion-2: FeatureUnion?Documentation for featureunion-2: FeatureUnion<pre>FeatureUnion(transformer_list=[('featureunion',\n                                FeatureUnion(transformer_list=[('normalizer',\n                                                                Normalizer(norm='l1')),\n                                                               ('nystroem',\n                                                                Nystroem(gamma=0.5186832611359,\n                                                                         kernel='polynomial',\n                                                                         n_components=3))])),\n                               ('passthrough', Passthrough())])</pre> featureunionnormalizer\u00a0Normalizer?Documentation for Normalizer<pre>Normalizer(norm='l1')</pre> nystroem\u00a0Nystroem?Documentation for Nystroem<pre>Nystroem(gamma=0.5186832611359, kernel='polynomial', n_components=3)</pre> passthroughPassthrough<pre>Passthrough()</pre> \u00a0SGDClassifier?Documentation for SGDClassifier<pre>SGDClassifier(alpha=0.0024802032445, eta0=0.2824117602653,\n              l1_ratio=0.281711265998, loss='modified_huber', n_jobs=1,\n              penalty='elasticnet')</pre>"},{"location":"Tutorial/4_Genetic_Feature_Selection/#geneticfeatureselectornode","title":"GeneticFeatureSelectorNode\u00b6","text":"<p>Whereas the <code>FSSNode</code> selects from a predefined list of subsets of features, the <code>GeneticFeatureSelectorNode</code> uses evolutionary algorithms to optimize a novel subset of features from scratch. This is useful where there is no predefined grouping of features.</p> <p>To initalize the <code>GeneticFeatureSelectorNode</code> you simply need to pass in the total number of features (i.e number of columns) in your dataset.</p>"},{"location":"Tutorial/4_Genetic_Feature_Selection/#training","title":"Training\u00b6","text":""},{"location":"Tutorial/4_Genetic_Feature_Selection/#custom-objective-function-to-minimize-number-of-selected-features","title":"Custom objective function to minimize number of selected features\u00b6","text":"<p>We can create a custom objective function that returns the number of features selected per pipeline. The <code>other_objective_functions</code> parameter is for objective functions that do not require fitted pipelines and do not require cross validation. Since we know that the selector instance gets its features from its parameters, not through fitting, we can create an objective for the <code>other_objective_functions</code> parameter. We set the weights to -1 because we would like to minimize the number of features selected. We also give it a name so that we can more easily access it in the <code>evaluated_individuals</code> dataframe.</p>"},{"location":"Tutorial/4_Genetic_Feature_Selection/#other-examples","title":"Other Examples\u00b6","text":"<p>As with all search spaces, GeneticFeatureSelectorNode can be combined with any other search space.</p> <p>You can also pair this with the existing prebuilt templates, for example:</p>"},{"location":"Tutorial/4_Genetic_Feature_Selection/#getting-fancy","title":"Getting Fancy\u00b6","text":"<p>If you want to get fancy, you can combine more search spaces in order to set up unique preprocessing pipelines per feature set. Here's an example:</p>"},{"location":"Tutorial/5_GraphPipeline/","title":"GraphPipeline","text":"In\u00a0[4]: Copied! <pre>from sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nimport networkx as nx\nfrom tpot import GraphPipeline\nimport sklearn.metrics\n\nX, y = make_classification(random_state=0)\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    random_state=0)\n\n\ng = nx.DiGraph()\n\ng.add_node(\"scaler\", instance=StandardScaler())\ng.add_node(\"svc\", instance=SVC())\ng.add_node(\"LogisticRegression\", instance=LogisticRegression())\ng.add_node(\"LogisticRegression2\", instance=LogisticRegression())\n\ng.add_edge(\"svc\",\"scaler\")\ng.add_edge(\"LogisticRegression\", \"scaler\")\ng.add_edge(\"LogisticRegression2\", \"LogisticRegression\")\ng.add_edge(\"LogisticRegression2\", \"svc\")\n\n\nest = GraphPipeline(g)\nest.plot()\nest.fit(X_train, y_train)\nprint(\"score\")\nprint(sklearn.metrics.roc_auc_score(y_test, est.predict_proba(X_test)[:,1]))\n</pre> from sklearn.svm import SVC from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline import networkx as nx from tpot import GraphPipeline import sklearn.metrics  X, y = make_classification(random_state=0) X_train, X_test, y_train, y_test = train_test_split(X, y,                                                     random_state=0)   g = nx.DiGraph()  g.add_node(\"scaler\", instance=StandardScaler()) g.add_node(\"svc\", instance=SVC()) g.add_node(\"LogisticRegression\", instance=LogisticRegression()) g.add_node(\"LogisticRegression2\", instance=LogisticRegression())  g.add_edge(\"svc\",\"scaler\") g.add_edge(\"LogisticRegression\", \"scaler\") g.add_edge(\"LogisticRegression2\", \"LogisticRegression\") g.add_edge(\"LogisticRegression2\", \"svc\")   est = GraphPipeline(g) est.plot() est.fit(X_train, y_train) print(\"score\") print(sklearn.metrics.roc_auc_score(y_test, est.predict_proba(X_test)[:,1])) <pre>score\n0.8974358974358974\n</pre> In\u00a0[5]: Copied! <pre>est = GraphPipeline(g, cross_val_predict_cv=10)\nest.plot()\nest.fit(X_train, y_train)\nprint(\"score\")\nprint(sklearn.metrics.roc_auc_score(y_test, est.predict_proba(X_test)[:,1]))\n</pre> est = GraphPipeline(g, cross_val_predict_cv=10) est.plot() est.fit(X_train, y_train) print(\"score\") print(sklearn.metrics.roc_auc_score(y_test, est.predict_proba(X_test)[:,1])) <pre>score\n0.9166666666666666\n</pre> <p>You can access individual steps of a GraphPipeline using the label of each node.</p> In\u00a0[6]: Copied! <pre>svc = est.graph.nodes[\"svc\"][\"instance\"]\nsvc\n</pre> svc = est.graph.nodes[\"svc\"][\"instance\"] svc Out[6]: <pre>SVC()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0SVC?Documentation for SVCiFitted<pre>SVC()</pre>"},{"location":"Tutorial/5_GraphPipeline/#graphpipeline","title":"GraphPipeline\u00b6","text":"<p>GraphPipelines (<code>tpot.GraphPipeline</code>) work similarly to the scikit-learn Pipeline class. Rather than provide a list of steps, in GraphPipeline you provide a directed acyclic graph (<code>networkx.DiGraph</code>) of steps using networkx. In GraphPipeline, parents get their inputs from their children (i.e the leafs get the raw inputs (X,y), and the roots are the final classifiers/regressors).</p> <p>The label of the nodes can be anything, but must unique per instance of an sklearn estimator. Each node has an attribute called \"instance\" for the instance of the scikit-learn estimator.</p> <p>GraphPipeline allows for classifiers and regressors in the middle of the pipeline. In this case, GraphPipeline will will try to use the outputs of predict_proba, decision_function, or predict in that order. If cross_val_predict_cv is set, the downstream models are trained with the output of <code>sklearn.model_selection.cross_val_predict</code> (final results are predicted using the models trained on the full data).</p> <pre><code>Parameters\n----------\n\ngraph: networkx.DiGraph\n    A directed graph where the nodes are sklearn estimators and the edges are the inputs to those estimators.\n\ncross_val_predict_cv: int, cross-validation generator or an iterable, optional\n    Determines the cross-validation splitting strategy used in inner classifiers or regressors\n\nmethod: str, optional\n    The prediction method to use for the inner classifiers or regressors. If 'auto', it will try to use predict_proba, decision_function, or predict in that order.\n\nmemory: str or object with the joblib.Memory interface, optional\n    Used to cache the input and outputs of nodes to prevent refitting or computationally heavy transformations. By default, no caching is performed. If a string is given, it is the path to the caching directory.\n\nuse_label_encoder: bool, optional\n    If True, the label encoder is used to encode the labels to be 0 to N. If False, the label encoder is not used.\n    Mainly useful for classifiers (XGBoost) that require labels to be ints from 0 to N.\n\n    Can also be a sklearn.preprocessing.LabelEncoder object. If so, that label encoder is used.</code></pre>"},{"location":"Tutorial/5_GraphPipeline/#cross-val-predict","title":"Cross val predict\u00b6","text":"<p>Using cross_val_predict_cv can improve performance in some cases.</p>"},{"location":"Tutorial/6_Symbolic_Regression_and_Classification/","title":"Symbolic Regression and Classification","text":"In\u00a0[1]: Copied! <pre>import tpot\nfrom tpot.search_spaces.pipelines import GraphSearchPipeline\nfrom tpot.search_spaces.nodes import FSSNode\nfrom tpot.config import get_search_space\nimport sklearn.datasets\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n</pre> import tpot from tpot.search_spaces.pipelines import GraphSearchPipeline from tpot.search_spaces.nodes import FSSNode from tpot.config import get_search_space import sklearn.datasets from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split import numpy as np <pre>/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[2]: Copied! <pre>X, y = sklearn.datasets.make_classification(n_samples=1000, n_features=100, n_informative=6, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)\n</pre> X, y = sklearn.datasets.make_classification(n_samples=1000, n_features=100, n_informative=6, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None) X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25) In\u00a0[3]: Copied! <pre>symbolic_classification_search_space = GraphSearchPipeline(\n    root_search_space= get_search_space(\"LogisticRegression\"),\n    leaf_search_space = FSSNode(subsets=X_train.shape[1]), \n    inner_search_space = get_search_space([\"arithmatic\"]),\n    max_size = 20,\n)\n\n#example pipelines randomly sampled\nind = symbolic_classification_search_space.generate(rng=5)\nfor i in range(3):\n    ind.mutate(rng=1)\nest_example = ind.export_pipeline()\nest_example.plot()\n</pre> symbolic_classification_search_space = GraphSearchPipeline(     root_search_space= get_search_space(\"LogisticRegression\"),     leaf_search_space = FSSNode(subsets=X_train.shape[1]),      inner_search_space = get_search_space([\"arithmatic\"]),     max_size = 20, )  #example pipelines randomly sampled ind = symbolic_classification_search_space.generate(rng=5) for i in range(3):     ind.mutate(rng=1) est_example = ind.export_pipeline() est_example.plot() In\u00a0[4]: Copied! <pre>est = tpot.TPOTEstimator(  generations=20, \n                            max_time_mins=None,\n                            scorers=['roc_auc_ovr'],\n                            scorers_weights=[1],\n                            other_objective_functions=[tpot.objectives.number_of_nodes_objective],\n                            other_objective_functions_weights=[-1],\n                            n_jobs=32,\n                            classification=True,\n                            search_space = symbolic_classification_search_space,\n                            verbose=1,\n                            )\n\nscorer = sklearn.metrics.get_scorer('roc_auc_ovo')\n\nest.fit(X_train, y_train)\nprint(scorer(est, X_test, y_test))\nest.fitted_pipeline_.plot()\n</pre> est = tpot.TPOTEstimator(  generations=20,                              max_time_mins=None,                             scorers=['roc_auc_ovr'],                             scorers_weights=[1],                             other_objective_functions=[tpot.objectives.number_of_nodes_objective],                             other_objective_functions_weights=[-1],                             n_jobs=32,                             classification=True,                             search_space = symbolic_classification_search_space,                             verbose=1,                             )  scorer = sklearn.metrics.get_scorer('roc_auc_ovo')  est.fit(X_train, y_train) print(scorer(est, X_test, y_test)) est.fitted_pipeline_.plot() <pre>Generation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:40&lt;00:00,  2.01s/it]\n</pre> <pre>0.71168\n</pre> In\u00a0[5]: Copied! <pre>import seaborn as sns\nimport matplotlib.pyplot as plt\ndf = est.evaluated_individuals\n#replace nans in pareto front with 0\nfig, ax = plt.subplots(figsize=(5,5))\nsns.scatterplot(df[df['Pareto_Front']!=1], y='roc_auc_score', x='number_of_nodes_objective', label='other', ax=ax)\nsns.scatterplot(df[df['Pareto_Front']==1], y='roc_auc_score', x='number_of_nodes_objective', label='Pareto Front', ax=ax)\nax.title.set_text('Performance of all pipelines')\n#log scale y\nplt.show()\n\n#replace nans in pareto front with 0\nfig, ax = plt.subplots(figsize=(10,5))\nsns.scatterplot(df[df['Pareto_Front']==1], y='roc_auc_score', x='number_of_nodes_objective', label='Pareto Front', ax=ax)\nax.title.set_text('Performance of only the Pareto Front')\n#log scale y\n# ax.set_yscale('log')\nplt.show()\n</pre> import seaborn as sns import matplotlib.pyplot as plt df = est.evaluated_individuals #replace nans in pareto front with 0 fig, ax = plt.subplots(figsize=(5,5)) sns.scatterplot(df[df['Pareto_Front']!=1], y='roc_auc_score', x='number_of_nodes_objective', label='other', ax=ax) sns.scatterplot(df[df['Pareto_Front']==1], y='roc_auc_score', x='number_of_nodes_objective', label='Pareto Front', ax=ax) ax.title.set_text('Performance of all pipelines') #log scale y plt.show()  #replace nans in pareto front with 0 fig, ax = plt.subplots(figsize=(10,5)) sns.scatterplot(df[df['Pareto_Front']==1], y='roc_auc_score', x='number_of_nodes_objective', label='Pareto Front', ax=ax) ax.title.set_text('Performance of only the Pareto Front') #log scale y # ax.set_yscale('log') plt.show() <p>Symbolic Regression</p> In\u00a0[6]: Copied! <pre>import tpot\nimport sklearn.datasets\n\nscorer = sklearn.metrics.get_scorer('neg_mean_squared_error')\nX, y = sklearn.datasets.load_diabetes(return_X_y=True)\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)\n\ngraph_search_space = tpot.search_spaces.pipelines.GraphSearchPipeline(\n    root_search_space= tpot.config.get_search_space(\"SGDRegressor\"),\n    leaf_search_space = tpot.search_spaces.nodes.FSSNode(subsets=X_train.shape[1]), \n    inner_search_space = tpot.config.get_search_space([\"arithmatic\"]),\n    max_size = 10,\n)\n\nest = tpot.TPOTEstimator(  generations=20, \n                            max_time_mins=None,\n                            scorers=['neg_mean_squared_error'],\n                            scorers_weights=[1],\n                            other_objective_functions=[tpot.objectives.number_of_nodes_objective],\n                            other_objective_functions_weights=[-1],\n                            n_jobs=32,\n                            classification=False,\n                            search_space = graph_search_space ,\n                            verbose=2,\n                            )\n\n\n\nest.fit(X_train, y_train)\nprint(scorer(est, X_test, y_test))\nest.fitted_pipeline_.plot()\n</pre> import tpot import sklearn.datasets  scorer = sklearn.metrics.get_scorer('neg_mean_squared_error') X, y = sklearn.datasets.load_diabetes(return_X_y=True) X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)  graph_search_space = tpot.search_spaces.pipelines.GraphSearchPipeline(     root_search_space= tpot.config.get_search_space(\"SGDRegressor\"),     leaf_search_space = tpot.search_spaces.nodes.FSSNode(subsets=X_train.shape[1]),      inner_search_space = tpot.config.get_search_space([\"arithmatic\"]),     max_size = 10, )  est = tpot.TPOTEstimator(  generations=20,                              max_time_mins=None,                             scorers=['neg_mean_squared_error'],                             scorers_weights=[1],                             other_objective_functions=[tpot.objectives.number_of_nodes_objective],                             other_objective_functions_weights=[-1],                             n_jobs=32,                             classification=False,                             search_space = graph_search_space ,                             verbose=2,                             )    est.fit(X_train, y_train) print(scorer(est, X_test, y_test)) est.fitted_pipeline_.plot() <pre>Generation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:30&lt;00:00,  1.51s/it]\n</pre> <pre>-3073.9914754941187\n</pre> In\u00a0[7]: Copied! <pre>import seaborn as sns\nimport matplotlib.pyplot as plt\ndf = est.evaluated_individuals\ndf['mean_squared_error'] = -df['mean_squared_error']\n#replace nans in pareto front with 0\nfig, ax = plt.subplots(figsize=(5,5))\nsns.scatterplot(df[df['Pareto_Front']!=1], y='mean_squared_error', x='number_of_nodes_objective', label='other', ax=ax)\nsns.scatterplot(df[df['Pareto_Front']==1], y='mean_squared_error', x='number_of_nodes_objective', label='Pareto Front', ax=ax)\nax.title.set_text('Performance of all pipelines')\n#log scale y\nax.set_yscale('log')\nplt.show()\n\n#replace nans in pareto front with 0\nfig, ax = plt.subplots(figsize=(10,5))\nsns.scatterplot(df[df['Pareto_Front']==1], y='mean_squared_error', x='number_of_nodes_objective', label='Pareto Front', ax=ax)\nax.title.set_text('Performance of only the Pareto Front')\n#log scale y\n# ax.set_yscale('log')\nplt.show()\n</pre> import seaborn as sns import matplotlib.pyplot as plt df = est.evaluated_individuals df['mean_squared_error'] = -df['mean_squared_error'] #replace nans in pareto front with 0 fig, ax = plt.subplots(figsize=(5,5)) sns.scatterplot(df[df['Pareto_Front']!=1], y='mean_squared_error', x='number_of_nodes_objective', label='other', ax=ax) sns.scatterplot(df[df['Pareto_Front']==1], y='mean_squared_error', x='number_of_nodes_objective', label='Pareto Front', ax=ax) ax.title.set_text('Performance of all pipelines') #log scale y ax.set_yscale('log') plt.show()  #replace nans in pareto front with 0 fig, ax = plt.subplots(figsize=(10,5)) sns.scatterplot(df[df['Pareto_Front']==1], y='mean_squared_error', x='number_of_nodes_objective', label='Pareto Front', ax=ax) ax.title.set_text('Performance of only the Pareto Front') #log scale y # ax.set_yscale('log') plt.show()"},{"location":"Tutorial/6_Symbolic_Regression_and_Classification/#symbolic-regression-and-classification","title":"Symbolic Regression and Classification\u00b6","text":"<p>Symbolic Regression and Classification seek to optimize an interpretable algebraic equation. TPOT allows you to combine this approach with classical machine learning operations.</p> <p>We can construct a search space for symbolic equations using either the TreePipeline or GraphSearchPipeline as neither have a fixed pipeline structure and instead optimize their own sequences and structure.</p> <p>The strategy is to set the leaves to select a single feature (Using FSSNode), have all inner nodes be arithmetic operators, and have the root node be a classifier or regressor.</p> <p>Note: This is still experimental. There are lots of opportunities to optimize the optimization process. In the future, symbolic regression/classification may have their own dedicated search space class.</p>"},{"location":"Tutorial/6_Symbolic_Regression_and_Classification/#symbolic-classification","title":"Symbolic Classification\u00b6","text":""},{"location":"Tutorial/7_dask_parallelization/","title":"Parallelization","text":"In\u00a0[\u00a0]: Copied! <pre>import tpot\nimport sklearn\nimport sklearn.datasets\nimport numpy as np\nscorer = sklearn.metrics.get_scorer('roc_auc_ovr')\nX, y = sklearn.datasets.load_iris(return_X_y=True)\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)\n\n\ngraph_search_space = tpot.search_spaces.pipelines.GraphSearchPipeline(\n    root_search_space= tpot.config.get_search_space([\"KNeighborsClassifier\", \"LogisticRegression\", \"DecisionTreeClassifier\"]),\n    leaf_search_space = tpot.config.get_search_space(\"selectors\"), \n    inner_search_space = tpot.config.get_search_space([\"transformers\"]),\n    max_size = 10,\n    )\n\nest = tpot.TPOTEstimator(\n    scorers = [\"roc_auc_ovr\"],\n    scorers_weights = [1],\n    classification = True,\n    cv = 10,\n    search_space = graph_search_space,\n    max_time_mins = 60,\n    verbose = 2,\n    n_jobs=16,\n    memory_limit=\"4GB\"\n)\n\nest.fit(X_train, y_train)\nprint(scorer(est, X_test, y_test))\n</pre> import tpot import sklearn import sklearn.datasets import numpy as np scorer = sklearn.metrics.get_scorer('roc_auc_ovr') X, y = sklearn.datasets.load_iris(return_X_y=True) X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)   graph_search_space = tpot.search_spaces.pipelines.GraphSearchPipeline(     root_search_space= tpot.config.get_search_space([\"KNeighborsClassifier\", \"LogisticRegression\", \"DecisionTreeClassifier\"]),     leaf_search_space = tpot.config.get_search_space(\"selectors\"),      inner_search_space = tpot.config.get_search_space([\"transformers\"]),     max_size = 10,     )  est = tpot.TPOTEstimator(     scorers = [\"roc_auc_ovr\"],     scorers_weights = [1],     classification = True,     cv = 10,     search_space = graph_search_space,     max_time_mins = 60,     verbose = 2,     n_jobs=16,     memory_limit=\"4GB\" )  est.fit(X_train, y_train) print(scorer(est, X_test, y_test)) <pre>/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nGeneration: : 8it [01:00,  7.57s/it]\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:595: UserWarning: n_components is too large: it will be set to 4\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n</pre> <pre>0.997905982905983\n</pre> <p>Initializing a basic dask local cluster</p> In\u00a0[2]: Copied! <pre>from dask.distributed import Client, LocalCluster\n\nn_jobs = 4\nmemory_limit = \"4GB\"\n\ncluster = LocalCluster(n_workers=n_jobs, #if no client is passed in and no global client exists, create our own\n                        threads_per_worker=1,\n                        memory_limit=memory_limit)\nclient = Client(cluster)\n</pre> from dask.distributed import Client, LocalCluster  n_jobs = 4 memory_limit = \"4GB\"  cluster = LocalCluster(n_workers=n_jobs, #if no client is passed in and no global client exists, create our own                         threads_per_worker=1,                         memory_limit=memory_limit) client = Client(cluster) <p>Get the link to view the dask Dashboard.</p> In\u00a0[3]: Copied! <pre>client.dashboard_link\n</pre> client.dashboard_link Out[3]: <pre>'http://127.0.0.1:8787/status'</pre> In\u00a0[\u00a0]: Copied! <pre>graph_search_space = tpot.search_spaces.pipelines.GraphSearchPipeline(\n    root_search_space= tpot.config.get_search_space([\"KNeighborsClassifier\", \"LogisticRegression\", \"DecisionTreeClassifier\"]),\n    leaf_search_space = tpot.config.get_search_space(\"selectors\"), \n    inner_search_space = tpot.config.get_search_space([\"transformers\"]),\n    max_size = 10,\n    )\n\nest = tpot.TPOTEstimator(\n    client = client,\n    scorers = [\"roc_auc_ovr\"],\n    scorers_weights = [1],\n    classification = True,\n    cv = 10,\n    search_space = graph_search_space,\n    max_time_mins = 60,\n    early_stop=10,\n    verbose = 2,\n)\n\n\n# this is equivalent to: \n# est = tpot.TPOTClassifier(population_size= 8, generations=5, n_jobs=4, memory_limit=\"4GB\", verbose=1)\nest.fit(X_train, y_train)\nprint(scorer(est, X_test, y_test))\n\n#It is good to close the client and cluster when you are done with them\nclient.close()\ncluster.close()\n</pre> graph_search_space = tpot.search_spaces.pipelines.GraphSearchPipeline(     root_search_space= tpot.config.get_search_space([\"KNeighborsClassifier\", \"LogisticRegression\", \"DecisionTreeClassifier\"]),     leaf_search_space = tpot.config.get_search_space(\"selectors\"),      inner_search_space = tpot.config.get_search_space([\"transformers\"]),     max_size = 10,     )  est = tpot.TPOTEstimator(     client = client,     scorers = [\"roc_auc_ovr\"],     scorers_weights = [1],     classification = True,     cv = 10,     search_space = graph_search_space,     max_time_mins = 60,     early_stop=10,     verbose = 2, )   # this is equivalent to:  # est = tpot.TPOTClassifier(population_size= 8, generations=5, n_jobs=4, memory_limit=\"4GB\", verbose=1) est.fit(X_train, y_train) print(scorer(est, X_test, y_test))  #It is good to close the client and cluster when you are done with them client.close() cluster.close() <pre>Generation: : 8it [01:01,  7.69s/it]\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n2025-02-21 16:37:55,843 - distributed.worker.state_machine - WARNING - Async instruction for &lt;Task cancelled name=\"execute('eval_objective_list-8bdcf8a1c1f54374fc47664011238a6d')\" coro=&lt;Worker.execute() done, defined at /opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/distributed/worker_state_machine.py:3607&gt;&gt; ended with CancelledError\n</pre> <pre>0.997905982905983\n</pre> <p>Option 2</p> <p>You can initialize the cluster and client with a context manager that will automatically close them.</p> In\u00a0[\u00a0]: Copied! <pre>from dask.distributed import Client, LocalCluster\nimport tpot\nimport sklearn\nimport sklearn.datasets\nimport numpy as np\n\nscorer = sklearn.metrics.get_scorer('roc_auc_ovr')\nX, y = sklearn.datasets.load_iris(return_X_y=True)\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)\n\n\nn_jobs = 4\nmemory_limit = \"4GB\"\n\nwith LocalCluster(  \n    n_workers=n_jobs,\n    threads_per_worker=1,\n    memory_limit='4GB',\n) as cluster, Client(cluster) as client:\n    graph_search_space = tpot.search_spaces.pipelines.GraphSearchPipeline(\n        root_search_space= tpot.config.get_search_space([\"KNeighborsClassifier\", \"LogisticRegression\", \"DecisionTreeClassifier\"]),\n        leaf_search_space = tpot.config.get_search_space(\"selectors\"), \n        inner_search_space = tpot.config.get_search_space([\"transformers\"]),\n        max_size = 10,\n        )\n\n    est = tpot.TPOTEstimator(\n        client = client,\n        scorers = [\"roc_auc_ovr\"],\n        scorers_weights = [1],\n        classification = True,\n        cv = 5,\n        search_space = graph_search_space,\n        max_time_mins = 60,\n        early_stop=10,\n        verbose = 2,\n        )\n    est.fit(X_train, y_train)\n    print(scorer(est, X_test, y_test))\n</pre> from dask.distributed import Client, LocalCluster import tpot import sklearn import sklearn.datasets import numpy as np  scorer = sklearn.metrics.get_scorer('roc_auc_ovr') X, y = sklearn.datasets.load_iris(return_X_y=True) X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)   n_jobs = 4 memory_limit = \"4GB\"  with LocalCluster(       n_workers=n_jobs,     threads_per_worker=1,     memory_limit='4GB', ) as cluster, Client(cluster) as client:     graph_search_space = tpot.search_spaces.pipelines.GraphSearchPipeline(         root_search_space= tpot.config.get_search_space([\"KNeighborsClassifier\", \"LogisticRegression\", \"DecisionTreeClassifier\"]),         leaf_search_space = tpot.config.get_search_space(\"selectors\"),          inner_search_space = tpot.config.get_search_space([\"transformers\"]),         max_size = 10,         )      est = tpot.TPOTEstimator(         client = client,         scorers = [\"roc_auc_ovr\"],         scorers_weights = [1],         classification = True,         cv = 5,         search_space = graph_search_space,         max_time_mins = 60,         early_stop=10,         verbose = 2,         )     est.fit(X_train, y_train)     print(scorer(est, X_test, y_test)) <pre>Generation: : 10it [01:00,  6.07s/it]\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n2025-02-21 16:38:57,976 - distributed.worker.state_machine - WARNING - Async instruction for &lt;Task cancelled name=\"execute('eval_objective_list-87c6eded7038f6c8291a3ee9879aef3f')\" coro=&lt;Worker.execute() done, defined at /opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/distributed/worker_state_machine.py:3607&gt;&gt; ended with CancelledError\n</pre> <pre>1.0\n</pre> <pre>2025-02-21 16:39:01,975 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n</pre> In\u00a0[\u00a0]: Copied! <pre>from dask.distributed import Client, LocalCluster\nimport sklearn\nimport sklearn.datasets\nimport sklearn.metrics\nimport sklearn.model_selection\nimport tpot\nfrom dask_jobqueue import SGECluster # or SLURMCluster, PBSCluster, etc. Replace SGE with your scheduler.\nimport os\n\nif os.system(\"which qsub\") != 0:\n    print(\"Sun Grid Engine is not installed. This example requires Sun Grid Engine to be installed.\")\nelse:\n    print(\"Sun Grid Engine is installed.\")\n\n    \n    cluster = SGECluster(\n        queue='all.q',\n        cores=2,\n        memory=\"50 GB\"\n\n    )\n\n    cluster.adapt(minimum_jobs=10, maximum_jobs=100)  # auto-scale between 10 and 100 jobs\n\n    client = Client(cluster)\n\n    scorer = sklearn.metrics.get_scorer('roc_auc_ovr')\n    X, y = sklearn.datasets.load_digits(return_X_y=True)\n    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)\n\n    graph_search_space = tpot.search_spaces.pipelines.GraphSearchPipeline(\n    root_search_space= tpot.config.get_search_space([\"KNeighborsClassifier\", \"LogisticRegression\", \"DecisionTreeClassifier\"]),\n    leaf_search_space = tpot.config.get_search_space(\"selectors\"), \n    inner_search_space = tpot.config.get_search_space([\"transformers\"]),\n    max_size = 10,\n    )\n\n    est = tpot.TPOTEstimator(\n        client = client,\n        scorers = [\"roc_auc\"],\n        scorers_weights = [1],\n        classification = True,\n        cv = 10,\n        search_space = graph_search_space,\n        max_time_mins = 60,\n        early_stop=10,\n        verbose = 2,\n        )\n    est.fit(X_train, y_train)\n    # this is equivalent to: \n    # est = tpot.TPOTClassifier(population_size= 8, generations=5, n_jobs=4, memory_limit=\"4GB\", verbose=1)\n    est.fit(X_train, y_train)\n    print(scorer(est, X_test, y_test))\n\n    #It is good to close the client and cluster when you are done with them\n    client.close()\n    cluster.close()\n</pre> from dask.distributed import Client, LocalCluster import sklearn import sklearn.datasets import sklearn.metrics import sklearn.model_selection import tpot from dask_jobqueue import SGECluster # or SLURMCluster, PBSCluster, etc. Replace SGE with your scheduler. import os  if os.system(\"which qsub\") != 0:     print(\"Sun Grid Engine is not installed. This example requires Sun Grid Engine to be installed.\") else:     print(\"Sun Grid Engine is installed.\")           cluster = SGECluster(         queue='all.q',         cores=2,         memory=\"50 GB\"      )      cluster.adapt(minimum_jobs=10, maximum_jobs=100)  # auto-scale between 10 and 100 jobs      client = Client(cluster)      scorer = sklearn.metrics.get_scorer('roc_auc_ovr')     X, y = sklearn.datasets.load_digits(return_X_y=True)     X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)      graph_search_space = tpot.search_spaces.pipelines.GraphSearchPipeline(     root_search_space= tpot.config.get_search_space([\"KNeighborsClassifier\", \"LogisticRegression\", \"DecisionTreeClassifier\"]),     leaf_search_space = tpot.config.get_search_space(\"selectors\"),      inner_search_space = tpot.config.get_search_space([\"transformers\"]),     max_size = 10,     )      est = tpot.TPOTEstimator(         client = client,         scorers = [\"roc_auc\"],         scorers_weights = [1],         classification = True,         cv = 10,         search_space = graph_search_space,         max_time_mins = 60,         early_stop=10,         verbose = 2,         )     est.fit(X_train, y_train)     # this is equivalent to:      # est = tpot.TPOTClassifier(population_size= 8, generations=5, n_jobs=4, memory_limit=\"4GB\", verbose=1)     est.fit(X_train, y_train)     print(scorer(est, X_test, y_test))      #It is good to close the client and cluster when you are done with them     client.close()     cluster.close() <pre>Sun Grid Engine is not installed. This example requires Sun Grid Engine to be installed.\n</pre>"},{"location":"Tutorial/7_dask_parallelization/#parallelization","title":"Parallelization\u00b6","text":"<p>This tutorial covers advanced setups for parallelizing TPOT with Dask. If you just want to parallelize TPOT within a single computer with multiple processes, set the n_jobs parameter to the number of threads you want to use and skip this tutorial.</p> <p>TPOT uses Dask for parallelization and defaults to using a dask.distributed.LocalCluster for local parallelization. A user can pass in a custom Dask client or cluster for advanced usage. For example, a multi-node parallelization is possible using the dask-jobqueue package.</p> <p>TPOT can be easily parallelized on a local computer by setting the n_jobs and memory_limit parameters.</p> <p><code>n_jobs</code> dictates how many dask workers to launch. In TPOT this corresponds to the number of pipelines to evaluate in parallel.</p> <p><code>memory_limit</code> is the amount of RAM to use per worker.</p>"},{"location":"Tutorial/7_dask_parallelization/#tpot-with-python-scripts","title":"TPOT with Python Scripts\u00b6","text":"<p>When running tpot from an .py script, it is important to protect code with <code>if __name__==\"__main__\":</code></p> <p>This is due to how parallelization is handled in Python. In short, when Python spawns new processes, each new process reimports code from the relevant .py files, including rerunning code. The context under <code>if __name__==\"__main__\":</code> ensures the code under it only executed by the main process and only once. More info here.</p>"},{"location":"Tutorial/7_dask_parallelization/#manual-dask-clients-and-dashboard","title":"Manual Dask Clients and Dashboard\u00b6","text":"<p>You can also manually initialize a dask client. This can be useful to gain additional control over the parallelization, debugging, as well as viewing a dashboard of the live performance of TPOT.</p> <p>You can find more details in the official documentation here.</p> <p>Dask Python Tutorial Dask Dashboard</p> <p>Note that the if a client is passed in manually, TPOT will ignore n_jobs and memory_limit. If there is no client passed in, TPOT will ignore any global/existing client and create its own.</p>"},{"location":"Tutorial/7_dask_parallelization/#dask-multi-node-parallelization-on-hpc","title":"Dask multi node parallelization on HPC\u00b6","text":"<p>Dask can parallelize across multiple nodes via job queueing systems. This is done using the Dask-Jobqueue package. More information can be found in the official documentation here.</p> <p>To parallelize TPOT with Dask-Jobqueue, simply pass in a client based on a Jobqueue cluster with desired settings into the client parameter. Each job will evaluate a single pipeline.</p> <p>Note that TPOT will ignore n_jobs and memory_limit as these should be set inside the Dask cluster.</p> <p>The following example is specific to the Sun Grid Engine. Other supported clusters can be found in the Dask-Jobqueue documentation here</p>"},{"location":"Tutorial/8_SH_and_cv_early_pruning/","title":"Strategies for reducing computational load","text":"<p>This tutorial covers two strategies for pruning the computational load of TPOT to decrease run time.</p> <p>The following cell illustrates how the population size and budget change over time with the given settings. (Note that tpot happens to converge on this dataset fairly quickly, but we turn off early stop to get the full run. )</p> In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport tpot\n\npopulation_size=30\ninitial_population_size=100\npopulation_scaling = .5\ngenerations_until_end_population = 50\n\nbudget_range = [.3,1]\ngenerations_until_end_budget=50\nbudget_scaling = .5\nstepwise_steps = 5\n\n#Population and budget use stepwise\nfig, ax1 = plt.subplots()\nax2 = ax1.twinx()\n\ninterpolated_values_population = tpot.utils.beta_interpolation(start=initial_population_size, end=population_size, n=generations_until_end_population, n_steps=stepwise_steps, scale=population_scaling)\ninterpolated_values_budget = tpot.utils.beta_interpolation(start=budget_range[0], end=budget_range[1], n=generations_until_end_budget, n_steps=stepwise_steps, scale=budget_scaling)\nax1.step(list(range(len(interpolated_values_population))), interpolated_values_population, label=f\"population size\")\nax2.step(list(range(len(interpolated_values_budget))), interpolated_values_budget, label=f\"budget\", color='r')\nax1.set_xlabel(\"generation\")\nax1.set_ylabel(\"population size\")\nax2.set_ylabel(\"bugdet\")\n\nax1.legend(loc='center left', bbox_to_anchor=(1.1, 0.4))\nax2.legend(loc='center left', bbox_to_anchor=(1.1, 0.3))\nplt.show()\n</pre> import matplotlib.pyplot as plt import tpot  population_size=30 initial_population_size=100 population_scaling = .5 generations_until_end_population = 50  budget_range = [.3,1] generations_until_end_budget=50 budget_scaling = .5 stepwise_steps = 5  #Population and budget use stepwise fig, ax1 = plt.subplots() ax2 = ax1.twinx()  interpolated_values_population = tpot.utils.beta_interpolation(start=initial_population_size, end=population_size, n=generations_until_end_population, n_steps=stepwise_steps, scale=population_scaling) interpolated_values_budget = tpot.utils.beta_interpolation(start=budget_range[0], end=budget_range[1], n=generations_until_end_budget, n_steps=stepwise_steps, scale=budget_scaling) ax1.step(list(range(len(interpolated_values_population))), interpolated_values_population, label=f\"population size\") ax2.step(list(range(len(interpolated_values_budget))), interpolated_values_budget, label=f\"budget\", color='r') ax1.set_xlabel(\"generation\") ax1.set_ylabel(\"population size\") ax2.set_ylabel(\"bugdet\")  ax1.legend(loc='center left', bbox_to_anchor=(1.1, 0.4)) ax2.legend(loc='center left', bbox_to_anchor=(1.1, 0.3)) plt.show()  <pre>/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[2]: Copied! <pre># A Graph pipeline starting with at least one selector as a leaf, potentially followed by a series\n# of stacking classifiers or transformers, and ending with a classifier. The graph will have at most 15 nodes and a max depth of 6.\n\nimport tpot\nimport sklearn\nimport sklearn.datasets\nimport numpy as np\nimport time\nimport tpot\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nimport sklearn\n\nX, y = sklearn.datasets.load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1)\nscorer = sklearn.metrics.make_scorer(sklearn.metrics.roc_auc_score, needs_proba=True, multi_class='ovr')\n\n\nest = tpot.TPOTEstimator(\n    generations=50,\n    max_time_mins=None,\n    scorers=['roc_auc_ovr'],\n    scorers_weights=[1],\n    classification=True,\n    search_space = 'linear',\n    n_jobs=32,\n    cv=10,\n    verbose=3,\n\n    population_size=population_size,\n    initial_population_size=initial_population_size,\n    population_scaling = population_scaling,\n    generations_until_end_population = generations_until_end_population,\n    \n    budget_range = budget_range,\n    generations_until_end_budget=generations_until_end_budget,\n    )\n\n\n\nstart = time.time()\nest.fit(X_train, y_train)\nprint(f\"total time: {time.time()-start}\")\n\nprint(\"test score: \", scorer(est, X_test, y_test))\n</pre> # A Graph pipeline starting with at least one selector as a leaf, potentially followed by a series # of stacking classifiers or transformers, and ending with a classifier. The graph will have at most 15 nodes and a max depth of 6.  import tpot import sklearn import sklearn.datasets import numpy as np import time import tpot import pandas as pd import numpy as np from sklearn.linear_model import LogisticRegression import sklearn  X, y = sklearn.datasets.load_breast_cancer(return_X_y=True) X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1) scorer = sklearn.metrics.make_scorer(sklearn.metrics.roc_auc_score, needs_proba=True, multi_class='ovr')   est = tpot.TPOTEstimator(     generations=50,     max_time_mins=None,     scorers=['roc_auc_ovr'],     scorers_weights=[1],     classification=True,     search_space = 'linear',     n_jobs=32,     cv=10,     verbose=3,      population_size=population_size,     initial_population_size=initial_population_size,     population_scaling = population_scaling,     generations_until_end_population = generations_until_end_population,          budget_range = budget_range,     generations_until_end_budget=generations_until_end_budget,     )    start = time.time() est.fit(X_train, y_train) print(f\"total time: {time.time()-start}\")  print(\"test score: \", scorer(est, X_test, y_test)) <pre>/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n  warnings.warn(\nGeneration:   2%|\u258f         | 1/50 [00:20&lt;16:51, 20.64s/it]</pre> <pre>Generation:  1\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:   4%|\u258d         | 2/50 [00:45&lt;18:39, 23.32s/it]</pre> <pre>Generation:  2\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:   6%|\u258c         | 3/50 [01:19&lt;22:03, 28.17s/it]</pre> <pre>Generation:  3\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:   8%|\u258a         | 4/50 [01:57&lt;24:30, 31.97s/it]</pre> <pre>Generation:  4\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  10%|\u2588         | 5/50 [02:24&lt;22:44, 30.32s/it]</pre> <pre>Generation:  5\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  12%|\u2588\u258f        | 6/50 [03:09&lt;25:40, 35.02s/it]</pre> <pre>Generation:  6\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  14%|\u2588\u258d        | 7/50 [03:50&lt;26:29, 36.96s/it]</pre> <pre>Generation:  7\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  16%|\u2588\u258c        | 8/50 [04:27&lt;26:04, 37.26s/it]</pre> <pre>Generation:  8\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  18%|\u2588\u258a        | 9/50 [05:20&lt;28:45, 42.08s/it]</pre> <pre>Generation:  9\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  20%|\u2588\u2588        | 10/50 [06:03&lt;28:09, 42.25s/it]</pre> <pre>Generation:  10\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  22%|\u2588\u2588\u258f       | 11/50 [07:16&lt;33:43, 51.88s/it]</pre> <pre>Generation:  11\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  24%|\u2588\u2588\u258d       | 12/50 [08:04&lt;31:55, 50.42s/it]</pre> <pre>Generation:  12\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  26%|\u2588\u2588\u258c       | 13/50 [09:13&lt;34:35, 56.10s/it]</pre> <pre>Generation:  13\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  28%|\u2588\u2588\u258a       | 14/50 [10:15&lt;34:49, 58.04s/it]</pre> <pre>Generation:  14\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  30%|\u2588\u2588\u2588       | 15/50 [11:36&lt;37:49, 64.85s/it]</pre> <pre>Generation:  15\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  32%|\u2588\u2588\u2588\u258f      | 16/50 [12:59&lt;39:47, 70.21s/it]</pre> <pre>Generation:  16\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  34%|\u2588\u2588\u2588\u258d      | 17/50 [14:05&lt;37:58, 69.05s/it]</pre> <pre>Generation:  17\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  36%|\u2588\u2588\u2588\u258c      | 18/50 [15:23&lt;38:13, 71.66s/it]</pre> <pre>Generation:  18\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  38%|\u2588\u2588\u2588\u258a      | 19/50 [17:03&lt;41:30, 80.33s/it]</pre> <pre>Generation:  19\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  40%|\u2588\u2588\u2588\u2588      | 20/50 [18:32&lt;41:28, 82.96s/it]</pre> <pre>Generation:  20\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  42%|\u2588\u2588\u2588\u2588\u258f     | 21/50 [22:13&lt;1:00:02, 124.23s/it]</pre> <pre>Generation:  21\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  44%|\u2588\u2588\u2588\u2588\u258d     | 22/50 [24:54&lt;1:03:11, 135.40s/it]</pre> <pre>Generation:  22\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  46%|\u2588\u2588\u2588\u2588\u258c     | 23/50 [27:03&lt;1:00:01, 133.40s/it]</pre> <pre>Generation:  23\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  48%|\u2588\u2588\u2588\u2588\u258a     | 24/50 [29:09&lt;56:48, 131.10s/it]  </pre> <pre>Generation:  24\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  50%|\u2588\u2588\u2588\u2588\u2588     | 25/50 [31:26&lt;55:27, 133.09s/it]</pre> <pre>Generation:  25\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 26/50 [33:27&lt;51:48, 129.50s/it]</pre> <pre>Generation:  26\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 27/50 [35:51&lt;51:12, 133.60s/it]</pre> <pre>Generation:  27\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 28/50 [38:40&lt;52:54, 144.28s/it]</pre> <pre>Generation:  28\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 29/50 [40:49&lt;48:55, 139.80s/it]</pre> <pre>Generation:  29\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 30/50 [43:49&lt;50:36, 151.83s/it]</pre> <pre>Generation:  30\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 31/50 [48:19&lt;59:20, 187.37s/it]</pre> <pre>Generation:  31\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 32/50 [50:18&lt;50:01, 166.77s/it]</pre> <pre>Generation:  32\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 33/50 [52:22&lt;43:38, 154.01s/it]</pre> <pre>Generation:  33\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 34/50 [54:38&lt;39:35, 148.46s/it]</pre> <pre>Generation:  34\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 35/50 [57:43&lt;39:52, 159.52s/it]</pre> <pre>Generation:  35\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 36/50 [1:00:16&lt;36:44, 157.44s/it]</pre> <pre>Generation:  36\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 37/50 [1:04:37&lt;40:51, 188.57s/it]</pre> <pre>Generation:  37\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 38/50 [1:27:43&lt;1:49:32, 547.68s/it]</pre> <pre>Generation:  38\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 39/50 [1:29:40&lt;1:16:43, 418.49s/it]</pre> <pre>Generation:  39\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 40/50 [1:32:56&lt;58:39, 351.95s/it]  </pre> <pre>Generation:  40\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 41/50 [1:36:55&lt;47:41, 317.92s/it]</pre> <pre>Generation:  41\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 42/50 [1:38:54&lt;34:27, 258.41s/it]</pre> <pre>Generation:  42\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 43/50 [1:40:59&lt;25:28, 218.30s/it]</pre> <pre>Generation:  43\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 44/50 [1:42:38&lt;18:14, 182.39s/it]</pre> <pre>Generation:  44\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 45/50 [1:44:18&lt;13:08, 157.68s/it]</pre> <pre>Generation:  45\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 46/50 [1:46:13&lt;09:39, 144.91s/it]</pre> <pre>Generation:  46\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 47/50 [1:48:29&lt;07:06, 142.24s/it]</pre> <pre>Generation:  47\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 48/50 [1:50:06&lt;04:17, 128.67s/it]</pre> <pre>Generation:  48\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 49/50 [1:52:18&lt;02:09, 129.85s/it]</pre> <pre>Generation:  49\nBest roc_auc_score score: 1.0\n</pre> <pre>Generation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [1:54:14&lt;00:00, 137.09s/it]</pre> <pre>Generation:  50\nBest roc_auc_score score: 1.0\n</pre> <pre>\n</pre> <pre>total time: 6862.724096059799\ntest score:  0.9917355371900827\n</pre> In\u00a0[3]: Copied! <pre>import matplotlib.pyplot as plt\nimport tpot\nimport time\nimport sklearn\nimport sklearn.datasets\n\nthreshold_evaluation_pruning = [30, 90]\nthreshold_evaluation_scaling = .2 #.5\ncv = 10\n\n#Population and budget use stepwise\nfig, ax1 = plt.subplots()\n\ninterpolated_values = tpot.utils.beta_interpolation(start=threshold_evaluation_pruning[0], end=threshold_evaluation_pruning[-1], n=cv, n_steps=cv, scale=threshold_evaluation_scaling)\nax1.step(list(range(len(interpolated_values))), interpolated_values, label=f\"threshold\")\nax1.set_xlabel(\"fold\")\nax1.set_ylabel(\"percentile\")\n#ax1.legend(loc='center left', bbox_to_anchor=(1.1, 0.4))\nplt.show()\n</pre> import matplotlib.pyplot as plt import tpot import time import sklearn import sklearn.datasets  threshold_evaluation_pruning = [30, 90] threshold_evaluation_scaling = .2 #.5 cv = 10  #Population and budget use stepwise fig, ax1 = plt.subplots()  interpolated_values = tpot.utils.beta_interpolation(start=threshold_evaluation_pruning[0], end=threshold_evaluation_pruning[-1], n=cv, n_steps=cv, scale=threshold_evaluation_scaling) ax1.step(list(range(len(interpolated_values))), interpolated_values, label=f\"threshold\") ax1.set_xlabel(\"fold\") ax1.set_ylabel(\"percentile\") #ax1.legend(loc='center left', bbox_to_anchor=(1.1, 0.4)) plt.show()  In\u00a0[4]: Copied! <pre>import tpot\nfrom tpot.search_spaces.pipelines import *\nfrom tpot.search_spaces.nodes import *\nfrom tpot.config.get_configspace import get_search_space\nimport sklearn.model_selection\nimport sklearn\n\n\nselectors = get_search_space([\"selectors\",\"selectors_classification\", \"Passthrough\"], random_state=42,)\nestimators = get_search_space(['XGBClassifier'],random_state=42,)\n\nscalers = get_search_space([\"scalers\",\"Passthrough\"],random_state=42,)\n\ntransformers_layer =UnionPipeline([\n                        ChoicePipeline([\n                            DynamicUnionPipeline(get_search_space([\"transformers\"], random_state=42,)),\n                            get_search_space(\"SkipTransformer\"),\n                        ]),\n                        get_search_space(\"Passthrough\")\n                        ]\n                    )\n    \nsearch_space = SequentialPipeline(search_spaces=[\n                                            scalers,\n                                            selectors, \n                                            transformers_layer,\n                                            estimators,\n                                            ])\n</pre> import tpot from tpot.search_spaces.pipelines import * from tpot.search_spaces.nodes import * from tpot.config.get_configspace import get_search_space import sklearn.model_selection import sklearn   selectors = get_search_space([\"selectors\",\"selectors_classification\", \"Passthrough\"], random_state=42,) estimators = get_search_space(['XGBClassifier'],random_state=42,)  scalers = get_search_space([\"scalers\",\"Passthrough\"],random_state=42,)  transformers_layer =UnionPipeline([                         ChoicePipeline([                             DynamicUnionPipeline(get_search_space([\"transformers\"], random_state=42,)),                             get_search_space(\"SkipTransformer\"),                         ]),                         get_search_space(\"Passthrough\")                         ]                     )      search_space = SequentialPipeline(search_spaces=[                                             scalers,                                             selectors,                                              transformers_layer,                                             estimators,                                             ]) In\u00a0[5]: Copied! <pre>import matplotlib.pyplot as plt\nimport tpot\nimport time\nimport sklearn\nimport sklearn.datasets\n\nscorer = sklearn.metrics.make_scorer(sklearn.metrics.roc_auc_score, needs_proba=True, multi_class='ovr')\n\nX, y = sklearn.datasets.make_classification(n_samples=5000, n_features=20, n_classes=5, random_state=1, n_informative=15, n_redundant=5, n_repeated=0, n_clusters_per_class=3, class_sep=.8)\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1)\n\n# search_space = tpot.config.template_search_spaces.get_template_search_spaces(\"linear\",inner_predictors=False, random_state=42)\n</pre> import matplotlib.pyplot as plt import tpot import time import sklearn import sklearn.datasets  scorer = sklearn.metrics.make_scorer(sklearn.metrics.roc_auc_score, needs_proba=True, multi_class='ovr')  X, y = sklearn.datasets.make_classification(n_samples=5000, n_features=20, n_classes=5, random_state=1, n_informative=15, n_redundant=5, n_repeated=0, n_clusters_per_class=3, class_sep=.8) X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1)  # search_space = tpot.config.template_search_spaces.get_template_search_spaces(\"linear\",inner_predictors=False, random_state=42)  <pre>/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n  warnings.warn(\n</pre> In\u00a0[6]: Copied! <pre># no pruning\nest = tpot.TPOTEstimator(  \n                            generations=10,\n                            max_time_mins=None,\n                            scorers=['roc_auc_ovr'],\n                            scorers_weights=[1],\n                            classification=True,\n                            search_space = search_space,\n                            population_size=100,\n                            n_jobs=32,\n                            cv=cv,\n                            verbose=3,\n                            random_state=42,\n                            )\n\n\nstart = time.time()\nest.fit(X_train, y_train)\nprint(f\"total time: {time.time()-start}\")\nprint(\"test score: \", scorer(est, X_test, y_test))\n</pre> # no pruning est = tpot.TPOTEstimator(                               generations=10,                             max_time_mins=None,                             scorers=['roc_auc_ovr'],                             scorers_weights=[1],                             classification=True,                             search_space = search_space,                             population_size=100,                             n_jobs=32,                             cv=cv,                             verbose=3,                             random_state=42,                             )   start = time.time() est.fit(X_train, y_train) print(f\"total time: {time.time()-start}\") print(\"test score: \", scorer(est, X_test, y_test)) <pre>Generation:  10%|\u2588         | 1/10 [02:42&lt;24:26, 162.98s/it]</pre> <pre>Generation:  1\nBest roc_auc_score score: 0.9212394545585599\n</pre> <pre>Generation:  20%|\u2588\u2588        | 2/10 [06:10&lt;25:14, 189.31s/it]</pre> <pre>Generation:  2\nBest roc_auc_score score: 0.921316057689257\n</pre> <pre>Generation:  30%|\u2588\u2588\u2588       | 3/10 [10:07&lt;24:37, 211.00s/it]</pre> <pre>Generation:  3\nBest roc_auc_score score: 0.9291812014325632\n</pre> <pre>Generation:  40%|\u2588\u2588\u2588\u2588      | 4/10 [16:26&lt;27:43, 277.33s/it]</pre> <pre>Generation:  4\nBest roc_auc_score score: 0.9291812014325632\n</pre> <pre>Generation:  50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [21:24&lt;23:44, 284.90s/it]</pre> <pre>Generation:  5\nBest roc_auc_score score: 0.9309353469187138\n</pre> <pre>Generation:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [28:02&lt;21:32, 323.19s/it]</pre> <pre>Generation:  6\nBest roc_auc_score score: 0.9328394699598583\n</pre> <pre>Generation:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [36:02&lt;18:43, 374.57s/it]</pre> <pre>Generation:  7\nBest roc_auc_score score: 0.9341963775600117\n</pre> <pre>Generation:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [45:34&lt;14:34, 437.41s/it]</pre> <pre>Generation:  8\nBest roc_auc_score score: 0.9341963775600117\n</pre> <pre>Generation:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [54:40&lt;07:51, 471.27s/it]</pre> <pre>Generation:  9\nBest roc_auc_score score: 0.9356175936945494\n</pre> <pre>Generation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [1:03:45&lt;00:00, 382.55s/it]</pre> <pre>Generation:  10\nBest roc_auc_score score: 0.9371852416832148\n</pre> <pre>\n</pre> <pre>total time: 3836.4180731773376\ntest score:  0.9422368174356803\n</pre> In\u00a0[7]: Copied! <pre>import tpot.config\nimport tpot.config.template_search_spaces\nimport tpot.search_spaces\n\n\n\n# search_space = tpot.config.get_search_space([\"RandomForestClassifier\"])\n\nest = tpot.TPOTEstimator(  \n                            generations=10,\n                            max_time_mins=None,\n                            scorers=['roc_auc_ovr'],\n                            scorers_weights=[1],\n                            classification=True,\n                            search_space = search_space,\n                            population_size=100,\n                            n_jobs=32,\n                            cv=cv,\n                            verbose=3,\n                            random_state=42,\n\n                            threshold_evaluation_pruning = threshold_evaluation_pruning,\n                            threshold_evaluation_scaling = threshold_evaluation_scaling,\n                            )\n\n\nstart = time.time()\nest.fit(X_train, y_train)\nprint(f\"total time: {time.time()-start}\")\nprint(\"test score: \", scorer(est, X_test, y_test))\n</pre> import tpot.config import tpot.config.template_search_spaces import tpot.search_spaces    # search_space = tpot.config.get_search_space([\"RandomForestClassifier\"])  est = tpot.TPOTEstimator(                               generations=10,                             max_time_mins=None,                             scorers=['roc_auc_ovr'],                             scorers_weights=[1],                             classification=True,                             search_space = search_space,                             population_size=100,                             n_jobs=32,                             cv=cv,                             verbose=3,                             random_state=42,                              threshold_evaluation_pruning = threshold_evaluation_pruning,                             threshold_evaluation_scaling = threshold_evaluation_scaling,                             )   start = time.time() est.fit(X_train, y_train) print(f\"total time: {time.time()-start}\") print(\"test score: \", scorer(est, X_test, y_test)) <pre>Generation:  10%|\u2588         | 1/10 [02:57&lt;26:40, 177.87s/it]</pre> <pre>Generation:  1\nBest roc_auc_score score: 0.9212394545585602\n</pre> <pre>Generation:  20%|\u2588\u2588        | 2/10 [03:57&lt;14:24, 108.05s/it]</pre> <pre>Generation:  2\nBest roc_auc_score score: 0.9212394545585602\n</pre> <pre>Generation:  30%|\u2588\u2588\u2588       | 3/10 [05:58&lt;13:18, 114.13s/it]</pre> <pre>Generation:  3\nBest roc_auc_score score: 0.9212394545585602\n</pre> <pre>Generation:  40%|\u2588\u2588\u2588\u2588      | 4/10 [07:54&lt;11:29, 114.96s/it]</pre> <pre>Generation:  4\nBest roc_auc_score score: 0.9212394545585602\n</pre> <pre>Generation:  50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [10:43&lt;11:11, 134.34s/it]</pre> <pre>Generation:  5\nBest roc_auc_score score: 0.921316057689257\n</pre> <pre>Generation:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [13:16&lt;09:23, 140.78s/it]</pre> <pre>Generation:  6\nBest roc_auc_score score: 0.921316057689257\n</pre> <pre>Generation:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [15:05&lt;06:31, 130.43s/it]</pre> <pre>Generation:  7\nBest roc_auc_score score: 0.921316057689257\n</pre> <pre>Generation:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [18:01&lt;04:49, 144.72s/it]</pre> <pre>Generation:  8\nBest roc_auc_score score: 0.9255953925256337\n</pre> <pre>Generation:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [19:53&lt;02:14, 134.59s/it]</pre> <pre>Generation:  9\nBest roc_auc_score score: 0.9255953925256337\n</pre> <pre>Generation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [21:24&lt;00:00, 128.50s/it]</pre> <pre>Generation:  10\nBest roc_auc_score score: 0.9255953925256337\n</pre> <pre>\n</pre> <pre>total time: 1295.825649023056\ntest score:  0.9320499022897322\n</pre> In\u00a0[8]: Copied! <pre>import matplotlib.pyplot as plt\nimport tpot\n\nselection_evaluation_pruning = [.9, .3]\nselection_evaluation_scaling = .2\n\n#Population and budget use stepwise\nfig, ax1 = plt.subplots()\n\ninterpolated_values = tpot.utils.beta_interpolation(start=selection_evaluation_pruning[0], end=selection_evaluation_pruning[-1], n=cv, n_steps=cv, scale=selection_evaluation_scaling)\nax1.step(list(range(len(interpolated_values))), interpolated_values, label=f\"threshold\")\nax1.set_xlabel(\"fold\")\nax1.set_ylabel(\"percent to select\")\n#ax1.legend(loc='center left', bbox_to_anchor=(1.1, 0.4))\nplt.show()\n</pre> import matplotlib.pyplot as plt import tpot  selection_evaluation_pruning = [.9, .3] selection_evaluation_scaling = .2  #Population and budget use stepwise fig, ax1 = plt.subplots()  interpolated_values = tpot.utils.beta_interpolation(start=selection_evaluation_pruning[0], end=selection_evaluation_pruning[-1], n=cv, n_steps=cv, scale=selection_evaluation_scaling) ax1.step(list(range(len(interpolated_values))), interpolated_values, label=f\"threshold\") ax1.set_xlabel(\"fold\") ax1.set_ylabel(\"percent to select\") #ax1.legend(loc='center left', bbox_to_anchor=(1.1, 0.4)) plt.show()  In\u00a0[9]: Copied! <pre>est = tpot.TPOTEstimator(  \n                            generations=10,\n                            max_time_mins=None,\n                            scorers=['roc_auc_ovr'],\n                            scorers_weights=[1],\n                            classification=True,\n                            search_space = search_space,\n                            population_size=100,\n                            n_jobs=32,\n                            cv=cv,\n                            verbose=3,\n                            random_state=42,\n\n                            selection_evaluation_pruning  = selection_evaluation_pruning,\n                            selection_evaluation_scaling = selection_evaluation_scaling,\n                            )\n\n\nstart = time.time()\nest.fit(X_train, y_train)\nprint(f\"total time: {time.time()-start}\")\nprint(\"test score: \", scorer(est, X_test, y_test))\n</pre> est = tpot.TPOTEstimator(                               generations=10,                             max_time_mins=None,                             scorers=['roc_auc_ovr'],                             scorers_weights=[1],                             classification=True,                             search_space = search_space,                             population_size=100,                             n_jobs=32,                             cv=cv,                             verbose=3,                             random_state=42,                              selection_evaluation_pruning  = selection_evaluation_pruning,                             selection_evaluation_scaling = selection_evaluation_scaling,                             )   start = time.time() est.fit(X_train, y_train) print(f\"total time: {time.time()-start}\") print(\"test score: \", scorer(est, X_test, y_test)) <pre>Generation:  10%|\u2588         | 1/10 [02:23&lt;21:31, 143.50s/it]</pre> <pre>Generation:  1\nBest roc_auc_score score: 0.9212394545585602\n</pre> <pre>Generation:  20%|\u2588\u2588        | 2/10 [04:00&lt;15:30, 116.31s/it]</pre> <pre>Generation:  2\nBest roc_auc_score score: 0.9212394545585602\n</pre> <pre>Generation:  30%|\u2588\u2588\u2588       | 3/10 [05:42&lt;12:48, 109.73s/it]</pre> <pre>Generation:  3\nBest roc_auc_score score: 0.9212394545585602\n</pre> <pre>Generation:  40%|\u2588\u2588\u2588\u2588      | 4/10 [07:36&lt;11:08, 111.45s/it]</pre> <pre>Generation:  4\nBest roc_auc_score score: 0.9212394545585602\n</pre> <pre>Generation:  50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [09:12&lt;08:48, 105.72s/it]</pre> <pre>Generation:  5\nBest roc_auc_score score: 0.9212394545585602\n</pre> <pre>Generation:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [11:04&lt;07:11, 107.81s/it]</pre> <pre>Generation:  6\nBest roc_auc_score score: 0.9212394545585602\n</pre> <pre>Generation:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [12:54&lt;05:26, 108.71s/it]</pre> <pre>Generation:  7\nBest roc_auc_score score: 0.9212394545585602\n</pre> <pre>Generation:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [14:45&lt;03:38, 109.49s/it]</pre> <pre>Generation:  8\nBest roc_auc_score score: 0.925549420935039\n</pre> <pre>Generation:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [16:49&lt;01:54, 114.03s/it]</pre> <pre>Generation:  9\nBest roc_auc_score score: 0.925549420935039\n</pre> <pre>Generation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [18:36&lt;00:00, 111.67s/it]</pre> <pre>Generation:  10\nBest roc_auc_score score: 0.925549420935039\n</pre> <pre>\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:595: UserWarning: n_components is too large: it will be set to 20\n  warnings.warn(\n/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:128: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n  warnings.warn(\n</pre> <pre>total time: 1129.1526980400085\ntest score:  0.9324219154371735\n</pre> In\u00a0[10]: Copied! <pre>est.evaluated_individuals[est.evaluated_individuals['roc_auc_score_step_9']&gt;0]\n</pre> est.evaluated_individuals[est.evaluated_individuals['roc_auc_score_step_9']&gt;0] Out[10]: roc_auc_score Parents Variation_Function Individual Generation roc_auc_score_step_0 Submitted Timestamp Completed Timestamp Eval Error roc_auc_score_step_1 roc_auc_score_step_2 roc_auc_score_step_3 roc_auc_score_step_4 roc_auc_score_step_5 roc_auc_score_step_6 roc_auc_score_step_7 roc_auc_score_step_8 roc_auc_score_step_9 Pareto_Front Instance 0 0.812263 NaN NaN &lt;tpot.search_spaces.pipelines.sequential.Seque... 0.0 0.811153 1.740198e+09 1.740198e+09 None 0.799213 0.807710 0.813587 0.797528 0.820692 0.827614 0.815069 0.805447 0.824616 NaN (MinMaxScaler(), RFE(estimator=ExtraTreesClass... 1 0.848068 NaN NaN &lt;tpot.search_spaces.pipelines.sequential.Seque... 0.0 0.846478 1.740197e+09 1.740197e+09 None 0.839894 0.844619 0.848321 0.846915 0.857902 0.855875 0.827655 0.850938 0.862081 NaN (Passthrough(), RFE(estimator=ExtraTreesClassi... 4 0.831502 NaN NaN &lt;tpot.search_spaces.pipelines.sequential.Seque... 0.0 0.817219 1.740197e+09 1.740197e+09 None 0.827888 0.821911 0.825558 0.830020 0.831529 0.836955 0.844634 0.832499 0.846805 NaN (StandardScaler(), VarianceThreshold(threshold... 5 0.830374 NaN NaN &lt;tpot.search_spaces.pipelines.sequential.Seque... 0.0 0.817150 1.740197e+09 1.740197e+09 None 0.831885 0.820694 0.824899 0.824409 0.827861 0.833923 0.844308 0.832798 0.845818 NaN (MinMaxScaler(), SelectFromModel(estimator=Ext... 6 0.850091 NaN NaN &lt;tpot.search_spaces.pipelines.sequential.Seque... 0.0 0.843524 1.740197e+09 1.740197e+09 None 0.841176 0.840619 0.846209 0.849561 0.854367 0.858035 0.860165 0.845179 0.862077 NaN (Normalizer(norm='max'), SelectFwe(alpha=0.000... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 983 0.886974 (13, 13) ind_mutate &lt;tpot.search_spaces.pipelines.sequential.Seque... 9.0 0.871580 1.740198e+09 1.740198e+09 None 0.887762 0.882504 0.860872 0.898100 0.885523 0.893527 0.904779 0.884557 0.900537 NaN (StandardScaler(), SelectFromModel(estimator=E... 986 0.850281 (35, 470) ind_crossover &lt;tpot.search_spaces.pipelines.sequential.Seque... 9.0 0.837493 1.740198e+09 1.740198e+09 None 0.858289 0.844141 0.851260 0.848909 0.853002 0.856132 0.845356 0.847830 0.860393 NaN (StandardScaler(), SelectPercentile(percentile... 990 0.878811 (866, 866) ind_mutate &lt;tpot.search_spaces.pipelines.sequential.Seque... 9.0 0.875842 1.740198e+09 1.740198e+09 None 0.862567 0.881858 0.885539 0.874347 0.888858 0.891205 0.882103 0.863952 0.881838 NaN (Normalizer(norm='l1'), SelectPercentile(perce... 991 0.835669 (72, 855) ind_crossover &lt;tpot.search_spaces.pipelines.sequential.Seque... 9.0 0.838375 1.740198e+09 1.740198e+09 None 0.844572 0.837234 0.822799 0.818868 0.840971 0.845122 0.816390 0.840709 0.851650 NaN (MinMaxScaler(), SelectPercentile(percentile=4... 992 0.892459 (898, 898) ind_mutate &lt;tpot.search_spaces.pipelines.sequential.Seque... 9.0 0.881991 1.740198e+09 1.740198e+09 None 0.893987 0.882514 0.887394 0.902290 0.894360 0.903944 0.884672 0.889588 0.903849 NaN (RobustScaler(quantile_range=(0.0911728428421,... <p>326 rows \u00d7 20 columns</p> <p>All of the above methods can be used independently or simultaneously as done below:</p> In\u00a0[12]: Copied! <pre>est = tpot.TPOTEstimator(  \n                            generations=10,\n                            max_time_mins=None,\n                            scorers=['roc_auc_ovr'],\n                            scorers_weights=[1],\n                            classification=True,\n                            search_space = search_space,\n                            population_size=30,\n                            n_jobs=3,\n                            cv=cv,\n                            verbose=3,\n\n                            initial_population_size=initial_population_size,\n                            population_scaling = population_scaling,\n                            generations_until_end_population = generations_until_end_population,\n                            \n                            budget_range = budget_range,\n                            generations_until_end_budget=generations_until_end_budget,\n                            \n                            threshold_evaluation_pruning = threshold_evaluation_pruning,\n                            threshold_evaluation_scaling = threshold_evaluation_scaling,\n\n                            selection_evaluation_pruning  = selection_evaluation_pruning,\n                            selection_evaluation_scaling = selection_evaluation_scaling,\n                            )\n\n\nstart = time.time()\nest.fit(X_train, y_train)\nprint(f\"total time: {time.time()-start}\")\nprint(\"test score: \", scorer(est, X_test, y_test))\n</pre> est = tpot.TPOTEstimator(                               generations=10,                             max_time_mins=None,                             scorers=['roc_auc_ovr'],                             scorers_weights=[1],                             classification=True,                             search_space = search_space,                             population_size=30,                             n_jobs=3,                             cv=cv,                             verbose=3,                              initial_population_size=initial_population_size,                             population_scaling = population_scaling,                             generations_until_end_population = generations_until_end_population,                                                          budget_range = budget_range,                             generations_until_end_budget=generations_until_end_budget,                                                          threshold_evaluation_pruning = threshold_evaluation_pruning,                             threshold_evaluation_scaling = threshold_evaluation_scaling,                              selection_evaluation_pruning  = selection_evaluation_pruning,                             selection_evaluation_scaling = selection_evaluation_scaling,                             )   start = time.time() est.fit(X_train, y_train) print(f\"total time: {time.time()-start}\") print(\"test score: \", scorer(est, X_test, y_test)) <pre>Generation:  10%|\u2588         | 1/10 [01:34&lt;14:09, 94.40s/it]</pre> <pre>Generation:  1\nBest roc_auc_score score: 0.8515086951804098\n</pre> <pre>Generation:  20%|\u2588\u2588        | 2/10 [02:26&lt;09:14, 69.36s/it]</pre> <pre>Generation:  2\nBest roc_auc_score score: 0.8515086951804098\n</pre> <pre>Generation:  30%|\u2588\u2588\u2588       | 3/10 [03:41&lt;08:23, 71.97s/it]</pre> <pre>Generation:  3\nBest roc_auc_score score: 0.8515086951804098\n</pre> <pre>Generation:  40%|\u2588\u2588\u2588\u2588      | 4/10 [04:52&lt;07:09, 71.53s/it]</pre> <pre>Generation:  4\nBest roc_auc_score score: 0.8515086951804098\n</pre> <pre>Generation:  50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [05:52&lt;05:37, 67.57s/it]</pre> <pre>Generation:  5\nBest roc_auc_score score: 0.8515086951804098\n</pre> <pre>Generation:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [07:13&lt;04:48, 72.10s/it]</pre> <pre>Generation:  6\nBest roc_auc_score score: 0.8515086951804098\n</pre> <pre>Generation:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [08:06&lt;03:17, 65.84s/it]</pre> <pre>Generation:  7\nBest roc_auc_score score: 0.8515086951804098\n</pre> <pre>Generation:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [08:57&lt;02:02, 61.13s/it]</pre> <pre>Generation:  8\nBest roc_auc_score score: 0.8515086951804098\n</pre> <pre>Generation:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [09:39&lt;00:55, 55.14s/it]</pre> <pre>Generation:  9\nBest roc_auc_score score: 0.8515086951804098\n</pre> <pre>Generation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [10:17&lt;00:00, 61.70s/it]</pre> <pre>Generation:  10\nBest roc_auc_score score: 0.8515086951804098\n</pre> <pre>\n</pre> <pre>total time: 621.607882976532\ntest score:  0.9084772293865335\n</pre>"},{"location":"Tutorial/8_SH_and_cv_early_pruning/#strategies-for-reducing-computational-load","title":"Strategies for reducing computational load\u00b6","text":""},{"location":"Tutorial/8_SH_and_cv_early_pruning/#successive-halving","title":"Successive Halving\u00b6","text":"<p>This idea was first tested with TPOT by Parmentier et al. in \"TPOT-SH: a Faster Optimization Algorithm to Solve the AutoML Problem on Large Datasets\". The algorithm operates in two stages. Initially, it trains early generations using a small data subset and a large population size. Later generations then evaluate a smaller set of promising pipelines on larger, or even full, data portions. This approach rapidly identifies top-performing pipeline configurations through initial rough evaluations, followed by more comprehensive assessments. More information on this strategy in Tutorial 8.</p> <p>In this tutorial, we will cover the following parameters:</p> <p><code>population_size</code></p> <p><code>initial_population_size</code></p> <p><code>population_scaling</code></p> <p><code>generations_until_end_population</code></p> <p><code>budget_range</code></p> <p><code>generations_until_end_budget</code></p> <p><code>budget_scaling</code></p> <p><code>stepwise_steps</code></p> <p>Population size is the number of individuals evaluated each generation. Budget refers to the proportion of data to sample. By manipulating these parameters, we can control how quickly the budget increases and how population size changes over time. Most often, this will be used to start the algorithm by evaluating a large number of pipelines on small subsets of the data to quickly narrow now best models, before later getting a better estimate with larger samples on fewer datasets. This can reduce overall computational cost by not spending as much time evaluating poor performing pipelines.</p> <p><code>population_size</code> determines the number of individuals to evalaute each generation. Sometimes we may want to evaluate more or fewer individuals in the earlier generations. The <code>initial_population_size</code> parameter specifies the starting size of the population. The population size will gradually move from <code>initial_population_size</code> to <code>population_size</code> over the course of <code>generations_until_end_population</code> generations. <code>population_scaling</code> dictates how fast that scaling takes place. The interpolation over <code>generations_until_end_population</code> is done stepwise with the number of steps specified by <code>stepwise_steps</code>.</p> <p>The same process goes for the budget scaling.</p>"},{"location":"Tutorial/8_SH_and_cv_early_pruning/#cv-early-pruning","title":"CV early pruning\u00b6","text":"<p>Most often, we will be evaluating pipelines using cross validation. However, we can often tell within the first few folds whether or not the pipeline is going have a reasonable change of outperforming the previous best pipelines. For example, if the best score so far is .92 AUROC and the average score of the first five folds of our current pipeline is only around .61, we can be reasonably confident that the next five folds are unlikely to this pipeline ahead of the others. We can save a significant amount of compute by not computing the rest of the folds. There are two strategies that TPOT can use to accomplish this (More information on these strategies in Tutorial 8).</p> <ol> <li>Threshold Pruning: Pipelines must achieve a score above a predefined percentile threshold (based on previous pipeline scores) to proceed in each cross-validation (CV) fold.</li> <li>Selection Pruning: Within each population, only the top N% of pipelines (ranked by performance in the previous CV fold) are selected to evaluate in the next fold.\"</li> </ol> <p>We can further reduce computational load by terminating the evaluation of individual pipelines early if the first few CV scores are not promising. Note that this is different than early stopping of the full algorithm. In this section we will cover:</p> <p><code>threshold_evaluation_pruning</code></p> <p><code>threshold_evaluation_scaling</code></p> <p><code>min_history_threshold</code></p> <p><code>selection_evaluation_pruning</code></p> <p><code>selection_evaluation_scaling</code></p> <p>Threshold early stopping uses previous scores to identify and terminate the cross validation evaluation of poorly performing pipelines. We calculate the percentile scores from the previously evaluated pipelines. A pipeline must reach the given percentile each fold for the next to be evaluated, otherwise the pipeline is discarded.</p> <p>The <code>threshold_evaluation_pruning</code> parameter is a list that specifies the starting and ending percentiles to use as a threshold for the evaluation early stopping. W The <code>threshold_evaluation_scaling</code> parameter is a float that controls the rate at which the threshold moves from the start to end percentile. The <code>min_history_threshold</code> parameter specifies the minimum number of previous scores needed before using threshold early stopping. This ensures that the algorithm has enough historical data to make an informed decision about when to stop evaluating pipelines.</p> <p>Selection early stopping uses a selection algorithm after each fold to select which algorithms will be evaluated for the next fold. For example, after evaluating 100 individuals on fold 1, we may want to only evaluate the best 50 for the remaining folds.</p> <p>The <code>selection_evaluation_pruning</code> parameter is a list that specifies the lower and upper percentage of the population size to select each round of CV. This is used to determine which individuals to evaluate in the next generation. The <code>selection_evaluation_scaling</code> parameter is a float that controls the rate at which the selection threshold moves from the start to end percentile.</p> <p>By manipulating these parameters, we can control how the algorithm selects individuals to evaluate in the next generation and when to stop evaluating pipelines that are not performing well.</p> <p>In practice, the values of these parameters will depend on the specific problem and the available computational resources.</p> <p>In the following sections, we will show you how to set and manipulate these parameters using Python code in a Jupyter Notebook. We will also provide examples of how these parameters can affect the performance of the algorithm.</p> <p>(Note that in these small test cases, you may not notice much or any performance improvements, these are more likely to be more beneficial in real world scenarios with larger datasets and slower evaluating pipelines.)</p> <p>Considerations: It is important to be aware of how CV pruning interacts with the evolutionary algorithm. When pipelines are pruned with one of these methods, they are removed from the live population and thus are no longer used to inform the TPOT algorithm. If too many pipelines are pruned, this could reduce the diversity of pipelines per generation, and limit TPOT's ability to learn. Additionally, the pruning methods may interact with how long it takes TPOT to run. If the pruning algorithm removes the slightly less performant but faster running pipelines, TPOT will most likely fill the next generation with only slower running pipelines, thus technically increasing the total runtime. This may be acceptable since more compute is dedicated to the higher performing pipelines.</p>"},{"location":"Tutorial/9_Genetic_Algorithm_Overview/","title":"9 Genetic Algorithm Overview","text":"<p>Objective functions can optionally take in step, budget, and generations.</p> <p>step - The same objective function will be run for #evaluation_early_stop_steps, the current step will be passed into the function as an interger. (This is useful for getting a single fold of cross validation for example).</p> <p>budget - A parameter that varies over the course of the generations. Gets passed into the objective function as a float between 0 and 1. If the budget of the previous evaluation is less than the current budget, it will get re-evaluated. Useful for using smaller datasets earlier in training.</p> <p>generations - an int corresponding to the current generation number.</p> In\u00a0[1]: Copied! <pre>#knapsack problem\nimport numpy as np\nimport tpot\nimport random\nimport matplotlib.pyplot as plt\nfrom dask.distributed import Client, LocalCluster\n\nclass SubsetSelector(tpot.individual.BaseIndividual):\n    def __init__(   self,\n                    values,\n                    initial_set = None,\n                    k=1, #step size for shuffling\n                ):\n\n        if isinstance(values, int):\n            self.values = set(range(0,values))\n        else:\n            self.values = set(values)\n\n\n        if initial_set is None:\n            self.subsets = set(random.choices(values, k=k))\n        else:\n            self.subsets = set(initial_set)\n\n        self.k = k\n\n        self.mutation_list = [self._mutate_add, self._mutate_remove]\n        self.crossover_list = [self._crossover_swap]\n        \n\n    def mutate(self, rng=None):\n        mutation_list_copy = self.mutation_list.copy()\n        random.shuffle(mutation_list_copy)\n        for func in mutation_list_copy:\n            if func():\n                return True\n        return False\n\n    def crossover(self, ind2, rng=None):\n        crossover_list_copy = self.crossover_list.copy()\n        random.shuffle(crossover_list_copy)\n        for func in crossover_list_copy:\n            if func(ind2):\n                return True\n        return False\n\n    def _mutate_add(self,):\n        not_included = list(self.values.difference(self.subsets))\n        if len(not_included) &gt; 1:\n            self.subsets.update(random.sample(not_included, k=min(self.k, len(not_included))))\n            return True\n        else:\n            return False\n\n    def _mutate_remove(self,):\n        if len(self.subsets) &gt; 1:\n            self.subsets = self.subsets - set(random.sample(list(self.subsets), k=min(self.k, len(self.subsets)-1) ))\n\n    def _crossover_swap(self, ss2):\n        diffs = self.subsets.symmetric_difference(ss2.subsets)\n\n        if len(diffs) == 0:\n            return False\n        for v in diffs:\n            self.subsets.discard(v)\n            ss2.subsets.discard(v)\n            random.choice([self.subsets, ss2.subsets]).add(v)\n        \n        return True\n\n    def unique_id(self):\n        return str(tuple(sorted(self.subsets)))\n\ndef individual_generator():\n    while True:\n        yield SubsetSelector(values=np.arange(len(values)))\n\n\nvalues = np.random.randint(200,size=100)\nweights = np.random.random(200)*10\nmax_weight = 50\n\ndef simple_objective(ind, **kwargs):\n    subset = np.array(list(ind.subsets))\n    if len(subset) == 0:\n        return 0, 0\n\n    total_weight = np.sum(weights[subset])\n    total_value = np.sum(values[subset])\n\n    if total_weight &gt; max_weight:\n        total_value = 0\n\n    return total_value, total_weight\n\nobjective_names = [\"Value\", \"Weight\"]\nobjective_function_weights = [1,-1]\n\n\n\nevolver = tpot.evolvers.BaseEvolver(   individual_generator=individual_generator(), \n                                objective_functions=[simple_objective],\n                                objective_function_weights = objective_function_weights,\n                                bigger_is_better = True,\n                                population_size= 100,\n                                objective_names = objective_names,\n                                generations= 100,\n                                n_jobs=32,\n                                verbose = 1,\n\n)\n\nevolver.optimize()\n</pre> #knapsack problem import numpy as np import tpot import random import matplotlib.pyplot as plt from dask.distributed import Client, LocalCluster  class SubsetSelector(tpot.individual.BaseIndividual):     def __init__(   self,                     values,                     initial_set = None,                     k=1, #step size for shuffling                 ):          if isinstance(values, int):             self.values = set(range(0,values))         else:             self.values = set(values)           if initial_set is None:             self.subsets = set(random.choices(values, k=k))         else:             self.subsets = set(initial_set)          self.k = k          self.mutation_list = [self._mutate_add, self._mutate_remove]         self.crossover_list = [self._crossover_swap]               def mutate(self, rng=None):         mutation_list_copy = self.mutation_list.copy()         random.shuffle(mutation_list_copy)         for func in mutation_list_copy:             if func():                 return True         return False      def crossover(self, ind2, rng=None):         crossover_list_copy = self.crossover_list.copy()         random.shuffle(crossover_list_copy)         for func in crossover_list_copy:             if func(ind2):                 return True         return False      def _mutate_add(self,):         not_included = list(self.values.difference(self.subsets))         if len(not_included) &gt; 1:             self.subsets.update(random.sample(not_included, k=min(self.k, len(not_included))))             return True         else:             return False      def _mutate_remove(self,):         if len(self.subsets) &gt; 1:             self.subsets = self.subsets - set(random.sample(list(self.subsets), k=min(self.k, len(self.subsets)-1) ))      def _crossover_swap(self, ss2):         diffs = self.subsets.symmetric_difference(ss2.subsets)          if len(diffs) == 0:             return False         for v in diffs:             self.subsets.discard(v)             ss2.subsets.discard(v)             random.choice([self.subsets, ss2.subsets]).add(v)                  return True      def unique_id(self):         return str(tuple(sorted(self.subsets)))  def individual_generator():     while True:         yield SubsetSelector(values=np.arange(len(values)))   values = np.random.randint(200,size=100) weights = np.random.random(200)*10 max_weight = 50  def simple_objective(ind, **kwargs):     subset = np.array(list(ind.subsets))     if len(subset) == 0:         return 0, 0      total_weight = np.sum(weights[subset])     total_value = np.sum(values[subset])      if total_weight &gt; max_weight:         total_value = 0      return total_value, total_weight  objective_names = [\"Value\", \"Weight\"] objective_function_weights = [1,-1]    evolver = tpot.evolvers.BaseEvolver(   individual_generator=individual_generator(),                                  objective_functions=[simple_objective],                                 objective_function_weights = objective_function_weights,                                 bigger_is_better = True,                                 population_size= 100,                                 objective_names = objective_names,                                 generations= 100,                                 n_jobs=32,                                 verbose = 1,  )  evolver.optimize() <pre>/opt/anaconda3/envs/tpotenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nGeneration: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [01:43&lt;00:00,  1.03s/it]\n</pre> In\u00a0[2]: Copied! <pre>final_population_results = evolver.population.evaluated_individuals\nfinal_population_results.reset_index(inplace=True)\nfinal_population_results = final_population_results.rename(columns = {'index':'Selected Index'})\n\nbest_idx = final_population_results[\"Value\"].idxmax()\nbest_individual = final_population_results.loc[best_idx]['Individual']\nprint(\"best subset\", best_individual.subsets)\nprint(\"Best value {0}, weight {1}\".format(final_population_results.loc[best_idx, \"Value\"],final_population_results.loc[best_idx, \"Weight\"]))\nprint()\n\nprint(\"All results\")\nfinal_population_results\n</pre> final_population_results = evolver.population.evaluated_individuals final_population_results.reset_index(inplace=True) final_population_results = final_population_results.rename(columns = {'index':'Selected Index'})  best_idx = final_population_results[\"Value\"].idxmax() best_individual = final_population_results.loc[best_idx]['Individual'] print(\"best subset\", best_individual.subsets) print(\"Best value {0}, weight {1}\".format(final_population_results.loc[best_idx, \"Value\"],final_population_results.loc[best_idx, \"Weight\"])) print()  print(\"All results\") final_population_results <pre>best subset {1, 8, 9, 16, 17, 22, 23, 24, 28, 29, 31, 42, 43, 48, 50, 61, 62, 68, 80, 89, 91, 97, 98}\nBest value 3070.0, weight 49.01985602703945\n\nAll results\n</pre> Out[2]: Selected Index Value Weight Parents Variation_Function Individual Generation Submitted Timestamp Completed Timestamp Eval Error Pareto_Front 0 (40,) 89.0 9.883465 NaN NaN &lt;__main__.SubsetSelector object at 0x32aa80eb0&gt; 0.0 1.740209e+09 1.740209e+09 None NaN 1 (45,) 116.0 6.643557 NaN NaN &lt;__main__.SubsetSelector object at 0x32aa83b50&gt; 0.0 1.740209e+09 1.740209e+09 None NaN 2 (52,) 172.0 9.273163 NaN NaN &lt;__main__.SubsetSelector object at 0x32aa81210&gt; 0.0 1.740209e+09 1.740209e+09 None NaN 3 (33,) 112.0 1.594347 NaN NaN &lt;__main__.SubsetSelector object at 0x32aa838e0&gt; 0.0 1.740209e+09 1.740209e+09 None NaN 4 (37,) 90.0 3.273826 NaN NaN &lt;__main__.SubsetSelector object at 0x32aa83e50&gt; 0.0 1.740209e+09 1.740209e+09 None NaN ... ... ... ... ... ... ... ... ... ... ... ... 9995 (1, 9, 16, 23, 24, 31, 77, 79) 998.0 11.622582 ((1, 9, 16, 17, 23, 24, 31, 77), (1, 9, 16, 17... ind_mutate &lt;__main__.SubsetSelector object at 0x3a739b010&gt; 99.0 1.740209e+09 1.740209e+09 None NaN 9996 (1, 8, 9, 16, 22, 23, 24, 28, 29, 31, 48, 49, ... 0.0 51.400433 ((1, 8, 9, 16, 17, 22, 23, 24, 28, 29, 31, 48,... ind_mutate &lt;__main__.SubsetSelector object at 0x3af9a4460&gt; 99.0 1.740209e+09 1.740209e+09 None NaN 9997 (1, 4, 8, 9, 16, 17, 23, 24, 31, 49, 68, 77, 8... 1728.0 15.997430 ((1, 4, 8, 9, 16, 17, 23, 24, 31, 68, 77, 88, ... ind_mutate &lt;__main__.SubsetSelector object at 0x3aa303430&gt; 99.0 1.740209e+09 1.740209e+09 None 1.0 9998 (8, 9, 17, 23, 24, 25, 31, 51, 77) 972.0 11.991547 ((8, 9, 17, 23, 24, 31, 77, 88), (8, 9, 17, 23... ind_mutate &lt;__main__.SubsetSelector object at 0x3a7399600&gt; 99.0 1.740209e+09 1.740209e+09 None NaN 9999 (8, 23, 24, 73, 79) 648.0 12.109013 ((8, 16, 17, 23, 24), (8, 16, 17, 23, 24)) ind_mutate &lt;__main__.SubsetSelector object at 0x3a88d4430&gt; 99.0 1.740209e+09 1.740209e+09 None NaN <p>10000 rows \u00d7 11 columns</p> In\u00a0[3]: Copied! <pre>from scipy.stats import binned_statistic_2d\n\ny = final_population_results[\"Value\"]\nx = final_population_results[\"Weight\"]\nc = final_population_results[\"Generation\"]\n\nx_bins = np.linspace(0, 100, 100)\ny_bins = np.linspace(0, 3000, 100)\n\nret = binned_statistic_2d(x, y, c, statistic=np.mean, bins=[x_bins, y_bins])\n\nfig, ax1 = plt.subplots(1, 1, figsize=(12, 4))\n\nim = ax1.imshow(ret.statistic.T, origin='lower', extent=(0,100,0,3000), vmin=0, vmax=100, aspect=.03)\nax1.set_xlabel(\"Weight\")\nax1.set_ylabel(\"Value\")\nax1.set_title(\"Binned Average Generation\")\n\ncbar = fig.colorbar(im,)\ncbar.set_label('Generation')\nplt.tight_layout()\n</pre> from scipy.stats import binned_statistic_2d  y = final_population_results[\"Value\"] x = final_population_results[\"Weight\"] c = final_population_results[\"Generation\"]  x_bins = np.linspace(0, 100, 100) y_bins = np.linspace(0, 3000, 100)  ret = binned_statistic_2d(x, y, c, statistic=np.mean, bins=[x_bins, y_bins])  fig, ax1 = plt.subplots(1, 1, figsize=(12, 4))  im = ax1.imshow(ret.statistic.T, origin='lower', extent=(0,100,0,3000), vmin=0, vmax=100, aspect=.03) ax1.set_xlabel(\"Weight\") ax1.set_ylabel(\"Value\") ax1.set_title(\"Binned Average Generation\")  cbar = fig.colorbar(im,) cbar.set_label('Generation') plt.tight_layout()"},{"location":"Tutorial/amltk_search_space_parser_example/","title":"Amltk search space parser example","text":"<p>The AMLTK (https://github.com/automl/amltk) provides a framework for developing AutoML systems. One component of this system is the search space definitions.</p> <p>TPOT provides a function called tpot.utils.tpot_parser which can convert a search space defined in the AMLTK API into the search space class used by TPOT. This allows users to define a single search space to be used by both algorithms, facilitating better comparisons. Below is an example of a few search spaces defined in AMLTK and how to use them in TPOT.</p> <p>Note: this feature is still experimental and not all features present in the AMLTK API are fully supported in TPOT yet. (For example, automated splitting based on categorical vs numeric with amltk.pipeline.Split is not currently implemented in the parser.)</p> In\u00a0[1]: Copied! <pre>from sklearn.compose import make_column_selector\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.svm import SVC\nfrom amltk.pipeline import Choice, Component, Sequential, Split\nimport tpot\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.compose import make_column_transformer\nimport tpot\nimport numpy as np\nimport sklearn\nimport sklearn.datasets\nimport pandas as pd\n# create dummy pandas dataset with both categorical and numerical columns\nX, y = sklearn.datasets.make_classification(n_samples=100, n_features=5, n_informative=3, n_classes=2, random_state=42)\nX = pd.DataFrame(X, columns=[f\"num_{i}\" for i in range(5)])\n# add 5 categorical columns\nfor i in range(5):\n    X[f\"cat_{i}\"] = np.random.choice([\"A\", \"B\", \"C\"], size=100)\ny = y.flatten()\n# train test split\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.5)\n\n# TODO: implement support for this condition\n# select_categories = make_column_selector(dtype_include=object)\n# select_numerical = make_column_selector(dtype_include=np.number)\n\n# split_imputation = Split(\n#     {\n#         \"categories\": [SimpleImputer(strategy=\"constant\", fill_value=\"missing\"), OneHotEncoder(drop=\"first\")],\n#         \"numerics\": Component(SimpleImputer, space={\"strategy\": [\"mean\", \"median\"]}),\n#     },\n#     config={\"categories\": select_categories, \"numerics\": select_numerical}, #not yet supported\n#     name=\"feature_preprocessing\",\n# )\n# split_imputation\n\nselect_categories = make_column_selector(dtype_include=object)\nselect_numerical = make_column_selector(dtype_include=np.number)\n\ncat_selector = make_column_transformer((\"passthrough\", select_categories))\nnum_selector = make_column_transformer((\"passthrough\", select_numerical))\n\n\nsplit_imputation = Split(\n    {\n        \"categories\": [cat_selector,SimpleImputer(strategy=\"constant\", fill_value=\"missing\"), OneHotEncoder(drop=\"first\", sparse_output=False)],\n        \"numerics\": [num_selector, Component(SimpleImputer, space={\"strategy\": [\"mean\", \"median\"]})],\n    },\n    name=\"split_imputation\",\n)\nsplit_imputation\n</pre> from sklearn.compose import make_column_selector import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.impute import SimpleImputer from sklearn.preprocessing import OneHotEncoder from sklearn.svm import SVC from amltk.pipeline import Choice, Component, Sequential, Split import tpot from sklearn.preprocessing import FunctionTransformer from sklearn.compose import make_column_transformer import tpot import numpy as np import sklearn import sklearn.datasets import pandas as pd # create dummy pandas dataset with both categorical and numerical columns X, y = sklearn.datasets.make_classification(n_samples=100, n_features=5, n_informative=3, n_classes=2, random_state=42) X = pd.DataFrame(X, columns=[f\"num_{i}\" for i in range(5)]) # add 5 categorical columns for i in range(5):     X[f\"cat_{i}\"] = np.random.choice([\"A\", \"B\", \"C\"], size=100) y = y.flatten() # train test split X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.5)  # TODO: implement support for this condition # select_categories = make_column_selector(dtype_include=object) # select_numerical = make_column_selector(dtype_include=np.number)  # split_imputation = Split( #     { #         \"categories\": [SimpleImputer(strategy=\"constant\", fill_value=\"missing\"), OneHotEncoder(drop=\"first\")], #         \"numerics\": Component(SimpleImputer, space={\"strategy\": [\"mean\", \"median\"]}), #     }, #     config={\"categories\": select_categories, \"numerics\": select_numerical}, #not yet supported #     name=\"feature_preprocessing\", # ) # split_imputation  select_categories = make_column_selector(dtype_include=object) select_numerical = make_column_selector(dtype_include=np.number)  cat_selector = make_column_transformer((\"passthrough\", select_categories)) num_selector = make_column_transformer((\"passthrough\", select_numerical))   split_imputation = Split(     {         \"categories\": [cat_selector,SimpleImputer(strategy=\"constant\", fill_value=\"missing\"), OneHotEncoder(drop=\"first\", sparse_output=False)],         \"numerics\": [num_selector, Component(SimpleImputer, space={\"strategy\": [\"mean\", \"median\"]})],     },     name=\"split_imputation\", ) split_imputation <pre>\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[1], line 7\n      5 from sklearn.preprocessing import OneHotEncoder\n      6 from sklearn.svm import SVC\n----&gt; 7 from amltk.pipeline import Choice, Component, Sequential, Split\n      8 import tpot\n      9 from sklearn.preprocessing import FunctionTransformer\n\nModuleNotFoundError: No module named 'amltk'</pre> In\u00a0[\u00a0]: Copied! <pre>from tpot.builtin_modules import Passthrough, ZeroCount\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.feature_selection import VarianceThreshold, SelectKBest\n\nselectors = Choice(\n    Component(VarianceThreshold, space={\"threshold\": (0.1,1)}),\n    Component(SelectKBest, space={\"k\": (1, 10)}),\n    name=\"selectors\",\n)\n\n\ntransformers = Split(\n    {\n        \"passthrough\": Passthrough(),\n        \"polynomial\": Component(PolynomialFeatures, space={\"degree\": [2, 3]}),\n        \"zerocount\" : ZeroCount(),\n    },\n    # config={\"categories\": select_categories, \"numerics\": select_numerical},\n    name=\"transformers\",\n)\n\npipeline = (\n    Sequential(name=\"my_pipeline\")\n    &gt;&gt; split_imputation\n    # &gt;&gt; Component(SimpleImputer, space={\"strategy\": [\"mean\", \"median\"]})  # Choose either mean or median\n    \n    &gt;&gt; selectors\n    &gt;&gt; transformers\n    &gt;&gt; Choice(\n        # Our pipeline can choose between two different estimators\n        Component(\n            RandomForestClassifier,\n            space={\"n_estimators\": (10, 100), \"criterion\": [\"gini\", \"log_loss\"]},\n            config={\"max_depth\": 3},\n        ),\n        Component(SVC, space={\"kernel\": [\"linear\", \"rbf\", \"poly\"]}),\n        name=\"estimator\",\n    )\n)\n\n# Display the amltk Pipeline\npipeline\n</pre> from tpot.builtin_modules import Passthrough, ZeroCount from sklearn.preprocessing import PolynomialFeatures from sklearn.decomposition import PCA  from sklearn.feature_selection import VarianceThreshold, SelectKBest  selectors = Choice(     Component(VarianceThreshold, space={\"threshold\": (0.1,1)}),     Component(SelectKBest, space={\"k\": (1, 10)}),     name=\"selectors\", )   transformers = Split(     {         \"passthrough\": Passthrough(),         \"polynomial\": Component(PolynomialFeatures, space={\"degree\": [2, 3]}),         \"zerocount\" : ZeroCount(),     },     # config={\"categories\": select_categories, \"numerics\": select_numerical},     name=\"transformers\", )  pipeline = (     Sequential(name=\"my_pipeline\")     &gt;&gt; split_imputation     # &gt;&gt; Component(SimpleImputer, space={\"strategy\": [\"mean\", \"median\"]})  # Choose either mean or median          &gt;&gt; selectors     &gt;&gt; transformers     &gt;&gt; Choice(         # Our pipeline can choose between two different estimators         Component(             RandomForestClassifier,             space={\"n_estimators\": (10, 100), \"criterion\": [\"gini\", \"log_loss\"]},             config={\"max_depth\": 3},         ),         Component(SVC, space={\"kernel\": [\"linear\", \"rbf\", \"poly\"]}),         name=\"estimator\",     ) )  # Display the amltk Pipeline pipeline <pre>\u256d\u2500 Sequential(my_pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Split(split_imputation) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 \u256d\u2500 Sequential(categories) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Sequential(numerics) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502\n\u2502 \u2502 \u2502 \u256d\u2500 Fixed(ColumnTransformer) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u256d\u2500 Fixed(ColumnTransformer) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 \u2502 item ColumnTransformer(transformers=[('pass\u2026 \u2502 \u2502 \u2502 \u2502 item ColumnTransformer(transformers=[('passth\u2026 \u2502 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 \u2502      'passthrough',                          \u2502 \u2502 \u2502 \u2502      'passthrough',                            \u2502 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 \u2502                                       &lt;skle\u2026 \u2502 \u2502 \u2502 \u2502                                       &lt;sklear\u2026 \u2502 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 \u2502      object at 0x7d354d946290&gt;)])            \u2502 \u2502 \u2502 \u2502      object at 0x7d34edf94fa0&gt;)])              \u2502 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                        \u2193                         \u2502 \u2502                         \u2193                          \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 \u256d\u2500 Fixed(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u256d\u2500 Component(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e         \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 \u2502 item SimpleImputer(fill_value='missing',     \u2502 \u2502 \u2502 \u2502 item  class SimpleImputer(...)         \u2502         \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 \u2502      strategy='constant')                    \u2502 \u2502 \u2502 \u2502 space {'strategy': ['mean', 'median']} \u2502         \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f         \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                        \u2193                         \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502\n\u2502 \u2502 \u2502 \u256d\u2500 Fixed(OneHotEncoder) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502                                                        \u2502 \u2502\n\u2502 \u2502 \u2502 \u2502 item OneHotEncoder(drop='first',             \u2502 \u2502                                                        \u2502 \u2502\n\u2502 \u2502 \u2502 \u2502      sparse_output=False)                    \u2502 \u2502                                                        \u2502 \u2502\n\u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502                                                        \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502                                                        \u2193                                                        \u2502\n\u2502 \u256d\u2500 Choice(selectors) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e                                     \u2502\n\u2502 \u2502 \u256d\u2500 Component(SelectKBest) \u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Component(VarianceThreshold) \u2500\u2500\u2500\u2500\u2500\u256e \u2502                                     \u2502\n\u2502 \u2502 \u2502 item  class SelectKBest(...) \u2502 \u2502 item  class VarianceThreshold(...) \u2502 \u2502                                     \u2502\n\u2502 \u2502 \u2502 space {'k': (1, 10)}         \u2502 \u2502 space {'threshold': (0.1, 1)}      \u2502 \u2502                                     \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502                                     \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                     \u2502\n\u2502                                                        \u2193                                                        \u2502\n\u2502 \u256d\u2500 Split(transformers) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e       \u2502\n\u2502 \u2502 \u256d\u2500 Sequential(passthrough) \u2500\u256e \u256d\u2500 Sequential(polynomial) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Sequential(zerocount) \u2500\u256e \u2502       \u2502\n\u2502 \u2502 \u2502 \u256d\u2500 Fixed(Passthrough) \u2500\u256e  \u2502 \u2502 \u256d\u2500 Component(PolynomialFeatures) \u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u256d\u2500 Fixed(ZeroCount) \u2500\u256e  \u2502 \u2502       \u2502\n\u2502 \u2502 \u2502 \u2502 item Passthrough()   \u2502  \u2502 \u2502 \u2502 item  class PolynomialFeatures(...) \u2502 \u2502 \u2502 \u2502 item ZeroCount()   \u2502  \u2502 \u2502       \u2502\n\u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f  \u2502 \u2502 \u2502 space {'degree': [2, 3]}            \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f  \u2502 \u2502       \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502       \u2502\n\u2502 \u2502                               \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                             \u2502       \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f       \u2502\n\u2502                                                        \u2193                                                        \u2502\n\u2502 \u256d\u2500 Choice(estimator) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 \u256d\u2500 Component(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Component(SVC) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e            \u2502 \u2502\n\u2502 \u2502 \u2502 item   class RandomForestClassifier(...)     \u2502 \u2502 item  class SVC(...)                        \u2502            \u2502 \u2502\n\u2502 \u2502 \u2502 config {'max_depth': 3}                      \u2502 \u2502 space {'kernel': ['linear', 'rbf', 'poly']} \u2502            \u2502 \u2502\n\u2502 \u2502 \u2502 space  {                                     \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f            \u2502 \u2502\n\u2502 \u2502 \u2502            'n_estimators': (10, 100),        \u2502                                                            \u2502 \u2502\n\u2502 \u2502 \u2502            'criterion': [                    \u2502                                                            \u2502 \u2502\n\u2502 \u2502 \u2502                'gini',                       \u2502                                                            \u2502 \u2502\n\u2502 \u2502 \u2502                'log_loss'                    \u2502                                                            \u2502 \u2502\n\u2502 \u2502 \u2502            ]                                 \u2502                                                            \u2502 \u2502\n\u2502 \u2502 \u2502        }                                     \u2502                                                            \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                            \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> Out[\u00a0]: In\u00a0[\u00a0]: Copied! <pre>#convert to tpot search space\ntpot_search_space = tpot.utils.tpot_parser(pipeline)\n\n# sample a pipeline from the tpot search space\ntpot_search_space.generate().export_pipeline()\n</pre> #convert to tpot search space tpot_search_space = tpot.utils.tpot_parser(pipeline)  # sample a pipeline from the tpot search space tpot_search_space.generate().export_pipeline() Out[\u00a0]: <pre>Pipeline(steps=[('featureunion-1',\n                 FeatureUnion(transformer_list=[('pipeline-1',\n                                                 Pipeline(steps=[('columntransformer',\n                                                                  ColumnTransformer(transformers=[('passthrough',\n                                                                                                   'passthrough',\n                                                                                                   &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7d354d946290&gt;)])),\n                                                                 ('simpleimputer',\n                                                                  SimpleImputer(fill_value='missing',\n                                                                                strategy='constant')),\n                                                                 ('onehotencode...\n                 VarianceThreshold(threshold=0.6738938110936)),\n                ('featureunion-2',\n                 FeatureUnion(transformer_list=[('pipeline-1',\n                                                 Pipeline(steps=[('passthrough',\n                                                                  Passthrough())])),\n                                                ('pipeline-2',\n                                                 Pipeline(steps=[('polynomialfeatures',\n                                                                  PolynomialFeatures(degree=3))])),\n                                                ('pipeline-3',\n                                                 Pipeline(steps=[('zerocount',\n                                                                  ZeroCount())]))])),\n                ('randomforestclassifier',\n                 RandomForestClassifier(n_estimators=16))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('featureunion-1',\n                 FeatureUnion(transformer_list=[('pipeline-1',\n                                                 Pipeline(steps=[('columntransformer',\n                                                                  ColumnTransformer(transformers=[('passthrough',\n                                                                                                   'passthrough',\n                                                                                                   &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7d354d946290&gt;)])),\n                                                                 ('simpleimputer',\n                                                                  SimpleImputer(fill_value='missing',\n                                                                                strategy='constant')),\n                                                                 ('onehotencode...\n                 VarianceThreshold(threshold=0.6738938110936)),\n                ('featureunion-2',\n                 FeatureUnion(transformer_list=[('pipeline-1',\n                                                 Pipeline(steps=[('passthrough',\n                                                                  Passthrough())])),\n                                                ('pipeline-2',\n                                                 Pipeline(steps=[('polynomialfeatures',\n                                                                  PolynomialFeatures(degree=3))])),\n                                                ('pipeline-3',\n                                                 Pipeline(steps=[('zerocount',\n                                                                  ZeroCount())]))])),\n                ('randomforestclassifier',\n                 RandomForestClassifier(n_estimators=16))])</pre> \u00a0featureunion-1: FeatureUnion?Documentation for featureunion-1: FeatureUnion<pre>FeatureUnion(transformer_list=[('pipeline-1',\n                                Pipeline(steps=[('columntransformer',\n                                                 ColumnTransformer(transformers=[('passthrough',\n                                                                                  'passthrough',\n                                                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7d354d946290&gt;)])),\n                                                ('simpleimputer',\n                                                 SimpleImputer(fill_value='missing',\n                                                               strategy='constant')),\n                                                ('onehotencoder',\n                                                 OneHotEncoder(drop='first',\n                                                               sparse_output=False))])),\n                               ('pipeline-2',\n                                Pipeline(steps=[('columntransformer',\n                                                 ColumnTransformer(transformers=[('passthrough',\n                                                                                  'passthrough',\n                                                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7d34edf94fa0&gt;)])),\n                                                ('simpleimputer',\n                                                 SimpleImputer(strategy='median'))]))])</pre> pipeline-1\u00a0columntransformer: ColumnTransformer?Documentation for columntransformer: ColumnTransformer<pre>ColumnTransformer(transformers=[('passthrough', 'passthrough',\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7d354d946290&gt;)])</pre> passthrough<pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7d354d946290&gt;</pre> passthrough<pre>passthrough</pre> \u00a0SimpleImputer?Documentation for SimpleImputer<pre>SimpleImputer(fill_value='missing', strategy='constant')</pre> \u00a0OneHotEncoder?Documentation for OneHotEncoder<pre>OneHotEncoder(drop='first', sparse_output=False)</pre> pipeline-2\u00a0columntransformer: ColumnTransformer?Documentation for columntransformer: ColumnTransformer<pre>ColumnTransformer(transformers=[('passthrough', 'passthrough',\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7d34edf94fa0&gt;)])</pre> passthrough<pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7d34edf94fa0&gt;</pre> passthrough<pre>passthrough</pre> \u00a0SimpleImputer?Documentation for SimpleImputer<pre>SimpleImputer(strategy='median')</pre> \u00a0VarianceThreshold?Documentation for VarianceThreshold<pre>VarianceThreshold(threshold=0.6738938110936)</pre> \u00a0featureunion-2: FeatureUnion?Documentation for featureunion-2: FeatureUnion<pre>FeatureUnion(transformer_list=[('pipeline-1',\n                                Pipeline(steps=[('passthrough',\n                                                 Passthrough())])),\n                               ('pipeline-2',\n                                Pipeline(steps=[('polynomialfeatures',\n                                                 PolynomialFeatures(degree=3))])),\n                               ('pipeline-3',\n                                Pipeline(steps=[('zerocount', ZeroCount())]))])</pre> pipeline-1Passthrough<pre>Passthrough()</pre> pipeline-2\u00a0PolynomialFeatures?Documentation for PolynomialFeatures<pre>PolynomialFeatures(degree=3)</pre> pipeline-3ZeroCount<pre>ZeroCount()</pre> \u00a0RandomForestClassifier?Documentation for RandomForestClassifier<pre>RandomForestClassifier(n_estimators=16)</pre> In\u00a0[\u00a0]: Copied! <pre>est = tpot.TPOTEstimator(\n    scorers = [\"roc_auc\"],\n    scorers_weights = [1],\n    classification = True,\n    cv = 5,\n    search_space = tpot_search_space, #converted search space goes here\n    population_size= 10,\n    generations = 2,\n    max_eval_time_mins = 60*5,\n    verbose = 5,\n    n_jobs=10,\n)\n\nest.fit(X_train, y_train)\n</pre>    est = tpot.TPOTEstimator(     scorers = [\"roc_auc\"],     scorers_weights = [1],     classification = True,     cv = 5,     search_space = tpot_search_space, #converted search space goes here     population_size= 10,     generations = 2,     max_eval_time_mins = 60*5,     verbose = 5,     n_jobs=10, )  est.fit(X_train, y_train) <pre>Generation:  50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:02&lt;00:02,  2.60s/it]</pre> <pre>Generation:  1\nBest roc_auc_score score: 0.976\n</pre> <pre>Generation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:03&lt;00:00,  1.57s/it]\n2024-09-09 17:25:40,301 - distributed.scheduler - ERROR - Removing worker 'tcp://127.0.0.1:39897' caused the cluster to lose scattered data, which can't be recovered: {'ndarray-3f2f44921e6e9cc40ef07cfcd8ae90fb', 'DataFrame-5551f84174fd651642ff10eb71e30b22'} (stimulus_id='handle-worker-cleanup-1725927940.3010821')\n</pre> <pre>Generation:  2\nBest roc_auc_score score: 0.984\n</pre> Out[\u00a0]: <pre>TPOTEstimator(classification=True, generations=2, max_eval_time_mins=300,\n              n_jobs=10, population_size=10, scorers=['roc_auc'],\n              scorers_weights=[1],\n              search_space=&lt;tpot.search_spaces.pipelines.sequential.SequentialPipeline object at 0x7d34ec1efbb0&gt;,\n              verbose=5)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0TPOTEstimatoriFitted<pre>TPOTEstimator(classification=True, generations=2, max_eval_time_mins=300,\n              n_jobs=10, population_size=10, scorers=['roc_auc'],\n              scorers_weights=[1],\n              search_space=&lt;tpot.search_spaces.pipelines.sequential.SequentialPipeline object at 0x7d34ec1efbb0&gt;,\n              verbose=5)</pre> In\u00a0[\u00a0]: Copied! <pre>est.fitted_pipeline_\n</pre> est.fitted_pipeline_ Out[\u00a0]: <pre>Pipeline(steps=[('featureunion-1',\n                 FeatureUnion(transformer_list=[('pipeline-1',\n                                                 Pipeline(steps=[('columntransformer',\n                                                                  ColumnTransformer(transformers=[('passthrough',\n                                                                                                   'passthrough',\n                                                                                                   &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7d34eb307cd0&gt;)])),\n                                                                 ('simpleimputer',\n                                                                  SimpleImputer(fill_value='missing',\n                                                                                strategy='constant')),\n                                                                 ('onehotencode...\n                 VarianceThreshold(threshold=0.1557560591318)),\n                ('featureunion-2',\n                 FeatureUnion(transformer_list=[('pipeline-1',\n                                                 Pipeline(steps=[('passthrough',\n                                                                  Passthrough())])),\n                                                ('pipeline-2',\n                                                 Pipeline(steps=[('polynomialfeatures',\n                                                                  PolynomialFeatures())])),\n                                                ('pipeline-3',\n                                                 Pipeline(steps=[('zerocount',\n                                                                  ZeroCount())]))])),\n                ('randomforestclassifier',\n                 RandomForestClassifier(criterion='log_loss',\n                                        n_estimators=80))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiFitted<pre>Pipeline(steps=[('featureunion-1',\n                 FeatureUnion(transformer_list=[('pipeline-1',\n                                                 Pipeline(steps=[('columntransformer',\n                                                                  ColumnTransformer(transformers=[('passthrough',\n                                                                                                   'passthrough',\n                                                                                                   &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7d34eb307cd0&gt;)])),\n                                                                 ('simpleimputer',\n                                                                  SimpleImputer(fill_value='missing',\n                                                                                strategy='constant')),\n                                                                 ('onehotencode...\n                 VarianceThreshold(threshold=0.1557560591318)),\n                ('featureunion-2',\n                 FeatureUnion(transformer_list=[('pipeline-1',\n                                                 Pipeline(steps=[('passthrough',\n                                                                  Passthrough())])),\n                                                ('pipeline-2',\n                                                 Pipeline(steps=[('polynomialfeatures',\n                                                                  PolynomialFeatures())])),\n                                                ('pipeline-3',\n                                                 Pipeline(steps=[('zerocount',\n                                                                  ZeroCount())]))])),\n                ('randomforestclassifier',\n                 RandomForestClassifier(criterion='log_loss',\n                                        n_estimators=80))])</pre> \u00a0featureunion-1: FeatureUnion?Documentation for featureunion-1: FeatureUnion<pre>FeatureUnion(transformer_list=[('pipeline-1',\n                                Pipeline(steps=[('columntransformer',\n                                                 ColumnTransformer(transformers=[('passthrough',\n                                                                                  'passthrough',\n                                                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7d34eb307cd0&gt;)])),\n                                                ('simpleimputer',\n                                                 SimpleImputer(fill_value='missing',\n                                                               strategy='constant')),\n                                                ('onehotencoder',\n                                                 OneHotEncoder(drop='first',\n                                                               sparse_output=False))])),\n                               ('pipeline-2',\n                                Pipeline(steps=[('columntransformer',\n                                                 ColumnTransformer(transformers=[('passthrough',\n                                                                                  'passthrough',\n                                                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7d34eb307d30&gt;)])),\n                                                ('simpleimputer',\n                                                 SimpleImputer(strategy='median'))]))])</pre> pipeline-1\u00a0columntransformer: ColumnTransformer?Documentation for columntransformer: ColumnTransformer<pre>ColumnTransformer(transformers=[('passthrough', 'passthrough',\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7d34eb307cd0&gt;)])</pre> passthrough<pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7d34eb307cd0&gt;</pre> passthrough<pre>passthrough</pre> \u00a0SimpleImputer?Documentation for SimpleImputer<pre>SimpleImputer(fill_value='missing', strategy='constant')</pre> \u00a0OneHotEncoder?Documentation for OneHotEncoder<pre>OneHotEncoder(drop='first', sparse_output=False)</pre> pipeline-2\u00a0columntransformer: ColumnTransformer?Documentation for columntransformer: ColumnTransformer<pre>ColumnTransformer(transformers=[('passthrough', 'passthrough',\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7d34eb307d30&gt;)])</pre> passthrough<pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7d34eb307d30&gt;</pre> passthrough<pre>passthrough</pre> \u00a0SimpleImputer?Documentation for SimpleImputer<pre>SimpleImputer(strategy='median')</pre> \u00a0VarianceThreshold?Documentation for VarianceThreshold<pre>VarianceThreshold(threshold=0.1557560591318)</pre> \u00a0featureunion-2: FeatureUnion?Documentation for featureunion-2: FeatureUnion<pre>FeatureUnion(transformer_list=[('pipeline-1',\n                                Pipeline(steps=[('passthrough',\n                                                 Passthrough())])),\n                               ('pipeline-2',\n                                Pipeline(steps=[('polynomialfeatures',\n                                                 PolynomialFeatures())])),\n                               ('pipeline-3',\n                                Pipeline(steps=[('zerocount', ZeroCount())]))])</pre> pipeline-1Passthrough<pre>Passthrough()</pre> pipeline-2\u00a0PolynomialFeatures?Documentation for PolynomialFeatures<pre>PolynomialFeatures()</pre> pipeline-3ZeroCount<pre>ZeroCount()</pre> \u00a0RandomForestClassifier?Documentation for RandomForestClassifier<pre>RandomForestClassifier(criterion='log_loss', n_estimators=80)</pre> In\u00a0[\u00a0]: Copied! <pre>est.predict(X_test)\n</pre> est.predict(X_test) Out[\u00a0]: <pre>array([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n       0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n       1, 0, 0, 0, 0, 0])</pre>"},{"location":"archived/","title":"Index","text":"\u26a0\ufe0f Warning <p>This documentation is for the archived version of TPOT, which is no longer maintained. For the latest version, click here.</p> <p>Consider TPOT your Data Science Assistant. TPOT is a Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming.</p> <p></p> <p></p> <p>TPOT will automate the most tedious part of machine learning by intelligently exploring thousands of possible pipelines to find the best one for your data.</p> <p></p> An example machine learning pipeline <p></p> <p>Once TPOT is finished searching (or you get tired of waiting), it provides you with the Python code for the best pipeline it found so you can tinker with the pipeline from there.</p> <p></p> An example TPOT pipeline <p></p> <p>TPOT is built on top of scikit-learn, so all of the code it generates should look familiar... if you're familiar with scikit-learn, anyway.</p> <p>TPOT is still under active development and we encourage you to check back on this repository regularly for updates.</p>"},{"location":"archived/api/","title":"Api","text":"\u26a0\ufe0f Warning <p>This documentation is for the archived version of TPOT, which is no longer maintained. For the latest version, click here.</p>"},{"location":"archived/api/#tpot-api","title":"TPOT API","text":""},{"location":"archived/api/#classification","title":"Classification","text":"<pre>class tpot.TPOTClassifier(generations=100, population_size=100,\n                          offspring_size=None, mutation_rate=0.9,\n                          crossover_rate=0.1,\n                          scoring='accuracy', cv=5,\n                          subsample=1.0, n_jobs=1,\n                          max_time_mins=None, max_eval_time_mins=5,\n                          random_state=None, config_dict=None,\n                          template=None,\n                          warm_start=False,\n                          memory=None,\n                          use_dask=False,\n                          periodic_checkpoint_folder=None,\n                          early_stop=None,\n                          verbosity=0,\n                          disable_update_check=False,\n                          log_file=None\n                          )</pre> source <p>Automated machine learning for supervised classification tasks.</p> <p>The TPOTClassifier performs an intelligent search over machine learning pipelines that can contain supervised classification models, preprocessors, feature selection techniques, and any other estimator or transformer that follows the scikit-learn API. The TPOTClassifier will also search over the hyperparameters of all objects in the pipeline.</p> <p>By default, TPOTClassifier will search over a broad range of supervised classification algorithms, transformers, and their parameters. However, the algorithms, transformers, and hyperparameters that the TPOTClassifier searches over can be fully customized using the <code>config_dict</code> parameter.</p> <p>Read more in the User Guide.</p> Parameters: generations: int or None optional (default=100)  Number of iterations to the run pipeline optimization process. It must be a positive number or None. If None, the parameter max_time_mins must be defined as the runtime limit.  Generally, TPOT will work better when you give it more generations (and therefore time) to optimize the pipeline.  TPOT will evaluate population_size + generations \u00d7 offspring_size pipelines in total.  population_size: int, optional (default=100)  Number of individuals to retain in the genetic programming population every generation. Must be a positive number.  Generally, TPOT will work better when you give it more individuals with which to optimize the pipeline.  offspring_size: int, optional (default=None)  Number of offspring to produce in each genetic programming generation. Must be a positive number. By default, the number of offspring is equal to the number of population size.  mutation_rate: float, optional (default=0.9)  Mutation rate for the genetic programming algorithm in the range [0.0, 1.0]. This parameter tells the GP algorithm how many pipelines to apply random changes to every generation.  mutation_rate + crossover_rate cannot exceed 1.0.  We recommend using the default parameter unless you understand how the mutation rate affects GP algorithms.  crossover_rate: float, optional (default=0.1)  Crossover rate for the genetic programming algorithm in the range [0.0, 1.0]. This parameter tells the genetic programming algorithm how many pipelines to \"breed\" every generation.  mutation_rate + crossover_rate cannot exceed 1.0.  We recommend using the default parameter unless you understand how the crossover rate affects GP algorithms.  scoring: string or callable, optional (default='accuracy')  Function used to evaluate the quality of a given pipeline for the classification problem. The following built-in scoring functions can be used:  'accuracy', 'adjusted_rand_score', 'average_precision', 'balanced_accuracy', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'neg_log_loss', 'precision' etc. (suffixes apply as with \u2018f1\u2019), 'recall' etc. (suffixes apply as with \u2018f1\u2019), \u2018jaccard\u2019 etc. (suffixes apply as with \u2018f1\u2019), 'roc_auc', \u2018roc_auc_ovr\u2019, \u2018roc_auc_ovo\u2019, \u2018roc_auc_ovr_weighted\u2019, \u2018roc_auc_ovo_weighted\u2019  If you would like to use a custom scorer, you can pass the callable object/function with signature scorer(estimator, X, y).  See the section on scoring functions for more details.   cv: int, cross-validation generator, or an iterable, optional (default=5)  Cross-validation strategy used when evaluating pipelines.  Possible inputs: <ul> <li>integer, to specify the number of folds in an unshuffled StratifiedKFold,</li> <li>An object to be used as a cross-validation generator, or</li> <li>An iterable yielding train/test splits.</li> subsample: float, optional (default=1.0)  Fraction of training samples that are used during the TPOT optimization process. Must be in the range (0.0, 1.0].  Setting subsample=0.5 tells TPOT to use a random subsample of half of the training data. This subsample will remain the same during the entire pipeline optimization process.  n_jobs: integer, optional (default=1)  Number of processes to use in parallel for evaluating pipelines during the TPOT optimization process.  Setting n_jobs=-1 will use as many cores as available on the computer. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. Beware that using multiple processes on the same machine may cause memory issues for large datasets.  max_time_mins: integer or None, optional (default=None)  How many minutes TPOT has to optimize the pipeline.  If not None, this setting will allow TPOT to run until max_time_mins minutes elapsed and then stop. TPOT will stop earlier if generations is set and all generations are already evaluated.  max_eval_time_mins: float, optional (default=5)  How many minutes TPOT has to evaluate a single pipeline.  Setting this parameter to higher values will allow TPOT to evaluate more complex pipelines, but will also allow TPOT to run longer. Use this parameter to help prevent TPOT from wasting time on evaluating time-consuming pipelines.  random_state: integer or None, optional (default=None)  The seed of the pseudo random number generator used in TPOT.  Use this parameter to make sure that TPOT will give you the same results each time you run it against the same data set with that seed.  config_dict: Python dictionary, string, or None, optional (default=None)  A configuration dictionary for customizing the operators and parameters that TPOT searches in the optimization process.  Possible inputs are: <ul> <li>Python dictionary, TPOT will use your custom configuration,</li> <li>string 'TPOT light', TPOT will use a built-in configuration with only fast models and preprocessors, or</li> <li>string 'TPOT MDR', TPOT will use a built-in configuration specialized for genomic studies, or</li> <li>string 'TPOT sparse': TPOT will use a configuration dictionary with a one-hot encoder and the operators normally included in TPOT that also support sparse matrices, or</li> <li>None, TPOT will use the default TPOTClassifier configuration.</li> </ul> See the built-in configurations section for the list of configurations included with TPOT, and the custom configuration section for more information and examples of how to create your own TPOT configurations.  template: string (default=None)  Template of predefined pipeline structure. The option is for specifying a desired structure for the machine learning pipeline evaluated in TPOT.   So far this option only supports linear pipeline structure. Each step in the pipeline should be a main class of operators (Selector, Transformer, Classifier) or a specific operator (e.g. `SelectPercentile`) defined in TPOT operator configuration. If one step is a main class, TPOT will randomly assign all subclass operators (subclasses of [`SelectorMixin`](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_selection/base.py#L17), [`TransformerMixin`](https://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html), [`ClassifierMixin`](https://scikit-learn.org/stable/modules/generated/sklearn.base.ClassifierMixin.html) in scikit-learn) to that step. Steps in the template are delimited by \"-\", e.g. \"SelectPercentile-Transformer-Classifier\". By default value of template is None, TPOT generates tree-based pipeline randomly.  See the  template option in tpot section for more details.  warm_start: boolean, optional (default=False)  Flag indicating whether the TPOT instance will reuse the population from previous calls to fit().  Setting warm_start=True can be useful for running TPOT for a short time on a dataset, checking the results, then resuming the TPOT run from where it left off.  memory: a joblib.Memory object or string, optional (default=None)  If supplied, pipeline will cache each transformer after calling fit. This feature is used to avoid computing the fit transformers within a pipeline if the parameters and input data are identical with another fitted pipeline during optimization process. More details about memory caching in scikit-learn documentation  Possible inputs are: <ul> <li>String 'auto': TPOT uses memory caching with a temporary directory and cleans it up upon shutdown, or</li> <li>Path of a caching directory, TPOT uses memory caching with the provided directory and TPOT does NOT clean the caching directory up upon shutdown, or</li> <li>Memory object, TPOT uses the instance of joblib.Memory for memory caching and TPOT does NOT clean the caching directory up upon shutdown, or</li> <li>None, TPOT does not use memory caching.</li> </ul> use_dask: boolean, optional (default: False)  Whether to use Dask-ML's pipeline optimiziations. This avoid re-fitting the same estimator on the same split of data multiple times. It will also provide more detailed diagnostics when using Dask's distributed scheduler.  See avoid repeated work for more details.  periodic_checkpoint_folder: path string, optional (default: None)  If supplied, a folder in which TPOT will periodically save pipelines in pareto front so far while optimizing. Currently once per generation but not more often than once per 30 seconds. Useful in multiple cases: <ul> <li>Sudden death before TPOT could save optimized pipeline</li> <li>Track its progress</li> <li>Grab pipelines while it's still optimizing</li> </ul> early_stop: integer, optional (default: None)  How many generations TPOT checks whether there is no improvement in optimization process.  Ends the optimization process if there is no improvement in the given number of generations.  verbosity: integer, optional (default=0)  How much information TPOT communicates while it's running.  Possible inputs are: <ul> <li>0, TPOT will print nothing,</li> <li>1, TPOT will print minimal information,</li> <li>2, TPOT will print more information and provide a progress bar, or</li> <li>3, TPOT will print everything and provide a progress bar.</li> </ul> disable_update_check: boolean, optional (default=False)  Flag indicating whether the TPOT version checker should be disabled.  The update checker will tell you when a new version of TPOT has been released.  log_file: file-like class (io.TextIOWrapper or io.StringIO) or string, optional (default: None)   Save progress content to a file. If it is a string for the path and file name of the desired output file, TPOT will create the file and write log into it. If it is None, TPOT will output log into sys.stdout  Attributes: fitted_pipeline_: scikit-learn Pipeline object  The best pipeline that TPOT discovered during the pipeline optimization process, fitted on the entire training dataset.  pareto_front_fitted_pipelines_: Python dictionary  Dictionary containing the all pipelines on the TPOT Pareto front, where the key is the string representation of the pipeline and the value is the corresponding pipeline fitted on the entire training dataset.  The TPOT Pareto front provides a trade-off between pipeline complexity (i.e., the number of steps in the pipeline) and the predictive performance of the pipeline.  Note: pareto_front_fitted_pipelines_ is only available when verbosity=3.  evaluated_individuals_: Python dictionary  Dictionary containing all pipelines that were evaluated during the pipeline optimization process, where the key is the string representation of the pipeline and the value is a tuple containing (# of steps in pipeline, accuracy metric for the pipeline).  This attribute is primarily for internal use, but may be useful for looking at the other pipelines that TPOT evaluated.  <p>Example</p> <pre><code>from tpot import TPOTClassifier\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\n\ndigits = load_digits()\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,\n                                                    train_size=0.75, test_size=0.25)\n\ntpot = TPOTClassifier(generations=5, population_size=50, verbosity=2)\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_test, y_test))\ntpot.export('tpot_digits_pipeline.py')\n</code></pre> <p>Functions</p> fit(features, classes[, sample_weight, groups]) Run the TPOT optimization process on the given training data. predict(features) Use the optimized pipeline to predict the classes for a feature set. predict_proba(features) Use the optimized pipeline to estimate the class probabilities for a feature set. score(testing_features, testing_classes) Returns the optimized pipeline's score on the given testing data using the user-specified scoring function. export(output_file_name) Export the optimized pipeline as Python code. <p> <pre><code>fit(features, classes, sample_weight=None, groups=None)\n</code></pre></p>  Run the TPOT optimization process on the given training data.  Uses genetic programming to optimize a machine learning pipeline that maximizes the score on the provided features and target. This pipeline optimization procedure uses internal k-fold cross-validaton to avoid overfitting on the provided data. At the end of the pipeline optimization procedure, the best pipeline is then trained on the entire set of provided samples.  Parameters: features: array-like {n_samples, n_features}  Feature matrix  TPOT and all scikit-learn algorithms assume that the features will be numerical and there will be no missing values. As such, when a feature matrix is provided to TPOT, all missing values will automatically be replaced (i.e., imputed) using median value imputation.  If you wish to use a different imputation strategy than median imputation, please make sure to apply imputation to your feature set prior to passing it to TPOT.  classes: array-like {n_samples}  List of class labels for prediction  sample_weight: array-like {n_samples}, optional  Per-sample weights. Higher weights indicate more importance. If specified, sample_weight will be passed to any pipeline element whose fit() function accepts a sample_weight argument. By default, using sample_weight does not affect tpot's scoring functions, which determine preferences between pipelines.  groups: array-like, with shape {n_samples, }, optional  Group labels for the samples used when performing cross-validation.  This parameter should only be used in conjunction with sklearn's Group cross-validation functions, such as sklearn.model_selection.GroupKFold.  Returns: self: object  Returns a copy of the fitted TPOT object  <p> <pre><code>predict(features)\n</code></pre></p>  Use the optimized pipeline to predict the classes for a feature set.  Parameters: features: array-like {n_samples, n_features}  Feature matrix  Returns: predictions: array-like {n_samples}  Predicted classes for the samples in the feature matrix  <p> <pre><code>predict_proba(features)\n</code></pre></p>  Use the optimized pipeline to estimate the class probabilities for a feature set.  Note: This function will only work for pipelines whose final classifier supports the predict_proba function. TPOT will raise an error otherwise.  Parameters: features: array-like {n_samples, n_features}  Feature matrix  Returns: predictions: array-like {n_samples, n_classes}  The class probabilities of the input samples  <p> <pre><code>score(testing_features, testing_classes)\n</code></pre></p>  Returns the optimized pipeline's score on the given testing data using the user-specified scoring function.  The default scoring function for TPOTClassifier is 'accuracy'.  Parameters: testing_features: array-like {n_samples, n_features}  Feature matrix of the testing set  testing_classes: array-like {n_samples}  List of class labels for prediction in the testing set  Returns: accuracy_score: float  The estimated test set accuracy according to the user-specified scoring function.  <p> <pre><code>export(output_file_name, data_file_path)\n</code></pre></p>  Export the optimized pipeline as Python code.  See the usage documentation for example usage of the export function.  Parameters: output_file_name: string  String containing the path and file name of the desired output file  data_file_path: string  By default, the path of input dataset is 'PATH/TO/DATA/FILE' by default. If data_file_path is another string, the path will be replaced.  Returns: exported_code_string: string  The whole pipeline text as a string should be returned if output_file_name is not specified."},{"location":"archived/api/#regression","title":"Regression","text":"<pre>class tpot.TPOTRegressor(generations=100, population_size=100,\n                         offspring_size=None, mutation_rate=0.9,\n                         crossover_rate=0.1,\n                         scoring='neg_mean_squared_error', cv=5,\n                         subsample=1.0, n_jobs=1,\n                         max_time_mins=None, max_eval_time_mins=5,\n                         random_state=None, config_dict=None,\n                         template=None,\n                         warm_start=False,\n                         memory=None,\n                         use_dask=False,\n                         periodic_checkpoint_folder=None,\n                         early_stop=None,\n                         verbosity=0,\n                         disable_update_check=False)</pre> source <p>Automated machine learning for supervised regression tasks.</p> <p>The TPOTRegressor performs an intelligent search over machine learning pipelines that can contain supervised regression models, preprocessors, feature selection techniques, and any other estimator or transformer that follows the scikit-learn API. The TPOTRegressor will also search over the hyperparameters of all objects in the pipeline.</p> <p>By default, TPOTRegressor will search over a broad range of supervised regression models, transformers, and their hyperparameters. However, the models, transformers, and parameters that the TPOTRegressor searches over can be fully customized using the <code>config_dict</code> parameter.</p> <p>Read more in the User Guide.</p> Parameters: generations: int or None, optional (default=100)  Number of iterations to the run pipeline optimization process. It must be a positive number or None. If None, the parameter max_time_mins must be defined as the runtime limit.  Generally, TPOT will work better when you give it more generations (and therefore time) to optimize the pipeline.  TPOT will evaluate population_size + generations \u00d7 offspring_size pipelines in total.  population_size: int, optional (default=100)  Number of individuals to retain in the genetic programming population every generation. Must be a positive number.  Generally, TPOT will work better when you give it more individuals with which to optimize the pipeline.  offspring_size: int, optional (default=None)  Number of offspring to produce in each genetic programming generation. Must be a positive number. By default, the number of offspring is equal to the number of population size.  mutation_rate: float, optional (default=0.9)  Mutation rate for the genetic programming algorithm in the range [0.0, 1.0]. This parameter tells the GP algorithm how many pipelines to apply random changes to every generation.  mutation_rate + crossover_rate cannot exceed 1.0.  We recommend using the default parameter unless you understand how the mutation rate affects GP algorithms.  crossover_rate: float, optional (default=0.1)  Crossover rate for the genetic programming algorithm in the range [0.0, 1.0]. This parameter tells the genetic programming algorithm how many pipelines to \"breed\" every generation.  mutation_rate + crossover_rate cannot exceed 1.0.  We recommend using the default parameter unless you understand how the crossover rate affects GP algorithms.  scoring: string or callable, optional (default='neg_mean_squared_error')  Function used to evaluate the quality of a given pipeline for the regression problem. The following built-in scoring functions can be used:  'neg_median_absolute_error', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'r2'  Note that we recommend using the neg version of mean squared error and related metrics so TPOT will minimize (instead of maximize) the metric.  If you would like to use a custom scorer, you can pass the callable object/function with signature scorer(estimator, X, y).  See the section on scoring functions for more details.  cv: int, cross-validation generator, or an iterable, optional (default=5)  Cross-validation strategy used when evaluating pipelines.  Possible inputs: <ul> <li>integer, to specify the number of folds in an unshuffled KFold,</li> <li>An object to be used as a cross-validation generator, or</li> <li>An iterable yielding train/test splits.</li> </ul> subsample: float, optional (default=1.0)  Fraction of training samples that are used during the TPOT optimization process. Must be in the range (0.0, 1.0].  Setting subsample=0.5 tells TPOT to use a random subsample of half of the training data. This subsample will remain the same during the entire pipeline optimization process.  n_jobs: integer, optional (default=1)  Number of processes to use in parallel for evaluating pipelines during the TPOT optimization process.  Setting n_jobs=-1 will use as many cores as available on the computer. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. Beware that using multiple processes on the same machine may cause memory issues for large datasets  max_time_mins: integer or None, optional (default=None)  How many minutes TPOT has to optimize the pipeline.  If not None, this setting will allow TPOT to run until max_time_mins minutes elapsed and then stop. TPOT will stop earlier if generations is set and all generations are already evaluated.  max_eval_time_mins: float, optional (default=5)  How many minutes TPOT has to evaluate a single pipeline.  Setting this parameter to higher values will allow TPOT to evaluate more complex pipelines, but will also allow TPOT to run longer. Use this parameter to help prevent TPOT from wasting time on evaluating time-consuming pipelines.  random_state: integer or None, optional (default=None)  The seed of the pseudo random number generator used in TPOT.  Use this parameter to make sure that TPOT will give you the same results each time you run it against the same data set with that seed.  config_dict: Python dictionary, string, or None, optional (default=None)  A configuration dictionary for customizing the operators and parameters that TPOT searches in the optimization process.  Possible inputs are: <ul> <li>Python dictionary, TPOT will use your custom configuration,</li> <li>string 'TPOT light', TPOT will use a built-in configuration with only fast models and preprocessors, or</li> <li>string 'TPOT MDR', TPOT will use a built-in configuration specialized for genomic studies, or</li> <li>string 'TPOT sparse': TPOT will use a configuration dictionary with a one-hot encoder and the operators normally included in TPOT that also support sparse matrices, or</li> <li>None, TPOT will use the default TPOTRegressor configuration.</li> </ul> See the built-in configurations section for the list of configurations included with TPOT, and the custom configuration section for more information and examples of how to create your own TPOT configurations.  template: string (default=None)  Template of predefined pipeline structure. The option is for specifying a desired structure for the machine learning pipeline evaluated in TPOT.   So far this option only supports linear pipeline structure. Each step in the pipeline should be a main class of operators (Selector, Transformer or Regressor) or a specific operator (e.g. `SelectPercentile`) defined in TPOT operator configuration. If one step is a main class, TPOT will randomly assign all subclass operators (subclasses of [`SelectorMixin`](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_selection/base.py#L17), [`TransformerMixin`](https://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html) or [`RegressorMixin`](https://scikit-learn.org/stable/modules/generated/sklearn.base.RegressorMixin.html) in scikit-learn) to that step. Steps in the template are delimited by \"-\", e.g. \"SelectPercentile-Transformer-Regressor\". By default value of template is None, TPOT generates tree-based pipeline randomly.  See the  template option in tpot section for more details.  warm_start: boolean, optional (default=False)  Flag indicating whether the TPOT instance will reuse the population from previous calls to fit().  Setting warm_start=True can be useful for running TPOT for a short time on a dataset, checking the results, then resuming the TPOT run from where it left off.  memory: a joblib.Memory object or string, optional (default=None)  If supplied, pipeline will cache each transformer after calling fit. This feature is used to avoid computing the fit transformers within a pipeline if the parameters and input data are identical with another fitted pipeline during optimization process. More details about memory caching in scikit-learn documentation  Possible inputs are: <ul> <li>String 'auto': TPOT uses memory caching with a temporary directory and cleans it up upon shutdown, or</li> <li>Path of a caching directory, TPOT uses memory caching with the provided directory and TPOT does NOT clean the caching directory up upon shutdown, or</li> <li>Memory object, TPOT uses the instance of joblib.Memory for memory caching and TPOT does NOT clean the caching directory up upon shutdown, or</li> <li>None, TPOT does not use memory caching.</li> </ul> use_dask: boolean, optional (default: False)  Whether to use Dask-ML's pipeline optimiziations. This avoid re-fitting the same estimator on the same split of data multiple times. It will also provide more detailed diagnostics when using Dask's distributed scheduler.  See avoid repeated work for more details.  periodic_checkpoint_folder: path string, optional (default: None)  If supplied, a folder in which TPOT will periodically save pipelines in pareto front so far while optimizing. Currently once per generation but not more often than once per 30 seconds. Useful in multiple cases: <ul> <li>Sudden death before TPOT could save optimized pipeline</li> <li>Track its progress</li> <li>Grab pipelines while it's still optimizing</li> </ul> early_stop: integer, optional (default: None)  How many generations TPOT checks whether there is no improvement in optimization process.  Ends the optimization process if there is no improvement in the given number of generations.  verbosity: integer, optional (default=0)  How much information TPOT communicates while it's running.  Possible inputs are: <ul> <li>0, TPOT will print nothing,</li> <li>1, TPOT will print minimal information,</li> <li>2, TPOT will print more information and provide a progress bar, or</li> <li>3, TPOT will print everything and provide a progress bar.</li> </ul> disable_update_check: boolean, optional (default=False)  Flag indicating whether the TPOT version checker should be disabled.  The update checker will tell you when a new version of TPOT has been released.  Attributes: fitted_pipeline_: scikit-learn Pipeline object  The best pipeline that TPOT discovered during the pipeline optimization process, fitted on the entire training dataset.  pareto_front_fitted_pipelines_: Python dictionary  Dictionary containing the all pipelines on the TPOT Pareto front, where the key is the string representation of the pipeline and the value is the corresponding pipeline fitted on the entire training dataset.  The TPOT Pareto front provides a trade-off between pipeline complexity (i.e., the number of steps in the pipeline) and the predictive performance of the pipeline.  Note: _pareto_front_fitted_pipelines is only available when verbosity=3.  evaluated_individuals_: Python dictionary  Dictionary containing all pipelines that were evaluated during the pipeline optimization process, where the key is the string representation of the pipeline and the value is a tuple containing (# of steps in pipeline, accuracy metric for the pipeline).  This attribute is primarily for internal use, but may be useful for looking at the other pipelines that TPOT evaluated.  <p>Example</p> <pre><code>from tpot import TPOTRegressor\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\n\ndigits = load_boston()\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,\n                                                    train_size=0.75, test_size=0.25)\n\ntpot = TPOTRegressor(generations=5, population_size=50, verbosity=2)\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_test, y_test))\ntpot.export('tpot_boston_pipeline.py')\n</code></pre> <p>Functions</p> fit(features, target[, sample_weight, groups]) Run the TPOT optimization process on the given training data. predict(features) Use the optimized pipeline to predict the target values for a feature set. score(testing_features, testing_target) Returns the optimized pipeline's score on the given testing data using the user-specified scoring function. export(output_file_name) Export the optimized pipeline as Python code. <p> <pre><code>fit(features, target, sample_weight=None, groups=None)\n</code></pre></p>  Run the TPOT optimization process on the given training data.  Uses genetic programming to optimize a machine learning pipeline that maximizes the score on the provided features and target. This pipeline optimization procedure uses internal k-fold cross-validaton to avoid overfitting on the provided data. At the end of the pipeline optimization procedure, the best pipeline is then trained on the entire set of provided samples.  Parameters: features: array-like {n_samples, n_features}  Feature matrix  TPOT and all scikit-learn algorithms assume that the features will be numerical and there will be no missing values. As such, when a feature matrix is provided to TPOT, all missing values will automatically be replaced (i.e., imputed) using median value imputation.  If you wish to use a different imputation strategy than median imputation, please make sure to apply imputation to your feature set prior to passing it to TPOT.  target: array-like {n_samples}  List of target labels for prediction  sample_weight: array-like {n_samples}, optional  Per-sample weights. Higher weights indicate more importance. If specified, sample_weight will be passed to any pipeline element whose fit() function accepts a sample_weight argument. By default, using sample_weight does not affect tpot's scoring functions, which determine preferences between pipelines.  groups: array-like, with shape {n_samples, }, optional  Group labels for the samples used when performing cross-validation.  This parameter should only be used in conjunction with sklearn's Group cross-validation functions, such as sklearn.model_selection.GroupKFold.  Returns: self: object  Returns a copy of the fitted TPOT object  <p> <pre><code>predict(features)\n</code></pre></p>  Use the optimized pipeline to predict the target values for a feature set.  Parameters: features: array-like {n_samples, n_features}  Feature matrix  Returns: predictions: array-like {n_samples}  Predicted target values for the samples in the feature matrix  <p> <pre><code>score(testing_features, testing_target)\n</code></pre></p>  Returns the optimized pipeline's score on the given testing data using the user-specified scoring function.  The default scoring function for TPOTRegressor is 'mean_squared_error'.  Parameters: testing_features: array-like {n_samples, n_features}  Feature matrix of the testing set  testing_target: array-like {n_samples}  List of target labels for prediction in the testing set  Returns: accuracy_score: float  The estimated test set accuracy according to the user-specified scoring function.  <p> <pre><code>export(output_file_name)\n</code></pre></p>  Export the optimized pipeline as Python code.  See the usage documentation for example usage of the export function.  Parameters: output_file_name: string  String containing the path and file name of the desired output file  data_file_path: string  By default, the path of input dataset is 'PATH/TO/DATA/FILE' by default. If data_file_path is another string, the path will be replaced.  Returns: exported_code_string: string  The whole pipeline text as a string should be returned if output_file_name is not specified."},{"location":"archived/citing/","title":"Citing","text":"\u26a0\ufe0f Warning <p>This documentation is for the archived version of TPOT, which is no longer maintained. For the latest version, click here.</p>"},{"location":"archived/citing/#citing-tpot","title":"Citing TPOT","text":"<p>If you use TPOT in a scientific publication, please consider citing at least one of the following papers:</p> <p>Trang T. Le, Weixuan Fu and Jason H. Moore (2020). Scaling tree-based automated machine learning to biomedical big data with a feature set selector. Bioinformatics.36(1): 250-256.</p> <p>BibTeX entry:</p> <pre><code>@article{le2020scaling,\n  title={Scaling tree-based automated machine learning to biomedical big data with a feature set selector},\n  author={Le, Trang T and Fu, Weixuan and Moore, Jason H},\n  journal={Bioinformatics},\n  volume={36},\n  number={1},\n  pages={250--256},\n  year={2020},\n  publisher={Oxford University Press}\n}\n</code></pre> <p>Randal S. Olson, Ryan J. Urbanowicz, Peter C. Andrews, Nicole A. Lavender, La Creis Kidd, and Jason H. Moore (2016). Automating biomedical data science through tree-based pipeline optimization. Applications of Evolutionary Computation, pages 123-137.</p> <p>BibTeX entry:</p> <pre><code>@inbook{Olson2016EvoBio,\n    author={Olson, Randal S. and Urbanowicz, Ryan J. and Andrews, Peter C. and Lavender, Nicole A. and Kidd, La Creis and Moore, Jason H.},\n    editor={Squillero, Giovanni and Burelli, Paolo},\n    chapter={Automating Biomedical Data Science Through Tree-Based Pipeline Optimization},\n    title={Applications of Evolutionary Computation: 19th European Conference, EvoApplications 2016, Porto, Portugal, March 30 -- April 1, 2016, Proceedings, Part I},\n    year={2016},\n    publisher={Springer International Publishing},\n    pages={123--137},\n    isbn={978-3-319-31204-0},\n    doi={10.1007/978-3-319-31204-0_9},\n    url={http://dx.doi.org/10.1007/978-3-319-31204-0_9}\n}\n</code></pre> <p>Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science</p> <p>Randal S. Olson, Nathan Bartley, Ryan J. Urbanowicz, and Jason H. Moore (2016). Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science. Proceedings of GECCO 2016, pages 485-492.</p> <p>BibTeX entry:</p> <pre><code>@inproceedings{OlsonGECCO2016,\n    author = {Olson, Randal S. and Bartley, Nathan and Urbanowicz, Ryan J. and Moore, Jason H.},\n    title = {Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science},\n    booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference 2016},\n    series = {GECCO '16},\n    year = {2016},\n    isbn = {978-1-4503-4206-3},\n    location = {Denver, Colorado, USA},\n    pages = {485--492},\n    numpages = {8},\n    url = {http://doi.acm.org/10.1145/2908812.2908918},\n    doi = {10.1145/2908812.2908918},\n    acmid = {2908918},\n    publisher = {ACM},\n    address = {New York, NY, USA},\n}\n</code></pre> <p>Alternatively, you can cite the repository directly with the following DOI:</p> <p>DOI</p>"},{"location":"archived/contributing/","title":"Contributing","text":"\u26a0\ufe0f Warning <p>This documentation is for the archived version of TPOT, which is no longer maintained. For the latest version, click here.</p>"},{"location":"archived/contributing/#contribution-guide","title":"Contribution Guide","text":"<p>We welcome you to check the existing issues for bugs or enhancements to work on. If you have an idea for an extension to TPOT, please file a new issue so we can discuss it.</p>"},{"location":"archived/contributing/#project-layout","title":"Project layout","text":"<p>The latest stable release of TPOT is on the master branch, whereas the latest version of TPOT in development is on the development branch. Make sure you are looking at and working on the correct branch if you're looking to contribute code.</p> <p>In terms of directory structure:</p> <ul> <li>All of TPOT's code sources are in the <code>tpot</code> directory</li> <li>The documentation sources are in the <code>docs_sources</code> directory</li> <li>Images in the documentation are in the <code>images</code> directory</li> <li>Tutorials for TPOT are in the <code>tutorials</code> directory</li> <li>Unit tests for TPOT are in the <code>tests.py</code> file</li> </ul> <p>Make sure to familiarize yourself with the project layout before making any major contributions, and especially make sure to send all code changes to the <code>development</code> branch.</p>"},{"location":"archived/contributing/#how-to-contribute","title":"How to contribute","text":"<p>The preferred way to contribute to TPOT is to fork the main repository on GitHub:</p> <ol> <li> <p>Fork the project repository:    click on the 'Fork' button near the top of the page. This creates    a copy of the code under your account on the GitHub server.</p> </li> <li> <p>Clone this copy to your local disk:</p> <pre><code>  $ git clone git@github.com:YourUsername/tpot.git\n  $ cd tpot\n</code></pre> </li> <li> <p>Create a branch to hold your changes:</p> <pre><code>  $ git checkout -b my-contribution\n</code></pre> </li> <li> <p>Make sure your local environment is setup correctly for development. Installation instructions are almost identical to the user instructions except that TPOT should not be installed. If you have TPOT installed on your computer then make sure you are using a virtual environment that does not have TPOT installed. Furthermore, you should make sure you have installed the <code>nose</code> package into your development environment so that you can test changes locally.</p> <pre><code>  $ conda install nose\n</code></pre> </li> <li> <p>Start making changes on your newly created branch, remembering to never work on the <code>master</code> branch! Work on this copy on your computer using Git to do the version control.</p> </li> <li> <p>Once some changes are saved locally, you can use your tweaked version of TPOT by navigating to the project's base directory and running TPOT directly from the command line:</p> <pre><code>  $ python -m tpot.driver\n</code></pre> <p>or by running script that imports and uses the TPOT module with code similar to <code>from tpot import TPOTClassifier</code></p> </li> <li> <p>To check your changes haven't broken any existing tests and to check new tests you've added pass run the following (note, you must have the <code>nose</code> package installed within your dev environment for this to work):</p> <pre><code>  $ nosetests -s -v\n</code></pre> </li> <li> <p>When you're done editing and local testing, run:</p> <pre><code>  $ git add modified_files\n  $ git commit\n</code></pre> </li> </ol> <p>to record your changes in Git, then push them to GitHub with:</p> <pre><code>      $ git push -u origin my-contribution\n</code></pre> <p>Finally, go to the web page of your fork of the TPOT repo, and click 'Pull Request' (PR) to send your changes to the maintainers for review. Make sure that you send your PR to the <code>development</code> branch, as the <code>master</code> branch is reserved for the latest stable release. This will start the CI server to check all the project's unit tests run and send an email to the maintainers.</p> <p>(If any of the above seems like magic to you, then look up the Git documentation on the web.)</p>"},{"location":"archived/contributing/#before-submitting-your-pull-request","title":"Before submitting your pull request","text":"<p>Before you submit a pull request for your contribution, please work through this checklist to make sure that you have done everything necessary so we can efficiently review and accept your changes.</p> <p>If your contribution changes TPOT in any way:</p> <ul> <li> <p>Update the documentation so all of your changes are reflected there.</p> </li> <li> <p>Update the README if anything there has changed.</p> </li> </ul> <p>If your contribution involves any code changes:</p> <ul> <li> <p>Update the project unit tests to test your code changes.</p> </li> <li> <p>Make sure that your code is properly commented with docstrings and comments explaining your rationale behind non-obvious coding practices.</p> </li> <li> <p>If your code affected any of the pipeline operators, make sure that the corresponding export functionality reflects those changes.</p> </li> </ul> <p>If your contribution requires a new library dependency:</p> <ul> <li> <p>Double-check that the new dependency is easy to install via <code>pip</code> or Anaconda and supports both Python 2 and 3. If the dependency requires a complicated installation, then we most likely won't merge your changes because we want to keep TPOT easy to install.</p> </li> <li> <p>Add the required version of the library to .travis.yml</p> </li> <li> <p>Add a line to pip install the library to .travis_install.sh</p> </li> <li> <p>Add a line to print the version of the library to .travis_install.sh</p> </li> <li> <p>Similarly add a line to print the version of the library to .travis_test.sh</p> </li> </ul>"},{"location":"archived/contributing/#after-submitting-your-pull-request","title":"After submitting your pull request","text":"<p>After submitting your pull request, Travis-CI will automatically run unit tests on your changes and make sure that your updated code builds and runs on Python 2 and 3. We also use services that automatically check code quality and test coverage.</p> <p>Check back shortly after submitting your pull request to make sure that your code passes these checks. If any of the checks come back with a red X, then do your best to address the errors.</p>"},{"location":"archived/examples/","title":"Examples","text":"\u26a0\ufe0f Warning <p>This documentation is for the archived version of TPOT, which is no longer maintained. For the latest version, click here.</p>"},{"location":"archived/examples/#overview","title":"Overview","text":"<p>The following sections illustrate the usage of TPOT with various datasets, each belonging to a typical class of machine learning tasks.</p> Dataset Task Task class Dataset description Jupyter notebook Iris flower classification classification link link Optical Recognition of Handwritten Digits digit recognition (image) classification link link Boston housing prices modeling regression link N/A Titanic survival analysis classification link link Bank Marketing subscription prediction classification link link MAGIC Gamma Telescope event detection classification link link cuML Classification Example random classification problem classification link link cuML Regression Example random regression problem regression link link <p>Notes: - For details on how the <code>fit()</code>, <code>score()</code> and <code>export()</code> methods work, refer to the usage documentation. - Upon re-running the experiments, your resulting pipelines may differ (to some extent) from the ones demonstrated here.</p>"},{"location":"archived/examples/#iris-flower-classification","title":"Iris flower classification","text":"<p>The following code illustrates how TPOT can be employed for performing a simple classification task over the Iris dataset.</p> <pre><code>from tpot import TPOTClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\niris = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris.data.astype(np.float64),\n    iris.target.astype(np.float64), train_size=0.75, test_size=0.25, random_state=42)\n\ntpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, random_state=42)\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_test, y_test))\ntpot.export('tpot_iris_pipeline.py')\n</code></pre> <p>Running this code should discover a pipeline (exported as <code>tpot_iris_pipeline.py</code>) that achieves about 97% test accuracy:</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Normalizer\nfrom tpot.export_utils import set_param_recursive\n\n# NOTE: Make sure that the outcome column is labeled 'target' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\nfeatures = tpot_data.drop('target', axis=1)\ntraining_features, testing_features, training_target, testing_target = \\\n            train_test_split(features, tpot_data['target'], random_state=42)\n\n# Average CV score on the training set was: 0.9826086956521738\nexported_pipeline = make_pipeline(\n    Normalizer(norm=\"l2\"),\n    KNeighborsClassifier(n_neighbors=5, p=2, weights=\"distance\")\n)\n# Fix random state for all the steps in exported pipeline\nset_param_recursive(exported_pipeline.steps, 'random_state', 42)\n\nexported_pipeline.fit(training_features, training_target)\nresults = exported_pipeline.predict(testing_features)\n</code></pre>"},{"location":"archived/examples/#digits-dataset","title":"Digits dataset","text":"<p>Below is a minimal working example with the optical recognition of handwritten digits dataset, which is an image classification problem.</p> <pre><code>from tpot import TPOTClassifier\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\n\ndigits = load_digits()\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,\n                                                    train_size=0.75, test_size=0.25, random_state=42)\n\ntpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, random_state=42)\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_test, y_test))\ntpot.export('tpot_digits_pipeline.py')\n</code></pre> <p>Running this code should discover a pipeline (exported as <code>tpot_digits_pipeline.py</code>) that achieves about 98% test accuracy:</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom tpot.builtins import StackingEstimator\nfrom tpot.export_utils import set_param_recursive\n\n# NOTE: Make sure that the outcome column is labeled 'target' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\nfeatures = tpot_data.drop('target', axis=1)\ntraining_features, testing_features, training_target, testing_target = \\\n            train_test_split(features, tpot_data['target'], random_state=42)\n\n# Average CV score on the training set was: 0.9799428471757372\nexported_pipeline = make_pipeline(\n    PolynomialFeatures(degree=2, include_bias=False, interaction_only=False),\n    StackingEstimator(estimator=LogisticRegression(C=0.1, dual=False, penalty=\"l1\")),\n    RandomForestClassifier(bootstrap=True, criterion=\"entropy\", max_features=0.35000000000000003, min_samples_leaf=20, min_samples_split=19, n_estimators=100)\n)\n# Fix random state for all the steps in exported pipeline\nset_param_recursive(exported_pipeline.steps, 'random_state', 42)\n\nexported_pipeline.fit(training_features, training_target)\nresults = exported_pipeline.predict(testing_features)\n</code></pre>"},{"location":"archived/examples/#boston-housing-prices-modeling","title":"Boston housing prices modeling","text":"<p>The following code illustrates how TPOT can be employed for performing a regression task over the Boston housing prices dataset.</p> <pre><code>from tpot import TPOTRegressor\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\n\nhousing = load_boston()\nX_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target,\n                                                    train_size=0.75, test_size=0.25, random_state=42)\n\ntpot = TPOTRegressor(generations=5, population_size=50, verbosity=2, random_state=42)\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_test, y_test))\ntpot.export('tpot_boston_pipeline.py')\n</code></pre> <p>Running this code should discover a pipeline (exported as <code>tpot_boston_pipeline.py</code>) that achieves at least 10 mean squared error (MSE) on the test set:</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom tpot.export_utils import set_param_recursive\n\n# NOTE: Make sure that the outcome column is labeled 'target' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\nfeatures = tpot_data.drop('target', axis=1)\ntraining_features, testing_features, training_target, testing_target = \\\n            train_test_split(features, tpot_data['target'], random_state=42)\n\n# Average CV score on the training set was: -10.812040755234403\nexported_pipeline = make_pipeline(\n    PolynomialFeatures(degree=2, include_bias=False, interaction_only=False),\n    ExtraTreesRegressor(bootstrap=False, max_features=0.5, min_samples_leaf=2, min_samples_split=3, n_estimators=100)\n)\n# Fix random state for all the steps in exported pipeline\nset_param_recursive(exported_pipeline.steps, 'random_state', 42)\n\nexported_pipeline.fit(training_features, training_target)\nresults = exported_pipeline.predict(testing_features)\n</code></pre>"},{"location":"archived/examples/#titanic-survival-analysis","title":"Titanic survival analysis","text":"<p>To see the TPOT applied the Titanic Kaggle dataset, see the Jupyter notebook here. This example shows how to take a messy dataset and preprocess it such that it can be used in scikit-learn and TPOT.</p>"},{"location":"archived/examples/#portuguese-bank-marketing","title":"Portuguese Bank Marketing","text":"<p>The corresponding Jupyter notebook, containing the associated data preprocessing and analysis, can be found here.</p>"},{"location":"archived/examples/#magic-gamma-telescope","title":"MAGIC Gamma Telescope","text":"<p>The corresponding Jupyter notebook, containing the associated data preprocessing and analysis, can be found here.</p>"},{"location":"archived/examples/#neural-network-classifier-using-tpot-nn","title":"Neural network classifier using TPOT-NN","text":"<p>By loading the TPOT-NN configuration dictionary, PyTorch estimators will be included for classification. Users can also create their own NN configuration dictionary that includes <code>tpot.builtins.PytorchLRClassifier</code> and/or <code>tpot.builtins.PytorchMLPClassifier</code>, or they can specify them using a template string, as shown in the following example:</p> <pre><code>from tpot import TPOTClassifier\nfrom sklearn.datasets import make_blobs\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_blobs(n_samples=100, centers=2, n_features=3, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25)\n\nclf = TPOTClassifier(config_dict='TPOT NN', template='Selector-Transformer-PytorchLRClassifier',\n                     verbosity=2, population_size=10, generations=10)\nclf.fit(X_train, y_train)\nprint(clf.score(X_test, y_test))\nclf.export('tpot_nn_demo_pipeline.py')\n</code></pre> <p>This example is somewhat trivial, but it should result in nearly 100% classification accuracy.</p>"},{"location":"archived/installing/","title":"Installing","text":"\u26a0\ufe0f Warning <p>This documentation is for the archived version of TPOT, which is no longer maintained. For the latest version, click here.</p>"},{"location":"archived/installing/#installation","title":"Installation","text":"<p>TPOT is built on top of several existing Python libraries, including:</p> <ul> <li> <p>NumPy</p> </li> <li> <p>SciPy</p> </li> <li> <p>scikit-learn</p> </li> <li> <p>DEAP</p> </li> <li> <p>update_checker</p> </li> <li> <p>tqdm</p> </li> <li> <p>stopit</p> </li> <li> <p>pandas</p> </li> <li> <p>joblib</p> </li> <li> <p>xgboost</p> </li> </ul> <p>Most of the necessary Python packages can be installed via the Anaconda Python distribution, which we strongly recommend that you use. Support for Python 3.4 and below has been officially dropped since version 0.11.0.</p> <p>You can install TPOT using <code>pip</code> or <code>conda-forge</code>.</p>"},{"location":"archived/installing/#pip","title":"pip","text":"<p>NumPy, SciPy, scikit-learn, pandas, joblib, and PyTorch can be installed in Anaconda via the command:</p> <pre><code>conda install numpy scipy scikit-learn pandas joblib pytorch\n</code></pre> <p>DEAP, update_checker, tqdm, stopit and xgboost can be installed with <code>pip</code> via the command:</p> <pre><code>pip install deap update_checker tqdm stopit xgboost\n</code></pre> <p>Windows users: pip installation may not work on some Windows environments, and it may cause unexpected errors. If you have issues installing XGBoost, check the XGBoost installation documentation.</p> <p>If you plan to use Dask for parallel training, make sure to install dask[delay] and dask[dataframe] and dask_ml. It is noted that dask-ml&gt;=1.7 requires distributed&gt;=2.4.0 and scikit-learn&gt;=0.23.0.</p> <pre><code>pip install dask[delayed] dask[dataframe] dask-ml fsspec&gt;=0.3.3 distributed&gt;=2.10.0\n</code></pre> <p>If you plan to use the TPOT-MDR configuration, make sure to install scikit-mdr and scikit-rebate:</p> <pre><code>pip install scikit-mdr skrebate\n</code></pre> <p>To enable support for PyTorch-based neural networks (TPOT-NN), you will need to install PyTorch. TPOT-NN will work with either CPU or GPU PyTorch, but we strongly recommend using a GPU version, if possible, as CPU PyTorch models tend to train very slowly.</p> <p>We recommend following PyTorch's installation instructions customized for your operating system and Python distribution.</p> <p>Finally to install TPOT itself, run the following command:</p> <pre><code>pip install tpot\n</code></pre>"},{"location":"archived/installing/#conda-forge","title":"conda-forge","text":"<p>To install tpot and its core dependencies you can use:</p> <pre><code>conda install -c conda-forge tpot\n</code></pre> <p>To install additional dependencies you can use:</p> <pre><code>conda install -c conda-forge tpot xgboost dask dask-ml scikit-mdr skrebate\n</code></pre> <p>As mentioned above, we recommend following PyTorch's installation instructions for installing it to enable support for PyTorch-based neural networks (TPOT-NN).</p>"},{"location":"archived/installing/#installation-for-using-tpot-cuml-configuration","title":"Installation for using TPOT-cuML configuration","text":"<p>With \"TPOT cuML\" configuration (see built-in configurations), TPOT will search over a restricted configuration using the GPU-accelerated estimators in RAPIDS cuML and DMLC XGBoost. This configuration requires an NVIDIA Pascal architecture or better GPU with compute capability 6.0+, and that the library cuML is installed. With this configuration, all model training and predicting will be GPU-accelerated. This configuration is particularly useful for medium-sized and larger datasets on which CPU-based estimators are a common bottleneck, and works for both the <code>TPOTClassifier</code> and <code>TPOTRegressor</code>.</p> <p>Please download this conda environment yml file to install TPOT for using TPOT-cuML configuration.</p> <pre><code>conda env create -f tpot-cuml.yml -n tpot-cuml\nconda activate tpot-cuml\n</code></pre>"},{"location":"archived/installing/#installation-problems","title":"Installation problems","text":"<p>Please file a new issue if you run into installation problems.</p>"},{"location":"archived/related/","title":"Related","text":"\u26a0\ufe0f Warning <p>This documentation is for the archived version of TPOT, which is no longer maintained. For the latest version, click here.</p> <p>Other Automated Machine Learning (AutoML) tools and related projects:</p> Name Language License Description Auto-WEKA Java GPL-v3 Automated model selection and hyper-parameter tuning for Weka models. auto-sklearn Python BSD-3-Clause An automated machine learning toolkit and a drop-in replacement for a scikit-learn estimator. auto_ml Python MIT Automated machine learning for analytics &amp; production. Supports manual feature type declarations. H2O AutoML Java with Python, Scala &amp; R APIs and web GUI Apache 2.0 Automated: data prep, hyperparameter tuning, random grid search and stacked ensembles in a distributed ML platform. devol Python MIT Automated deep neural network design via genetic programming. MLBox Python BSD-3-Clause Accurate hyper-parameter optimization in high-dimensional space with support for distributed computing. Recipe C GPL-v3 Machine-learning pipeline optimization through genetic programming. Uses grammars to define pipeline structure. Xcessiv Python Apache 2.0 A web-based application for quick, scalable, and automated hyper-parameter tuning and stacked ensembling in Python. GAMA Python Apache 2.0 Machine-learning pipeline optimization through asynchronous evaluation based genetic programming."},{"location":"archived/releases/","title":"Releases","text":"\u26a0\ufe0f Warning <p>This documentation is for the archived version of TPOT, which is no longer maintained. For the latest version, click here.</p>"},{"location":"archived/releases/#release-notes","title":"Release Notes","text":""},{"location":"archived/releases/#version-0120","title":"Version 0.12.0","text":"<ul> <li>Fix numpy compatibility</li> <li>Dask optimizations</li> <li>Minor bug fixes</li> </ul>"},{"location":"archived/releases/#version-0117","title":"Version 0.11.7","text":"<ul> <li>Fix compatibility issue with scikit-learn 0.24 and xgboost 1.3.0</li> <li>Fix a bug causing that TPOT does not work when classifying more than 50 classes</li> <li>Add initial support <code>Resampler</code> from <code>imblearn</code></li> <li>Fix minor bugs</li> </ul>"},{"location":"archived/releases/#version-0116","title":"Version 0.11.6","text":"<ul> <li>Fix a bug causing point mutation function does not work properly with using <code>template</code> option</li> <li>Add a new built configuration called \"TPOT cuML\" which TPOT will search over a restricted configuration using the GPU-accelerated estimators in RAPIDS cuML and DMLC XGBoost. This configuration requires an NVIDIA Pascal architecture or better GPU with compute capability 6.0+, and that the library cuML is installed.</li> <li>Add string path support for log/log_file parameter</li> <li>Fix a bug in version 0.11.5 causing no update in stdout after each generation</li> <li>Fix minor bugs</li> </ul>"},{"location":"archived/releases/#version-0115","title":"Version 0.11.5","text":"<ul> <li>Make <code>Pytorch</code> as an optional dependency</li> <li>Refine installation documentation</li> </ul>"},{"location":"archived/releases/#version-0114","title":"Version 0.11.4","text":"<ul> <li>Add a new built configuration \"TPOT NN\" which includes all operators in \"Default TPOT\" plus additional neural network estimators written in PyTorch (currently <code>tpot.builtins.PytorchLRClassifier</code> and <code>tpot.builtins.PytorchMLPClassifier</code> for classification tasks only)</li> <li>Refine <code>log_file</code> parameter's behavior</li> </ul>"},{"location":"archived/releases/#version-0113","title":"Version 0.11.3","text":"<ul> <li>Fix a bug in TPOTRegressor in v0.11.2</li> <li>Add <code>-log</code> option in command line interface to save process log to a file.</li> </ul>"},{"location":"archived/releases/#version-0112","title":"Version 0.11.2","text":"<ul> <li>Fix <code>early_stop</code> parameter does not work properly</li> <li>TPOT built-in <code>OneHotEncoder</code> can refit to different datasets</li> <li>Fix the issue that the attribute <code>evaluated_individuals_</code> cannot record correct generation info.</li> <li>Add a new parameter <code>log_file</code> to output logs to a file instead of <code>sys.stdout</code></li> <li>Fix some code quality issues and mistakes in documentations</li> <li>Fix minor bugs</li> </ul>"},{"location":"archived/releases/#version-0111","title":"Version 0.11.1","text":"<ul> <li>Fix compatibility issue with scikit-learn v0.22</li> <li><code>warm_start</code> now saves both Primitive Sets and evaluated_pipelines_ from previous runs;</li> <li>Fix the error that TPOT assign wrong fitness scores to non-evaluated pipelines (interrupted by <code>max_min_mins</code> or <code>KeyboardInterrupt</code>) ;</li> <li>Fix the bug that mutation operator cannot generate new pipeline when template is not default value and <code>warm_start</code> is True;</li> <li>Fix the bug that <code>max_time_mins</code> cannot stop optimization process when search space is limited.  </li> <li>Fix a bug in exported codes when the exported pipeline is only 1 estimator</li> <li>Fix spelling mistakes in documentations</li> <li>Fix some code quality issues</li> </ul>"},{"location":"archived/releases/#version-0110","title":"Version 0.11.0","text":"<ul> <li>Support for Python 3.4 and below has been officially dropped. Also support for scikit-learn 0.20 or below has been dropped.</li> <li>The support of a metric function with the signature <code>score_func(y_true, y_pred)</code> for <code>scoring parameter</code> has been dropped.</li> <li>Refine <code>StackingEstimator</code> for not stacking NaN/Infinity predication probabilities.</li> <li>Fix a bug that population doesn't persist by <code>warm_start=True</code> when <code>max_time_mins</code> is not default value.</li> <li>Now the <code>random_state</code> parameter in TPOT is used for pipeline evaluation instead of using a fixed random seed of 42 before. The <code>set_param_recursive</code> function has been moved to <code>export_utils.py</code> and it can be used in exported codes for setting <code>random_state</code> recursively in scikit-learn Pipeline. It is used to set <code>random_state</code> in <code>fitted_pipeline_</code> attribute and exported pipelines.</li> <li>TPOT can independently use <code>generations</code> and <code>max_time_mins</code> to limit the optimization process through using one of the parameters or both.</li> <li><code>.export()</code> function will return string of exported pipeline if output filename is not specified.</li> <li>Add <code>SGDClassifier</code> and <code>SGDRegressor</code> into TPOT default configs.</li> <li>Documentation has been updated</li> <li>Fix minor bugs.</li> </ul>"},{"location":"archived/releases/#version-0102","title":"Version 0.10.2","text":"<ul> <li>TPOT v0.10.2 is the last version to support Python 2.7 and Python 3.4.</li> <li>Minor updates for fixing compatibility issues with the latest version of scikit-learn (version &gt; 0.21) and xgboost (v0.90)</li> <li>Default value of <code>template</code> parameter is changed to <code>None</code> instead.</li> <li>Fix errors in documentation</li> </ul>"},{"location":"archived/releases/#version-0101","title":"Version 0.10.1","text":"<ul> <li>Add <code>data_file_path</code> option into <code>expert</code> function for replacing <code>'PATH/TO/DATA/FILE'</code> to customized dataset path in exported scripts. (Related issue #838)</li> <li>Change python version in CI tests to 3.7</li> <li>Add CI tests for macOS.</li> </ul>"},{"location":"archived/releases/#version-0100","title":"Version 0.10.0","text":"<ul> <li>Add a new <code>template</code> option to specify a desired structure for machine learning pipeline in TPOT. Check TPOT API (it will be updated once it is merge to master branch).</li> <li>Add <code>FeatureSetSelector</code> operator into TPOT for feature selection based on priori export knowledge. Please check our preprint paper for more details (Note: it was named <code>DatasetSelector</code> in 1st version paper but we will rename to FeatureSetSelector in next version of the paper)</li> <li>Refine <code>n_jobs</code> parameter to accept value below -1. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used.</li> <li>Now <code>memory</code>  parameter can create memory cache directory if it does not exist.</li> <li>Fix minor bugs.</li> </ul>"},{"location":"archived/releases/#version-096","title":"Version 0.9.6","text":"<ul> <li>Fix a bug causing that <code>max_time_mins</code> parameter doesn't work when <code>use_dask=True</code> in TPOT 0.9.5</li> <li>Now TPOT saves best pareto values best pareto pipeline s in checkpoint folder</li> <li>TPOT raises <code>ImportError</code> if operators in the TPOT configuration are not available when <code>verbosity&gt;2</code></li> <li>Thank @PGijsbers for the suggestions. Now TPOT can save scores of individuals already evaluated in any generation even the evaluation process of that generation is interrupted/stopped. But it is noted that, in this case, TPOT will raise this warning message: <code>WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.</code>, because the pipelines in early generation, e.g. 1st generation, are evolved/modified very limited times via evolutionary algorithm.</li> <li>Fix bugs in configuration of <code>TPOTRegressor</code></li> <li>Error fixes in documentation</li> </ul>"},{"location":"archived/releases/#version-095","title":"Version 0.9.5","text":"<ul> <li> <p>TPOT now supports integration with Dask for parallelization + smart caching. Big thanks to the Dask dev team for making this happen!</p> </li> <li> <p>TPOT now supports for imputation/sparse matrices into <code>predict</code> and <code>predict_proba</code> functions.</p> </li> <li> <p><code>TPOTClassifier</code> and <code>TPOTRegressor</code> now follows scikit-learn estimator API.</p> </li> <li> <p>We refined scoring parameter in TPOT API for accepting <code>Scorer</code> object.</p> </li> <li> <p>We refined parameters in VarianceThreshold and FeatureAgglomeration.</p> </li> <li> <p>TPOT now supports using memory caching within a Pipeline via an optional <code>memory</code> parameter.</p> </li> <li> <p>We improved documentation of TPOT.</p> </li> </ul>"},{"location":"archived/releases/#version-09","title":"Version 0.9","text":"<ul> <li> <p>TPOT now supports sparse matrices with a new built-in TPOT configuration, \"TPOT sparse\". We are using a custom OneHotEncoder implementation that supports missing values and continuous features.</p> </li> <li> <p>We have added an \"early stopping\" option for stopping the optimization process if no improvement is made within a set number of generations. Look up the <code>early_stop</code> parameter to access this functionality.</p> </li> <li> <p>TPOT now reduces the number of duplicated pipelines between generations, which saves you time during the optimization process.</p> </li> <li> <p>TPOT now supports custom scoring functions via the command-line mode.</p> </li> <li> <p>We have added a new optional argument, <code>periodic_checkpoint_folder</code>, that allows TPOT to periodically save the best pipeline so far to a local folder during optimization process.</p> </li> <li> <p>TPOT no longer uses <code>sklearn.externals.joblib</code> when <code>n_jobs=1</code> to avoid the potential freezing issue that scikit-learn suffers from.</p> </li> <li> <p>We have added <code>pandas</code> as a dependency to read input datasets instead of <code>numpy.recfromcsv</code>. NumPy's <code>recfromcsv</code> function is unable to parse datasets with complex data types.</p> </li> <li> <p>Fixed a bug that <code>DEFAULT</code> in the parameter(s) of nested estimator raises <code>KeyError</code> when exporting pipelines.</p> </li> <li> <p>Fixed a bug related to setting <code>random_state</code> in nested estimators. The issue would happen with pipeline with <code>SelectFromModel</code> (<code>ExtraTreesClassifier</code> as nested estimator) or <code>StackingEstimator</code> if nested estimator has <code>random_state</code> parameter.</p> </li> <li> <p>Fixed a bug in the missing value imputation function in TPOT to impute along columns instead rows.</p> </li> <li> <p>Refined input checking for sparse matrices in TPOT.</p> </li> <li> <p>Refined the TPOT pipeline mutation operator.</p> </li> </ul>"},{"location":"archived/releases/#version-08","title":"Version 0.8","text":"<ul> <li> <p>TPOT now detects whether there are missing values in your dataset and replaces them with the median value of the column.</p> </li> <li> <p>TPOT now allows you to set a <code>group</code> parameter in the <code>fit</code> function so you can use the GroupKFold cross-validation strategy.</p> </li> <li> <p>TPOT now allows you to set a subsample ratio of the training instance with the <code>subsample</code> parameter. For example, setting <code>subsample</code>=0.5 tells TPOT to create a fixed subsample of half of the training data for the pipeline optimization process. This parameter can be useful for speeding up the pipeline optimization process, but may give less accurate performance estimates from cross-validation.</p> </li> <li> <p>TPOT now has more built-in configurations, including TPOT MDR and TPOT light, for both classification and regression problems.</p> </li> <li> <p><code>TPOTClassifier</code> and <code>TPOTRegressor</code> now expose three useful internal attributes, <code>fitted_pipeline_</code>, <code>pareto_front_fitted_pipelines_</code>, and <code>evaluated_individuals_</code>. These attributes are described in the API documentation.</p> </li> <li> <p>Oh, TPOT now has thorough API documentation. Check it out!</p> </li> <li> <p>Fixed a reproducibility issue where setting <code>random_seed</code> didn't necessarily result in the same results every time. This bug was present since TPOT v0.7.</p> </li> <li> <p>Refined input checking in TPOT.</p> </li> <li> <p>Removed Python 2 uncompliant code.</p> </li> </ul>"},{"location":"archived/releases/#version-07","title":"Version 0.7","text":"<ul> <li> <p>TPOT now has multiprocessing support. TPOT allows you to use multiple processes in parallel to accelerate the pipeline optimization process in TPOT with the <code>n_jobs</code> parameter.</p> </li> <li> <p>TPOT now allows you to customize the operators and parameters considered during the optimization process, which can be accomplished with the new <code>config_dict</code> parameter. The format of this customized dictionary can be found in the online documentation, along with a list of built-in configurations.</p> </li> <li> <p>TPOT now allows you to specify a time limit for evaluating a single pipeline  (default limit is 5 minutes) in optimization process with the <code>max_eval_time_mins</code> parameter, so TPOT won't spend hours evaluating overly-complex pipelines.</p> </li> <li> <p>We tweaked TPOT's underlying evolutionary optimization algorithm to work even better, including using the mu+lambda algorithm. This algorithm gives you more control of how many pipelines are generated every iteration with the <code>offspring_size</code> parameter.</p> </li> <li> <p>Refined the default operators and parameters in TPOT, so TPOT 0.7 should work even better than 0.6.</p> </li> <li> <p>TPOT now supports sample weights in the fitness function if some if your samples are more important to classify correctly than others. The sample weights option works the same as in scikit-learn, e.g., <code>tpot.fit(x_train, y_train, sample_weights=sample_weights)</code>.</p> </li> <li> <p>The default scoring metric in TPOT has been changed from balanced accuracy to accuracy, the same default metric for classification algorithms in scikit-learn. Balanced accuracy can still be used by setting <code>scoring='balanced_accuracy'</code> when creating a TPOT instance.</p> </li> </ul>"},{"location":"archived/releases/#version-06","title":"Version 0.6","text":"<ul> <li> <p>TPOT now supports regression problems! We have created two separate <code>TPOTClassifier</code> and <code>TPOTRegressor</code> classes to support classification and regression problems, respectively. The command-line interface also supports this feature through the <code>-mode</code> parameter.</p> </li> <li> <p>TPOT now allows you to specify a time limit for the optimization process with the <code>max_time_mins</code> parameter, so you don't need to guess how long TPOT will take any more to recommend a pipeline to you.</p> </li> <li> <p>Added a new operator that performs feature selection using ExtraTrees feature importance scores.</p> </li> <li> <p>XGBoost has been added as an optional dependency to TPOT. If you have XGBoost installed, TPOT will automatically detect your installation and use the <code>XGBoostClassifier</code> and <code>XGBoostRegressor</code> in its pipelines.</p> </li> <li> <p>TPOT now offers a verbosity level of 3 (\"science mode\"), which outputs the entire Pareto front instead of only the current best score. This feature may be useful for users looking to make a trade-off between pipeline complexity and score.</p> </li> </ul>"},{"location":"archived/releases/#version-05","title":"Version 0.5","text":"<ul> <li>Major refactor: Each operator is defined in a separate class file. Hooray for easier-to-maintain code!</li> <li>TPOT now exports directly to scikit-learn Pipelines instead of hacky code.</li> <li>Internal representation of individuals now uses scikit-learn pipelines.</li> <li>Parameters for each operator have been optimized so TPOT spends less time exploring useless parameters.</li> <li>We have removed pandas as a dependency and instead use numpy matrices to store the data.</li> <li>TPOT now uses k-fold cross-validation when evaluating pipelines, with a default k = 3. This k parameter can be tuned when creating a new TPOT instance.</li> <li>Improved scoring function support: Even though TPOT uses balanced accuracy by default, you can now have TPOT use any of the scoring functions that <code>cross_val_score</code> supports.</li> <li>Added the scikit-learn Normalizer preprocessor.</li> <li>Minor text fixes.</li> </ul>"},{"location":"archived/releases/#version-04","title":"Version 0.4","text":"<p>In TPOT 0.4, we've made some major changes to the internals of TPOT and added some convenience functions. We've summarized the changes below.</p> <ul> <li>Added new sklearn models and preprocessors  <ul> <li>AdaBoostClassifier</li> <li>BernoulliNB</li> <li>ExtraTreesClassifier</li> <li>GaussianNB</li> <li>MultinomialNB</li> <li>LinearSVC</li> <li>PassiveAggressiveClassifier</li> <li>GradientBoostingClassifier</li> <li>RBFSampler</li> <li>FastICA</li> <li>FeatureAgglomeration</li> <li>Nystroem</li> </ul></li> <li>Added operator that inserts virtual features for the count of features with values of zero</li> <li>Reworked parameterization of TPOT operators <ul> <li>Reduced parameter search space with information from a scikit-learn benchmark</li> <li>TPOT no longer generates arbitrary parameter values, but uses a fixed parameter set instead</li> </ul></li> <li>Removed XGBoost as a dependency <ul> <li>Too many users were having install issues with XGBoost</li> <li>Replaced with scikit-learn's GradientBoostingClassifier</li> </ul></li> <li>Improved descriptiveness of TPOT command line parameter documentation</li> <li>Removed min/max/avg details during fit() when verbosity &gt; 1  <ul> <li>Replaced with tqdm progress bar</li> <li>Added tqdm as a dependency</li> </ul></li> <li>Added <code>fit_predict()</code> convenience function</li> <li>Added <code>get_params()</code> function so TPOT can operate in scikit-learn's <code>cross_val_score</code> &amp; related functions</li> </ul>"},{"location":"archived/releases/#version-03","title":"Version 0.3","text":"<ul> <li>We revised the internal optimization process of TPOT to make it more efficient, in particular in regards to the model parameters that TPOT optimizes over.</li> </ul>"},{"location":"archived/releases/#version-02","title":"Version 0.2","text":"<ul> <li> <p>TPOT now has the ability to export the optimized pipelines to sklearn code.</p> </li> <li> <p>Logistic regression, SVM, and k-nearest neighbors classifiers were added as pipeline operators. Previously, TPOT only included decision tree and random forest classifiers.</p> </li> <li> <p>TPOT can now use arbitrary scoring functions for the optimization process.</p> </li> <li> <p>TPOT now performs multi-objective Pareto optimization to balance model complexity (i.e., # of pipeline operators) and the score of the pipeline.</p> </li> </ul>"},{"location":"archived/releases/#version-01","title":"Version 0.1","text":"<ul> <li> <p>First public release of TPOT.</p> </li> <li> <p>Optimizes pipelines with decision trees and random forest classifiers as the model, and uses a handful of feature preprocessors.</p> </li> </ul>"},{"location":"archived/support/","title":"Support","text":"\u26a0\ufe0f Warning <p>This documentation is for the archived version of TPOT, which is no longer maintained. For the latest version, click here.</p> <p>TPOT was developed in the Computational Genetics Lab at the University of Pennsylvania with funding from the NIH under grant R01 AI117694. We are incredibly grateful for the support of the NIH and the University of Pennsylvania during the development of this project.</p> <p>The TPOT logo was designed by Todd Newmuis, who generously donated his time to the project.</p>"},{"location":"archived/using/","title":"Using","text":"\u26a0\ufe0f Warning <p>This documentation is for the archived version of TPOT, which is no longer maintained. For the latest version, click here.</p>"},{"location":"archived/using/#using-tpot","title":"Using TPOT","text":""},{"location":"archived/using/#what-to-expect-from-automl-software","title":"What to expect from AutoML software","text":"<p>Automated machine learning (AutoML) takes a higher-level approach to machine learning than most practitioners are used to, so we've gathered a handful of guidelines on what to expect when running AutoML software such as TPOT.</p> AutoML algorithms aren't intended to run for only a few minutes <p>Of course, you can run TPOT for only a few minutes and it will find a reasonably good pipeline for your dataset. However, if you don't run TPOT for long enough, it may not find the best possible pipeline for your dataset. It may even not find any suitable pipeline at all, in which case a <code>RuntimeError('A pipeline has not yet been optimized. Please call fit() first.')</code> will be raised. Often it is worthwhile to run multiple instances of TPOT in parallel for a long time (hours to days) to allow TPOT to thoroughly search the pipeline space for your dataset.</p> AutoML algorithms can take a long time to finish their search <p>AutoML algorithms aren't as simple as fitting one model on the dataset; they are considering multiple machine learning algorithms (random forests, linear models, SVMs, etc.) in a pipeline with multiple preprocessing steps (missing value imputation, scaling, PCA, feature selection, etc.), the hyperparameters for all of the models and preprocessing steps, as well as multiple ways to ensemble or stack the algorithms within the pipeline.</p> <p>As such, TPOT will take a while to run on larger datasets, but it's important to realize why. With the default TPOT settings (100 generations with 100 population size), TPOT will evaluate 10,000 pipeline configurations before finishing. To put this number into context, think about a grid search of 10,000 hyperparameter combinations for a machine learning algorithm and how long that grid search will take. That is 10,000 model configurations to evaluate with 10-fold cross-validation, which means that roughly 100,000 models are fit and evaluated on the training data in one grid search. That's a time-consuming procedure, even for simpler models like decision trees.</p> <p>Typical TPOT runs will take hours to days to finish (unless it's a small dataset), but you can always interrupt the run partway through and see the best results so far. TPOT also provides a <code>warm_start</code> parameter that lets you restart a TPOT run from where it left off.</p> AutoML algorithms can recommend different solutions for the same dataset <p>If you're working with a reasonably complex dataset or run TPOT for a short amount of time, different TPOT runs may result in different pipeline recommendations. TPOT's optimization algorithm is stochastic in nature, which means that it uses randomness (in part) to search the possible pipeline space. When two TPOT runs recommend different pipelines, this means that the TPOT runs didn't converge due to lack of time or that multiple pipelines perform more-or-less the same on your dataset.</p> <p>This is actually an advantage over fixed grid search techniques: TPOT is meant to be an assistant that gives you ideas on how to solve a particular machine learning problem by exploring pipeline configurations that you might have never considered, then leaves the fine-tuning to more constrained parameter tuning techniques such as grid search.</p>"},{"location":"archived/using/#tpot-with-code","title":"TPOT with code","text":"<p>We've taken care to design the TPOT interface to be as similar as possible to scikit-learn.</p> <p>TPOT can be imported just like any regular Python module. To import TPOT, type:</p> <pre><code>from tpot import TPOTClassifier\n</code></pre> <p>then create an instance of TPOT as follows:</p> <pre><code>pipeline_optimizer = TPOTClassifier()\n</code></pre> <p>It's also possible to use TPOT for regression problems with the <code>TPOTRegressor</code> class. Other than the class name, a <code>TPOTRegressor</code> is used the same way as a <code>TPOTClassifier</code>. You can read more about the <code>TPOTClassifier</code> and <code>TPOTRegressor</code> classes in the API documentation.</p> <p>Some example code with custom TPOT parameters might look like:</p> <pre><code>pipeline_optimizer = TPOTClassifier(generations=5, population_size=20, cv=5,\n                                    random_state=42, verbosity=2)\n</code></pre> <p>Now TPOT is ready to optimize a pipeline for you. You can tell TPOT to optimize a pipeline based on a data set with the <code>fit</code> function:</p> <pre><code>pipeline_optimizer.fit(X_train, y_train)\n</code></pre> <p>The <code>fit</code> function initializes the genetic programming algorithm to find the highest-scoring pipeline based on average k-fold cross-validation Then, the pipeline is trained on the entire set of provided samples, and the TPOT instance can be used as a fitted model.</p> <p>You can then proceed to evaluate the final pipeline on the testing set with the <code>score</code> function:</p> <pre><code>print(pipeline_optimizer.score(X_test, y_test))\n</code></pre> <p>Finally, you can tell TPOT to export the corresponding Python code for the optimized pipeline to a text file with the <code>export</code> function:</p> <pre><code>pipeline_optimizer.export('tpot_exported_pipeline.py')\n</code></pre> <p>Once this code finishes running, <code>tpot_exported_pipeline.py</code> will contain the Python code for the optimized pipeline.</p> <p>Below is a full example script using TPOT to optimize a pipeline, score it, and export the best pipeline to a file.</p> <pre><code>from tpot import TPOTClassifier\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\n\ndigits = load_digits()\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,\n                                                    train_size=0.75, test_size=0.25)\n\npipeline_optimizer = TPOTClassifier(generations=5, population_size=20, cv=5,\n                                    random_state=42, verbosity=2)\npipeline_optimizer.fit(X_train, y_train)\nprint(pipeline_optimizer.score(X_test, y_test))\npipeline_optimizer.export('tpot_exported_pipeline.py')\n</code></pre> <p>Check our examples to see TPOT applied to some specific data sets.</p>"},{"location":"archived/using/#tpot-on-the-command-line","title":"TPOT on the command line","text":"<p>To use TPOT via the command line, enter the following command with a path to the data file:</p> <pre><code>tpot /path_to/data_file.csv\n</code></pre> <p>An example command-line call to TPOT may look like:</p> <pre><code>tpot data/mnist.csv -is , -target class -o tpot_exported_pipeline.py -g 5 -p 20 -cv 5 -s 42 -v 2\n</code></pre> <p>TPOT offers several arguments that can be provided at the command line. To see brief descriptions of these arguments, enter the following command:</p> <pre><code>tpot --help\n</code></pre> <p>Detailed descriptions of the command-line arguments are below.</p> Argument Parameter Valid values Effect -is INPUT_SEPARATOR Any string Character used to separate columns in the input file. -target TARGET_NAME Any string Name of the target column in the input file. -mode TPOT_MODE ['classification', 'regression'] Whether TPOT is being used for a supervised classification or regression problem. -o OUTPUT_FILE String path to a file File to export the code for the final optimized pipeline. -g GENERATIONS Any positive integer or None Number of iterations to run the pipeline optimization process. It must be a positive number or None. If None, the parameter max_time_mins must be defined as the runtime limit. Generally, TPOT will work better when you give it more generations (and therefore time) to optimize the pipeline.  TPOT will evaluate POPULATION_SIZE + GENERATIONS x OFFSPRING_SIZE pipelines in total. -p POPULATION_SIZE Any positive integer Number of individuals to retain in the GP population every generation. Generally, TPOT will work better when you give it more individuals (and therefore time) to optimize the pipeline.  TPOT will evaluate POPULATION_SIZE + GENERATIONS x OFFSPRING_SIZE pipelines in total. -os OFFSPRING_SIZE Any positive integer Number of offspring to produce in each GP generation.  By default, OFFSPRING_SIZE = POPULATION_SIZE. -mr MUTATION_RATE [0.0, 1.0] GP mutation rate in the range [0.0, 1.0]. This tells the GP algorithm how many pipelines to apply random changes to every generation.  We recommend using the default parameter unless you understand how the mutation rate affects GP algorithms. -xr CROSSOVER_RATE [0.0, 1.0] GP crossover rate in the range [0.0, 1.0]. This tells the GP algorithm how many pipelines to \"breed\" every generation.  We recommend using the default parameter unless you understand how the crossover rate affects GP algorithms. -scoring SCORING_FN 'accuracy', 'adjusted_rand_score', 'average_precision', 'balanced_accuracy','f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_median_absolute_error', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted','r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'my_module.scorer_name*' Function used to evaluate the quality of a given pipeline for the problem. By default, accuracy is used for classification and mean squared error (MSE) is used for regression.  TPOT assumes that any function with \"error\" or \"loss\" in the name is meant to be minimized, whereas any other functions will be maximized.  my_module.scorer_name: You can also specify your own function or a full python path to an existing one.  See the section on scoring functions for more details. -cv CV Any integer &gt; 1 Number of folds to evaluate each pipeline over in k-fold cross-validation during the TPOT optimization process. -sub SUBSAMPLE (0.0, 1.0] Subsample ratio of the training instance. Setting it to 0.5 means that TPOT randomly collects half of training samples for pipeline optimization process. -njobs NUM_JOBS Any positive integer or -1 Number of CPUs for evaluating pipelines in parallel during the TPOT optimization process.  Assigning this to -1 will use as many cores as available on the computer. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. -maxtime MAX_TIME_MINS Any positive integer How many minutes TPOT has to optimize the pipeline.  How many minutes TPOT has to optimize the pipeline.If not None, this setting will allow TPOT to run until max_time_mins minutes elapsed and then stop. TPOT will stop earlier if generationsis set and all generations are already evaluated. -maxeval MAX_EVAL_MINS Any positive float How many minutes TPOT has to evaluate a single pipeline.  Setting this parameter to higher values will allow TPOT to consider more complex pipelines but will also allow TPOT to run longer. -s RANDOM_STATE Any positive integer Random number generator seed for reproducibility.  Set this seed if you want your TPOT run to be reproducible with the same seed and data set in the future. -config CONFIG_FILE String or file path Operators and parameter configurations in TPOT:  <ul> <li>Path for configuration file: TPOT will use the path to a configuration file for customizing the operators and parameters that TPOT uses in the optimization process</li> <li>string 'TPOT light', TPOT will use a built-in configuration with only fast models and preprocessors</li> <li>string 'TPOT MDR', TPOT will use a built-in configuration specialized for genomic studies</li> <li>string 'TPOT sparse': TPOT will use a configuration dictionary with a one-hot encoder and the operators normally included in TPOT that also support sparse matrices.</li> </ul> See the built-in configurations section for the list of configurations included with TPOT, and the custom configuration section for more information and examples of how to create your own TPOT configurations.  -template TEMPLATE String Template of predefined pipeline structure. The option is for specifying a desired structure for the machine learning pipeline evaluated in TPOT. So far this option only supports linear pipeline structure. Each step in the pipeline should be a main class of operators (Selector, Transformer, Classifier or Regressor) or a specific operator (e.g. `SelectPercentile`) defined in TPOT operator configuration. If one step is a main class, TPOT will randomly assign all subclass operators (subclasses of [`SelectorMixin`](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_selection/base.py#L17), [`TransformerMixin`](https://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html), [`ClassifierMixin`](https://scikit-learn.org/stable/modules/generated/sklearn.base.ClassifierMixin.html) or [`RegressorMixin`](https://scikit-learn.org/stable/modules/generated/sklearn.base.RegressorMixin.html) in scikit-learn) to that step. Steps in the template are delimited by \"-\", e.g. \"SelectPercentile-Transformer-Classifier\". By default value of template is None, TPOT generates tree-based pipeline randomly.  See the  template option in tpot section for more details.  -memory MEMORY String or file path If supplied, pipeline will cache each transformer after calling fit. This feature is used to avoid computing the fit transformers within a pipeline if the parameters and input data are identical with another fitted pipeline during optimization process. Memory caching mode in TPOT:  <ul> <li>Path for a caching directory: TPOT uses memory caching with the provided directory and TPOT does NOT clean the caching directory up upon shutdown.</li> <li>string 'auto': TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.</li> </ul> -cf CHECKPOINT_FOLDER Folder path  If supplied, a folder you created, in which tpot will periodically save pipelines in pareto front so far while optimizing.  This is useful in multiple cases: <ul> <li>sudden death before tpot could save an optimized pipeline</li> <li>progress tracking</li> <li>grabbing a pipeline while tpot is working</li> </ul>  Example:  mkdir my_checkpoints  -cf ./my_checkpoints  -es EARLY_STOP Any positive integer  How many generations TPOT checks whether there is no improvement in optimization process.  End optimization process if there is no improvement in the set number of generations.  -v VERBOSITY {0, 1, 2, 3} How much information TPOT communicates while it is running.  0 = none, 1 = minimal, 2 = high, 3 = all.  A setting of 2 or higher will add a progress bar during the optimization procedure. -log LOG Folder path Save progress content to a file. --no-update-check Flag indicating whether the TPOT version checker should be disabled. --version Show TPOT's version number and exit. --help Show TPOT's help documentation and exit."},{"location":"archived/using/#scoring-functions","title":"Scoring functions","text":"<p>TPOT makes use of <code>sklearn.model_selection.cross_val_score</code> for evaluating pipelines, and as such offers the same support for scoring functions. There are two ways to make use of scoring functions with TPOT:</p> <ul> <li> <p>You can pass in a string to the <code>scoring</code> parameter from the list above. Any other strings will cause TPOT to throw an exception.</p> </li> <li> <p>You can pass the callable object/function with signature <code>scorer(estimator, X, y)</code>, where <code>estimator</code> is trained estimator to use for scoring, <code>X</code> are features that will be passed to <code>estimator.predict</code> and <code>y</code> are target values for <code>X</code>. To do this, you should implement your own function. See the example below for further explanation.</p> </li> </ul> <pre><code>from tpot import TPOTClassifier\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import make_scorer\n\ndigits = load_digits()\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,\n                                                    train_size=0.75, test_size=0.25)\n# Make a custom metric function\ndef my_custom_accuracy(y_true, y_pred):\n    return float(sum(y_pred == y_true)) / len(y_true)\n\n# Make a custom a scorer from the custom metric function\n# Note: greater_is_better=False in make_scorer below would mean that the scoring function should be minimized.\nmy_custom_scorer = make_scorer(my_custom_accuracy, greater_is_better=True)\n\ntpot = TPOTClassifier(generations=5, population_size=20, verbosity=2,\n                      scoring=my_custom_scorer)\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_test, y_test))\ntpot.export('tpot_digits_pipeline.py')\n</code></pre> <ul> <li>my_module.scorer_name: You can also use a custom <code>score_func(y_true, y_pred)</code> or <code>scorer(estimator, X, y)</code> function through the command line by adding the argument <code>-scoring my_module.scorer</code> to your command-line call. TPOT will import your module and use the custom scoring function from there. TPOT will include your current working directory when importing the module, so you can place it in the same directory where you are going to run TPOT. Example: <code>-scoring sklearn.metrics.auc</code> will use the function auc from sklearn.metrics module.</li> </ul>"},{"location":"archived/using/#built-in-tpot-configurations","title":"Built-in TPOT configurations","text":"<p>TPOT comes with a handful of default operators and parameter configurations that we believe work well for optimizing machine learning pipelines. Below is a list of the current built-in configurations that come with TPOT.</p> Configuration Name Description Operators Default TPOT TPOT will search over a broad range of preprocessors, feature constructors, feature selectors, models, and parameters to find a series of operators that minimize the error of the model predictions. Some of these operators are complex and may take a long time to run, especially on larger datasets.  Note: This is the default configuration for TPOT. To use this configuration, use the default value (None) for the config_dict parameter. Classification Regression TPOT light TPOT will search over a restricted range of preprocessors, feature constructors, feature selectors, models, and parameters to find a series of operators that minimize the error of the model predictions. Only simpler and fast-running operators will be used in these pipelines, so TPOT light is useful for finding quick and simple pipelines for a classification or regression problem.  This configuration works for both the TPOTClassifier and TPOTRegressor. Classification Regression TPOT MDR TPOT will search over a series of feature selectors and Multifactor Dimensionality Reduction models to find a series of operators that maximize prediction accuracy. The TPOT MDR configuration is specialized for genome-wide association studies (GWAS), and is described in detail online here.  Note that TPOT MDR may be slow to run because the feature selection routines are computationally expensive, especially on large datasets. Classification Regression TPOT sparse TPOT uses a configuration dictionary with a one-hot encoder and the operators normally included in TPOT that also support sparse matrices.  This configuration works for both the TPOTClassifier and TPOTRegressor. Classification Regression TPOT NN TPOT uses the same configuration as \"Default TPOT\" plus additional neural network estimators written in PyTorch (currently only `tpot.builtins.PytorchLRClassifier` and `tpot.builtins.PytorchMLPClassifier`).  Currently only classification is supported, but future releases will include regression estimators. Classification TPOT cuML TPOT will search over a restricted configuration using the GPU-accelerated estimators in RAPIDS cuML and DMLC XGBoost. This configuration requires an NVIDIA Pascal architecture or better GPU with compute capability 6.0+, and that the library cuML is installed. With this configuration, all model training and predicting will be GPU-accelerated.  This configuration is particularly useful for medium-sized and larger datasets on which CPU-based estimators are a common bottleneck, and works for both the TPOTClassifier and TPOTRegressor. Classification Regression <p>To use any of these configurations, simply pass the string name of the configuration to the <code>config_dict</code> parameter (or <code>-config</code> on the command line). For example, to use the \"TPOT light\" configuration:</p> <pre><code>from tpot import TPOTClassifier\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\n\ndigits = load_digits()\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,\n                                                    train_size=0.75, test_size=0.25)\n\ntpot = TPOTClassifier(generations=5, population_size=20, verbosity=2,\n                      config_dict='TPOT light')\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_test, y_test))\ntpot.export('tpot_digits_pipeline.py')\n</code></pre>"},{"location":"archived/using/#customizing-tpots-operators-and-parameters","title":"Customizing TPOT's operators and parameters","text":"<p>Beyond the default configurations that come with TPOT, in some cases it is useful to limit the algorithms and parameters that TPOT considers. For that reason, we allow users to provide TPOT with a custom configuration for its operators and parameters.</p> <p>The custom TPOT configuration must be in nested dictionary format, where the first level key is the path and name of the operator (e.g., <code>sklearn.naive_bayes.MultinomialNB</code>) and the second level key is the corresponding parameter name for that operator (e.g., <code>fit_prior</code>). The second level key should point to a list of parameter values for that parameter, e.g., <code>'fit_prior': [True, False]</code>.</p> <p>For a simple example, the configuration could be:</p> <pre><code>tpot_config = {\n    'sklearn.naive_bayes.GaussianNB': {\n    },\n\n    'sklearn.naive_bayes.BernoulliNB': {\n        'alpha': [1e-3, 1e-2, 1e-1, 1., 10., 100.],\n        'fit_prior': [True, False]\n    },\n\n    'sklearn.naive_bayes.MultinomialNB': {\n        'alpha': [1e-3, 1e-2, 1e-1, 1., 10., 100.],\n        'fit_prior': [True, False]\n    }\n}\n</code></pre> <p>in which case TPOT would only consider pipelines containing <code>GaussianNB</code>, <code>BernoulliNB</code>, <code>MultinomialNB</code>, and tune those algorithm's parameters in the ranges provided. This dictionary can be passed directly within the code to the <code>TPOTClassifier</code>/<code>TPOTRegressor</code> <code>config_dict</code> parameter, described above. For example:</p> <pre><code>from tpot import TPOTClassifier\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\n\ndigits = load_digits()\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,\n                                                    train_size=0.75, test_size=0.25)\n\ntpot_config = {\n    'sklearn.naive_bayes.GaussianNB': {\n    },\n\n    'sklearn.naive_bayes.BernoulliNB': {\n        'alpha': [1e-3, 1e-2, 1e-1, 1., 10., 100.],\n        'fit_prior': [True, False]\n    },\n\n    'sklearn.naive_bayes.MultinomialNB': {\n        'alpha': [1e-3, 1e-2, 1e-1, 1., 10., 100.],\n        'fit_prior': [True, False]\n    }\n}\n\ntpot = TPOTClassifier(generations=5, population_size=20, verbosity=2,\n                      config_dict=tpot_config)\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_test, y_test))\ntpot.export('tpot_digits_pipeline.py')\n</code></pre> <p>Command-line users must create a separate <code>.py</code> file with the custom configuration and provide the path to the file to the <code>tpot</code> call. For example, if the simple example configuration above is saved in <code>tpot_classifier_config.py</code>, that configuration could be used on the command line with the command:</p> <pre><code>tpot data/mnist.csv -is , -target class -config tpot_classifier_config.py -g 5 -p 20 -v 2 -o tpot_exported_pipeline.py\n</code></pre> <p>When using the command-line interface, the configuration file specified in the <code>-config</code> parameter must name its custom TPOT configuration <code>tpot_config</code>. Otherwise, TPOT will not be able to locate the configuration dictionary.</p> <p>For more detailed examples of how to customize TPOT's operator configuration, see the default configurations for classification and regression in TPOT's source code.</p> <p>Note that you must have all of the corresponding packages for the operators installed on your computer, otherwise TPOT will not be able to use them. For example, if XGBoost is not installed on your computer, then TPOT will simply not import nor use XGBoost in the pipelines it considers.</p>"},{"location":"archived/using/#template-option-in-tpot","title":"Template option in TPOT","text":"<p>Template option provides a way to specify a desired structure for machine learning pipeline, which may reduce TPOT computation time and potentially provide more interpretable results. Current implementation only supports linear pipelines.</p> <p>Below is a simple example to use <code>template</code> option. The pipelines generated/evaluated in TPOT will follow this structure: 1st step is a feature selector (a subclass of <code>SelectorMixin</code>), 2nd step is a feature transformer (a subclass of <code>TransformerMixin</code>) and 3rd step is a classifier for classification (a subclass of <code>ClassifierMixin</code>). The last step must be <code>Classifier</code> for <code>TPOTClassifier</code>'s template but <code>Regressor</code> for <code>TPOTRegressor</code>. Note: although <code>SelectorMixin</code> is subclass of <code>TransformerMixin</code> in scikit-learn, but <code>Transformer</code> in this option excludes those subclasses of <code>SelectorMixin</code>.</p> <pre><code>tpot_obj = TPOTClassifier(\n                template='Selector-Transformer-Classifier'\n                )\n</code></pre> <p>If a specific operator, e.g. <code>SelectPercentile</code>, is preferred for usage in the 1st step of the pipeline, the template can be defined like 'SelectPercentile-Transformer-Classifier'.</p>"},{"location":"archived/using/#featuresetselector-in-tpot","title":"FeatureSetSelector in TPOT","text":"<p><code>FeatureSetSelector</code> is a special new operator in TPOT. This operator enables feature selection based on priori expert knowledge. For example, in RNA-seq gene expression analysis, this operator can be used to select one or more gene (feature) set(s) based on GO (Gene Ontology) terms or annotated gene sets Molecular Signatures Database (MSigDB) in the 1st step of pipeline via <code>template</code> option above, in order to reduce dimensions and TPOT computation time. This operator requires a dataset list in csv format. In this csv file, there are only three columns: 1st column is feature set names, 2nd column is the total number of features in one set and 3rd column is a list of feature names (if input X is pandas.DataFrame) or indexes (if input X is numpy.ndarray) delimited by \";\". Below is an example how to use this operator in TPOT.</p> <p>Please check our preprint paper for more details.</p> <pre><code>from tpot import TPOTClassifier\nimport numpy as np\nimport pandas as pd\nfrom tpot.config import classifier_config_dict\ntest_data = pd.read_csv(\"https://raw.githubusercontent.com/EpistasisLab/tpot/master/tests/tests.csv\")\ntest_X = test_data.drop(\"class\", axis=1)\ntest_y = test_data['class']\n\n# add FeatureSetSelector into tpot configuration\nclassifier_config_dict['tpot.builtins.FeatureSetSelector'] = {\n    'subset_list': ['https://raw.githubusercontent.com/EpistasisLab/tpot/master/tests/subset_test.csv'],\n    'sel_subset': [0,1] # select only one feature set, a list of index of subset in the list above\n    #'sel_subset': list(combinations(range(3), 2)) # select two feature sets\n}\n\n\ntpot = TPOTClassifier(generations=5,\n                           population_size=50, verbosity=2,\n                           template='FeatureSetSelector-Transformer-Classifier',\n                           config_dict=classifier_config_dict)\ntpot.fit(test_X, test_y)\n</code></pre>"},{"location":"archived/using/#pipeline-caching-in-tpot","title":"Pipeline caching in TPOT","text":"<p>With the <code>memory</code> parameter, pipelines can cache the results of each transformer after fitting them. This feature is used to avoid repeated computation by transformers within a pipeline if the parameters and input data are identical to another fitted pipeline during optimization process. TPOT allows users to specify a custom directory path or <code>joblib.Memory</code> in case they want to re-use the memory cache in future TPOT runs (or a <code>warm_start</code> run).</p> <p>There are three methods for enabling memory caching in TPOT:</p> <pre><code>from tpot import TPOTClassifier\nfrom tempfile import mkdtemp\nfrom joblib import Memory\nfrom shutil import rmtree\n\n# Method 1, auto mode: TPOT uses memory caching with a temporary directory and cleans it up upon shutdown\ntpot = TPOTClassifier(memory='auto')\n\n# Method 2, with a custom directory for memory caching\ntpot = TPOTClassifier(memory='/to/your/path')\n\n# Method 3, with a Memory object\ncachedir = mkdtemp() # Create a temporary folder\nmemory = Memory(cachedir=cachedir, verbose=0)\ntpot = TPOTClassifier(memory=memory)\n\n# Clear the cache directory when you don't need it anymore\nrmtree(cachedir)\n</code></pre> <p>Note: TPOT does NOT clean up memory caches if users set a custom directory path or Memory object. We recommend that you clean up the memory caches when you don't need it anymore.</p>"},{"location":"archived/using/#crashfreeze-issue-with-n_jobs-1-under-osx-or-linux","title":"Crash/freeze issue with n_jobs &gt; 1 under OSX or Linux","text":"<p>Internally, TPOT uses joblib to fit estimators in parallel. This is the same parallelization framework used by scikit-learn. But it may crash/freeze with n_jobs &gt; 1 under OSX or Linux as scikit-learn does, especially with large datasets.</p> <p>One solution is to configure Python's <code>multiprocessing</code> module to use the <code>forkserver</code> start method (instead of the default <code>fork</code>) to manage the process pools. You can enable the <code>forkserver</code> mode globally for your program by putting the following codes into your main script:</p> <pre><code>import multiprocessing\n\n# other imports, custom code, load data, define model...\n\nif __name__ == '__main__':\n    multiprocessing.set_start_method('forkserver')\n\n    # call scikit-learn utils or tpot utils with n_jobs &gt; 1 here\n</code></pre> <p>More information about these start methods can be found in the multiprocessing documentation.</p>"},{"location":"archived/using/#parallel-training-with-dask","title":"Parallel Training with Dask","text":"<p>For large problems or working on Jupyter notebook, we highly recommend that you can distribute the work on a Dask cluster. The dask-examples binder has a runnable example with a small dask cluster.</p> <p>To use your Dask cluster to fit a TPOT model, specify the <code>use_dask</code> keyword when you create the TPOT estimator. Note: if <code>use_dask=True</code>, TPOT will use as many cores as available on the your Dask cluster. If <code>n_jobs</code> is specified, then it will control the chunk size (10*<code>n_jobs</code> if it is less then offspring size) of parallel training.</p> <pre><code>estimator = TPOTEstimator(use_dask=True, n_jobs=-1)\n</code></pre> <p>This will use all the workers on your cluster to do the training, and use Dask-ML's pipeline rewriting to avoid re-fitting estimators multiple times on the same set of data. It will also provide fine-grained diagnostics in the distributed scheduler UI.</p> <p>Alternatively, Dask implements a joblib backend. You can instruct TPOT to use the distributed backend during training by specifying a <code>joblib.parallel_backend</code>:</p> <pre><code>import joblib\nimport distributed.joblib\nfrom dask.distributed import Client\n\n# connect to the cluster\nclient = Client('schedueler-address')\n\n# create the estimator normally\nestimator = TPOTClassifier(n_jobs=-1)\n\n# perform the fit in this context manager\nwith joblib.parallel_backend(\"dask\"):\n    estimator.fit(X, y)\n</code></pre> <p>See dask's distributed joblib integration for more.</p>"},{"location":"archived/using/#neural-networks-in-tpot-tpotnn","title":"Neural Networks in TPOT (<code>tpot.nn</code>)","text":"<p>Support for neural network models and deep learning is an experimental feature newly added to TPOT. Available neural network architectures are provided by the <code>tpot.nn</code> module. Unlike regular <code>sklearn</code> estimators, these models need to be written by hand, and must also inherit the appropriate base classes provided by <code>sklearn</code> for all of their built-in modules. In other words, they need implement methods like <code>.fit()</code>, <code>fit_transform()</code>, <code>get_params()</code>, etc., as described in detail on Developing scikit-learn estimators.</p>"},{"location":"archived/using/#telling-tpot-to-use-built-in-pytorch-neural-network-models","title":"Telling TPOT to use built-in PyTorch neural network models","text":"<p>Mainly due to the issues described below, TPOT won't use its neural network models unless you explicitly tell it to do so. This is done as follows:</p> <ul> <li> <p>Use <code>import tpot.nn</code> before instantiating any TPOT estimators.</p> </li> <li> <p>Use a configuration dictionary that includes one or more <code>tpot.nn</code> estimators, either by writing one manually, including one from a file, or by importing the configuration in <code>tpot/config/classifier_nn.py</code>. A very simple example that will force TPOT to only use a PyTorch-based logistic regression classifier as its main estimator is as follows:</p> </li> </ul> <pre><code>tpot_config = {\n    'tpot.nn.PytorchLRClassifier': {\n        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.]\n    }\n}\n</code></pre> <ul> <li>Alternatively, use a template string including <code>PytorchLRClassifier</code> or <code>PytorchMLPClassifier</code> while loading the TPOT-NN configuration dictionary.</li> </ul> <p>Neural network models are notorious for being extremely sensitive to their initialization parameters, so you may need to heavily adjust <code>tpot.nn</code> configuration dictionaries in order to attain good performance on your dataset.</p> <p>A simple example of using TPOT-NN is shown in examples.</p>"},{"location":"archived/using/#important-caveats","title":"Important caveats","text":"<ul> <li> <p>Neural network models (especially when they reach moderately large sizes) take a notoriously large amount of time and computing power to train. You should expect <code>tpot.nn</code> neural networks to train several orders of magnitude slower than their <code>sklearn</code> alternatives. This can be alleviated somewhat by training the models on computers with CUDA-enabled GPUs.</p> </li> <li> <p>TPOT will occasionally learn pipelines that stack several <code>sklearn</code> estimators. Mathematically, these can be nearly identical to some deep learning models. For example, by stacking several <code>sklearn.linear_model.LogisticRegression</code>s, you end up with a very close approximation of a Multilayer Perceptron; one of the simplest and most well known deep learning architectures. TPOT's genetic programming algorithms generally optimize these 'networks' much faster than PyTorch, which typically uses a more brute-force convex optimization approach.</p> </li> <li> <p>The problem of 'black box' model introspection is one of the most substantial criticisms and challenges of deep learning. This problem persists in <code>tpot.nn</code>, whereas TPOT's default estimators often are far easier to introspect.</p> </li> </ul>"},{"location":"documentation/tpot/_version/","title":"version","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/graphsklearn/","title":"Graphsklearn","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/graphsklearn/#tpot.graphsklearn.GraphPipeline","title":"<code>GraphPipeline</code>","text":"<p>               Bases: <code>_BaseComposition</code></p> Source code in <code>tpot/graphsklearn.py</code> <pre><code>class GraphPipeline(_BaseComposition):\n    def __init__(\n                self,\n                graph,\n                cross_val_predict_cv=0, #signature function(estimator, X, y=none)\n                method='auto',\n                memory=None,\n                use_label_encoder=False,\n                **kwargs,\n                ):\n        super().__init__(**kwargs)\n        '''\n        An sklearn baseestimator that uses genetic programming to optimize a pipeline.\n\n        Parameters\n        ----------\n\n        graph: networkx.DiGraph\n            A directed graph where the nodes are sklearn estimators and the edges are the inputs to those estimators.\n\n        cross_val_predict_cv: int, cross-validation generator or an iterable, optional\n            Determines the cross-validation splitting strategy used in inner classifiers or regressors\n\n        method: str, optional\n            The prediction method to use for the inner classifiers or regressors. If 'auto', it will try to use predict_proba, decision_function, or predict in that order.\n\n        memory: str or object with the joblib.Memory interface, optional\n            Used to cache the input and outputs of nodes to prevent refitting or computationally heavy transformations. By default, no caching is performed. If a string is given, it is the path to the caching directory.\n\n        use_label_encoder: bool, optional\n            If True, the label encoder is used to encode the labels to be 0 to N. If False, the label encoder is not used.\n            Mainly useful for classifiers (XGBoost) that require labels to be ints from 0 to N.\n\n            Can also be a sklearn.preprocessing.LabelEncoder object. If so, that label encoder is used.\n\n        '''\n\n        self.graph = graph\n        self.cross_val_predict_cv = cross_val_predict_cv\n        self.method = method\n        self.memory = memory\n        self.use_label_encoder = use_label_encoder\n\n        setup_ordered_successors(graph)\n\n        self.topo_sorted_nodes = list(nx.topological_sort(self.graph))\n        self.topo_sorted_nodes.reverse()\n\n        self.root = self.topo_sorted_nodes[-1]\n\n        if self.use_label_encoder:\n            if type(self.use_label_encoder) == LabelEncoder:\n                self.label_encoder = self.use_label_encoder\n            else:\n                self.label_encoder = LabelEncoder()\n\n\n        #TODO clean this up\n        try:\n            nx.find_cycle(self.G)\n            raise BaseException \n        except: \n            pass\n\n    def __str__(self):\n        if len(self.graph.edges) &gt; 0:\n            return str(self.graph.edges)\n        else:\n            return str(self.graph.nodes)\n\n    def fit(self, X, y):\n\n\n        if self.use_label_encoder:\n            if type(self.use_label_encoder) == LabelEncoder:\n                y = self.label_encoder.transform(y)\n            else:\n                y = self.label_encoder.fit_transform(y)\n\n\n\n        fit_sklearn_digraph(   graph=self.graph,\n                                X=X,\n                                y=y,\n                                method=self.method,\n                                cross_val_predict_cv = self.cross_val_predict_cv,\n                                memory = self.memory,\n                                topo_sort = self.topo_sorted_nodes,\n                                )\n\n        return self\n\n    def plot(self, ):\n        plot(graph = self.graph)\n\n    def __sklearn_is_fitted__(self):\n        '''Indicate whether pipeline has been fit.'''\n        try:\n            # check if the last step of the pipeline is fitted\n            # we only check the last step since if the last step is fit, it\n            # means the previous steps should also be fit. This is faster than\n            # checking if every step of the pipeline is fit.\n            sklearn.utils.validation.check_is_fitted(self.graph.nodes[self.root][\"instance\"])\n            return True\n        except sklearn.exceptions.NotFittedError:\n            return False\n\n    @available_if(_estimator_has('predict'))\n    def predict(self, X, **predict_params):\n\n\n        this_X = get_inputs_to_node(self.graph,\n                    X, \n                    self.root,\n                    method = self.method,\n                    topo_sort = self.topo_sorted_nodes,\n                    )\n\n        preds = self.graph.nodes[self.root][\"instance\"].predict(this_X, **predict_params)\n\n        if self.use_label_encoder:\n            preds = self.label_encoder.inverse_transform(preds)\n\n        return preds\n\n    @available_if(_estimator_has('predict_proba'))\n    def predict_proba(self, X, **predict_params):\n\n\n        this_X = get_inputs_to_node(self.graph,\n                    X, \n                    self.root,\n                    method = self.method,\n                    topo_sort = self.topo_sorted_nodes,\n                    )\n        return self.graph.nodes[self.root][\"instance\"].predict_proba(this_X, **predict_params)\n\n    @available_if(_estimator_has('decision_function'))\n    def decision_function(self, X, **predict_params):\n\n        this_X = get_inputs_to_node(self.graph,\n                    X, \n                    self.root,\n                    method = self.method,\n                    topo_sort = self.topo_sorted_nodes,\n                    )\n        return self.graph.nodes[self.root][\"instance\"].decision_function(this_X, **predict_params)\n\n    @available_if(_estimator_has('transform'))\n    def transform(self, X, **predict_params):\n\n        this_X = get_inputs_to_node(self.graph,\n                    X, \n                    self.root,\n                    method = self.method,\n                    topo_sort = self.topo_sorted_nodes,\n                    )\n        return self.graph.nodes[self.root][\"instance\"].transform(this_X, **predict_params)\n\n    @property\n    def classes_(self):\n        \"\"\"The classes labels. Only exist if the last step is a classifier.\"\"\"\n\n        if self.use_label_encoder:\n            return self.label_encoder.classes_\n        else:\n            return self.graph.nodes[self.root][\"instance\"].classes_\n\n    @property\n    def _estimator_type(self):\n        return self.graph.nodes[self.root][\"instance\"]._estimator_type\n</code></pre>"},{"location":"documentation/tpot/graphsklearn/#tpot.graphsklearn.GraphPipeline.classes_","title":"<code>classes_</code>  <code>property</code>","text":"<p>The classes labels. Only exist if the last step is a classifier.</p>"},{"location":"documentation/tpot/graphsklearn/#tpot.graphsklearn.GraphPipeline.__sklearn_is_fitted__","title":"<code>__sklearn_is_fitted__()</code>","text":"<p>Indicate whether pipeline has been fit.</p> Source code in <code>tpot/graphsklearn.py</code> <pre><code>def __sklearn_is_fitted__(self):\n    '''Indicate whether pipeline has been fit.'''\n    try:\n        # check if the last step of the pipeline is fitted\n        # we only check the last step since if the last step is fit, it\n        # means the previous steps should also be fit. This is faster than\n        # checking if every step of the pipeline is fit.\n        sklearn.utils.validation.check_is_fitted(self.graph.nodes[self.root][\"instance\"])\n        return True\n    except sklearn.exceptions.NotFittedError:\n        return False\n</code></pre>"},{"location":"documentation/tpot/individual/","title":"Individual","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/logbook/","title":"Logbook","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/population/","title":"Population","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/population/#tpot.population.Population","title":"<code>Population</code>","text":"<p>Primary usage is to keep track of evaluated individuals</p> <p>Parameters:</p> Name Type Description Default <code>initial_population</code> <code>list of BaseIndividuals</code> <p>Initial population to start with. If None, start with an empty population.</p> <code>list of BaseIndividuals</code> <code>use_unique_id</code> <code>Bool</code> <p>If True, individuals are treated as unique if they have the same unique_id(). If False, all new individuals are treated as unique.</p> <code>Bool</code> <code>callback</code> <code>function</code> <p>NOT YET IMPLEMENTED A function to call after each generation. The function should take a Population object as its only argument.</p> <code>function</code> <p>Attributes:</p> Name Type Description <code>population</code> <code>{list of BaseIndividuals}</code> <p>The current population of individuals. Contains the live instances of BaseIndividuals.</p> <code>evaluated_individuals</code> <code>{dict}</code> <p>A dictionary of dictionaries. The keys are the unique_id() or self of each BaseIndividual. Can be thought of as a table with the unique_id() as the row index and the inner dictionary keys as the columns.</p> Source code in <code>tpot/population.py</code> <pre><code>class Population():\n    '''\n    Primary usage is to keep track of evaluated individuals\n\n    Parameters\n    ----------\n    initial_population : {list of BaseIndividuals}, default=None\n        Initial population to start with. If None, start with an empty population.\n    use_unique_id : {Bool}, default=True\n        If True, individuals are treated as unique if they have the same unique_id().\n        If False, all new individuals are treated as unique.\n    callback : {function}, default=None\n        NOT YET IMPLEMENTED\n        A function to call after each generation. The function should take a Population object as its only argument.\n\n    Attributes\n    ----------\n    population : {list of BaseIndividuals}\n        The current population of individuals. Contains the live instances of BaseIndividuals.\n    evaluated_individuals : {dict}\n        A dictionary of dictionaries. The keys are the unique_id() or self of each BaseIndividual.\n        Can be thought of as a table with the unique_id() as the row index and the inner dictionary keys as the columns.\n    '''\n    def __init__(   self,\n                    column_names: typing.List[str] = None,\n                    n_jobs: int = 1,\n                    callback=None,\n                    ) -&gt; None:\n\n        if column_names is not None:\n\n            column_names = column_names+[\"Parents\", \"Variation_Function\"]\n        else:\n            column_names = [\"Parents\", \"Variation_Function\"]\n        self.evaluated_individuals = pd.DataFrame(columns=column_names)\n        self.evaluated_individuals[\"Parents\"] = self.evaluated_individuals[\"Parents\"].astype('object')\n\n        self.use_unique_id = True #Todo clean this up. perhaps pull unique_id() out of baseestimator and have it be supplied as a function\n        self.n_jobs = n_jobs\n        self.callback=callback\n        self.population = []\n\n    def survival_select(self, selector, weights, columns_names, n_survivors, rng, inplace=True):\n        rng = np.random.default_rng(rng)\n        weighted_scores = self.get_column(self.population, column_names=columns_names) * weights\n        new_population_index = np.ravel(selector(weighted_scores, k=n_survivors, rng=rng)) #TODO make it clear that we are concatenating scores...\n        new_population = np.array(self.population)[new_population_index]\n        if inplace:\n            self.set_population(new_population, rng=rng)\n        return new_population\n\n    def parent_select(self, selector, weights, columns_names, k, n_parents, rng):\n        rng = np.random.default_rng(rng)\n        weighted_scores = self.get_column(self.population, column_names=columns_names) * weights\n        parents_index = selector(weighted_scores, k=k, n_parents=n_parents, rng=rng)\n        parents = np.array(self.population)[parents_index]\n        return parents\n\n\n    #remove individuals that either do not have a column_name value or a nan in that value\n    #TODO take into account when the value is not a list/tuple?\n    #TODO make invalid a global variable?\n    def remove_invalid_from_population(self, column_names, invalid_value = \"INVALID\"):\n        '''\n        Remove individuals from the live population if either do not have a value in the column_name column or if the value contains np.nan.\n\n        Parameters\n        ----------\n        column_name : {str}\n            The name of the column to check for np.nan values.\n\n        Returns\n        -------\n        None\n        '''\n        if isinstance(column_names, str): #TODO check this\n            column_names = [column_names]\n        is_valid = lambda ind: ind.unique_id() not in self.evaluated_individuals.index or invalid_value not in self.evaluated_individuals.loc[ind.unique_id(),column_names].to_list()\n        self.population = [ind for ind in self.population if is_valid(ind)]\n\n\n\n    # takes the list of individuals and adds it to the live population list.\n    # if keep_repeats is False, repeated individuals are not added to the population\n    # returns a list of individuals added to the live population\n    #TODO make keep repeats allow for previously evaluated individuals,\n    #but make sure that the live population only includes one of each, no repeats\n    def add_to_population(self, individuals: typing.List[BaseIndividual], rng, keep_repeats=False, mutate_until_unique=True):\n        '''\n        Add individuals to the live population. Add individuals to the evaluated_individuals if they are not already there.\n\n        Parameters:\n        -----------\n        individuals : {list of BaseIndividuals}\n            The individuals to add to the live population.\n        keep_repeats : {bool}, default=False\n            If True, allow the population to have repeated individuals.\n            If False, only add individuals that have not yet been added to geneology.\n        '''\n\n        rng = np.random.default_rng(rng)\n\n        if not isinstance(individuals, collections.abc.Iterable):\n            individuals = [individuals]\n\n        new_individuals = []\n        #TODO check for proper inputs\n        for individual in individuals:\n            key = individual.unique_id()\n\n            if key not in self.evaluated_individuals.index: #If its new, we always add it\n                self.evaluated_individuals.loc[key] = np.nan\n                self.evaluated_individuals.loc[key,\"Individual\"] = copy.deepcopy(individual)\n                self.population.append(individual)\n                new_individuals.append(individual)\n\n            else:#If its old\n                if keep_repeats: #If we want to keep repeats, we add it\n                    self.population.append(individual)\n                    new_individuals.append(individual)\n                elif mutate_until_unique: #If its old and we don't want repeats, we can optionally mutate it until it is unique\n                    for _ in range(20):\n                        individual = copy.deepcopy(individual)\n                        individual.mutate(rng=rng)\n                        key = individual.unique_id()\n                        if key not in self.evaluated_individuals.index:\n                            self.evaluated_individuals.loc[key] = np.nan\n                            self.evaluated_individuals.loc[key,\"Individual\"] = copy.deepcopy(individual)\n                            self.population.append(individual)\n                            new_individuals.append(individual)\n                            break\n\n        return new_individuals\n\n\n    def update_column(self, individual, column_names, data):\n        '''\n        Update the column_name column in the evaluated_individuals with the data.\n        If the data is a list, it must be the same length as the evaluated_individuals.\n        If the data is a single value, it will be applied to all individuals in the evaluated_individuals.\n        '''\n        if isinstance(individual, collections.abc.Iterable):\n            if self.use_unique_id:\n                key = [ind.unique_id() for ind in individual]\n            else:\n                key = individual\n        else:\n            if self.use_unique_id:\n                key = individual.unique_id()\n            else:\n                key = individual\n\n        self.evaluated_individuals.loc[key,column_names] = data\n\n\n    def get_column(self, individual, column_names=None, to_numpy=True):\n        '''\n        Update the column_name column in the evaluated_individuals with the data.\n        If the data is a list, it must be the same length as the evaluated_individuals.\n        If the data is a single value, it will be applied to all individuals in the evaluated_individuals.\n        '''\n        if isinstance(individual, collections.abc.Iterable):\n            if self.use_unique_id:\n                key = [ind.unique_id() for ind in individual]\n            else:\n                key = individual\n        else:\n            if self.use_unique_id:\n                key = individual.unique_id()\n            else:\n                key = individual\n\n        if column_names is not None:\n            slice = self.evaluated_individuals.loc[key,column_names]\n        else:\n            slice = self.evaluated_individuals.loc[key]\n        if to_numpy:\n            slice.reset_index(drop=True, inplace=True)\n            return slice.to_numpy()\n        else:\n            return slice\n\n\n    #returns the individuals without a 'column' as a key in geneology\n    #TODO make sure not to get repeats in this list even if repeats are in the \"live\" population\n    def get_unevaluated_individuals(self, column_names, individual_list=None):\n        if individual_list is None:\n            individual_list = self.population\n\n        if self.use_unique_id:\n            unevaluated_filter = lambda individual: individual.unique_id() not in self.evaluated_individuals.index or any(self.evaluated_individuals.loc[individual.unique_id(), column_names].isna())\n        else:\n            unevaluated_filter = lambda individual: individual not in self.evaluated_individuals.index or any(self.evaluated_individuals.loc[individual.unique_id(), column_names].isna())\n\n        return [individual for individual in individual_list if unevaluated_filter(individual)]\n\n    # def get_valid_evaluated_individuals_df(self, column_names_to_check, invalid_values=[\"TIMEOUT\",\"INVALID\"]):\n    #     '''\n    #     Returns a dataframe of the evaluated individuals that do no have invalid_values in column_names_to_check.\n    #     '''\n    #     return self.evaluated_individuals[~self.evaluated_individuals[column_names_to_check].isin(invalid_values).any(axis=1)]\n\n    #the live population empied and is set to new_population\n    def set_population(self,  new_population, rng, keep_repeats=True):\n        '''\n        sets population to new population\n        for selection?\n        '''\n        rng = np.random.default_rng(rng)\n        self.population = []\n        self.add_to_population(new_population, rng=rng, keep_repeats=keep_repeats)\n\n    #TODO should we just generate one offspring per crossover?\n    def create_offspring(self, parents_list, var_op_list, rng, add_to_population=True, keep_repeats=False, mutate_until_unique=True, n_jobs=1):\n        '''\n        parents_list: a list of lists of parents.\n        var_op_list: a list of var_ops to apply to each list of parents. Should be the same length as parents_list.\n\n        for example:\n        parents_list = [[parent1, parent2], [parent3]]\n        var_op_list = [\"crossover\", \"mutate\"]\n\n        This will apply crossover to parent1 and parent2 and mutate to parent3.\n\n        Creates offspring from parents using the var_op_list.\n        If string, will use a built in method\n            - \"crossover\" : crossover\n            - \"mutate\" : mutate\n            - \"mutate_and_crossover\" : mutate_and_crossover\n            - \"cross_and_mutate\" : cross_and_mutate\n        '''\n        rng = np.random.default_rng(rng)\n        new_offspring = []\n        all_offspring = parallel_create_offspring(parents_list, var_op_list, rng=rng, n_jobs=n_jobs)\n\n        for parents, offspring, var_op in zip(parents_list, all_offspring, var_op_list):\n\n            # if var_op in built_in_var_ops_dict:\n            #     var_op = built_in_var_ops_dict[var_op]\n\n            # offspring = copy.deepcopy(parents)\n            # offspring = var_op(offspring)\n            # if isinstance(offspring, collections.abc.Iterable):\n            #     offspring = offspring[0]\n\n            if add_to_population:\n                added = self.add_to_population(offspring, rng=rng, keep_repeats=keep_repeats, mutate_until_unique=mutate_until_unique)\n                if len(added) &gt; 0:\n                    for new_child in added:\n                        parent_keys = [parent.unique_id() for parent in parents]\n                        if not pd.api.types.is_object_dtype(self.evaluated_individuals[\"Parents\"]): #TODO Is there a cleaner way of doing this? Not required for some python environments?\n                            self.evaluated_individuals[\"Parents\"] = self.evaluated_individuals[\"Parents\"].astype('object')\n                        if not pd.api.types.is_object_dtype(self.evaluated_individuals[\"Variation_Function\"]):#TODO Is there a cleaner way of doing this? Not required for some python environments?\n                            self.evaluated_individuals[\"Variation_Function\"] = self.evaluated_individuals[\"Variation_Function\"].astype('object')\n                        self.evaluated_individuals.at[new_child.unique_id(),\"Parents\"] = tuple(parent_keys)\n\n                        #if var_op is a function\n                        if hasattr(var_op, '__call__'):\n                            self.evaluated_individuals.at[new_child.unique_id(),\"Variation_Function\"] = var_op.__name__\n                        else:\n                            self.evaluated_individuals.at[new_child.unique_id(),\"Variation_Function\"] = str(var_op)\n\n\n                        new_offspring.append(new_child)\n\n            else:\n                new_offspring.append(offspring)\n\n\n        return new_offspring\n\n\n    #TODO should we just generate one offspring per crossover?\n    def create_offspring2(self, parents_list, var_op_list, mutation_functions,mutation_function_weights, crossover_functions,crossover_function_weights, rng, add_to_population=True, keep_repeats=False, mutate_until_unique=True):\n\n        rng = np.random.default_rng(rng)\n        new_offspring = []\n\n        all_offspring = []\n        chosen_ops = []\n\n        for parents, var_op in zip(parents_list,var_op_list):\n            #TODO put this loop in population class\n            if var_op == \"mutate\":\n                mutation_op = rng.choice(mutation_functions, p=mutation_function_weights)\n                all_offspring.append(copy_and_mutate(parents[0], mutation_op, rng=rng))\n                chosen_ops.append(mutation_op.__name__)\n\n\n            elif var_op == \"crossover\":\n                crossover_op = rng.choice(crossover_functions, p=crossover_function_weights)\n                all_offspring.append(copy_and_crossover(parents, crossover_op, rng=rng))\n                chosen_ops.append(crossover_op.__name__)\n            elif var_op == \"mutate_then_crossover\":\n\n                mutation_op1 = rng.choice(mutation_functions, p=mutation_function_weights)\n                mutation_op2 = rng.choice(mutation_functions, p=mutation_function_weights)\n                crossover_op = rng.choice(crossover_functions, p=crossover_function_weights)\n                p1 = copy_and_mutate(parents[0], mutation_op1, rng=rng)\n                p2 = copy_and_mutate(parents[1], mutation_op2, rng=rng)\n                crossover_op(p1,p2,rng=rng)\n                all_offspring.append(p1)\n                chosen_ops.append(f\"{mutation_op1.__name__} , {mutation_op2.__name__} , {crossover_op.__name__}\")\n            elif var_op == \"crossover_then_mutate\":\n                crossover_op = rng.choice(crossover_functions, p=crossover_function_weights)\n                child = copy_and_crossover(parents, crossover_op, rng=rng)\n                mutation_op = rng.choice(mutation_functions, p=mutation_function_weights)\n                mutation_op(child, rng=rng)\n                all_offspring.append(child)\n                chosen_ops.append(f\"{crossover_op.__name__} , {mutation_op.__name__}\")\n\n\n        for parents, offspring, var_op in zip(parents_list, all_offspring, chosen_ops):\n\n            # if var_op in built_in_var_ops_dict:\n            #     var_op = built_in_var_ops_dict[var_op]\n\n            # offspring = copy.deepcopy(parents)\n            # offspring = var_op(offspring)\n            # if isinstance(offspring, collections.abc.Iterable):\n            #     offspring = offspring[0]\n\n            if add_to_population:\n                added = self.add_to_population(offspring, rng=rng, keep_repeats=keep_repeats, mutate_until_unique=mutate_until_unique)\n                if len(added) &gt; 0:\n                    for new_child in added:\n                        parent_keys = [parent.unique_id() for parent in parents]\n                        if not pd.api.types.is_object_dtype(self.evaluated_individuals[\"Parents\"]): #TODO Is there a cleaner way of doing this? Not required for some python environments?\n                            self.evaluated_individuals[\"Parents\"] = self.evaluated_individuals[\"Parents\"].astype('object')\n                        self.evaluated_individuals.at[new_child.unique_id(),\"Parents\"] = tuple(parent_keys)\n\n                        #check if Variation_Function variable is an object type\n                        if not pd.api.types.is_object_dtype(self.evaluated_individuals[\"Variation_Function\"]): #TODO Is there a cleaner way of doing this? Not required for some python environments?\n                            self.evaluated_individuals[\"Variation_Function\"] = self.evaluated_individuals[\"Variation_Function\"].astype('object')\n\n                        #if var_op is a function\n                        if hasattr(var_op, '__call__'):\n                            self.evaluated_individuals.at[new_child.unique_id(),\"Variation_Function\"] = var_op.__name__\n                        else:\n                            self.evaluated_individuals.at[new_child.unique_id(),\"Variation_Function\"] = str(var_op)\n\n\n                        new_offspring.append(new_child)\n\n            else:\n                new_offspring.append(offspring)\n\n\n        return new_offspring\n</code></pre>"},{"location":"documentation/tpot/population/#tpot.population.Population.add_to_population","title":"<code>add_to_population(individuals, rng, keep_repeats=False, mutate_until_unique=True)</code>","text":"<p>Add individuals to the live population. Add individuals to the evaluated_individuals if they are not already there.</p> Parameters: <p>individuals : {list of BaseIndividuals}     The individuals to add to the live population. keep_repeats : {bool}, default=False     If True, allow the population to have repeated individuals.     If False, only add individuals that have not yet been added to geneology.</p> Source code in <code>tpot/population.py</code> <pre><code>def add_to_population(self, individuals: typing.List[BaseIndividual], rng, keep_repeats=False, mutate_until_unique=True):\n    '''\n    Add individuals to the live population. Add individuals to the evaluated_individuals if they are not already there.\n\n    Parameters:\n    -----------\n    individuals : {list of BaseIndividuals}\n        The individuals to add to the live population.\n    keep_repeats : {bool}, default=False\n        If True, allow the population to have repeated individuals.\n        If False, only add individuals that have not yet been added to geneology.\n    '''\n\n    rng = np.random.default_rng(rng)\n\n    if not isinstance(individuals, collections.abc.Iterable):\n        individuals = [individuals]\n\n    new_individuals = []\n    #TODO check for proper inputs\n    for individual in individuals:\n        key = individual.unique_id()\n\n        if key not in self.evaluated_individuals.index: #If its new, we always add it\n            self.evaluated_individuals.loc[key] = np.nan\n            self.evaluated_individuals.loc[key,\"Individual\"] = copy.deepcopy(individual)\n            self.population.append(individual)\n            new_individuals.append(individual)\n\n        else:#If its old\n            if keep_repeats: #If we want to keep repeats, we add it\n                self.population.append(individual)\n                new_individuals.append(individual)\n            elif mutate_until_unique: #If its old and we don't want repeats, we can optionally mutate it until it is unique\n                for _ in range(20):\n                    individual = copy.deepcopy(individual)\n                    individual.mutate(rng=rng)\n                    key = individual.unique_id()\n                    if key not in self.evaluated_individuals.index:\n                        self.evaluated_individuals.loc[key] = np.nan\n                        self.evaluated_individuals.loc[key,\"Individual\"] = copy.deepcopy(individual)\n                        self.population.append(individual)\n                        new_individuals.append(individual)\n                        break\n\n    return new_individuals\n</code></pre>"},{"location":"documentation/tpot/population/#tpot.population.Population.create_offspring","title":"<code>create_offspring(parents_list, var_op_list, rng, add_to_population=True, keep_repeats=False, mutate_until_unique=True, n_jobs=1)</code>","text":"<p>parents_list: a list of lists of parents. var_op_list: a list of var_ops to apply to each list of parents. Should be the same length as parents_list.</p> <p>for example: parents_list = [[parent1, parent2], [parent3]] var_op_list = [\"crossover\", \"mutate\"]</p> <p>This will apply crossover to parent1 and parent2 and mutate to parent3.</p> <p>Creates offspring from parents using the var_op_list. If string, will use a built in method     - \"crossover\" : crossover     - \"mutate\" : mutate     - \"mutate_and_crossover\" : mutate_and_crossover     - \"cross_and_mutate\" : cross_and_mutate</p> Source code in <code>tpot/population.py</code> <pre><code>def create_offspring(self, parents_list, var_op_list, rng, add_to_population=True, keep_repeats=False, mutate_until_unique=True, n_jobs=1):\n    '''\n    parents_list: a list of lists of parents.\n    var_op_list: a list of var_ops to apply to each list of parents. Should be the same length as parents_list.\n\n    for example:\n    parents_list = [[parent1, parent2], [parent3]]\n    var_op_list = [\"crossover\", \"mutate\"]\n\n    This will apply crossover to parent1 and parent2 and mutate to parent3.\n\n    Creates offspring from parents using the var_op_list.\n    If string, will use a built in method\n        - \"crossover\" : crossover\n        - \"mutate\" : mutate\n        - \"mutate_and_crossover\" : mutate_and_crossover\n        - \"cross_and_mutate\" : cross_and_mutate\n    '''\n    rng = np.random.default_rng(rng)\n    new_offspring = []\n    all_offspring = parallel_create_offspring(parents_list, var_op_list, rng=rng, n_jobs=n_jobs)\n\n    for parents, offspring, var_op in zip(parents_list, all_offspring, var_op_list):\n\n        # if var_op in built_in_var_ops_dict:\n        #     var_op = built_in_var_ops_dict[var_op]\n\n        # offspring = copy.deepcopy(parents)\n        # offspring = var_op(offspring)\n        # if isinstance(offspring, collections.abc.Iterable):\n        #     offspring = offspring[0]\n\n        if add_to_population:\n            added = self.add_to_population(offspring, rng=rng, keep_repeats=keep_repeats, mutate_until_unique=mutate_until_unique)\n            if len(added) &gt; 0:\n                for new_child in added:\n                    parent_keys = [parent.unique_id() for parent in parents]\n                    if not pd.api.types.is_object_dtype(self.evaluated_individuals[\"Parents\"]): #TODO Is there a cleaner way of doing this? Not required for some python environments?\n                        self.evaluated_individuals[\"Parents\"] = self.evaluated_individuals[\"Parents\"].astype('object')\n                    if not pd.api.types.is_object_dtype(self.evaluated_individuals[\"Variation_Function\"]):#TODO Is there a cleaner way of doing this? Not required for some python environments?\n                        self.evaluated_individuals[\"Variation_Function\"] = self.evaluated_individuals[\"Variation_Function\"].astype('object')\n                    self.evaluated_individuals.at[new_child.unique_id(),\"Parents\"] = tuple(parent_keys)\n\n                    #if var_op is a function\n                    if hasattr(var_op, '__call__'):\n                        self.evaluated_individuals.at[new_child.unique_id(),\"Variation_Function\"] = var_op.__name__\n                    else:\n                        self.evaluated_individuals.at[new_child.unique_id(),\"Variation_Function\"] = str(var_op)\n\n\n                    new_offspring.append(new_child)\n\n        else:\n            new_offspring.append(offspring)\n\n\n    return new_offspring\n</code></pre>"},{"location":"documentation/tpot/population/#tpot.population.Population.get_column","title":"<code>get_column(individual, column_names=None, to_numpy=True)</code>","text":"<p>Update the column_name column in the evaluated_individuals with the data. If the data is a list, it must be the same length as the evaluated_individuals. If the data is a single value, it will be applied to all individuals in the evaluated_individuals.</p> Source code in <code>tpot/population.py</code> <pre><code>def get_column(self, individual, column_names=None, to_numpy=True):\n    '''\n    Update the column_name column in the evaluated_individuals with the data.\n    If the data is a list, it must be the same length as the evaluated_individuals.\n    If the data is a single value, it will be applied to all individuals in the evaluated_individuals.\n    '''\n    if isinstance(individual, collections.abc.Iterable):\n        if self.use_unique_id:\n            key = [ind.unique_id() for ind in individual]\n        else:\n            key = individual\n    else:\n        if self.use_unique_id:\n            key = individual.unique_id()\n        else:\n            key = individual\n\n    if column_names is not None:\n        slice = self.evaluated_individuals.loc[key,column_names]\n    else:\n        slice = self.evaluated_individuals.loc[key]\n    if to_numpy:\n        slice.reset_index(drop=True, inplace=True)\n        return slice.to_numpy()\n    else:\n        return slice\n</code></pre>"},{"location":"documentation/tpot/population/#tpot.population.Population.remove_invalid_from_population","title":"<code>remove_invalid_from_population(column_names, invalid_value='INVALID')</code>","text":"<p>Remove individuals from the live population if either do not have a value in the column_name column or if the value contains np.nan.</p> <p>Parameters:</p> Name Type Description Default <code>column_name</code> <code>str</code> <p>The name of the column to check for np.nan values.</p> <code>str</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>tpot/population.py</code> <pre><code>def remove_invalid_from_population(self, column_names, invalid_value = \"INVALID\"):\n    '''\n    Remove individuals from the live population if either do not have a value in the column_name column or if the value contains np.nan.\n\n    Parameters\n    ----------\n    column_name : {str}\n        The name of the column to check for np.nan values.\n\n    Returns\n    -------\n    None\n    '''\n    if isinstance(column_names, str): #TODO check this\n        column_names = [column_names]\n    is_valid = lambda ind: ind.unique_id() not in self.evaluated_individuals.index or invalid_value not in self.evaluated_individuals.loc[ind.unique_id(),column_names].to_list()\n    self.population = [ind for ind in self.population if is_valid(ind)]\n</code></pre>"},{"location":"documentation/tpot/population/#tpot.population.Population.set_population","title":"<code>set_population(new_population, rng, keep_repeats=True)</code>","text":"<p>sets population to new population for selection?</p> Source code in <code>tpot/population.py</code> <pre><code>def set_population(self,  new_population, rng, keep_repeats=True):\n    '''\n    sets population to new population\n    for selection?\n    '''\n    rng = np.random.default_rng(rng)\n    self.population = []\n    self.add_to_population(new_population, rng=rng, keep_repeats=keep_repeats)\n</code></pre>"},{"location":"documentation/tpot/population/#tpot.population.Population.update_column","title":"<code>update_column(individual, column_names, data)</code>","text":"<p>Update the column_name column in the evaluated_individuals with the data. If the data is a list, it must be the same length as the evaluated_individuals. If the data is a single value, it will be applied to all individuals in the evaluated_individuals.</p> Source code in <code>tpot/population.py</code> <pre><code>def update_column(self, individual, column_names, data):\n    '''\n    Update the column_name column in the evaluated_individuals with the data.\n    If the data is a list, it must be the same length as the evaluated_individuals.\n    If the data is a single value, it will be applied to all individuals in the evaluated_individuals.\n    '''\n    if isinstance(individual, collections.abc.Iterable):\n        if self.use_unique_id:\n            key = [ind.unique_id() for ind in individual]\n        else:\n            key = individual\n    else:\n        if self.use_unique_id:\n            key = individual.unique_id()\n        else:\n            key = individual\n\n    self.evaluated_individuals.loc[key,column_names] = data\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/","title":"Arithmetictransformer","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.AddTransformer","title":"<code>AddTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>class AddTransformer(BaseEstimator,TransformerMixin):\n    def __init__(self):\n          \"\"\"\n          A transformer that adds all elements along axis 1.\n          \"\"\"\n          pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        transformed_X = np.array(self.transform_helper(np.array(X)))\n        if transformed_X.dtype != float:\n            transformed_X = transformed_X.astype(float)\n\n        return transformed_X\n\n    def transform_helper(self, X):\n        X = np.array(X)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X,0)\n        return np.expand_dims(np.sum(X,1),1)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.AddTransformer.__init__","title":"<code>__init__()</code>","text":"<p>A transformer that adds all elements along axis 1.</p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>def __init__(self):\n      \"\"\"\n      A transformer that adds all elements along axis 1.\n      \"\"\"\n      pass\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.ArithmeticTransformer","title":"<code>ArithmeticTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>class ArithmeticTransformer(BaseEstimator,TransformerMixin):\n\n    #functions = [\"add\", \"mul_neg_1\", \"mul\", \"safe_reciprocal\", \"eq\",\"ne\",\"ge\",\"gt\",\"le\",\"lt\", \"min\",\"max\",\"0\",\"1\"]\n    def __init__(self, function,):\n        \"\"\"\n        A transformer that applies a function to the input array along axis 1.\n        Parameters\n        ----------\n\n        function : str\n            The function to apply to the input array. The following functions are supported:\n            - 'add' : Add all elements along axis 1\n            - 'mul_neg_1' : Multiply all elements along axis 1 by -1\n            - 'mul' : Multiply all elements along axis 1\n            - 'safe_reciprocal' : Take the reciprocal of all elements along axis 1, with a safe division by zero\n            - 'eq' : Check if all elements along axis 1 are equal\n            - 'ne' : Check if all elements along axis 1 are not equal\n            - 'ge' : Check if all elements along axis 1 are greater than or equal to 0\n            - 'gt' : Check if all elements along axis 1 are greater than 0\n            - 'le' : Check if all elements along axis 1 are less than or equal to 0\n            - 'lt' : Check if all elements along axis 1 are less than 0\n            - 'min' : Take the minimum of all elements along axis 1\n            - 'max' : Take the maximum of all elements along axis 1\n            - '0' : Return an array of zeros\n            - '1' : Return an array of ones\n        \"\"\"\n        self.function = function\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        transformed_X = np.array(self.transform_helper(np.array(X)))\n        if transformed_X.dtype != float:\n            transformed_X = transformed_X.astype(float)\n\n        return transformed_X\n\n    def transform_helper(self, X):\n        X = np.array(X)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X,0)\n        if self.function == \"add\":\n                return np.expand_dims(np.sum(X,1),1)\n        elif self.function == \"mul_neg_1\":\n                return X*-1\n        elif self.function == \"mul\":\n                return np.expand_dims(np.prod(X,1),1)\n\n        elif self.function == \"safe_reciprocal\":\n                results = np.divide(1.0, X.astype(float), out=np.zeros_like(X).astype(float), where=X!=0) #TODO remove astypefloat?\n                return results\n\n        elif self.function == \"eq\":\n                return np.expand_dims(np.all(X == X[0,:], axis = 1),1).astype(float)\n\n        elif self.function == \"ne\":\n                return 1- np.expand_dims(np.all(X == X[0,:], axis = 1),1).astype(float)\n\n        #TODO these could be \"sorted order\"\n        elif self.function == \"ge\":\n                result = X &gt;= 0\n                return  result.astype(float)\n\n        elif self.function == \"gt\":\n                result = X &gt; 0\n                return  result.astype(float)\n        elif self.function ==  \"le\":\n                result = X &lt;= 0\n                return  result.astype(float)\n        elif self.function ==  \"lt\":\n                result = X &lt; 0\n                return  result.astype(float)\n\n\n        elif self.function ==   \"min\":\n                return np.expand_dims(np.amin(X,1),1)\n        elif self.function ==  \"max\":\n                return np.expand_dims(np.amax(X,1),1)\n\n        elif self.function ==  \"0\":\n                return np.zeros((X.shape[0],1))\n        elif self.function ==  \"1\":\n                return np.ones((X.shape[0],1))\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.ArithmeticTransformer.__init__","title":"<code>__init__(function)</code>","text":"<p>A transformer that applies a function to the input array along axis 1.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>str</code> <p>The function to apply to the input array. The following functions are supported: - 'add' : Add all elements along axis 1 - 'mul_neg_1' : Multiply all elements along axis 1 by -1 - 'mul' : Multiply all elements along axis 1 - 'safe_reciprocal' : Take the reciprocal of all elements along axis 1, with a safe division by zero - 'eq' : Check if all elements along axis 1 are equal - 'ne' : Check if all elements along axis 1 are not equal - 'ge' : Check if all elements along axis 1 are greater than or equal to 0 - 'gt' : Check if all elements along axis 1 are greater than 0 - 'le' : Check if all elements along axis 1 are less than or equal to 0 - 'lt' : Check if all elements along axis 1 are less than 0 - 'min' : Take the minimum of all elements along axis 1 - 'max' : Take the maximum of all elements along axis 1 - '0' : Return an array of zeros - '1' : Return an array of ones</p> required Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>def __init__(self, function,):\n    \"\"\"\n    A transformer that applies a function to the input array along axis 1.\n    Parameters\n    ----------\n\n    function : str\n        The function to apply to the input array. The following functions are supported:\n        - 'add' : Add all elements along axis 1\n        - 'mul_neg_1' : Multiply all elements along axis 1 by -1\n        - 'mul' : Multiply all elements along axis 1\n        - 'safe_reciprocal' : Take the reciprocal of all elements along axis 1, with a safe division by zero\n        - 'eq' : Check if all elements along axis 1 are equal\n        - 'ne' : Check if all elements along axis 1 are not equal\n        - 'ge' : Check if all elements along axis 1 are greater than or equal to 0\n        - 'gt' : Check if all elements along axis 1 are greater than 0\n        - 'le' : Check if all elements along axis 1 are less than or equal to 0\n        - 'lt' : Check if all elements along axis 1 are less than 0\n        - 'min' : Take the minimum of all elements along axis 1\n        - 'max' : Take the maximum of all elements along axis 1\n        - '0' : Return an array of zeros\n        - '1' : Return an array of ones\n    \"\"\"\n    self.function = function\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.EQTransformer","title":"<code>EQTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>class EQTransformer(BaseEstimator,TransformerMixin):\n\n    def __init__(self):\n        \"\"\"\n        A transformer that takes checks if all elements in a row are equal.\n        \"\"\"\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        transformed_X = np.array(self.transform_helper(np.array(X)))\n        if transformed_X.dtype != float:\n            transformed_X = transformed_X.astype(float)\n\n        return transformed_X\n\n    def transform_helper(self, X):\n        X = np.array(X)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X,0)\n        return np.expand_dims(np.all(X == X[0,:], axis = 1),1).astype(float)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.EQTransformer.__init__","title":"<code>__init__()</code>","text":"<p>A transformer that takes checks if all elements in a row are equal.</p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    A transformer that takes checks if all elements in a row are equal.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.GETransformer","title":"<code>GETransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>class GETransformer(BaseEstimator,TransformerMixin):\n\n    def __init__(self):\n        \"\"\"\n        A transformer that takes checks if all elements in a row are greater than or equal to 0.\n        \"\"\"\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        transformed_X = np.array(self.transform_helper(np.array(X)))\n        if transformed_X.dtype != float:\n            transformed_X = transformed_X.astype(float)\n\n        return transformed_X\n\n    def transform_helper(self, X):\n        X = np.array(X)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X,0)\n        result = X &gt;= 0\n        return  result.astype(float)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.GETransformer.__init__","title":"<code>__init__()</code>","text":"<p>A transformer that takes checks if all elements in a row are greater than or equal to 0.</p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    A transformer that takes checks if all elements in a row are greater than or equal to 0.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.GTTransformer","title":"<code>GTTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>class GTTransformer(BaseEstimator,TransformerMixin):\n    def __init__(self):\n          \"\"\"\n          A transformer that takes checks if all elements in a row are greater than 0.\n          \"\"\"\n          pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        transformed_X = np.array(self.transform_helper(np.array(X)))\n        if transformed_X.dtype != float:\n            transformed_X = transformed_X.astype(float)\n\n        return transformed_X\n\n    def transform_helper(self, X):\n        X = np.array(X)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X,0)\n        result = X &gt; 0\n        return  result.astype(float)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.GTTransformer.__init__","title":"<code>__init__()</code>","text":"<p>A transformer that takes checks if all elements in a row are greater than 0.</p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>def __init__(self):\n      \"\"\"\n      A transformer that takes checks if all elements in a row are greater than 0.\n      \"\"\"\n      pass\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.LETransformer","title":"<code>LETransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>class LETransformer(BaseEstimator,TransformerMixin):\n    def __init__(self):\n        \"\"\"\n        A transformer that takes checks if all elements in a row are less than or equal to 0.\n        \"\"\"\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        transformed_X = np.array(self.transform_helper(np.array(X)))\n        if transformed_X.dtype != float:\n            transformed_X = transformed_X.astype(float)\n\n        return transformed_X\n\n    def transform_helper(self, X):\n        X = np.array(X)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X,0)\n        result = X &lt;= 0\n        return  result.astype(float)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.LETransformer.__init__","title":"<code>__init__()</code>","text":"<p>A transformer that takes checks if all elements in a row are less than or equal to 0.</p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    A transformer that takes checks if all elements in a row are less than or equal to 0.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.LTTransformer","title":"<code>LTTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>class LTTransformer(BaseEstimator,TransformerMixin):\n    def __init__(self):\n        \"\"\"\n        A transformer that takes checks if all elements in a row are less than 0.\n        \"\"\"\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        transformed_X = np.array(self.transform_helper(np.array(X)))\n        if transformed_X.dtype != float:\n            transformed_X = transformed_X.astype(float)\n\n        return transformed_X\n\n    def transform_helper(self, X):\n        X = np.array(X)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X,0)\n        result = X &lt; 0\n        return  result.astype(float)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.LTTransformer.__init__","title":"<code>__init__()</code>","text":"<p>A transformer that takes checks if all elements in a row are less than 0.</p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    A transformer that takes checks if all elements in a row are less than 0.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.MaxTransformer","title":"<code>MaxTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>class MaxTransformer(BaseEstimator,TransformerMixin):\n\n    def __init__(self):\n          \"\"\"\n          A transformer that takes the maximum of all elements in a row.\n          \"\"\"\n          pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        transformed_X = np.array(self.transform_helper(np.array(X)))\n        if transformed_X.dtype != float:\n            transformed_X = transformed_X.astype(float)\n\n        return transformed_X\n\n    def transform_helper(self, X):\n        X = np.array(X)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X,0)\n        return np.expand_dims(np.amax(X,1),1)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.MaxTransformer.__init__","title":"<code>__init__()</code>","text":"<p>A transformer that takes the maximum of all elements in a row.</p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>def __init__(self):\n      \"\"\"\n      A transformer that takes the maximum of all elements in a row.\n      \"\"\"\n      pass\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.MinTransformer","title":"<code>MinTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>class MinTransformer(BaseEstimator,TransformerMixin):\n    def __init__(self):\n        \"\"\"\n        A transformer that takes the minimum of all elements in a row.\n        \"\"\"\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        transformed_X = np.array(self.transform_helper(np.array(X)))\n        if transformed_X.dtype != float:\n            transformed_X = transformed_X.astype(float)\n\n        return transformed_X\n\n    def transform_helper(self, X):\n        X = np.array(X)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X,0)\n        return np.expand_dims(np.amin(X,1),1)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.MinTransformer.__init__","title":"<code>__init__()</code>","text":"<p>A transformer that takes the minimum of all elements in a row.</p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    A transformer that takes the minimum of all elements in a row.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.MulTransformer","title":"<code>MulTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>class MulTransformer(BaseEstimator,TransformerMixin):\n\n    def __init__(self):\n        \"\"\"\n        A transformer that multiplies all elements along axis 1.\n        \"\"\"\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        transformed_X = np.array(self.transform_helper(np.array(X)))\n        if transformed_X.dtype != float:\n            transformed_X = transformed_X.astype(float)\n\n        return transformed_X\n\n    def transform_helper(self, X):\n        X = np.array(X)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X,0)\n        return np.expand_dims(np.prod(X,1),1)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.MulTransformer.__init__","title":"<code>__init__()</code>","text":"<p>A transformer that multiplies all elements along axis 1.</p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    A transformer that multiplies all elements along axis 1.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.NETransformer","title":"<code>NETransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>class NETransformer(BaseEstimator,TransformerMixin):\n\n    def __init__(self):\n        \"\"\"\n        A transformer that takes checks if all elements in a row are not equal.\n        \"\"\"  \n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        transformed_X = np.array(self.transform_helper(np.array(X)))\n        if transformed_X.dtype != float:\n            transformed_X = transformed_X.astype(float)\n\n        return transformed_X\n\n    def transform_helper(self, X):\n        X = np.array(X)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X,0)\n        return 1- np.expand_dims(np.all(X == X[0,:], axis = 1),1).astype(float)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.NETransformer.__init__","title":"<code>__init__()</code>","text":"<p>A transformer that takes checks if all elements in a row are not equal.</p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    A transformer that takes checks if all elements in a row are not equal.\n    \"\"\"  \n    pass\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.NTransformer","title":"<code>NTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>class NTransformer(BaseEstimator,TransformerMixin):\n\n    def __init__(self, n):\n        \"\"\"\n        A transformer that returns an array of n.\n        \"\"\"\n        self.n = n\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        transformed_X = np.array(self.transform_helper(np.array(X)))\n        if transformed_X.dtype != float:\n            transformed_X = transformed_X.astype(float)\n\n        return transformed_X\n\n    def transform_helper(self, X):\n        X = np.array(X)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X,0)\n        return np.ones((X.shape[0],1))*self.n\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.NTransformer.__init__","title":"<code>__init__(n)</code>","text":"<p>A transformer that returns an array of n.</p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>def __init__(self, n):\n    \"\"\"\n    A transformer that returns an array of n.\n    \"\"\"\n    self.n = n\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.OneTransformer","title":"<code>OneTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>class OneTransformer(BaseEstimator,TransformerMixin):\n    def __init__(self):\n          \"\"\"\n          A transformer that returns an array of ones.\n          \"\"\"\n          pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        transformed_X = np.array(self.transform_helper(np.array(X)))\n        if transformed_X.dtype != float:\n            transformed_X = transformed_X.astype(float)\n\n        return transformed_X\n\n    def transform_helper(self, X):\n        X = np.array(X)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X,0)\n        return np.ones((X.shape[0],1))\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.OneTransformer.__init__","title":"<code>__init__()</code>","text":"<p>A transformer that returns an array of ones.</p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>def __init__(self):\n      \"\"\"\n      A transformer that returns an array of ones.\n      \"\"\"\n      pass\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.SafeReciprocalTransformer","title":"<code>SafeReciprocalTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>class SafeReciprocalTransformer(BaseEstimator,TransformerMixin):\n\n    def __init__(self):\n        \"\"\"\n        A transformer that takes the reciprocal of all elements, with a safe division by zero.\n        \"\"\"\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        transformed_X = np.array(self.transform_helper(np.array(X)))\n        if transformed_X.dtype != float:\n            transformed_X = transformed_X.astype(float)\n\n        return transformed_X\n\n    def transform_helper(self, X):\n        X = np.array(X)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X,0)\n        return np.divide(1.0, X.astype(float), out=np.zeros_like(X).astype(float), where=X!=0) #TODO remove astypefloat?\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.SafeReciprocalTransformer.__init__","title":"<code>__init__()</code>","text":"<p>A transformer that takes the reciprocal of all elements, with a safe division by zero.</p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    A transformer that takes the reciprocal of all elements, with a safe division by zero.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.ZeroTransformer","title":"<code>ZeroTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>class ZeroTransformer(BaseEstimator,TransformerMixin):\n\n    def __init__(self):\n          \"\"\"\n        A transformer that returns an array of zeros.\n          \"\"\"\n          pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        transformed_X = np.array(self.transform_helper(np.array(X)))\n        if transformed_X.dtype != float:\n            transformed_X = transformed_X.astype(float)\n\n        return transformed_X\n\n    def transform_helper(self, X):\n        X = np.array(X)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X,0)\n        return np.zeros((X.shape[0],1))\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.ZeroTransformer.__init__","title":"<code>__init__()</code>","text":"<p>A transformer that returns an array of zeros.</p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>def __init__(self):\n      \"\"\"\n    A transformer that returns an array of zeros.\n      \"\"\"\n      pass\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.mul_neg_1_Transformer","title":"<code>mul_neg_1_Transformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>class mul_neg_1_Transformer(BaseEstimator,TransformerMixin):\n    def __init__(self):\n        \"\"\"\n        A transformer that multiplies all elements by -1.\n        \"\"\"\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        transformed_X = np.array(self.transform_helper(np.array(X)))\n        if transformed_X.dtype != float:\n            transformed_X = transformed_X.astype(float)\n\n        return transformed_X\n\n    def transform_helper(self, X):\n        X = np.array(X)\n        if len(X.shape) == 1:\n            X = np.expand_dims(X,0)\n        return X*-1\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/arithmetictransformer/#tpot.builtin_modules.arithmetictransformer.mul_neg_1_Transformer.__init__","title":"<code>__init__()</code>","text":"<p>A transformer that multiplies all elements by -1.</p> Source code in <code>tpot/builtin_modules/arithmetictransformer.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    A transformer that multiplies all elements by -1.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/column_one_hot_encoder/","title":"Column one hot encoder","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/builtin_modules/column_one_hot_encoder/#tpot.builtin_modules.column_one_hot_encoder.ColumnOneHotEncoder","title":"<code>ColumnOneHotEncoder</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/column_one_hot_encoder.py</code> <pre><code>class ColumnOneHotEncoder(BaseEstimator, TransformerMixin):\n\n\n    def __init__(self, columns='auto', drop=None, handle_unknown='infrequent_if_exist', sparse_output=False, min_frequency=None,max_categories=None):\n        '''\n        A wrapper for OneHotEncoder that allows for onehot encoding of specific columns in a DataFrame or np array.\n\n        Parameters\n        ----------\n\n        columns : str, list, default='auto'\n            Determines which columns to onehot encode with sklearn.preprocessing.OneHotEncoder.\n            - 'auto' : Automatically select categorical features based on columns with less than 10 unique values\n            - 'categorical' : Automatically select categorical features\n            - 'numeric' : Automatically select numeric features\n            - 'all' : Select all features\n            - list : A list of columns to select\n\n        drop, handle_unknown, sparse_output, min_frequency, max_categories : see sklearn.preprocessing.OneHotEncoder\n\n        '''\n\n        self.columns = columns\n        self.drop = drop\n        self.handle_unknown = handle_unknown\n        self.sparse_output = sparse_output\n        self.min_frequency = min_frequency\n        self.max_categories = max_categories\n\n\n\n    def fit(self, X, y=None):\n        \"\"\"Fit OneHotEncoder to X, then transform X.\n\n        Equivalent to self.fit(X).transform(X), but more convenient and more\n        efficient. See fit for the parameters, transform for the return value.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape=(n_samples, n_features)\n            Dense array or sparse matrix.\n        y: array-like {n_samples,} (Optional, ignored)\n            Feature labels\n        \"\"\"\n\n        if (self.columns == \"categorical\" or self.columns == \"numeric\") and not isinstance(X, pd.DataFrame):\n            raise ValueError(f\"Invalid value for columns: {self.columns}. \"\n                             \"Only 'all' or &lt;list&gt; is supported for np arrays\")\n\n        if self.columns == \"categorical\":\n            self.columns_ = list(X.select_dtypes(exclude='number').columns)\n        elif self.columns == \"numeric\":\n            self.columns_ =  [col for col in X.columns if is_numeric_dtype(X[col])]\n        elif self.columns == \"auto\":\n            self.columns_ = auto_select_categorical_features(X)\n        elif self.columns == \"all\":\n            if isinstance(X, pd.DataFrame):\n                self.columns_ = X.columns\n            else:\n                self.columns_ = list(range(X.shape[1]))\n        elif isinstance(self.columns, list):\n            self.columns_ = self.columns\n        else:\n            raise ValueError(f\"Invalid value for columns: {self.columns}\")\n\n\n\n        if len(self.columns_) == 0:\n            return self\n\n        self.enc = sklearn.preprocessing.OneHotEncoder( categories='auto',   \n                                                        drop = self.drop,\n                                                        handle_unknown = self.handle_unknown,\n                                                        sparse_output = self.sparse_output,\n                                                        min_frequency = self.min_frequency,\n                                                        max_categories = self.max_categories)\n\n        #TODO make this more consistent with sklearn baseimputer/baseencoder\n        if isinstance(X, pd.DataFrame):\n            self.enc.set_output(transform=\"pandas\")\n            for col in X.columns:\n                # check if the column name is not a string\n                if not isinstance(col, str):\n                    # if it's not a string, rename the column with \"X\" prefix\n                    X.rename(columns={col: f\"X{col}\"}, inplace=True)\n\n\n        if len(self.columns_) == X.shape[1]:\n            X_sel = self.enc.fit(X)\n        else:\n            X_sel, X_not_sel = _X_selected(X, self.columns_)\n            X_sel = self.enc.fit(X_sel)\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Transform X using one-hot encoding.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape=(n_samples, n_features)\n            Dense array or sparse matrix.\n\n        Returns\n        -------\n        X_out : sparse matrix if sparse=True else a 2-d array, dtype=int\n            Transformed input.\n        \"\"\"\n\n\n        if len(self.columns_) == 0:\n            return X\n\n        #TODO make this more consistent with sklearn baseimputer/baseencoder\n        if isinstance(X, pd.DataFrame):\n            for col in X.columns:\n                # check if the column name is not a string\n                if not isinstance(col, str):\n                    # if it's not a string, rename the column with \"X\" prefix\n                    X.rename(columns={col: f\"X{col}\"}, inplace=True)\n\n        if len(self.columns_) == X.shape[1]:\n            return self.enc.transform(X)\n        else:\n\n            X_sel, X_not_sel= _X_selected(X, self.columns_)\n            X_sel = self.enc.transform(X_sel)\n\n            #If X is dataframe\n            if isinstance(X, pd.DataFrame):\n\n                X_sel = pd.DataFrame(X_sel, columns=self.enc.get_feature_names_out())\n                return pd.concat([X_not_sel.reset_index(drop=True), X_sel.reset_index(drop=True)], axis=1)\n            else:\n                return np.hstack((X_not_sel, X_sel))\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/column_one_hot_encoder/#tpot.builtin_modules.column_one_hot_encoder.ColumnOneHotEncoder.__init__","title":"<code>__init__(columns='auto', drop=None, handle_unknown='infrequent_if_exist', sparse_output=False, min_frequency=None, max_categories=None)</code>","text":"<p>A wrapper for OneHotEncoder that allows for onehot encoding of specific columns in a DataFrame or np array.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>(str, list)</code> <p>Determines which columns to onehot encode with sklearn.preprocessing.OneHotEncoder. - 'auto' : Automatically select categorical features based on columns with less than 10 unique values - 'categorical' : Automatically select categorical features - 'numeric' : Automatically select numeric features - 'all' : Select all features - list : A list of columns to select</p> <code>'auto'</code> <code>drop</code> <code>see sklearn.preprocessing.OneHotEncoder</code> <code>None</code> <code>handle_unknown</code> <code>see sklearn.preprocessing.OneHotEncoder</code> <code>None</code> <code>sparse_output</code> <code>see sklearn.preprocessing.OneHotEncoder</code> <code>None</code> <code>min_frequency</code> <code>see sklearn.preprocessing.OneHotEncoder</code> <code>None</code> <code>max_categories</code> <code>see sklearn.preprocessing.OneHotEncoder</code> <code>None</code> Source code in <code>tpot/builtin_modules/column_one_hot_encoder.py</code> <pre><code>def __init__(self, columns='auto', drop=None, handle_unknown='infrequent_if_exist', sparse_output=False, min_frequency=None,max_categories=None):\n    '''\n    A wrapper for OneHotEncoder that allows for onehot encoding of specific columns in a DataFrame or np array.\n\n    Parameters\n    ----------\n\n    columns : str, list, default='auto'\n        Determines which columns to onehot encode with sklearn.preprocessing.OneHotEncoder.\n        - 'auto' : Automatically select categorical features based on columns with less than 10 unique values\n        - 'categorical' : Automatically select categorical features\n        - 'numeric' : Automatically select numeric features\n        - 'all' : Select all features\n        - list : A list of columns to select\n\n    drop, handle_unknown, sparse_output, min_frequency, max_categories : see sklearn.preprocessing.OneHotEncoder\n\n    '''\n\n    self.columns = columns\n    self.drop = drop\n    self.handle_unknown = handle_unknown\n    self.sparse_output = sparse_output\n    self.min_frequency = min_frequency\n    self.max_categories = max_categories\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/column_one_hot_encoder/#tpot.builtin_modules.column_one_hot_encoder.ColumnOneHotEncoder.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit OneHotEncoder to X, then transform X.</p> <p>Equivalent to self.fit(X).transform(X), but more convenient and more efficient. See fit for the parameters, transform for the return value.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like or sparse matrix, shape=(n_samples, n_features)</code> <p>Dense array or sparse matrix.</p> required <code>y</code> <p>Feature labels</p> <code>None</code> Source code in <code>tpot/builtin_modules/column_one_hot_encoder.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Fit OneHotEncoder to X, then transform X.\n\n    Equivalent to self.fit(X).transform(X), but more convenient and more\n    efficient. See fit for the parameters, transform for the return value.\n\n    Parameters\n    ----------\n    X : array-like or sparse matrix, shape=(n_samples, n_features)\n        Dense array or sparse matrix.\n    y: array-like {n_samples,} (Optional, ignored)\n        Feature labels\n    \"\"\"\n\n    if (self.columns == \"categorical\" or self.columns == \"numeric\") and not isinstance(X, pd.DataFrame):\n        raise ValueError(f\"Invalid value for columns: {self.columns}. \"\n                         \"Only 'all' or &lt;list&gt; is supported for np arrays\")\n\n    if self.columns == \"categorical\":\n        self.columns_ = list(X.select_dtypes(exclude='number').columns)\n    elif self.columns == \"numeric\":\n        self.columns_ =  [col for col in X.columns if is_numeric_dtype(X[col])]\n    elif self.columns == \"auto\":\n        self.columns_ = auto_select_categorical_features(X)\n    elif self.columns == \"all\":\n        if isinstance(X, pd.DataFrame):\n            self.columns_ = X.columns\n        else:\n            self.columns_ = list(range(X.shape[1]))\n    elif isinstance(self.columns, list):\n        self.columns_ = self.columns\n    else:\n        raise ValueError(f\"Invalid value for columns: {self.columns}\")\n\n\n\n    if len(self.columns_) == 0:\n        return self\n\n    self.enc = sklearn.preprocessing.OneHotEncoder( categories='auto',   \n                                                    drop = self.drop,\n                                                    handle_unknown = self.handle_unknown,\n                                                    sparse_output = self.sparse_output,\n                                                    min_frequency = self.min_frequency,\n                                                    max_categories = self.max_categories)\n\n    #TODO make this more consistent with sklearn baseimputer/baseencoder\n    if isinstance(X, pd.DataFrame):\n        self.enc.set_output(transform=\"pandas\")\n        for col in X.columns:\n            # check if the column name is not a string\n            if not isinstance(col, str):\n                # if it's not a string, rename the column with \"X\" prefix\n                X.rename(columns={col: f\"X{col}\"}, inplace=True)\n\n\n    if len(self.columns_) == X.shape[1]:\n        X_sel = self.enc.fit(X)\n    else:\n        X_sel, X_not_sel = _X_selected(X, self.columns_)\n        X_sel = self.enc.fit(X_sel)\n\n    return self\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/column_one_hot_encoder/#tpot.builtin_modules.column_one_hot_encoder.ColumnOneHotEncoder.transform","title":"<code>transform(X)</code>","text":"<p>Transform X using one-hot encoding.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like or sparse matrix, shape=(n_samples, n_features)</code> <p>Dense array or sparse matrix.</p> required <p>Returns:</p> Name Type Description <code>X_out</code> <code>sparse matrix if sparse=True else a 2-d array, dtype=int</code> <p>Transformed input.</p> Source code in <code>tpot/builtin_modules/column_one_hot_encoder.py</code> <pre><code>def transform(self, X):\n    \"\"\"Transform X using one-hot encoding.\n\n    Parameters\n    ----------\n    X : array-like or sparse matrix, shape=(n_samples, n_features)\n        Dense array or sparse matrix.\n\n    Returns\n    -------\n    X_out : sparse matrix if sparse=True else a 2-d array, dtype=int\n        Transformed input.\n    \"\"\"\n\n\n    if len(self.columns_) == 0:\n        return X\n\n    #TODO make this more consistent with sklearn baseimputer/baseencoder\n    if isinstance(X, pd.DataFrame):\n        for col in X.columns:\n            # check if the column name is not a string\n            if not isinstance(col, str):\n                # if it's not a string, rename the column with \"X\" prefix\n                X.rename(columns={col: f\"X{col}\"}, inplace=True)\n\n    if len(self.columns_) == X.shape[1]:\n        return self.enc.transform(X)\n    else:\n\n        X_sel, X_not_sel= _X_selected(X, self.columns_)\n        X_sel = self.enc.transform(X_sel)\n\n        #If X is dataframe\n        if isinstance(X, pd.DataFrame):\n\n            X_sel = pd.DataFrame(X_sel, columns=self.enc.get_feature_names_out())\n            return pd.concat([X_not_sel.reset_index(drop=True), X_sel.reset_index(drop=True)], axis=1)\n        else:\n            return np.hstack((X_not_sel, X_sel))\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/column_one_hot_encoder/#tpot.builtin_modules.column_one_hot_encoder.ColumnOrdinalEncoder","title":"<code>ColumnOrdinalEncoder</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/column_one_hot_encoder.py</code> <pre><code>class ColumnOrdinalEncoder(BaseEstimator, TransformerMixin):\n\n\n    def __init__(self, columns='auto', handle_unknown='error', unknown_value = -1, encoded_missing_value = np.nan, min_frequency=None,max_categories=None):\n        '''\n\n        Parameters\n        ----------\n\n        columns : str, list, default='auto'\n            Determines which columns to onehot encode with sklearn.preprocessing.OneHotEncoder.\n            - 'auto' : Automatically select categorical features based on columns with less than 10 unique values\n            - 'categorical' : Automatically select categorical features\n            - 'numeric' : Automatically select numeric features\n            - 'all' : Select all features\n            - list : A list of columns to select\n\n        drop, handle_unknown, sparse_output, min_frequency, max_categories : see sklearn.preprocessing.OneHotEncoder\n\n        '''\n\n        self.columns = columns\n        self.handle_unknown = handle_unknown\n        self.unknown_value = unknown_value\n        self.encoded_missing_value = encoded_missing_value\n        self.min_frequency = min_frequency\n        self.max_categories = max_categories\n\n\n\n    def fit(self, X, y=None):\n        \"\"\"Fit OneHotEncoder to X, then transform X.\n\n        Equivalent to self.fit(X).transform(X), but more convenient and more\n        efficient. See fit for the parameters, transform for the return value.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape=(n_samples, n_features)\n            Dense array or sparse matrix.\n        y: array-like {n_samples,} (Optional, ignored)\n            Feature labels\n        \"\"\"\n\n        if (self.columns == \"categorical\" or self.columns == \"numeric\") and not isinstance(X, pd.DataFrame):\n            raise ValueError(f\"Invalid value for columns: {self.columns}. \"\n                             \"Only 'all' or &lt;list&gt; is supported for np arrays\")\n\n        if self.columns == \"categorical\":\n            self.columns_ = list(X.select_dtypes(exclude='number').columns)\n        elif self.columns == \"numeric\":\n            self.columns_ =  [col for col in X.columns if is_numeric_dtype(X[col])]\n        elif self.columns == \"auto\":\n            self.columns_ = auto_select_categorical_features(X)\n        elif self.columns == \"all\":\n            if isinstance(X, pd.DataFrame):\n                self.columns_ = X.columns\n            else:\n                self.columns_ = list(range(X.shape[1]))\n        elif isinstance(self.columns, list):\n            self.columns_ = self.columns\n        else:\n            raise ValueError(f\"Invalid value for columns: {self.columns}\")\n\n        if len(self.columns_) == 0:\n            return self\n\n        self.enc = sklearn.preprocessing.OrdinalEncoder(categories='auto',   \n                                                        handle_unknown = self.handle_unknown,\n                                                        unknown_value = self.unknown_value, \n                                                        encoded_missing_value = self.encoded_missing_value,\n                                                        min_frequency = self.min_frequency,\n                                                        max_categories = self.max_categories)\n        #TODO make this more consistent with sklearn baseimputer/baseencoder\n        '''\n        if isinstance(X, pd.DataFrame):\n            self.enc.set_output(transform=\"pandas\")\n            for col in X.columns:\n                # check if the column name is not a string\n                if not isinstance(col, str):\n                    # if it's not a string, rename the column with \"X\" prefix\n                    X.rename(columns={col: f\"X{col}\"}, inplace=True)\n        '''\n\n        if len(self.columns_) == X.shape[1]:\n            X_sel = self.enc.fit(X)\n        else:\n            X_sel, X_not_sel = _X_selected(X, self.columns_)\n            X_sel = self.enc.fit(X_sel)\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Transform X using one-hot encoding.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape=(n_samples, n_features)\n            Dense array or sparse matrix.\n\n        Returns\n        -------\n        X_out : sparse matrix if sparse=True else a 2-d array, dtype=int\n            Transformed input.\n        \"\"\"\n\n\n        if len(self.columns_) == 0:\n            return X\n\n        #TODO make this more consistent with sklearn baseimputer/baseencoder\n        '''\n        if isinstance(X, pd.DataFrame):\n            for col in X.columns:\n                # check if the column name is not a string\n                if not isinstance(col, str):\n                    # if it's not a string, rename the column with \"X\" prefix\n                    X.rename(columns={col: f\"X{col}\"}, inplace=True)\n        '''\n\n        if len(self.columns_) == X.shape[1]:\n            return self.enc.transform(X)\n        else:\n\n            X_sel, X_not_sel= _X_selected(X, self.columns_)\n            X_sel = self.enc.transform(X_sel)\n\n            #If X is dataframe\n            if isinstance(X, pd.DataFrame):\n\n                X_sel = pd.DataFrame(X_sel, columns=self.enc.get_feature_names_out())\n                return pd.concat([X_not_sel.reset_index(drop=True), X_sel.reset_index(drop=True)], axis=1)\n            else:\n                return np.hstack((X_not_sel, X_sel))\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/column_one_hot_encoder/#tpot.builtin_modules.column_one_hot_encoder.ColumnOrdinalEncoder.__init__","title":"<code>__init__(columns='auto', handle_unknown='error', unknown_value=-1, encoded_missing_value=np.nan, min_frequency=None, max_categories=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>columns</code> <code>(str, list)</code> <p>Determines which columns to onehot encode with sklearn.preprocessing.OneHotEncoder. - 'auto' : Automatically select categorical features based on columns with less than 10 unique values - 'categorical' : Automatically select categorical features - 'numeric' : Automatically select numeric features - 'all' : Select all features - list : A list of columns to select</p> <code>'auto'</code> <code>drop</code> <code>see sklearn.preprocessing.OneHotEncoder</code> <code>'error'</code> <code>handle_unknown</code> <code>see sklearn.preprocessing.OneHotEncoder</code> <code>'error'</code> <code>sparse_output</code> <code>see sklearn.preprocessing.OneHotEncoder</code> <code>'error'</code> <code>min_frequency</code> <code>see sklearn.preprocessing.OneHotEncoder</code> <code>'error'</code> <code>max_categories</code> <code>see sklearn.preprocessing.OneHotEncoder</code> <code>'error'</code> Source code in <code>tpot/builtin_modules/column_one_hot_encoder.py</code> <pre><code>def __init__(self, columns='auto', handle_unknown='error', unknown_value = -1, encoded_missing_value = np.nan, min_frequency=None,max_categories=None):\n    '''\n\n    Parameters\n    ----------\n\n    columns : str, list, default='auto'\n        Determines which columns to onehot encode with sklearn.preprocessing.OneHotEncoder.\n        - 'auto' : Automatically select categorical features based on columns with less than 10 unique values\n        - 'categorical' : Automatically select categorical features\n        - 'numeric' : Automatically select numeric features\n        - 'all' : Select all features\n        - list : A list of columns to select\n\n    drop, handle_unknown, sparse_output, min_frequency, max_categories : see sklearn.preprocessing.OneHotEncoder\n\n    '''\n\n    self.columns = columns\n    self.handle_unknown = handle_unknown\n    self.unknown_value = unknown_value\n    self.encoded_missing_value = encoded_missing_value\n    self.min_frequency = min_frequency\n    self.max_categories = max_categories\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/column_one_hot_encoder/#tpot.builtin_modules.column_one_hot_encoder.ColumnOrdinalEncoder.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit OneHotEncoder to X, then transform X.</p> <p>Equivalent to self.fit(X).transform(X), but more convenient and more efficient. See fit for the parameters, transform for the return value.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like or sparse matrix, shape=(n_samples, n_features)</code> <p>Dense array or sparse matrix.</p> required <code>y</code> <p>Feature labels</p> <code>None</code> Source code in <code>tpot/builtin_modules/column_one_hot_encoder.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Fit OneHotEncoder to X, then transform X.\n\n    Equivalent to self.fit(X).transform(X), but more convenient and more\n    efficient. See fit for the parameters, transform for the return value.\n\n    Parameters\n    ----------\n    X : array-like or sparse matrix, shape=(n_samples, n_features)\n        Dense array or sparse matrix.\n    y: array-like {n_samples,} (Optional, ignored)\n        Feature labels\n    \"\"\"\n\n    if (self.columns == \"categorical\" or self.columns == \"numeric\") and not isinstance(X, pd.DataFrame):\n        raise ValueError(f\"Invalid value for columns: {self.columns}. \"\n                         \"Only 'all' or &lt;list&gt; is supported for np arrays\")\n\n    if self.columns == \"categorical\":\n        self.columns_ = list(X.select_dtypes(exclude='number').columns)\n    elif self.columns == \"numeric\":\n        self.columns_ =  [col for col in X.columns if is_numeric_dtype(X[col])]\n    elif self.columns == \"auto\":\n        self.columns_ = auto_select_categorical_features(X)\n    elif self.columns == \"all\":\n        if isinstance(X, pd.DataFrame):\n            self.columns_ = X.columns\n        else:\n            self.columns_ = list(range(X.shape[1]))\n    elif isinstance(self.columns, list):\n        self.columns_ = self.columns\n    else:\n        raise ValueError(f\"Invalid value for columns: {self.columns}\")\n\n    if len(self.columns_) == 0:\n        return self\n\n    self.enc = sklearn.preprocessing.OrdinalEncoder(categories='auto',   \n                                                    handle_unknown = self.handle_unknown,\n                                                    unknown_value = self.unknown_value, \n                                                    encoded_missing_value = self.encoded_missing_value,\n                                                    min_frequency = self.min_frequency,\n                                                    max_categories = self.max_categories)\n    #TODO make this more consistent with sklearn baseimputer/baseencoder\n    '''\n    if isinstance(X, pd.DataFrame):\n        self.enc.set_output(transform=\"pandas\")\n        for col in X.columns:\n            # check if the column name is not a string\n            if not isinstance(col, str):\n                # if it's not a string, rename the column with \"X\" prefix\n                X.rename(columns={col: f\"X{col}\"}, inplace=True)\n    '''\n\n    if len(self.columns_) == X.shape[1]:\n        X_sel = self.enc.fit(X)\n    else:\n        X_sel, X_not_sel = _X_selected(X, self.columns_)\n        X_sel = self.enc.fit(X_sel)\n\n    return self\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/column_one_hot_encoder/#tpot.builtin_modules.column_one_hot_encoder.ColumnOrdinalEncoder.transform","title":"<code>transform(X)</code>","text":"<p>Transform X using one-hot encoding.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like or sparse matrix, shape=(n_samples, n_features)</code> <p>Dense array or sparse matrix.</p> required <p>Returns:</p> Name Type Description <code>X_out</code> <code>sparse matrix if sparse=True else a 2-d array, dtype=int</code> <p>Transformed input.</p> Source code in <code>tpot/builtin_modules/column_one_hot_encoder.py</code> <pre><code>def transform(self, X):\n    \"\"\"Transform X using one-hot encoding.\n\n    Parameters\n    ----------\n    X : array-like or sparse matrix, shape=(n_samples, n_features)\n        Dense array or sparse matrix.\n\n    Returns\n    -------\n    X_out : sparse matrix if sparse=True else a 2-d array, dtype=int\n        Transformed input.\n    \"\"\"\n\n\n    if len(self.columns_) == 0:\n        return X\n\n    #TODO make this more consistent with sklearn baseimputer/baseencoder\n    '''\n    if isinstance(X, pd.DataFrame):\n        for col in X.columns:\n            # check if the column name is not a string\n            if not isinstance(col, str):\n                # if it's not a string, rename the column with \"X\" prefix\n                X.rename(columns={col: f\"X{col}\"}, inplace=True)\n    '''\n\n    if len(self.columns_) == X.shape[1]:\n        return self.enc.transform(X)\n    else:\n\n        X_sel, X_not_sel= _X_selected(X, self.columns_)\n        X_sel = self.enc.transform(X_sel)\n\n        #If X is dataframe\n        if isinstance(X, pd.DataFrame):\n\n            X_sel = pd.DataFrame(X_sel, columns=self.enc.get_feature_names_out())\n            return pd.concat([X_not_sel.reset_index(drop=True), X_sel.reset_index(drop=True)], axis=1)\n        else:\n            return np.hstack((X_not_sel, X_sel))\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/estimatortransformer/","title":"Estimatortransformer","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/builtin_modules/estimatortransformer/#tpot.builtin_modules.estimatortransformer.EstimatorTransformer","title":"<code>EstimatorTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/estimatortransformer.py</code> <pre><code>class EstimatorTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, estimator, method='auto', passthrough=False, cross_val_predict_cv=None):\n        \"\"\"\n        A class for using a sklearn estimator as a transformer. When calling fit_transform, this class returns the out put of cross_val_predict\n        and trains the estimator on the full dataset. When calling transform, this class uses the estimator fit on the full dataset to transform the data.\n\n        Parameters\n        ----------\n        estimator : sklear.base. BaseEstimator\n            The estimator to use as a transformer.\n        method : str, default='auto'\n            The method to use for the transformation. If 'auto', will try to use predict_proba, decision_function, or predict in that order.\n            - predict_proba: use the predict_proba method of the estimator.\n            - decision_function: use the decision_function method of the estimator.\n            - predict: use the predict method of the estimator.\n        passthrough : bool, default=False\n            Whether to pass the original input through.\n        cross_val_predict_cv : int, default=0\n            Number of folds to use for the cross_val_predict function for inner classifiers and regressors. Estimators will still be fit on the full dataset, but the following node will get the outputs from cross_val_predict.\n\n            - 0-1 : When set to 0 or 1, the cross_val_predict function will not be used. The next layer will get the outputs from fitting and transforming the full dataset.\n            - &gt;=2 : When fitting pipelines with inner classifiers or regressors, they will still be fit on the full dataset.\n                    However, the output to the next node will come from cross_val_predict with the specified number of folds.\n\n        \"\"\"\n        self.estimator = estimator\n        self.method = method\n        self.passthrough = passthrough\n        self.cross_val_predict_cv = cross_val_predict_cv\n\n    def fit(self, X, y=None):\n        self.estimator.fit(X, y)\n        return self\n\n    def transform(self, X, y=None):\n        #Does not do cross val predict, just uses the estimator to transform the data. This is used for the actual transformation in practice, so the real transformation without fitting is needed\n        if self.method == 'auto':\n            if hasattr(self.estimator, 'predict_proba'):\n                method = 'predict_proba'\n            elif hasattr(self.estimator, 'decision_function'):\n                method = 'decision_function'\n            elif hasattr(self.estimator, 'predict'):\n                method = 'predict'\n            else:\n                raise ValueError('Estimator has no valid method')\n        else:\n            method = self.method\n\n        output = getattr(self.estimator, method)(X)\n        output=np.array(output)\n\n        if len(output.shape) == 1:\n            output = output.reshape(-1,1)\n\n        if self.passthrough:\n            return np.hstack((output, X))\n        else:\n            return output\n\n\n\n    def fit_transform(self, X, y=None):\n        #Does use cross_val_predict if cross_val_predict_cv is greater than 0. this function is only used in training the model. \n        self.estimator.fit(X,y)\n\n        if self.method == 'auto':\n            if hasattr(self.estimator, 'predict_proba'):\n                method = 'predict_proba'\n            elif hasattr(self.estimator, 'decision_function'):\n                method = 'decision_function'\n            elif hasattr(self.estimator, 'predict'):\n                method = 'predict'\n            else:\n                raise ValueError('Estimator has no valid method')\n        else:\n            method = self.method\n\n        if self.cross_val_predict_cv is not None:\n            output = cross_val_predict(self.estimator, X, y=y, cv=self.cross_val_predict_cv)\n        else:\n            output = getattr(self.estimator, method)(X)\n            #reshape if needed\n\n        if len(output.shape) == 1:\n            output = output.reshape(-1,1)\n\n        output=np.array(output)\n        if self.passthrough:\n            return np.hstack((output, X))\n        else:\n            return output\n\n    def _estimator_has(attr):\n        '''Check if we can delegate a method to the underlying estimator.\n        First, we check the first fitted final estimator if available, otherwise we\n        check the unfitted final estimator.\n        '''\n        return  lambda self: (self.estimator is not None and\n            hasattr(self.estimator, attr)\n        )\n\n    @available_if(_estimator_has('predict'))\n    def predict(self, X, **predict_params):\n        check_is_fitted(self.estimator)\n        #X = check_array(X)\n\n        preds = self.estimator.predict(X,**predict_params)\n        return preds\n\n    @available_if(_estimator_has('predict_proba'))\n    def predict_proba(self, X, **predict_params):\n        check_is_fitted(self.estimator)\n        #X = check_array(X)\n        return self.estimator.predict_proba(X,**predict_params)\n\n    @available_if(_estimator_has('decision_function'))\n    def decision_function(self, X, **predict_params):\n        check_is_fitted(self.estimator)\n        #X = check_array(X)\n        return self.estimator.decision_function(X,**predict_params)\n\n    def __sklearn_is_fitted__(self):\n        \"\"\"\n        Check fitted status and return a Boolean value.\n        \"\"\"\n        return check_is_fitted(self.estimator)\n\n\n    # @property\n    # def _estimator_type(self):\n    #     return self.estimator._estimator_type\n\n\n\n    @property\n    def classes_(self):\n        \"\"\"The classes labels. Only exist if the last step is a classifier.\"\"\"\n        return self.estimator._classes\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/estimatortransformer/#tpot.builtin_modules.estimatortransformer.EstimatorTransformer.classes_","title":"<code>classes_</code>  <code>property</code>","text":"<p>The classes labels. Only exist if the last step is a classifier.</p>"},{"location":"documentation/tpot/builtin_modules/estimatortransformer/#tpot.builtin_modules.estimatortransformer.EstimatorTransformer.__init__","title":"<code>__init__(estimator, method='auto', passthrough=False, cross_val_predict_cv=None)</code>","text":"<p>A class for using a sklearn estimator as a transformer. When calling fit_transform, this class returns the out put of cross_val_predict and trains the estimator on the full dataset. When calling transform, this class uses the estimator fit on the full dataset to transform the data.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>BaseEstimator</code> <p>The estimator to use as a transformer.</p> required <code>method</code> <code>str</code> <p>The method to use for the transformation. If 'auto', will try to use predict_proba, decision_function, or predict in that order. - predict_proba: use the predict_proba method of the estimator. - decision_function: use the decision_function method of the estimator. - predict: use the predict method of the estimator.</p> <code>'auto'</code> <code>passthrough</code> <code>bool</code> <p>Whether to pass the original input through.</p> <code>False</code> <code>cross_val_predict_cv</code> <code>int</code> <p>Number of folds to use for the cross_val_predict function for inner classifiers and regressors. Estimators will still be fit on the full dataset, but the following node will get the outputs from cross_val_predict.</p> <ul> <li>0-1 : When set to 0 or 1, the cross_val_predict function will not be used. The next layer will get the outputs from fitting and transforming the full dataset.</li> <li> <p>=2 : When fitting pipelines with inner classifiers or regressors, they will still be fit on the full dataset.         However, the output to the next node will come from cross_val_predict with the specified number of folds.</p> </li> </ul> <code>0</code> Source code in <code>tpot/builtin_modules/estimatortransformer.py</code> <pre><code>def __init__(self, estimator, method='auto', passthrough=False, cross_val_predict_cv=None):\n    \"\"\"\n    A class for using a sklearn estimator as a transformer. When calling fit_transform, this class returns the out put of cross_val_predict\n    and trains the estimator on the full dataset. When calling transform, this class uses the estimator fit on the full dataset to transform the data.\n\n    Parameters\n    ----------\n    estimator : sklear.base. BaseEstimator\n        The estimator to use as a transformer.\n    method : str, default='auto'\n        The method to use for the transformation. If 'auto', will try to use predict_proba, decision_function, or predict in that order.\n        - predict_proba: use the predict_proba method of the estimator.\n        - decision_function: use the decision_function method of the estimator.\n        - predict: use the predict method of the estimator.\n    passthrough : bool, default=False\n        Whether to pass the original input through.\n    cross_val_predict_cv : int, default=0\n        Number of folds to use for the cross_val_predict function for inner classifiers and regressors. Estimators will still be fit on the full dataset, but the following node will get the outputs from cross_val_predict.\n\n        - 0-1 : When set to 0 or 1, the cross_val_predict function will not be used. The next layer will get the outputs from fitting and transforming the full dataset.\n        - &gt;=2 : When fitting pipelines with inner classifiers or regressors, they will still be fit on the full dataset.\n                However, the output to the next node will come from cross_val_predict with the specified number of folds.\n\n    \"\"\"\n    self.estimator = estimator\n    self.method = method\n    self.passthrough = passthrough\n    self.cross_val_predict_cv = cross_val_predict_cv\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/estimatortransformer/#tpot.builtin_modules.estimatortransformer.EstimatorTransformer.__sklearn_is_fitted__","title":"<code>__sklearn_is_fitted__()</code>","text":"<p>Check fitted status and return a Boolean value.</p> Source code in <code>tpot/builtin_modules/estimatortransformer.py</code> <pre><code>def __sklearn_is_fitted__(self):\n    \"\"\"\n    Check fitted status and return a Boolean value.\n    \"\"\"\n    return check_is_fitted(self.estimator)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/feature_encoding_frequency_selector/","title":"Feature encoding frequency selector","text":"<p>From https://github.com/EpistasisLab/autoqtl</p>"},{"location":"documentation/tpot/builtin_modules/feature_encoding_frequency_selector/#tpot.builtin_modules.feature_encoding_frequency_selector.FeatureEncodingFrequencySelector","title":"<code>FeatureEncodingFrequencySelector</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>SelectorMixin</code></p> <p>Feature selector based on Encoding Frequency. Encoding frequency is the frequency of each unique element(0/1/2/3) present in a feature set.  Features are selected on the basis of a threshold assigned for encoding frequency. If frequency of any unique element is less than or equal to threshold, the feature is removed.</p> Source code in <code>tpot/builtin_modules/feature_encoding_frequency_selector.py</code> <pre><code>class FeatureEncodingFrequencySelector(BaseEstimator, SelectorMixin):\n    \"\"\"Feature selector based on Encoding Frequency. Encoding frequency is the frequency of each unique element(0/1/2/3) present in a feature set. \n     Features are selected on the basis of a threshold assigned for encoding frequency. If frequency of any unique element is less than or equal to threshold, the feature is removed.  \"\"\"\n\n    @property\n    def __name__(self):\n        \"\"\"Instance name is the same as the class name. \"\"\"\n        return self.__class__.__name__\n\n    def __init__(self, threshold):\n        \"\"\"Create a FeatureEncodingFrequencySelector object.\n\n        Parameters\n        ----------\n        threshold : float, required\n            Threshold value for allele frequency. If frequency of A or frequency of a is less than the threshold value then the feature is dropped.\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n        self.threshold = threshold\n\n    \"\"\"def fit(self, X, y=None):\n        Fit FeatureAlleleFrequencySelector for feature selection\n\n        Parameters\n        ----------\n        X : numpy ndarray, {n_samples, n_features}\n            The training input samples.\n        y : numpy array {n_samples,}\n            The training target values.\n\n        Returns\n        -------\n        self : object\n            Returns a copy of the estimator\n\n        self.selected_feature_indexes = []\n        self.no_of_features = X.shape[1]\n\n        # Finding the no of alleles in each feature column\n        for i in range(0, X.shape[1]):\n            no_of_AA_featurewise = np.count_nonzero(X[:,i]==0)\n            no_of_Aa_featurewise = np.count_nonzero(X[:,i]==1)\n            no_of_aa_featurewise = np.count_nonzero(X[:,i]==2)\n\n\n            frequency_A_featurewise = (2*no_of_AA_featurewise + no_of_Aa_featurewise) / (2*no_of_AA_featurewise + \n            2*no_of_Aa_featurewise + 2*no_of_aa_featurewise)\n\n            frequency_a_featurewise = 1 - frequency_A_featurewise\n\n            if(not(frequency_A_featurewise &lt;= self.threshold) and not(frequency_a_featurewise &lt;= self.threshold)):\n                self.selected_feature_indexes.append(i)\n        return self\"\"\"\n\n    \"\"\"def transform(self, X):\n        Make subset after fit\n\n        Parameters\n        ----------\n        X : numpy ndarray, {n_samples, n_features}\n            New data, where n_samples is the number of samples and n_features is the number of features.\n\n        Returns\n        -------\n        X_transformed : numpy ndarray, {n_samples, n_features}\n            The transformed feature set.\n\n\n        X_transformed = X[:, self.selected_feature_indexes]\n\n        return X_transformed\"\"\"\n\n    def fit(self, X, y=None) :\n        \"\"\"Fit FeatureEncodingFrequencySelector for feature selection. This function gets the appropriate features. \"\"\"\n\n        self.selected_feature_indexes = []\n        self.no_of_original_features = X.shape[1]\n\n        # Finding the frequency of all the unique elements present featurewise in the input variable X\n        for i in range(0, X.shape[1]):\n            unique, counts = np.unique(X[:,i], return_counts=True)\n            element_count_dict_featurewise = dict(zip(unique, counts))\n            element_frequency_dict_featurewise = {}\n            feature_column_selected = True\n\n            for x in unique:\n                x_frequency_featurewise = element_count_dict_featurewise[x] / sum(counts)\n                element_frequency_dict_featurewise[x] = x_frequency_featurewise\n\n            for frequency in element_frequency_dict_featurewise.values():\n                if frequency &lt;= self.threshold :\n                    feature_column_selected = False\n                    break\n\n            if feature_column_selected == True :\n                self.selected_feature_indexes.append(i)\n\n        if not len(self.selected_feature_indexes):\n            \"\"\"msg = \"No feature in X meets the encoding frequency threshold {0:.5f}\"\n            raise ValueError(msg.format(self.threshold))\"\"\"\n            for i in range(0, X.shape[1]):\n                self.selected_feature_indexes.append(i)\n\n        return self\n\n    def transform(self, X):\n        \"\"\" Make subset after fit. This function returns a transformed version of X.  \"\"\"\n        X_transformed = X[:, self.selected_feature_indexes]\n\n        return X_transformed\n\n\n    def _get_support_mask(self):\n        \"\"\"\n        Get the boolean mask indicating which features are selected\n        It is the abstractmethod\n\n        Returns\n        -------\n        support : boolean array of shape [# input features]\n            An element is True iff its corresponding feature is selected for retention.\n            \"\"\"\n        n_features = self.no_of_original_features\n        mask = np.zeros(n_features, dtype=bool)\n        mask[np.asarray(self.selected_feature_indexes)] = True\n\n        return mask\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/feature_encoding_frequency_selector/#tpot.builtin_modules.feature_encoding_frequency_selector.FeatureEncodingFrequencySelector.__name__","title":"<code>__name__</code>  <code>property</code>","text":"<p>Instance name is the same as the class name.</p>"},{"location":"documentation/tpot/builtin_modules/feature_encoding_frequency_selector/#tpot.builtin_modules.feature_encoding_frequency_selector.FeatureEncodingFrequencySelector.__init__","title":"<code>__init__(threshold)</code>","text":"<p>Create a FeatureEncodingFrequencySelector object.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>(float, required)</code> <p>Threshold value for allele frequency. If frequency of A or frequency of a is less than the threshold value then the feature is dropped.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>tpot/builtin_modules/feature_encoding_frequency_selector.py</code> <pre><code>def __init__(self, threshold):\n    \"\"\"Create a FeatureEncodingFrequencySelector object.\n\n    Parameters\n    ----------\n    threshold : float, required\n        Threshold value for allele frequency. If frequency of A or frequency of a is less than the threshold value then the feature is dropped.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    self.threshold = threshold\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/feature_encoding_frequency_selector/#tpot.builtin_modules.feature_encoding_frequency_selector.FeatureEncodingFrequencySelector.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit FeatureEncodingFrequencySelector for feature selection. This function gets the appropriate features.</p> Source code in <code>tpot/builtin_modules/feature_encoding_frequency_selector.py</code> <pre><code>def fit(self, X, y=None) :\n    \"\"\"Fit FeatureEncodingFrequencySelector for feature selection. This function gets the appropriate features. \"\"\"\n\n    self.selected_feature_indexes = []\n    self.no_of_original_features = X.shape[1]\n\n    # Finding the frequency of all the unique elements present featurewise in the input variable X\n    for i in range(0, X.shape[1]):\n        unique, counts = np.unique(X[:,i], return_counts=True)\n        element_count_dict_featurewise = dict(zip(unique, counts))\n        element_frequency_dict_featurewise = {}\n        feature_column_selected = True\n\n        for x in unique:\n            x_frequency_featurewise = element_count_dict_featurewise[x] / sum(counts)\n            element_frequency_dict_featurewise[x] = x_frequency_featurewise\n\n        for frequency in element_frequency_dict_featurewise.values():\n            if frequency &lt;= self.threshold :\n                feature_column_selected = False\n                break\n\n        if feature_column_selected == True :\n            self.selected_feature_indexes.append(i)\n\n    if not len(self.selected_feature_indexes):\n        \"\"\"msg = \"No feature in X meets the encoding frequency threshold {0:.5f}\"\n        raise ValueError(msg.format(self.threshold))\"\"\"\n        for i in range(0, X.shape[1]):\n            self.selected_feature_indexes.append(i)\n\n    return self\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/feature_encoding_frequency_selector/#tpot.builtin_modules.feature_encoding_frequency_selector.FeatureEncodingFrequencySelector.transform","title":"<code>transform(X)</code>","text":"<p>Make subset after fit. This function returns a transformed version of X.</p> Source code in <code>tpot/builtin_modules/feature_encoding_frequency_selector.py</code> <pre><code>def transform(self, X):\n    \"\"\" Make subset after fit. This function returns a transformed version of X.  \"\"\"\n    X_transformed = X[:, self.selected_feature_indexes]\n\n    return X_transformed\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/feature_set_selector/","title":"Feature set selector","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/builtin_modules/feature_set_selector/#tpot.builtin_modules.feature_set_selector.FeatureSetSelector","title":"<code>FeatureSetSelector</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>SelectorMixin</code></p> <p>Select predefined feature subsets.</p> Source code in <code>tpot/builtin_modules/feature_set_selector.py</code> <pre><code>class FeatureSetSelector(BaseEstimator, SelectorMixin):\n    \"\"\"\n    Select predefined feature subsets.\n\n\n    \"\"\"\n\n    def __init__(self, sel_subset=None, name=None):\n        \"\"\"Create a FeatureSetSelector object.\n\n        Parameters\n        ----------\n        sel_subset: list or int\n            If X is a dataframe, items in sel_subset list must correspond to column names\n            If X is a numpy array, items in sel_subset list must correspond to column indexes\n            int: index of a single column\n        Returns\n        -------\n        None\n\n        \"\"\"\n        self.name = name\n        self.sel_subset = sel_subset\n\n\n    def fit(self, X, y=None):\n        \"\"\"Fit FeatureSetSelector for feature selection\n\n        Parameters\n        ----------\n        X: array-like of shape (n_samples, n_features)\n            The training input samples.\n        y: array-like, shape (n_samples,)\n            The target values (integers that correspond to classes in classification, real numbers in regression).\n\n        Returns\n        -------\n        self: object\n            Returns a copy of the estimator\n        \"\"\"\n        if isinstance(self.sel_subset, int) or isinstance(self.sel_subset, str):\n            self.sel_subset = [self.sel_subset]\n\n        #generate  self.feat_list_idx\n        if isinstance(X, pd.DataFrame):\n            self.feature_names_in_ = X.columns.tolist()\n            self.feat_list_idx = sorted([self.feature_names_in_.index(feat) for feat in self.sel_subset])\n\n\n        elif isinstance(X, np.ndarray):\n            self.feature_names_in_ = None#list(range(X.shape[1]))\n\n            self.feat_list_idx = sorted(self.sel_subset)\n\n        n_features = X.shape[1]\n        self.mask = np.zeros(n_features, dtype=bool)\n        self.mask[np.asarray(self.feat_list_idx)] = True\n\n        return self\n\n    #TODO keep returned as dataframe if input is dataframe? may not be consistent with sklearn\n\n    # def transform(self, X):\n\n    def _get_tags(self):\n        tags = {\"allow_nan\": True, \"requires_y\": False}\n        return tags\n\n    def _get_support_mask(self):\n        \"\"\"\n        Get the boolean mask indicating which features are selected\n        Returns\n        -------\n        support : boolean array of shape [# input features]\n            An element is True iff its corresponding feature is selected for\n            retention.\n        \"\"\"\n        return self.mask\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/feature_set_selector/#tpot.builtin_modules.feature_set_selector.FeatureSetSelector.__init__","title":"<code>__init__(sel_subset=None, name=None)</code>","text":"<p>Create a FeatureSetSelector object.</p> <p>Parameters:</p> Name Type Description Default <code>sel_subset</code> <p>If X is a dataframe, items in sel_subset list must correspond to column names If X is a numpy array, items in sel_subset list must correspond to column indexes int: index of a single column</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>tpot/builtin_modules/feature_set_selector.py</code> <pre><code>def __init__(self, sel_subset=None, name=None):\n    \"\"\"Create a FeatureSetSelector object.\n\n    Parameters\n    ----------\n    sel_subset: list or int\n        If X is a dataframe, items in sel_subset list must correspond to column names\n        If X is a numpy array, items in sel_subset list must correspond to column indexes\n        int: index of a single column\n    Returns\n    -------\n    None\n\n    \"\"\"\n    self.name = name\n    self.sel_subset = sel_subset\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/feature_set_selector/#tpot.builtin_modules.feature_set_selector.FeatureSetSelector.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit FeatureSetSelector for feature selection</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>The training input samples.</p> required <code>y</code> <p>The target values (integers that correspond to classes in classification, real numbers in regression).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Returns a copy of the estimator</p> Source code in <code>tpot/builtin_modules/feature_set_selector.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Fit FeatureSetSelector for feature selection\n\n    Parameters\n    ----------\n    X: array-like of shape (n_samples, n_features)\n        The training input samples.\n    y: array-like, shape (n_samples,)\n        The target values (integers that correspond to classes in classification, real numbers in regression).\n\n    Returns\n    -------\n    self: object\n        Returns a copy of the estimator\n    \"\"\"\n    if isinstance(self.sel_subset, int) or isinstance(self.sel_subset, str):\n        self.sel_subset = [self.sel_subset]\n\n    #generate  self.feat_list_idx\n    if isinstance(X, pd.DataFrame):\n        self.feature_names_in_ = X.columns.tolist()\n        self.feat_list_idx = sorted([self.feature_names_in_.index(feat) for feat in self.sel_subset])\n\n\n    elif isinstance(X, np.ndarray):\n        self.feature_names_in_ = None#list(range(X.shape[1]))\n\n        self.feat_list_idx = sorted(self.sel_subset)\n\n    n_features = X.shape[1]\n    self.mask = np.zeros(n_features, dtype=bool)\n    self.mask[np.asarray(self.feat_list_idx)] = True\n\n    return self\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/feature_transformers/","title":"Feature transformers","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/builtin_modules/feature_transformers/#tpot.builtin_modules.feature_transformers.CategoricalSelector","title":"<code>CategoricalSelector</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Meta-transformer for selecting categorical features and transform them using OneHotEncoder.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>int</code> <p>Maximum number of unique values per feature to consider the feature to be categorical.</p> <code>10</code> <code>minimum_fraction</code> <p>Minimum fraction of unique values in a feature to consider the feature to be categorical.</p> <code>None</code> Source code in <code>tpot/builtin_modules/feature_transformers.py</code> <pre><code>class CategoricalSelector(BaseEstimator, TransformerMixin):\n    \"\"\"Meta-transformer for selecting categorical features and transform them using OneHotEncoder.\n\n    Parameters\n    ----------\n\n    threshold : int, default=10\n        Maximum number of unique values per feature to consider the feature\n        to be categorical.\n\n    minimum_fraction: float, default=None\n        Minimum fraction of unique values in a feature to consider the feature\n        to be categorical.\n    \"\"\"\n\n    def __init__(self, threshold=10, minimum_fraction=None):\n        \"\"\"Create a CategoricalSelector object.\"\"\"\n        self.threshold = threshold\n        self.minimum_fraction = minimum_fraction\n\n\n    def fit(self, X, y=None):\n        \"\"\"Do nothing and return the estimator unchanged\n        This method is just there to implement the usual API and hence\n        work in pipelines.\n        Parameters\n        ----------\n        X : array-like\n        \"\"\"\n        X = check_array(X, accept_sparse='csr')\n        return self\n\n\n    def transform(self, X):\n        \"\"\"Select categorical features and transform them using OneHotEncoder.\n\n        Parameters\n        ----------\n        X: numpy ndarray, {n_samples, n_components}\n            New data, where n_samples is the number of samples and n_components is the number of components.\n\n        Returns\n        -------\n        array-like, {n_samples, n_components}\n        \"\"\"\n        selected = auto_select_categorical_features(X, threshold=self.threshold)\n        X_sel, _, n_selected, _ = _X_selected(X, selected)\n\n        if n_selected == 0:\n            # No features selected.\n            raise ValueError('No categorical feature was found!')\n        else:\n            ohe = OneHotEncoder(categorical_features='all', sparse=False, minimum_fraction=self.minimum_fraction)\n            return ohe.fit_transform(X_sel)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/feature_transformers/#tpot.builtin_modules.feature_transformers.CategoricalSelector.__init__","title":"<code>__init__(threshold=10, minimum_fraction=None)</code>","text":"<p>Create a CategoricalSelector object.</p> Source code in <code>tpot/builtin_modules/feature_transformers.py</code> <pre><code>def __init__(self, threshold=10, minimum_fraction=None):\n    \"\"\"Create a CategoricalSelector object.\"\"\"\n    self.threshold = threshold\n    self.minimum_fraction = minimum_fraction\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/feature_transformers/#tpot.builtin_modules.feature_transformers.CategoricalSelector.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Do nothing and return the estimator unchanged This method is just there to implement the usual API and hence work in pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> required Source code in <code>tpot/builtin_modules/feature_transformers.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Do nothing and return the estimator unchanged\n    This method is just there to implement the usual API and hence\n    work in pipelines.\n    Parameters\n    ----------\n    X : array-like\n    \"\"\"\n    X = check_array(X, accept_sparse='csr')\n    return self\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/feature_transformers/#tpot.builtin_modules.feature_transformers.CategoricalSelector.transform","title":"<code>transform(X)</code>","text":"<p>Select categorical features and transform them using OneHotEncoder.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>New data, where n_samples is the number of samples and n_components is the number of components.</p> required <p>Returns:</p> Type Description <code>(array - like, {n_samples, n_components})</code> Source code in <code>tpot/builtin_modules/feature_transformers.py</code> <pre><code>def transform(self, X):\n    \"\"\"Select categorical features and transform them using OneHotEncoder.\n\n    Parameters\n    ----------\n    X: numpy ndarray, {n_samples, n_components}\n        New data, where n_samples is the number of samples and n_components is the number of components.\n\n    Returns\n    -------\n    array-like, {n_samples, n_components}\n    \"\"\"\n    selected = auto_select_categorical_features(X, threshold=self.threshold)\n    X_sel, _, n_selected, _ = _X_selected(X, selected)\n\n    if n_selected == 0:\n        # No features selected.\n        raise ValueError('No categorical feature was found!')\n    else:\n        ohe = OneHotEncoder(categorical_features='all', sparse=False, minimum_fraction=self.minimum_fraction)\n        return ohe.fit_transform(X_sel)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/feature_transformers/#tpot.builtin_modules.feature_transformers.ContinuousSelector","title":"<code>ContinuousSelector</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Meta-transformer for selecting continuous features and transform them using PCA.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>int</code> <p>Maximum number of unique values per feature to consider the feature to be categorical.</p> <code>10</code> <code>svd_solver</code> <code>string {'auto', 'full', 'arpack', 'randomized'}</code> <p>auto :     the solver is selected by a default policy based on <code>X.shape</code> and     <code>n_components</code>: if the input data is larger than 500x500 and the     number of components to extract is lower than 80% of the smallest     dimension of the data, then the more efficient 'randomized'     method is enabled. Otherwise the exact full SVD is computed and     optionally truncated afterwards. full :     run exact full SVD calling the standard LAPACK solver via     <code>scipy.linalg.svd</code> and select the components by postprocessing arpack :     run SVD truncated to n_components calling ARPACK solver via     <code>scipy.sparse.linalg.svds</code>. It requires strictly     0 &lt; n_components &lt; X.shape[1] randomized :     run randomized SVD by the method of Halko et al.</p> <code>'randomized'</code> <code>iterated_power</code> <code>int &gt;= 0, or 'auto', (default 'auto')</code> <p>Number of iterations for the power method computed by svd_solver == 'randomized'.</p> <code>'auto'</code> Source code in <code>tpot/builtin_modules/feature_transformers.py</code> <pre><code>class ContinuousSelector(BaseEstimator, TransformerMixin):\n    \"\"\"Meta-transformer for selecting continuous features and transform them using PCA.\n\n    Parameters\n    ----------\n\n    threshold : int, default=10\n        Maximum number of unique values per feature to consider the feature\n        to be categorical.\n\n    svd_solver : string {'auto', 'full', 'arpack', 'randomized'}\n        auto :\n            the solver is selected by a default policy based on `X.shape` and\n            `n_components`: if the input data is larger than 500x500 and the\n            number of components to extract is lower than 80% of the smallest\n            dimension of the data, then the more efficient 'randomized'\n            method is enabled. Otherwise the exact full SVD is computed and\n            optionally truncated afterwards.\n        full :\n            run exact full SVD calling the standard LAPACK solver via\n            `scipy.linalg.svd` and select the components by postprocessing\n        arpack :\n            run SVD truncated to n_components calling ARPACK solver via\n            `scipy.sparse.linalg.svds`. It requires strictly\n            0 &lt; n_components &lt; X.shape[1]\n        randomized :\n            run randomized SVD by the method of Halko et al.\n\n    iterated_power : int &gt;= 0, or 'auto', (default 'auto')\n        Number of iterations for the power method computed by\n        svd_solver == 'randomized'.\n\n    \"\"\"\n\n    def __init__(self, threshold=10, svd_solver='randomized' ,iterated_power='auto', random_state=42):\n        \"\"\"Create a ContinuousSelector object.\"\"\"\n        self.threshold = threshold\n        self.svd_solver = svd_solver\n        self.iterated_power = iterated_power\n        self.random_state = random_state\n\n\n    def fit(self, X, y=None):\n        \"\"\"Do nothing and return the estimator unchanged\n        This method is just there to implement the usual API and hence\n        work in pipelines.\n        Parameters\n        ----------\n        X : array-like\n        \"\"\"\n        X = check_array(X)\n        return self\n\n\n    def transform(self, X):\n        \"\"\"Select continuous features and transform them using PCA.\n\n        Parameters\n        ----------\n        X: numpy ndarray, {n_samples, n_components}\n            New data, where n_samples is the number of samples and n_components is the number of components.\n\n        Returns\n        -------\n        array-like, {n_samples, n_components}\n        \"\"\"\n        selected = auto_select_categorical_features(X, threshold=self.threshold)\n        _, X_sel, n_selected, _ = _X_selected(X, selected)\n\n        if n_selected == 0:\n            # No features selected.\n            raise ValueError('No continuous feature was found!')\n        else:\n            pca = PCA(svd_solver=self.svd_solver, iterated_power=self.iterated_power, random_state=self.random_state)\n            return pca.fit_transform(X_sel)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/feature_transformers/#tpot.builtin_modules.feature_transformers.ContinuousSelector.__init__","title":"<code>__init__(threshold=10, svd_solver='randomized', iterated_power='auto', random_state=42)</code>","text":"<p>Create a ContinuousSelector object.</p> Source code in <code>tpot/builtin_modules/feature_transformers.py</code> <pre><code>def __init__(self, threshold=10, svd_solver='randomized' ,iterated_power='auto', random_state=42):\n    \"\"\"Create a ContinuousSelector object.\"\"\"\n    self.threshold = threshold\n    self.svd_solver = svd_solver\n    self.iterated_power = iterated_power\n    self.random_state = random_state\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/feature_transformers/#tpot.builtin_modules.feature_transformers.ContinuousSelector.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Do nothing and return the estimator unchanged This method is just there to implement the usual API and hence work in pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> required Source code in <code>tpot/builtin_modules/feature_transformers.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Do nothing and return the estimator unchanged\n    This method is just there to implement the usual API and hence\n    work in pipelines.\n    Parameters\n    ----------\n    X : array-like\n    \"\"\"\n    X = check_array(X)\n    return self\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/feature_transformers/#tpot.builtin_modules.feature_transformers.ContinuousSelector.transform","title":"<code>transform(X)</code>","text":"<p>Select continuous features and transform them using PCA.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>New data, where n_samples is the number of samples and n_components is the number of components.</p> required <p>Returns:</p> Type Description <code>(array - like, {n_samples, n_components})</code> Source code in <code>tpot/builtin_modules/feature_transformers.py</code> <pre><code>def transform(self, X):\n    \"\"\"Select continuous features and transform them using PCA.\n\n    Parameters\n    ----------\n    X: numpy ndarray, {n_samples, n_components}\n        New data, where n_samples is the number of samples and n_components is the number of components.\n\n    Returns\n    -------\n    array-like, {n_samples, n_components}\n    \"\"\"\n    selected = auto_select_categorical_features(X, threshold=self.threshold)\n    _, X_sel, n_selected, _ = _X_selected(X, selected)\n\n    if n_selected == 0:\n        # No features selected.\n        raise ValueError('No continuous feature was found!')\n    else:\n        pca = PCA(svd_solver=self.svd_solver, iterated_power=self.iterated_power, random_state=self.random_state)\n        return pca.fit_transform(X_sel)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/genetic_encoders/","title":"Genetic encoders","text":"<p>Code from https://github.com/EpistasisLab/autoqtl This file contains the class definition for all the genetic encoders. All the genetic encoder classes inherit the Scikit learn BaseEstimator and TransformerMixin classes to follow the Scikit-learn paradigm.</p>"},{"location":"documentation/tpot/builtin_modules/genetic_encoders/#tpot.builtin_modules.genetic_encoders.DominantEncoder","title":"<code>DominantEncoder</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>This class contains the function definition for encoding the input features as a Dominant genetic model. The encoding used is AA(0)-&gt;1, Aa(1)-&gt;1, aa(2)-&gt;0.</p> Source code in <code>tpot/builtin_modules/genetic_encoders.py</code> <pre><code>class DominantEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"This class contains the function definition for encoding the input features as a Dominant genetic model.\n    The encoding used is AA(0)-&gt;1, Aa(1)-&gt;1, aa(2)-&gt;0. \"\"\"\n\n    def fit(self, X, y=None):\n        \"\"\"Do nothing and return the estimator unchanged.\n        Dummy function to fit in with the sklearn API and hence work in pipelines.\n\n        Parameters\n        ----------\n        X : array-like\n        \"\"\"\n        return self\n\n    def transform(self, X, y=None):\n        \"\"\"Transform the data by applying the Dominant encoding.\n\n        Parameters\n        ----------\n        X : numpy ndarray, {n_samples, n_components}\n            New data, where n_samples is the number of samples (number of individuals)\n            and n_components is the number of components (number of features).\n        y : None\n            Unused\n\n        Returns\n        -------\n        X_transformed: numpy ndarray, {n_samples, n_components}\n            The encoded feature set\n        \"\"\"\n        X = check_array(X)\n        map = {0: 1, 1: 1, 2: 0}\n        mapping_function = np.vectorize(lambda i: map[i] if i in map else i)\n\n        X_transformed = mapping_function(X)\n\n        return X_transformed\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/genetic_encoders/#tpot.builtin_modules.genetic_encoders.DominantEncoder.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Do nothing and return the estimator unchanged. Dummy function to fit in with the sklearn API and hence work in pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> required Source code in <code>tpot/builtin_modules/genetic_encoders.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Do nothing and return the estimator unchanged.\n    Dummy function to fit in with the sklearn API and hence work in pipelines.\n\n    Parameters\n    ----------\n    X : array-like\n    \"\"\"\n    return self\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/genetic_encoders/#tpot.builtin_modules.genetic_encoders.DominantEncoder.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Transform the data by applying the Dominant encoding.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy ndarray, {n_samples, n_components}</code> <p>New data, where n_samples is the number of samples (number of individuals) and n_components is the number of components (number of features).</p> required <code>y</code> <code>None</code> <p>Unused</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_transformed</code> <code>numpy ndarray, {n_samples, n_components}</code> <p>The encoded feature set</p> Source code in <code>tpot/builtin_modules/genetic_encoders.py</code> <pre><code>def transform(self, X, y=None):\n    \"\"\"Transform the data by applying the Dominant encoding.\n\n    Parameters\n    ----------\n    X : numpy ndarray, {n_samples, n_components}\n        New data, where n_samples is the number of samples (number of individuals)\n        and n_components is the number of components (number of features).\n    y : None\n        Unused\n\n    Returns\n    -------\n    X_transformed: numpy ndarray, {n_samples, n_components}\n        The encoded feature set\n    \"\"\"\n    X = check_array(X)\n    map = {0: 1, 1: 1, 2: 0}\n    mapping_function = np.vectorize(lambda i: map[i] if i in map else i)\n\n    X_transformed = mapping_function(X)\n\n    return X_transformed\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/genetic_encoders/#tpot.builtin_modules.genetic_encoders.HeterosisEncoder","title":"<code>HeterosisEncoder</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>This class contains the function definition for encoding the input features as a Heterozygote Advantage genetic model. The encoding used is AA(0)-&gt;0, Aa(1)-&gt;1, aa(2)-&gt;0.</p> Source code in <code>tpot/builtin_modules/genetic_encoders.py</code> <pre><code>class HeterosisEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"This class contains the function definition for encoding the input features as a Heterozygote Advantage genetic model.\n    The encoding used is AA(0)-&gt;0, Aa(1)-&gt;1, aa(2)-&gt;0. \"\"\"\n\n    def fit(self, X, y=None):\n        \"\"\"Do nothing and return the estimator unchanged.\n        Dummy function to fit in with the sklearn API and hence work in pipelines.\n\n        Parameters\n        ----------\n        X : array-like\n        \"\"\"\n        return self\n\n    def transform(self, X, y=None):\n        \"\"\"Transform the data by applying the Heterosis encoding.\n\n        Parameters\n        ----------\n        X : numpy ndarray, {n_samples, n_components}\n            New data, where n_samples is the number of samples (number of individuals)\n            and n_components is the number of components (number of features).\n        y : None\n            Unused\n\n        Returns\n        -------\n        X_transformed: numpy ndarray, {n_samples, n_components}\n            The encoded feature set\n        \"\"\"\n        X = check_array(X)\n        map = {0: 0, 1: 1, 2: 0}\n        mapping_function = np.vectorize(lambda i: map[i] if i in map else i)\n\n        X_transformed = mapping_function(X)\n\n        return X_transformed\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/genetic_encoders/#tpot.builtin_modules.genetic_encoders.HeterosisEncoder.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Do nothing and return the estimator unchanged. Dummy function to fit in with the sklearn API and hence work in pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> required Source code in <code>tpot/builtin_modules/genetic_encoders.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Do nothing and return the estimator unchanged.\n    Dummy function to fit in with the sklearn API and hence work in pipelines.\n\n    Parameters\n    ----------\n    X : array-like\n    \"\"\"\n    return self\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/genetic_encoders/#tpot.builtin_modules.genetic_encoders.HeterosisEncoder.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Transform the data by applying the Heterosis encoding.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy ndarray, {n_samples, n_components}</code> <p>New data, where n_samples is the number of samples (number of individuals) and n_components is the number of components (number of features).</p> required <code>y</code> <code>None</code> <p>Unused</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_transformed</code> <code>numpy ndarray, {n_samples, n_components}</code> <p>The encoded feature set</p> Source code in <code>tpot/builtin_modules/genetic_encoders.py</code> <pre><code>def transform(self, X, y=None):\n    \"\"\"Transform the data by applying the Heterosis encoding.\n\n    Parameters\n    ----------\n    X : numpy ndarray, {n_samples, n_components}\n        New data, where n_samples is the number of samples (number of individuals)\n        and n_components is the number of components (number of features).\n    y : None\n        Unused\n\n    Returns\n    -------\n    X_transformed: numpy ndarray, {n_samples, n_components}\n        The encoded feature set\n    \"\"\"\n    X = check_array(X)\n    map = {0: 0, 1: 1, 2: 0}\n    mapping_function = np.vectorize(lambda i: map[i] if i in map else i)\n\n    X_transformed = mapping_function(X)\n\n    return X_transformed\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/genetic_encoders/#tpot.builtin_modules.genetic_encoders.OverDominanceEncoder","title":"<code>OverDominanceEncoder</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>This class contains the function definition for encoding the input features as a Over Dominance genetic model. The encoding used is AA(0)-&gt;1, Aa(1)-&gt;2, aa(2)-&gt;0.</p> Source code in <code>tpot/builtin_modules/genetic_encoders.py</code> <pre><code>class OverDominanceEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"This class contains the function definition for encoding the input features as a Over Dominance genetic model.\n    The encoding used is AA(0)-&gt;1, Aa(1)-&gt;2, aa(2)-&gt;0. \"\"\"\n\n    def fit(self, X, y=None):\n        \"\"\"Do nothing and return the estimator unchanged.\n        Dummy function to fit in with the sklearn API and hence work in pipelines.\n\n        Parameters\n        ----------\n        X : array-like\n        \"\"\"\n        return self\n\n    def transform(self, X, y=None):\n        \"\"\"Transform the data by applying the Heterosis encoding.\n\n        Parameters\n        ----------\n        X : numpy ndarray, {n_samples, n_components}\n            New data, where n_samples is the number of samples (number of individuals)\n            and n_components is the number of components (number of features).\n        y : None\n            Unused\n\n        Returns\n        -------\n        X_transformed: numpy ndarray, {n_samples, n_components}\n            The encoded feature set\n        \"\"\"\n        X = check_array(X)\n        map = {0: 1, 1: 2, 2: 0}\n        mapping_function = np.vectorize(lambda i: map[i] if i in map else i)\n\n        X_transformed = mapping_function(X)\n\n        return X_transformed\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/genetic_encoders/#tpot.builtin_modules.genetic_encoders.OverDominanceEncoder.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Do nothing and return the estimator unchanged. Dummy function to fit in with the sklearn API and hence work in pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> required Source code in <code>tpot/builtin_modules/genetic_encoders.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Do nothing and return the estimator unchanged.\n    Dummy function to fit in with the sklearn API and hence work in pipelines.\n\n    Parameters\n    ----------\n    X : array-like\n    \"\"\"\n    return self\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/genetic_encoders/#tpot.builtin_modules.genetic_encoders.OverDominanceEncoder.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Transform the data by applying the Heterosis encoding.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy ndarray, {n_samples, n_components}</code> <p>New data, where n_samples is the number of samples (number of individuals) and n_components is the number of components (number of features).</p> required <code>y</code> <code>None</code> <p>Unused</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_transformed</code> <code>numpy ndarray, {n_samples, n_components}</code> <p>The encoded feature set</p> Source code in <code>tpot/builtin_modules/genetic_encoders.py</code> <pre><code>def transform(self, X, y=None):\n    \"\"\"Transform the data by applying the Heterosis encoding.\n\n    Parameters\n    ----------\n    X : numpy ndarray, {n_samples, n_components}\n        New data, where n_samples is the number of samples (number of individuals)\n        and n_components is the number of components (number of features).\n    y : None\n        Unused\n\n    Returns\n    -------\n    X_transformed: numpy ndarray, {n_samples, n_components}\n        The encoded feature set\n    \"\"\"\n    X = check_array(X)\n    map = {0: 1, 1: 2, 2: 0}\n    mapping_function = np.vectorize(lambda i: map[i] if i in map else i)\n\n    X_transformed = mapping_function(X)\n\n    return X_transformed\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/genetic_encoders/#tpot.builtin_modules.genetic_encoders.RecessiveEncoder","title":"<code>RecessiveEncoder</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>This class contains the function definition for encoding the input features as a Recessive genetic model. The encoding used is AA(0)-&gt;0, Aa(1)-&gt;1, aa(2)-&gt;1.</p> Source code in <code>tpot/builtin_modules/genetic_encoders.py</code> <pre><code>class RecessiveEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"This class contains the function definition for encoding the input features as a Recessive genetic model.\n    The encoding used is AA(0)-&gt;0, Aa(1)-&gt;1, aa(2)-&gt;1. \"\"\"\n\n    def fit(self, X, y=None):\n        \"\"\"Do nothing and return the estimator unchanged.\n        Dummy function to fit in with the sklearn API and hence work in pipelines.\n\n        Parameters\n        ----------\n        X : array-like\n        \"\"\"\n        return self\n\n    def transform(self, X, y=None):\n        \"\"\"Transform the data by applying the Recessive encoding.\n\n        Parameters\n        ----------\n        X : numpy ndarray, {n_samples, n_components}\n            New data, where n_samples is the number of samples (number of individuals)\n            and n_components is the number of components (number of features).\n        y : None\n            Unused\n\n        Returns\n        -------\n        X_transformed: numpy ndarray, {n_samples, n_components}\n            The encoded feature set\n        \"\"\"\n        X = check_array(X)\n        map = {0: 0, 1: 1, 2: 1}\n        mapping_function = np.vectorize(lambda i: map[i] if i in map else i)\n\n        X_transformed = mapping_function(X)\n\n        return X_transformed\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/genetic_encoders/#tpot.builtin_modules.genetic_encoders.RecessiveEncoder.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Do nothing and return the estimator unchanged. Dummy function to fit in with the sklearn API and hence work in pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> required Source code in <code>tpot/builtin_modules/genetic_encoders.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Do nothing and return the estimator unchanged.\n    Dummy function to fit in with the sklearn API and hence work in pipelines.\n\n    Parameters\n    ----------\n    X : array-like\n    \"\"\"\n    return self\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/genetic_encoders/#tpot.builtin_modules.genetic_encoders.RecessiveEncoder.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Transform the data by applying the Recessive encoding.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy ndarray, {n_samples, n_components}</code> <p>New data, where n_samples is the number of samples (number of individuals) and n_components is the number of components (number of features).</p> required <code>y</code> <code>None</code> <p>Unused</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_transformed</code> <code>numpy ndarray, {n_samples, n_components}</code> <p>The encoded feature set</p> Source code in <code>tpot/builtin_modules/genetic_encoders.py</code> <pre><code>def transform(self, X, y=None):\n    \"\"\"Transform the data by applying the Recessive encoding.\n\n    Parameters\n    ----------\n    X : numpy ndarray, {n_samples, n_components}\n        New data, where n_samples is the number of samples (number of individuals)\n        and n_components is the number of components (number of features).\n    y : None\n        Unused\n\n    Returns\n    -------\n    X_transformed: numpy ndarray, {n_samples, n_components}\n        The encoded feature set\n    \"\"\"\n    X = check_array(X)\n    map = {0: 0, 1: 1, 2: 1}\n    mapping_function = np.vectorize(lambda i: map[i] if i in map else i)\n\n    X_transformed = mapping_function(X)\n\n    return X_transformed\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/genetic_encoders/#tpot.builtin_modules.genetic_encoders.UnderDominanceEncoder","title":"<code>UnderDominanceEncoder</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>This class contains the function definition for encoding the input features as a Under Dominance genetic model. The encoding used is AA(0)-&gt;2, Aa(1)-&gt;0, aa(2)-&gt;1.</p> Source code in <code>tpot/builtin_modules/genetic_encoders.py</code> <pre><code>class UnderDominanceEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"This class contains the function definition for encoding the input features as a Under Dominance genetic model.\n    The encoding used is AA(0)-&gt;2, Aa(1)-&gt;0, aa(2)-&gt;1. \"\"\"\n\n    def fit(self, X, y=None):\n        \"\"\"Do nothing and return the estimator unchanged.\n        Dummy function to fit in with the sklearn API and hence work in pipelines.\n\n        Parameters\n        ----------\n        X : array-like\n        \"\"\"\n        return self\n\n    def transform(self, X, y=None):\n        \"\"\"Transform the data by applying the Heterosis encoding.\n\n        Parameters\n        ----------\n        X : numpy ndarray, {n_samples, n_components}\n            New data, where n_samples is the number of samples (number of individuals)\n            and n_components is the number of components (number of features).\n        y : None\n            Unused\n\n        Returns\n        -------\n        X_transformed: numpy ndarray, {n_samples, n_components}\n            The encoded feature set\n        \"\"\"\n        X = check_array(X)\n        map = {0: 2, 1: 0, 2: 1}\n        mapping_function = np.vectorize(lambda i: map[i] if i in map else i)\n\n        X_transformed = mapping_function(X)\n\n        return X_transformed\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/genetic_encoders/#tpot.builtin_modules.genetic_encoders.UnderDominanceEncoder.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Do nothing and return the estimator unchanged. Dummy function to fit in with the sklearn API and hence work in pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> required Source code in <code>tpot/builtin_modules/genetic_encoders.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Do nothing and return the estimator unchanged.\n    Dummy function to fit in with the sklearn API and hence work in pipelines.\n\n    Parameters\n    ----------\n    X : array-like\n    \"\"\"\n    return self\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/genetic_encoders/#tpot.builtin_modules.genetic_encoders.UnderDominanceEncoder.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Transform the data by applying the Heterosis encoding.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy ndarray, {n_samples, n_components}</code> <p>New data, where n_samples is the number of samples (number of individuals) and n_components is the number of components (number of features).</p> required <code>y</code> <code>None</code> <p>Unused</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_transformed</code> <code>numpy ndarray, {n_samples, n_components}</code> <p>The encoded feature set</p> Source code in <code>tpot/builtin_modules/genetic_encoders.py</code> <pre><code>def transform(self, X, y=None):\n    \"\"\"Transform the data by applying the Heterosis encoding.\n\n    Parameters\n    ----------\n    X : numpy ndarray, {n_samples, n_components}\n        New data, where n_samples is the number of samples (number of individuals)\n        and n_components is the number of components (number of features).\n    y : None\n        Unused\n\n    Returns\n    -------\n    X_transformed: numpy ndarray, {n_samples, n_components}\n        The encoded feature set\n    \"\"\"\n    X = check_array(X)\n    map = {0: 2, 1: 0, 2: 1}\n    mapping_function = np.vectorize(lambda i: map[i] if i in map else i)\n\n    X_transformed = mapping_function(X)\n\n    return X_transformed\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/imputer/","title":"Imputer","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/builtin_modules/imputer/#tpot.builtin_modules.imputer.ColumnSimpleImputer","title":"<code>ColumnSimpleImputer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/imputer.py</code> <pre><code>class ColumnSimpleImputer(BaseEstimator, TransformerMixin):\n    def __init__(self,  columns=\"all\",         \n                        missing_values=np.nan,\n                        strategy=\"mean\",\n                        fill_value=None,\n                        copy=True,\n                        add_indicator=False,\n                        keep_empty_features=False,):\n        \"\"\"\"\n        A wrapper for SimpleImputer that allows for imputation of specific columns in a DataFrame or np array.\n        Passes through columns that are not imputed.\n\n        Parameters\n        ----------\n        columns : str, list, default='all'\n            Determines which columns to impute with sklearn.impute.SimpleImputer.\n            - 'categorical' : Automatically select categorical features\n            - 'numeric' : Automatically select numeric features\n            - 'all' : Select all features\n            - list : A list of columns to select\n\n        # See documentation from sklearn.impute.SimpleImputer for the following parameters\n        missing_values, strategy, fill_value, copy, add_indicator, keep_empty_features\n\n        \"\"\"\n\n        self.columns = columns\n        self.missing_values = missing_values\n        self.strategy = strategy\n        self.fill_value = fill_value\n        self.copy = copy\n        self.add_indicator = add_indicator\n        self.keep_empty_features = keep_empty_features\n\n\n    def fit(self, X, y=None):\n        if (self.columns == \"categorical\" or self.columns == \"numeric\") and not isinstance(X, pd.DataFrame):\n            raise ValueError(f\"Invalid value for columns: {self.columns}. \"\n                             \"Only 'all' or &lt;list&gt; is supported for np arrays\")\n\n        if self.columns == \"categorical\":\n            self.columns_ = list(X.select_dtypes(exclude='number').columns)\n        elif self.columns == \"numeric\":\n            self.columns_ =  [col for col in X.columns if is_numeric_dtype(X[col])]\n        elif self.columns == \"all\":\n            if isinstance(X, pd.DataFrame):\n                self.columns_ = X.columns\n            else:\n                self.columns_ = list(range(X.shape[1]))\n        elif isinstance(self.columns, list):\n            self.columns_ = self.columns\n        else:\n            raise ValueError(f\"Invalid value for columns: {self.columns}\")\n\n        if len(self.columns_) == 0:\n            return self\n\n        self.imputer = sklearn.impute.SimpleImputer(missing_values=self.missing_values,\n                                                    strategy=self.strategy,\n                                                    fill_value=self.fill_value,\n                                                    copy=self.copy,\n                                                    add_indicator=self.add_indicator,\n                                                    keep_empty_features=self.keep_empty_features)\n\n        if isinstance(X, pd.DataFrame):\n            self.imputer.set_output(transform=\"pandas\")\n\n        if isinstance(X, pd.DataFrame):\n            self.imputer.fit(X[self.columns_], y)\n        else:\n            self.imputer.fit(X[:, self.columns_], y)\n\n        return self\n\n    def transform(self, X):\n        if len(self.columns_) == 0:\n            return X\n\n        if isinstance(X, pd.DataFrame):\n            X = X.copy()\n            X[self.columns_] = self.imputer.transform(X[self.columns_])\n            return X\n        else:\n            X = np.copy(X)\n            X[:, self.columns_] = self.imputer.transform(X[:, self.columns_])\n            return X\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/imputer/#tpot.builtin_modules.imputer.ColumnSimpleImputer.__init__","title":"<code>__init__(columns='all', missing_values=np.nan, strategy='mean', fill_value=None, copy=True, add_indicator=False, keep_empty_features=False)</code>","text":"<p>\" A wrapper for SimpleImputer that allows for imputation of specific columns in a DataFrame or np array. Passes through columns that are not imputed.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>(str, list)</code> <p>Determines which columns to impute with sklearn.impute.SimpleImputer. - 'categorical' : Automatically select categorical features - 'numeric' : Automatically select numeric features - 'all' : Select all features - list : A list of columns to select</p> <code>'all'</code> <code>missing_values</code> <code>nan</code> <code>strategy</code> <code>nan</code> <code>fill_value</code> <code>nan</code> <code>copy</code> <code>nan</code> <code>add_indicator</code> <code>nan</code> <code>keep_empty_features</code> <code>nan</code> Source code in <code>tpot/builtin_modules/imputer.py</code> <pre><code>def __init__(self,  columns=\"all\",         \n                    missing_values=np.nan,\n                    strategy=\"mean\",\n                    fill_value=None,\n                    copy=True,\n                    add_indicator=False,\n                    keep_empty_features=False,):\n    \"\"\"\"\n    A wrapper for SimpleImputer that allows for imputation of specific columns in a DataFrame or np array.\n    Passes through columns that are not imputed.\n\n    Parameters\n    ----------\n    columns : str, list, default='all'\n        Determines which columns to impute with sklearn.impute.SimpleImputer.\n        - 'categorical' : Automatically select categorical features\n        - 'numeric' : Automatically select numeric features\n        - 'all' : Select all features\n        - list : A list of columns to select\n\n    # See documentation from sklearn.impute.SimpleImputer for the following parameters\n    missing_values, strategy, fill_value, copy, add_indicator, keep_empty_features\n\n    \"\"\"\n\n    self.columns = columns\n    self.missing_values = missing_values\n    self.strategy = strategy\n    self.fill_value = fill_value\n    self.copy = copy\n    self.add_indicator = add_indicator\n    self.keep_empty_features = keep_empty_features\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/nn/","title":"Nn","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/builtin_modules/nn/#tpot.builtin_modules.nn.PytorchClassifier","title":"<code>PytorchClassifier</code>","text":"<p>               Bases: <code>PytorchEstimator</code>, <code>ClassifierMixin</code></p> Source code in <code>tpot/builtin_modules/nn.py</code> <pre><code>class PytorchClassifier(PytorchEstimator, ClassifierMixin):\n    @abstractmethod\n    def _init_model(self, X, y): # pragma: no cover\n        pass\n\n    def fit(self, X, y):\n        \"\"\"Generalizable method for fitting a PyTorch estimator to a training\n        set.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vector, where n_samples is the number of samples and\n            n_features is the number of features.\n        y : array-like of shape (n_samples,)\n            Target vector relative to X.\n\n        Returns\n        -------\n        self\n            Fitted estimator.\n        \"\"\"\n        # pylint: disable=no-member\n\n        self._init_model(X, y)\n\n        assert _pytorch_model_is_fully_initialized(self)\n\n        for epoch in range(self.num_epochs):\n            for i, (samples, labels) in enumerate(self.data_loader):\n                samples = samples.to(self.device)\n                labels = labels.to(self.device)\n\n                self.optimizer.zero_grad()\n                outputs = self.network(samples)\n\n                loss = self.loss_function(outputs, labels)\n                loss.backward()\n                self.optimizer.step()\n\n                if self.verbose and ((i + 1) % 100 == 0):\n                    print(\n                        \"Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f\"\n                        % (\n                            epoch + 1,\n                            self.num_epochs,\n                            i + 1,\n                            self.train_dset_len // self.batch_size,\n                            loss.item(),\n                        )\n                    )\n\n        # pylint: disable=attribute-defined-outside-init\n        self.is_fitted_ = True\n        return self\n\n    def validate_inputs(self, X, y):\n        # Things we don't want to allow until we've tested them:\n        # - Sparse inputs\n        # - Multiclass outputs (e.g., more than 2 classes in `y`)\n        # - Non-finite inputs\n        # - Complex inputs\n\n        X, y = check_X_y(X, y, accept_sparse=False, allow_nd=False)\n\n        # Throw a ValueError if X or y contains NaN or infinity.\n        assert_all_finite(X)\n        assert_all_finite(y)\n\n        if type_of_target(y) != 'binary':\n            raise ValueError(\"Non-binary targets not supported\")\n\n        if np.any(np.iscomplex(X)) or np.any(np.iscomplex(y)):\n            raise ValueError(\"Complex data not supported\")\n        if np.issubdtype(X.dtype, np.object_) or np.issubdtype(y.dtype, np.object_):\n            try:\n                X = X.astype(float)\n                y = y.astype(int)\n            except (TypeError, ValueError):\n                raise ValueError(\"argument must be a string.* number\")\n\n        return (X, y)\n\n    def predict(self, X):\n        # pylint: disable=no-member\n\n        X = check_array(X, accept_sparse=True)\n        check_is_fitted(self, 'is_fitted_')\n\n        X = torch.tensor(X, dtype=torch.float32).to(self.device)\n        predictions = np.empty(len(X), dtype=int)\n        for i, rows in enumerate(X):\n            rows = Variable(rows.view(-1, self.input_size))\n            outputs = self.network(rows)\n\n            _, predicted = torch.max(outputs.data, 1)\n            predictions[i] = int(predicted)\n        return predictions.reshape(-1, 1)\n\n    def transform(self, X):\n        return self.predict(X)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/nn/#tpot.builtin_modules.nn.PytorchClassifier.fit","title":"<code>fit(X, y)</code>","text":"<p>Generalizable method for fitting a PyTorch estimator to a training set.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>Training vector, where n_samples is the number of samples and n_features is the number of features.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>Target vector relative to X.</p> required <p>Returns:</p> Type Description <code>self</code> <p>Fitted estimator.</p> Source code in <code>tpot/builtin_modules/nn.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Generalizable method for fitting a PyTorch estimator to a training\n    set.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n    y : array-like of shape (n_samples,)\n        Target vector relative to X.\n\n    Returns\n    -------\n    self\n        Fitted estimator.\n    \"\"\"\n    # pylint: disable=no-member\n\n    self._init_model(X, y)\n\n    assert _pytorch_model_is_fully_initialized(self)\n\n    for epoch in range(self.num_epochs):\n        for i, (samples, labels) in enumerate(self.data_loader):\n            samples = samples.to(self.device)\n            labels = labels.to(self.device)\n\n            self.optimizer.zero_grad()\n            outputs = self.network(samples)\n\n            loss = self.loss_function(outputs, labels)\n            loss.backward()\n            self.optimizer.step()\n\n            if self.verbose and ((i + 1) % 100 == 0):\n                print(\n                    \"Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f\"\n                    % (\n                        epoch + 1,\n                        self.num_epochs,\n                        i + 1,\n                        self.train_dset_len // self.batch_size,\n                        loss.item(),\n                    )\n                )\n\n    # pylint: disable=attribute-defined-outside-init\n    self.is_fitted_ = True\n    return self\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/nn/#tpot.builtin_modules.nn.PytorchEstimator","title":"<code>PytorchEstimator</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Base class for Pytorch-based estimators (currently only classifiers) for use in TPOT.</p> <p>In the future, these will be merged into TPOT's main code base.</p> Source code in <code>tpot/builtin_modules/nn.py</code> <pre><code>class PytorchEstimator(BaseEstimator):\n    \"\"\"Base class for Pytorch-based estimators (currently only classifiers) for\n    use in TPOT.\n\n    In the future, these will be merged into TPOT's main code base.\n    \"\"\"\n\n    @abstractmethod\n    def fit(self, X, y): # pragma: no cover\n        pass\n\n    @abstractmethod\n    def transform(self, X): # pragma: no cover\n        pass\n\n    def predict(self, X):\n        return self.transform(X)\n\n    def fit_transform(self, X, y):\n        self.fit(X, y)\n        return self.transform(X)\n\n    def set_params(self, **parameters):\n        for parameter, value in parameters.items():\n            setattr(self, parameter, value)\n        return self\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/nn/#tpot.builtin_modules.nn.PytorchLRClassifier","title":"<code>PytorchLRClassifier</code>","text":"<p>               Bases: <code>PytorchClassifier</code></p> <p>Logistic Regression classifier, implemented in PyTorch, for use with TPOT.</p> <p>For examples on standalone use (i.e., non-TPOT) refer to: https://github.com/trang1618/tpot-nn/blob/master/tpot_nn/estimator_sandbox.py</p> Source code in <code>tpot/builtin_modules/nn.py</code> <pre><code>class PytorchLRClassifier(PytorchClassifier):\n    \"\"\"Logistic Regression classifier, implemented in PyTorch, for use with\n    TPOT.\n\n    For examples on standalone use (i.e., non-TPOT) refer to:\n    https://github.com/trang1618/tpot-nn/blob/master/tpot_nn/estimator_sandbox.py\n    \"\"\"\n\n    def __init__(\n        self,\n        num_epochs=10,\n        batch_size=16,\n        learning_rate=0.02,\n        weight_decay=1e-4,\n        verbose=False\n    ):\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.learning_rate = learning_rate\n        self.weight_decay = weight_decay\n        self.verbose = verbose\n\n        self.input_size = None\n        self.num_classes = None\n        self.network = None\n        self.loss_function = None\n        self.optimizer = None\n        self.data_loader = None\n        self.train_dset_len = None\n        self.device = None\n\n    def _init_model(self, X, y):\n        device = _get_cuda_device_if_available()\n\n        X, y = self.validate_inputs(X, y)\n\n        self.input_size = X.shape[-1]\n        self.num_classes = len(set(y))\n\n        X = torch.tensor(X, dtype=torch.float32)\n        y = torch.tensor(y, dtype=torch.long)\n\n        train_dset = TensorDataset(X, y)\n\n        # Set parameters of the network\n        self.network = _LR(self.input_size, self.num_classes).to(device)\n        self.loss_function = nn.CrossEntropyLoss()\n        self.optimizer = Adam(self.network.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n        self.data_loader = DataLoader(\n            train_dset, batch_size=self.batch_size, shuffle=True, num_workers=2\n        )\n        self.train_dset_len = len(train_dset)\n        self.device = device\n\n    def _more_tags(self):\n        return {'non_deterministic': True, 'binary_only': True}\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/nn/#tpot.builtin_modules.nn.PytorchMLPClassifier","title":"<code>PytorchMLPClassifier</code>","text":"<p>               Bases: <code>PytorchClassifier</code></p> <p>Multilayer Perceptron, implemented in PyTorch, for use with TPOT.</p> Source code in <code>tpot/builtin_modules/nn.py</code> <pre><code>class PytorchMLPClassifier(PytorchClassifier):\n    \"\"\"Multilayer Perceptron, implemented in PyTorch, for use with TPOT.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_epochs=10,\n        batch_size=8,\n        learning_rate=0.01,\n        weight_decay=0,\n        verbose=False\n    ):\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.learning_rate = learning_rate\n        self.weight_decay = weight_decay\n        self.verbose = verbose\n\n        self.input_size = None\n        self.num_classes = None\n        self.network = None\n        self.loss_function = None\n        self.optimizer = None\n        self.data_loader = None\n        self.train_dset_len = None\n        self.device = None\n\n    def _init_model(self, X, y):\n        device = _get_cuda_device_if_available()\n\n        X, y = self.validate_inputs(X, y)\n\n        self.input_size = X.shape[-1]\n        self.num_classes = len(set(y))\n\n        X = torch.tensor(X, dtype=torch.float32)\n        y = torch.tensor(y, dtype=torch.long)\n\n        train_dset = TensorDataset(X, y)\n\n        # Set parameters of the network\n        self.network = _MLP(self.input_size, self.num_classes).to(device)\n        self.loss_function = nn.CrossEntropyLoss()\n        self.optimizer = Adam(self.network.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n        self.data_loader = DataLoader(\n            train_dset, batch_size=self.batch_size, shuffle=True, num_workers=2\n        )\n        self.train_dset_len = len(train_dset)\n        self.device = device\n\n    def _more_tags(self):\n        return {'non_deterministic': True, 'binary_only': True}\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/passkbinsdiscretizer/","title":"Passkbinsdiscretizer","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/builtin_modules/passkbinsdiscretizer/#tpot.builtin_modules.passkbinsdiscretizer.PassKBinsDiscretizer","title":"<code>PassKBinsDiscretizer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot/builtin_modules/passkbinsdiscretizer.py</code> <pre><code>class PassKBinsDiscretizer(BaseEstimator, TransformerMixin):\n    def __init__(self, n_bins=5,  encode='onehot-dense', strategy='quantile', subsample=None, random_state=None):\n        self.n_bins = n_bins\n        self.encode = encode\n        self.strategy = strategy\n        self.subsample = subsample\n        self.random_state = random_state\n        \"\"\"\n        Same as sklearn.preprocessing.KBinsDiscretizer, but passes through columns that are not discretized due to having fewer than n_bins unique values instead of ignoring them.\n        See sklearn.preprocessing.KBinsDiscretizer for more information.\n        \"\"\"\n\n    def fit(self, X, y=None):\n        # Identify columns with more than n unique values\n        # Create a ColumnTransformer to select and discretize the chosen columns\n        self.selected_columns_ = select_features(X, min_unique=10)\n        if isinstance(X, pd.DataFrame):\n            self.not_selected_columns_ = [col for col in X.columns if col not in self.selected_columns_]\n        else:\n            self.not_selected_columns_ = [i for i in range(X.shape[1]) if i not in self.selected_columns_]\n\n        enc = KBinsDiscretizer(n_bins=self.n_bins, encode=self.encode, strategy=self.strategy, subsample=self.subsample, random_state=self.random_state)\n        self.transformer = ColumnTransformer([\n            ('discretizer', enc, self.selected_columns_),\n            ('passthrough', 'passthrough', self.not_selected_columns_)\n        ])\n        self.transformer.fit(X)\n        return self\n\n    def transform(self, X):\n        return self.transformer.transform(X)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/passkbinsdiscretizer/#tpot.builtin_modules.passkbinsdiscretizer.PassKBinsDiscretizer.random_state","title":"<code>random_state = random_state</code>  <code>instance-attribute</code>","text":"<p>Same as sklearn.preprocessing.KBinsDiscretizer, but passes through columns that are not discretized due to having fewer than n_bins unique values instead of ignoring them. See sklearn.preprocessing.KBinsDiscretizer for more information.</p>"},{"location":"documentation/tpot/builtin_modules/passkbinsdiscretizer/#tpot.builtin_modules.passkbinsdiscretizer.select_features","title":"<code>select_features(X, min_unique=10)</code>","text":"<p>Given a DataFrame or numpy array, return a list of column indices that have more than min_unique unique values.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>Data to select features from</p> required <code>min_unique</code> <p>Minimum number of unique values a column must have to be selected</p> <code>10</code> <p>Returns:</p> Type Description <code>list</code> <p>List of column indices that have more than min_unique unique values</p> Source code in <code>tpot/builtin_modules/passkbinsdiscretizer.py</code> <pre><code>def select_features(X, min_unique=10,):\n    \"\"\"\n    Given a DataFrame or numpy array, return a list of column indices that have more than min_unique unique values.\n\n    Parameters\n    ----------\n    X: DataFrame or numpy array\n        Data to select features from\n    min_unique: int, default=10\n        Minimum number of unique values a column must have to be selected\n\n    Returns\n    -------\n    list\n        List of column indices that have more than min_unique unique values\n\n    \"\"\"\n\n    if isinstance(X, pd.DataFrame):\n        return [col for col in X.columns if len(X[col].unique()) &gt; min_unique]\n    else:\n        return [i for i in range(X.shape[1]) if len(np.unique(X[:, i])) &gt; min_unique]\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/passthrough/","title":"Passthrough","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/builtin_modules/passthrough/#tpot.builtin_modules.passthrough.Passthrough","title":"<code>Passthrough</code>","text":"<p>               Bases: <code>TransformerMixin</code>, <code>BaseEstimator</code></p> <p>A transformer that does nothing. It just passes the input array as is.</p> Source code in <code>tpot/builtin_modules/passthrough.py</code> <pre><code>class Passthrough(TransformerMixin,BaseEstimator):\n    \"\"\"\n    A transformer that does nothing. It just passes the input array as is.\n    \"\"\"\n\n    def fit(self, X=None, y=None):\n        \"\"\"\n        Nothing to fit, just returns self.\n        \"\"\"\n        return self\n\n    def transform(self, X):\n        \"\"\"\n        returns the input array as is.\n        \"\"\"\n        return X\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/passthrough/#tpot.builtin_modules.passthrough.Passthrough.fit","title":"<code>fit(X=None, y=None)</code>","text":"<p>Nothing to fit, just returns self.</p> Source code in <code>tpot/builtin_modules/passthrough.py</code> <pre><code>def fit(self, X=None, y=None):\n    \"\"\"\n    Nothing to fit, just returns self.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/passthrough/#tpot.builtin_modules.passthrough.Passthrough.transform","title":"<code>transform(X)</code>","text":"<p>returns the input array as is.</p> Source code in <code>tpot/builtin_modules/passthrough.py</code> <pre><code>def transform(self, X):\n    \"\"\"\n    returns the input array as is.\n    \"\"\"\n    return X\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/passthrough/#tpot.builtin_modules.passthrough.SkipTransformer","title":"<code>SkipTransformer</code>","text":"<p>               Bases: <code>TransformerMixin</code>, <code>BaseEstimator</code></p> <p>A transformer returns an empty array. When combined with FeatureUnion, it can be used to skip a branch.</p> Source code in <code>tpot/builtin_modules/passthrough.py</code> <pre><code>class SkipTransformer(TransformerMixin,BaseEstimator):\n    \"\"\"\n    A transformer returns an empty array. When combined with FeatureUnion, it can be used to skip a branch.\n    \"\"\"\n    def fit(self, X=None, y=None):\n        \"\"\"\n        Nothing to fit, just returns self.\n        \"\"\"\n        return self\n\n    def transform(self, X):\n        \"\"\"\n        returns an empty array.\n        \"\"\"\n        return np.array([]).reshape(X.shape[0],0)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/passthrough/#tpot.builtin_modules.passthrough.SkipTransformer.fit","title":"<code>fit(X=None, y=None)</code>","text":"<p>Nothing to fit, just returns self.</p> Source code in <code>tpot/builtin_modules/passthrough.py</code> <pre><code>def fit(self, X=None, y=None):\n    \"\"\"\n    Nothing to fit, just returns self.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/passthrough/#tpot.builtin_modules.passthrough.SkipTransformer.transform","title":"<code>transform(X)</code>","text":"<p>returns an empty array.</p> Source code in <code>tpot/builtin_modules/passthrough.py</code> <pre><code>def transform(self, X):\n    \"\"\"\n    returns an empty array.\n    \"\"\"\n    return np.array([]).reshape(X.shape[0],0)\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/zero_count/","title":"Zero count","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/builtin_modules/zero_count/#tpot.builtin_modules.zero_count.ZeroCount","title":"<code>ZeroCount</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Adds the count of zeros and count of non-zeros per sample as features.</p> Source code in <code>tpot/builtin_modules/zero_count.py</code> <pre><code>class ZeroCount(BaseEstimator, TransformerMixin):\n    \"\"\"Adds the count of zeros and count of non-zeros per sample as features.\"\"\"\n\n    def fit(self, X, y=None):\n        \"\"\"Dummy function to fit in with the sklearn API.\"\"\"\n        return self\n\n    def transform(self, X, y=None):\n        \"\"\"Transform data by adding two virtual features.\n\n        Parameters\n        ----------\n        X: numpy ndarray, {n_samples, n_components}\n            New data, where n_samples is the number of samples and n_components\n            is the number of components.\n        y: None\n            Unused\n\n        Returns\n        -------\n        X_transformed: array-like, shape (n_samples, n_features)\n            The transformed feature set\n        \"\"\"\n        X = check_array(X)\n        n_features = X.shape[1]\n\n        X_transformed = np.copy(X)\n\n        non_zero_vector = np.count_nonzero(X_transformed, axis=1)\n        non_zero = np.reshape(non_zero_vector, (-1, 1))\n        zero_col = np.reshape(n_features - non_zero_vector, (-1, 1))\n\n        X_transformed = np.hstack((non_zero, X_transformed))\n        X_transformed = np.hstack((zero_col, X_transformed))\n\n        return X_transformed\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/zero_count/#tpot.builtin_modules.zero_count.ZeroCount.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Dummy function to fit in with the sklearn API.</p> Source code in <code>tpot/builtin_modules/zero_count.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Dummy function to fit in with the sklearn API.\"\"\"\n    return self\n</code></pre>"},{"location":"documentation/tpot/builtin_modules/zero_count/#tpot.builtin_modules.zero_count.ZeroCount.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Transform data by adding two virtual features.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>New data, where n_samples is the number of samples and n_components is the number of components.</p> required <code>y</code> <p>Unused</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_transformed</code> <code>(array - like, shape(n_samples, n_features))</code> <p>The transformed feature set</p> Source code in <code>tpot/builtin_modules/zero_count.py</code> <pre><code>def transform(self, X, y=None):\n    \"\"\"Transform data by adding two virtual features.\n\n    Parameters\n    ----------\n    X: numpy ndarray, {n_samples, n_components}\n        New data, where n_samples is the number of samples and n_components\n        is the number of components.\n    y: None\n        Unused\n\n    Returns\n    -------\n    X_transformed: array-like, shape (n_samples, n_features)\n        The transformed feature set\n    \"\"\"\n    X = check_array(X)\n    n_features = X.shape[1]\n\n    X_transformed = np.copy(X)\n\n    non_zero_vector = np.count_nonzero(X_transformed, axis=1)\n    non_zero = np.reshape(non_zero_vector, (-1, 1))\n    zero_col = np.reshape(n_features - non_zero_vector, (-1, 1))\n\n    X_transformed = np.hstack((non_zero, X_transformed))\n    X_transformed = np.hstack((zero_col, X_transformed))\n\n    return X_transformed\n</code></pre>"},{"location":"documentation/tpot/config/autoqtl_builtins/","title":"Autoqtl builtins","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/config/classifiers/","title":"Classifiers","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/config/classifiers_sklearnex/","title":"Classifiers sklearnex","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/config/get_configspace/","title":"Get configspace","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/config/get_configspace/#tpot.config.get_configspace.get_configspace","title":"<code>get_configspace(name, n_classes=3, n_samples=1000, n_features=100, random_state=None, n_jobs=1)</code>","text":"<p>This function returns the ConfigSpace.ConfigurationSpace with the hyperparameter ranges for the given scikit-learn method. It also uses the n_classes, n_samples, n_features, and random_state to set the hyperparameters that depend on these values.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The str name of the scikit-learn method for which to create the ConfigurationSpace. (e.g. 'RandomForestClassifier' for sklearn.ensemble.RandomForestClassifier)</p> required <code>n_classes</code> <code>int</code> <p>The number of classes in the target variable. Default is 3.</p> <code>3</code> <code>n_samples</code> <code>int</code> <p>The number of samples in the dataset. Default is 1000.</p> <code>1000</code> <code>n_features</code> <code>int</code> <p>The number of features in the dataset. Default is 100.</p> <code>100</code> <code>random_state</code> <code>int</code> <p>The random_state to use in the ConfigurationSpace. Default is None. If None, the random_state hyperparameter is not included in the ConfigurationSpace. Use this to set the random state for the individual methods if you want to ensure reproducibility.</p> <code>None</code> <code>n_jobs</code> <code>int(default=1)</code> <p>Sets the n_jobs parameter for estimators that have it. Default is 1.</p> <code>1</code> Source code in <code>tpot/config/get_configspace.py</code> <pre><code>def get_configspace(name, n_classes=3, n_samples=1000, n_features=100, random_state=None, n_jobs=1):\n    \"\"\"\n    This function returns the ConfigSpace.ConfigurationSpace with the hyperparameter ranges for the given\n    scikit-learn method. It also uses the n_classes, n_samples, n_features, and random_state to set the\n    hyperparameters that depend on these values.\n\n    Parameters\n    ----------\n    name : str\n        The str name of the scikit-learn method for which to create the ConfigurationSpace. (e.g. 'RandomForestClassifier' for sklearn.ensemble.RandomForestClassifier)\n    n_classes : int\n        The number of classes in the target variable. Default is 3.\n    n_samples : int\n        The number of samples in the dataset. Default is 1000.\n    n_features : int\n        The number of features in the dataset. Default is 100.\n    random_state : int\n        The random_state to use in the ConfigurationSpace. Default is None.\n        If None, the random_state hyperparameter is not included in the ConfigurationSpace.\n        Use this to set the random state for the individual methods if you want to ensure reproducibility.\n    n_jobs : int (default=1)\n        Sets the n_jobs parameter for estimators that have it. Default is 1.\n\n    \"\"\"\n    match name:\n\n        #autoqtl_builtins.py\n        case \"FeatureEncodingFrequencySelector\":\n            return autoqtl_builtins.FeatureEncodingFrequencySelector_ConfigurationSpace\n        case \"DominantEncoder\":\n            return {}\n        case \"RecessiveEncoder\":\n            return {}\n        case \"HeterosisEncoder\":\n            return {}\n        case \"UnderDominanceEncoder\":\n            return {}\n        case \"OverDominanceEncoder\":\n            return {}\n\n        case \"Passthrough\":\n            return {}\n        case \"SkipTransformer\":\n            return {}\n\n        #classifiers.py\n        case \"LinearDiscriminantAnalysis\":\n            return classifiers.get_LinearDiscriminantAnalysis_ConfigurationSpace()\n        case \"AdaBoostClassifier\":\n            return classifiers.get_AdaBoostClassifier_ConfigurationSpace(random_state=random_state)\n        case \"LogisticRegression\":\n            return classifiers.get_LogisticRegression_ConfigurationSpace(random_state=random_state, n_jobs=n_jobs)\n        case \"KNeighborsClassifier\":\n            return classifiers.get_KNeighborsClassifier_ConfigurationSpace(n_samples=n_samples, n_jobs=n_jobs)\n        case \"DecisionTreeClassifier\":\n            return classifiers.get_DecisionTreeClassifier_ConfigurationSpace(n_featues=n_features, random_state=random_state)\n        case \"SVC\":\n            return classifiers.get_SVC_ConfigurationSpace(random_state=random_state)\n        case \"LinearSVC\":\n            return classifiers.get_LinearSVC_ConfigurationSpace(random_state=random_state)\n        case \"RandomForestClassifier\":\n            return classifiers.get_RandomForestClassifier_ConfigurationSpace(random_state=random_state, n_jobs=n_jobs)\n        case \"GradientBoostingClassifier\":\n            return classifiers.get_GradientBoostingClassifier_ConfigurationSpace(n_classes=n_classes, random_state=random_state)\n        case \"HistGradientBoostingClassifier\":\n            return classifiers.get_HistGradientBoostingClassifier_ConfigurationSpace(random_state=random_state)\n        case \"XGBClassifier\":\n            return classifiers.get_XGBClassifier_ConfigurationSpace(random_state=random_state, n_jobs=n_jobs)\n        case \"LGBMClassifier\":\n            return classifiers.get_LGBMClassifier_ConfigurationSpace(random_state=random_state, n_jobs=n_jobs)\n        case \"ExtraTreesClassifier\":\n            return classifiers.get_ExtraTreesClassifier_ConfigurationSpace(random_state=random_state, n_jobs=n_jobs)\n        case \"SGDClassifier\":\n            return classifiers.get_SGDClassifier_ConfigurationSpace(random_state=random_state, n_jobs=n_jobs)\n        case \"MLPClassifier\":\n            return classifiers.get_MLPClassifier_ConfigurationSpace(random_state=random_state)\n        case \"BernoulliNB\":\n            return classifiers.get_BernoulliNB_ConfigurationSpace()\n        case \"MultinomialNB\":\n            return classifiers.get_MultinomialNB_ConfigurationSpace()\n        case \"GaussianNB\":\n            return {}\n        case \"LassoLarsCV\":\n            return {}\n        case \"ElasticNetCV\":\n            return regressors.ElasticNetCV_configspace\n        case \"RidgeCV\":\n            return {}\n        case \"PassiveAggressiveClassifier\":\n            return classifiers.get_PassiveAggressiveClassifier_ConfigurationSpace(random_state=random_state)\n        case \"QuadraticDiscriminantAnalysis\":\n            return classifiers.get_QuadraticDiscriminantAnalysis_ConfigurationSpace()\n        case \"GaussianProcessClassifier\":\n            return classifiers.get_GaussianProcessClassifier_ConfigurationSpace(n_features=n_features, random_state=random_state)\n        case \"BaggingClassifier\":\n            return classifiers.get_BaggingClassifier_ConfigurationSpace(random_state=random_state, n_jobs=n_jobs)\n\n        #regressors.py\n        case \"RandomForestRegressor\":\n            return regressors.get_RandomForestRegressor_ConfigurationSpace(random_state=random_state, n_jobs=n_jobs)\n        case \"SGDRegressor\":\n            return regressors.get_SGDRegressor_ConfigurationSpace(random_state=random_state)\n        case \"Ridge\":\n            return regressors.get_Ridge_ConfigurationSpace(random_state=random_state)\n        case \"Lasso\":\n            return regressors.get_Lasso_ConfigurationSpace(random_state=random_state)\n        case \"ElasticNet\":\n            return regressors.get_ElasticNet_ConfigurationSpace(random_state=random_state)\n        case \"Lars\":\n            return regressors.get_Lars_ConfigurationSpace(random_state=random_state)\n        case \"OthogonalMatchingPursuit\":\n            return regressors.get_OthogonalMatchingPursuit_ConfigurationSpace()\n        case \"BayesianRidge\":\n            return regressors.get_BayesianRidge_ConfigurationSpace()\n        case \"LassoLars\":\n            return regressors.get_LassoLars_ConfigurationSpace(random_state=random_state)\n        case \"BaggingRegressor\":\n            return regressors.get_BaggingRegressor_ConfigurationSpace(random_state=random_state)\n        case \"ARDRegression\":\n            return regressors.get_ARDRegression_ConfigurationSpace()\n        case \"TheilSenRegressor\":\n            return regressors.get_TheilSenRegressor_ConfigurationSpace(random_state=random_state)\n        case \"Perceptron\":\n            return regressors.get_Perceptron_ConfigurationSpace(random_state=random_state)\n        case \"DecisionTreeRegressor\":\n            return regressors.get_DecisionTreeRegressor_ConfigurationSpace(random_state=random_state)\n        case \"LinearSVR\":\n            return regressors.get_LinearSVR_ConfigurationSpace(random_state=random_state)\n        case \"SVR\":\n            return regressors.get_SVR_ConfigurationSpace()\n        case \"XGBRegressor\":\n            return regressors.get_XGBRegressor_ConfigurationSpace(random_state=random_state, n_jobs=n_jobs)\n        case \"AdaBoostRegressor\":\n            return regressors.get_AdaBoostRegressor_ConfigurationSpace(random_state=random_state)\n        case \"ExtraTreesRegressor\":\n            return regressors.get_ExtraTreesRegressor_ConfigurationSpace(random_state=random_state, n_jobs=n_jobs)\n        case \"GradientBoostingRegressor\":\n            return regressors.get_GradientBoostingRegressor_ConfigurationSpace(random_state=random_state)\n        case \"HistGradientBoostingRegressor\":\n            return regressors.get_HistGradientBoostingRegressor_ConfigurationSpace(random_state=random_state)\n        case \"MLPRegressor\":\n            return regressors.get_MLPRegressor_ConfigurationSpace(random_state=random_state)\n        case \"KNeighborsRegressor\":\n            return regressors.get_KNeighborsRegressor_ConfigurationSpace(n_samples=n_samples, n_jobs=n_jobs)\n        case \"GaussianProcessRegressor\":\n            return regressors.get_GaussianProcessRegressor_ConfigurationSpace(n_features=n_features, random_state=random_state)\n        case \"LGBMRegressor\":\n            return regressors.get_LGBMRegressor_ConfigurationSpace(random_state=random_state, n_jobs=n_jobs)\n        case \"BaggingRegressor\":\n            return regressors.get_BaggingRegressor_ConfigurationSpace(random_state=random_state, n_jobs=n_jobs)\n\n        #transformers.py\n        case \"Binarizer\":\n            return transformers.Binarizer_configspace\n        case \"Normalizer\":\n            return transformers.Normalizer_configspace\n        case \"PCA\":\n            return transformers.PCA_configspace\n        case \"ZeroCount\":\n            return transformers.ZeroCount_configspace\n        case \"FastICA\":\n            return transformers.get_FastICA_configspace(n_features=n_features, random_state=random_state)\n        case \"FeatureAgglomeration\":\n            return transformers.get_FeatureAgglomeration_configspace(n_features=n_features)\n        case \"Nystroem\":\n            return transformers.get_Nystroem_configspace(n_features=n_features, random_state=random_state)\n        case \"RBFSampler\":\n            return transformers.get_RBFSampler_configspace(n_features=n_features, random_state=random_state)\n        case \"MinMaxScaler\":\n            return {}\n        case \"PowerTransformer\":\n            return {}\n        case \"QuantileTransformer\":\n            return transformers.get_QuantileTransformer_configspace(n_samples=n_samples, random_state=random_state)\n        case \"RobustScaler\":\n            return transformers.RobustScaler_configspace\n        case \"MaxAbsScaler\":\n            return {}\n        case \"PolynomialFeatures\":\n            return transformers.PolynomialFeatures_configspace\n        case \"StandardScaler\":\n            return {}\n        case \"PassKBinsDiscretizer\":\n            return transformers.get_passkbinsdiscretizer_configspace(random_state=random_state)\n        case \"KBinsDiscretizer\":\n            return transformers.get_passkbinsdiscretizer_configspace(random_state=random_state)\n        case \"ColumnOneHotEncoder\":\n            return {}\n        case \"ColumnOrdinalEncoder\":\n            return {}\n\n        #selectors.py\n        case \"SelectFwe\":\n            return selectors.SelectFwe_configspace \n        case \"SelectPercentile\":\n            return selectors.SelectPercentile_configspace\n        case \"VarianceThreshold\":\n            return selectors.VarianceThreshold_configspace\n        case \"RFE\":\n            return selectors.RFE_configspace_part\n        case \"SelectFromModel\":\n            return selectors.SelectFromModel_configspace_part\n\n\n        #special_configs.py\n        case \"AddTransformer\":\n            return {}\n        case \"mul_neg_1_Transformer\":\n            return {}\n        case \"MulTransformer\":\n            return {}\n        case \"SafeReciprocalTransformer\":\n            return {}\n        case \"EQTransformer\":\n            return {}\n        case \"NETransformer\":\n            return {}\n        case \"GETransformer\":\n            return {}\n        case \"GTTransformer\":\n            return {}\n        case \"LETransformer\":\n            return {}\n        case \"LTTransformer\":\n            return {}        \n        case \"MinTransformer\":\n            return {}\n        case \"MaxTransformer\":\n            return {}\n        case \"ZeroTransformer\":\n            return {}\n        case \"OneTransformer\":\n            return {}\n        case \"NTransformer\":\n            return ConfigurationSpace(\n\n                space = {\n\n                    'n': Float(\"n\", bounds=(-1e2, 1e2)),\n                }\n            ) \n\n        #imputers.py\n        case \"SimpleImputer\":\n            return imputers.simple_imputer_cs\n        case \"IterativeImputer\":\n            return imputers.get_IterativeImputer_config_space(n_features=n_features, random_state=random_state)\n        case \"IterativeImputer_no_estimator\":\n            return imputers.get_IterativeImputer_config_space_no_estimator(n_features=n_features, random_state=random_state)\n\n        case \"KNNImputer\":\n            return imputers.get_KNNImputer_config_space(n_samples=n_samples)\n\n        #mdr_configs.py\n        case \"MDR\":\n            return mdr_configs.MDR_configspace\n        case \"ContinuousMDR\":\n            return mdr_configs.MDR_configspace\n        case \"ReliefF\":\n            return mdr_configs.get_skrebate_ReliefF_config_space(n_features=n_features)\n        case \"SURF\":\n            return mdr_configs.get_skrebate_SURF_config_space(n_features=n_features)\n        case \"SURFstar\":\n            return mdr_configs.get_skrebate_SURFstar_config_space(n_features=n_features)\n        case \"MultiSURF\":\n            return mdr_configs.get_skrebate_MultiSURF_config_space(n_features=n_features)\n\n        #classifiers_sklearnex.py\n        case \"RandomForestClassifier_sklearnex\":\n            return classifiers_sklearnex.get_RandomForestClassifier_ConfigurationSpace(random_state=random_state, n_jobs=n_jobs)\n        case \"LogisticRegression_sklearnex\":\n            return classifiers_sklearnex.get_LogisticRegression_ConfigurationSpace(random_state=random_state)\n        case \"KNeighborsClassifier_sklearnex\":\n            return classifiers_sklearnex.get_KNeighborsClassifier_ConfigurationSpace(n_samples=n_samples)\n        case \"SVC_sklearnex\":\n            return classifiers_sklearnex.get_SVC_ConfigurationSpace(random_state=random_state)\n        case \"NuSVC_sklearnex\":\n            return classifiers_sklearnex.get_NuSVC_ConfigurationSpace(random_state=random_state)\n\n        #regressors_sklearnex.py\n        case \"LinearRegression_sklearnex\":\n            return {}\n        case \"Ridge_sklearnex\":\n            return regressors_sklearnex.get_Ridge_ConfigurationSpace(random_state=random_state)\n        case \"Lasso_sklearnex\":\n            return regressors_sklearnex.get_Lasso_ConfigurationSpace(random_state=random_state)\n        case \"ElasticNet_sklearnex\":\n            return regressors_sklearnex.get_ElasticNet_ConfigurationSpace(random_state=random_state)\n        case \"SVR_sklearnex\":\n            return regressors_sklearnex.get_SVR_ConfigurationSpace(random_state=random_state)\n        case \"NuSVR_sklearnex\":\n            return regressors_sklearnex.get_NuSVR_ConfigurationSpace(random_state=random_state)\n        case \"RandomForestRegressor_sklearnex\":\n            return regressors_sklearnex.get_RandomForestRegressor_ConfigurationSpace(random_state=random_state)\n        case \"KNeighborsRegressor_sklearnex\":\n            return regressors_sklearnex.get_KNeighborsRegressor_ConfigurationSpace(n_samples=n_samples)\n\n    #raise error\n    raise ValueError(f\"Could not find configspace for {name}\")\n</code></pre>"},{"location":"documentation/tpot/config/get_configspace/#tpot.config.get_configspace.get_node","title":"<code>get_node(name, n_classes=3, n_samples=100, n_features=100, random_state=None, base_node=EstimatorNode, n_jobs=1)</code>","text":"<p>Helper function for get_search_space. Returns a single EstimatorNode for the given scikit-learn method. Also includes special cases for nodes that require custom parsing of the hyperparameters or methods that wrap other methods.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str or list</code> <p>The name of the scikit-learn method or group of methods for which to create the search space. - str: The name of the scikit-learn method. (e.g. 'RandomForestClassifier' for sklearn.ensemble.RandomForestClassifier) Alternatively, the name of a group of methods. (e.g. 'classifiers' for all classifiers). - list: A list of scikit-learn method names. (e.g. ['RandomForestClassifier', 'ExtraTreesClassifier'])</p> required <code>n_classes</code> <code>int(default=3)</code> <p>The number of classes in the target variable.</p> <code>3</code> <code>n_samples</code> <code>int(default=1000)</code> <p>The number of samples in the dataset.</p> <code>100</code> <code>n_features</code> <code>int(default=100)</code> <p>The number of features in the dataset.</p> <code>100</code> <code>random_state</code> <code>int(default=None)</code> <p>A fixed random_state to pass through to all methods that have a random_state hyperparameter.</p> <code>None</code> <code>return_choice_pipeline</code> <code>bool(default=True)</code> <p>If False, returns a list of TPOT.search_spaces.nodes.EstimatorNode objects. If True, returns a single TPOT.search_spaces.pipelines.ChoicePipeline that includes and samples from all EstimatorNodes.</p> required <code>base_node</code> <p>The SearchSpace to pass the configuration space to. If you want to experiment with custom mutation/crossover operators, you can pass a custom SearchSpace node here.</p> <code>EstimatorNode</code> <code>n_jobs</code> <code>int(default=1)</code> <p>Sets the n_jobs parameter for estimators that have it. Default is 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>    Returns an SearchSpace object that can be optimized by TPOT.</code> <ul> <li>TPOT.search_spaces.nodes.EstimatorNode (or base_node).</li> <li>TPOT.search_spaces.pipelines.WrapperPipeline object if the method requires a wrapped estimator.</li> </ul> Source code in <code>tpot/config/get_configspace.py</code> <pre><code>def get_node(name, n_classes=3, n_samples=100, n_features=100, random_state=None, base_node=EstimatorNode, n_jobs=1):\n    \"\"\"\n    Helper function for get_search_space. Returns a single EstimatorNode for the given scikit-learn method. Also includes special cases for nodes that require custom parsing of the hyperparameters or methods that wrap other methods.\n\n    Parameters\n    ----------\n\n    name : str or list\n        The name of the scikit-learn method or group of methods for which to create the search space.\n        - str: The name of the scikit-learn method. (e.g. 'RandomForestClassifier' for sklearn.ensemble.RandomForestClassifier)\n        Alternatively, the name of a group of methods. (e.g. 'classifiers' for all classifiers).\n        - list: A list of scikit-learn method names. (e.g. ['RandomForestClassifier', 'ExtraTreesClassifier'])\n    n_classes : int (default=3)\n        The number of classes in the target variable.\n    n_samples : int (default=1000)\n        The number of samples in the dataset.\n    n_features : int (default=100)\n        The number of features in the dataset.\n    random_state : int (default=None)\n        A fixed random_state to pass through to all methods that have a random_state hyperparameter. \n    return_choice_pipeline : bool (default=True)\n        If False, returns a list of TPOT.search_spaces.nodes.EstimatorNode objects.\n        If True, returns a single TPOT.search_spaces.pipelines.ChoicePipeline that includes and samples from all EstimatorNodes.\n    base_node: TPOT.search_spaces.base.SearchSpace (default=TPOT.search_spaces.nodes.EstimatorNode)\n        The SearchSpace to pass the configuration space to. If you want to experiment with custom mutation/crossover operators, you can pass a custom SearchSpace node here.\n    n_jobs : int (default=1)\n        Sets the n_jobs parameter for estimators that have it. Default is 1.\n\n    Returns\n    -------\n        Returns an SearchSpace object that can be optimized by TPOT.\n        - TPOT.search_spaces.nodes.EstimatorNode (or base_node).\n        - TPOT.search_spaces.pipelines.WrapperPipeline object if the method requires a wrapped estimator.\n\n\n    \"\"\"\n\n    if name == \"LinearSVC_wrapped\":\n        ext = get_node(\"LinearSVC\", n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return WrapperPipeline(estimator_search_space=ext, method=sklearn.calibration.CalibratedClassifierCV, space={})\n    if name == \"RFE_classification\":\n        rfe_sp = get_configspace(name=\"RFE\", n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        ext = get_node(\"ExtraTreesClassifier\", n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return WrapperPipeline(estimator_search_space=ext, method=RFE, space=rfe_sp)\n    if name == \"RFE_regression\":\n        rfe_sp = get_configspace(name=\"RFE\", n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        ext = get_node(\"ExtraTreesRegressor\", n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return WrapperPipeline(estimator_search_space=ext, method=RFE, space=rfe_sp)\n    if name == \"SelectFromModel_classification\":\n        sfm_sp = get_configspace(name=\"SelectFromModel\", n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        ext = get_node(\"ExtraTreesClassifier\", n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return WrapperPipeline(estimator_search_space=ext, method=SelectFromModel, space=sfm_sp)\n    if name == \"SelectFromModel_regression\":\n        sfm_sp = get_configspace(name=\"SelectFromModel\", n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        ext = get_node(\"ExtraTreesRegressor\", n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return WrapperPipeline(estimator_search_space=ext, method=SelectFromModel, space=sfm_sp)\n    # TODO Add IterativeImputer with more estimator methods\n    if name == \"IterativeImputer_learned_estimators\":\n        iteative_sp = get_configspace(name=\"IterativeImputer_no_estimator\", n_features=n_features, random_state=random_state, n_jobs=n_jobs)\n        regressor_searchspace = get_node(\"ExtraTreesRegressor\", n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return WrapperPipeline(estimator_search_space=regressor_searchspace, method=IterativeImputer, space=iteative_sp)\n\n    #these are nodes that have special search spaces which require custom parsing of the hyperparameters\n    if name == \"IterativeImputer\":\n        configspace = get_configspace(name, n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return EstimatorNode(STRING_TO_CLASS[name], configspace, hyperparameter_parser=imputers.IterativeImputer_hyperparameter_parser)\n    if name == \"RobustScaler\":\n        configspace = get_configspace(name, n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return base_node(STRING_TO_CLASS[name], configspace, hyperparameter_parser=transformers.robust_scaler_hyperparameter_parser)\n    if name == \"GradientBoostingClassifier\":\n        configspace = get_configspace(name, n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return base_node(STRING_TO_CLASS[name], configspace, hyperparameter_parser=classifiers.GradientBoostingClassifier_hyperparameter_parser)\n    if name == \"HistGradientBoostingClassifier\":\n        configspace = get_configspace(name, n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return base_node(STRING_TO_CLASS[name], configspace, hyperparameter_parser=classifiers.HistGradientBoostingClassifier_hyperparameter_parser)\n    if name == \"GradientBoostingRegressor\":\n        configspace = get_configspace(name, n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return base_node(STRING_TO_CLASS[name], configspace, hyperparameter_parser=regressors.GradientBoostingRegressor_hyperparameter_parser)\n    if  name == \"HistGradientBoostingRegressor\":\n        configspace = get_configspace(name, n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return base_node(STRING_TO_CLASS[name], configspace, hyperparameter_parser=regressors.HistGradientBoostingRegressor_hyperparameter_parser)\n    if name == \"MLPClassifier\":\n        configspace = get_configspace(name, n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return base_node(STRING_TO_CLASS[name], configspace, hyperparameter_parser=classifiers.MLPClassifier_hyperparameter_parser)\n    if name == \"MLPRegressor\":\n        configspace = get_configspace(name, n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return base_node(STRING_TO_CLASS[name], configspace, hyperparameter_parser=regressors.MLPRegressor_hyperparameter_parser)\n    if name == \"GaussianProcessRegressor\":\n        configspace = get_configspace(name, n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return base_node(STRING_TO_CLASS[name], configspace, hyperparameter_parser=regressors.GaussianProcessRegressor_hyperparameter_parser)\n    if name == \"GaussianProcessClassifier\":\n        configspace = get_configspace(name, n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return base_node(STRING_TO_CLASS[name], configspace, hyperparameter_parser=classifiers.GaussianProcessClassifier_hyperparameter_parser)\n    if name == \"FeatureAgglomeration\":\n        configspace = get_configspace(name, n_classes=n_classes, n_samples=n_samples, random_state=random_state, n_jobs=n_jobs)\n        return base_node(STRING_TO_CLASS[name], configspace, hyperparameter_parser=transformers.FeatureAgglomeration_hyperparameter_parser)\n\n    configspace = get_configspace(name, n_classes=n_classes, n_samples=n_samples, n_features=n_features, random_state=random_state, n_jobs=n_jobs)\n    if configspace is None:\n        #raise warning\n        warnings.warn(f\"Could not find configspace for {name}\")\n        return None\n\n    return base_node(STRING_TO_CLASS[name], configspace)\n</code></pre>"},{"location":"documentation/tpot/config/get_configspace/#tpot.config.get_configspace.get_search_space","title":"<code>get_search_space(name, n_classes=3, n_samples=1000, n_features=100, random_state=None, return_choice_pipeline=True, base_node=EstimatorNode, n_jobs=1)</code>","text":"<p>Returns a TPOT search space for a given scikit-learn method or group of methods.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str or list</code> <p>The name of the scikit-learn method or group of methods for which to create the search space. - str: The name of the scikit-learn method. (e.g. 'RandomForestClassifier' for sklearn.ensemble.RandomForestClassifier) Alternatively, the name of a group of methods. (e.g. 'classifiers' for all classifiers). - list: A list of scikit-learn method names. (e.g. ['RandomForestClassifier', 'ExtraTreesClassifier'])</p> required <code>n_classes</code> <code>int(default=3)</code> <p>The number of classes in the target variable.</p> <code>3</code> <code>n_samples</code> <code>int(default=1000)</code> <p>The number of samples in the dataset.</p> <code>1000</code> <code>n_features</code> <code>int(default=100)</code> <p>The number of features in the dataset.</p> <code>100</code> <code>random_state</code> <code>int(default=None)</code> <p>A fixed random_state to pass through to all methods that have a random_state hyperparameter.</p> <code>None</code> <code>return_choice_pipeline</code> <code>bool(default=True)</code> <p>If False, returns a list of TPOT.search_spaces.nodes.EstimatorNode objects. If True, returns a single TPOT.search_spaces.pipelines.ChoicePipeline that includes and samples from all EstimatorNodes.</p> <code>True</code> <code>base_node</code> <p>The SearchSpace to pass the configuration space to. If you want to experiment with custom mutation/crossover operators, you can pass a custom SearchSpace node here.</p> <code>EstimatorNode</code> <code>n_jobs</code> <code>int(default=1)</code> <p>Sets the n_jobs parameter for estimators that have it. Default is 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>    Returns an SearchSpace object that can be optimized by TPOT.</code> <ul> <li>TPOT.search_spaces.nodes.EstimatorNode (or base_node) if there is only one search space.</li> <li>List of TPOT.search_spaces.nodes.EstimatorNode (or base_node) objects if there are multiple search spaces.</li> <li>TPOT.search_spaces.pipelines.ChoicePipeline object if return_choice_pipeline is True. Note: for some special cases with methods using wrapped estimators, the returned search space is a TPOT.search_spaces.pipelines.WrapperPipeline object.</li> </ul> Source code in <code>tpot/config/get_configspace.py</code> <pre><code>def get_search_space(name, n_classes=3, n_samples=1000, n_features=100, random_state=None, return_choice_pipeline=True, base_node=EstimatorNode, n_jobs=1):\n    \"\"\"\n    Returns a TPOT search space for a given scikit-learn method or group of methods.\n\n    Parameters\n    ----------\n    name : str or list\n        The name of the scikit-learn method or group of methods for which to create the search space.\n        - str: The name of the scikit-learn method. (e.g. 'RandomForestClassifier' for sklearn.ensemble.RandomForestClassifier)\n        Alternatively, the name of a group of methods. (e.g. 'classifiers' for all classifiers).\n        - list: A list of scikit-learn method names. (e.g. ['RandomForestClassifier', 'ExtraTreesClassifier'])\n    n_classes : int (default=3)\n        The number of classes in the target variable.\n    n_samples : int (default=1000)\n        The number of samples in the dataset.\n    n_features : int (default=100)\n        The number of features in the dataset.\n    random_state : int (default=None)\n        A fixed random_state to pass through to all methods that have a random_state hyperparameter. \n    return_choice_pipeline : bool (default=True)\n        If False, returns a list of TPOT.search_spaces.nodes.EstimatorNode objects.\n        If True, returns a single TPOT.search_spaces.pipelines.ChoicePipeline that includes and samples from all EstimatorNodes.\n    base_node: TPOT.search_spaces.base.SearchSpace (default=TPOT.search_spaces.nodes.EstimatorNode)\n        The SearchSpace to pass the configuration space to. If you want to experiment with custom mutation/crossover operators, you can pass a custom SearchSpace node here.\n    n_jobs : int (default=1)\n        Sets the n_jobs parameter for estimators that have it. Default is 1.\n\n    Returns\n    -------\n        Returns an SearchSpace object that can be optimized by TPOT.\n        - TPOT.search_spaces.nodes.EstimatorNode (or base_node) if there is only one search space.\n        - List of TPOT.search_spaces.nodes.EstimatorNode (or base_node) objects if there are multiple search spaces.\n        - TPOT.search_spaces.pipelines.ChoicePipeline object if return_choice_pipeline is True.\n        Note: for some special cases with methods using wrapped estimators, the returned search space is a TPOT.search_spaces.pipelines.WrapperPipeline object.\n\n    \"\"\"\n    name = flatten_group_names(name)\n\n    #if list of names, return a list of EstimatorNodes\n    if isinstance(name, list) or isinstance(name, np.ndarray):\n        search_spaces = [get_search_space(n, n_classes=n_classes, n_samples=n_samples, n_features=n_features, random_state=random_state, return_choice_pipeline=False, base_node=base_node, n_jobs=n_jobs) for n in name]\n        #remove Nones\n        search_spaces = [s for s in search_spaces if s is not None]\n\n        if return_choice_pipeline:\n            return ChoicePipeline(search_spaces=np.hstack(search_spaces))\n        else:\n            return np.hstack(search_spaces)\n\n    # if name in GROUPNAMES:\n    #     name_list = GROUPNAMES[name]\n    #     return get_search_space(name_list, n_classes=n_classes, n_samples=n_samples, n_features=n_features, random_state=random_state, return_choice_pipeline=return_choice_pipeline, base_node=base_node)\n\n    return get_node(name, n_classes=n_classes, n_samples=n_samples, n_features=n_features, random_state=random_state, base_node=base_node, n_jobs=n_jobs)\n</code></pre>"},{"location":"documentation/tpot/config/imputers/","title":"Imputers","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/config/mdr_configs/","title":"Mdr configs","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/config/regressors/","title":"Regressors","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/config/regressors_sklearnex/","title":"Regressors sklearnex","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/config/selectors/","title":"Selectors","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/config/special_configs/","title":"Special configs","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/config/template_search_spaces/","title":"Template search spaces","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.DynamicUnionPipeline","title":"<code>DynamicUnionPipeline</code>","text":"<p>               Bases: <code>SearchSpace</code></p> Source code in <code>tpot/search_spaces/pipelines/dynamicunion.py</code> <pre><code>class DynamicUnionPipeline(SearchSpace):\n    def __init__(self, search_space : SearchSpace, max_estimators=None, allow_repeats=False ) -&gt; None:\n        \"\"\"\n        Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.\n        \"\"\"\n\n        self.search_space = search_space\n        self.max_estimators = max_estimators\n        self.allow_repeats = allow_repeats\n\n    def generate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        return DynamicUnionPipelineIndividual(self.search_space, max_estimators=self.max_estimators, allow_repeats=self.allow_repeats, rng=rng)\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.DynamicUnionPipeline.__init__","title":"<code>__init__(search_space, max_estimators=None, allow_repeats=False)</code>","text":"<p>Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.</p> Source code in <code>tpot/search_spaces/pipelines/dynamicunion.py</code> <pre><code>def __init__(self, search_space : SearchSpace, max_estimators=None, allow_repeats=False ) -&gt; None:\n    \"\"\"\n    Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.\n    \"\"\"\n\n    self.search_space = search_space\n    self.max_estimators = max_estimators\n    self.allow_repeats = allow_repeats\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.DynamicUnionPipelineIndividual","title":"<code>DynamicUnionPipelineIndividual</code>","text":"<p>               Bases: <code>SklearnIndividual</code></p> <p>Takes in one search space. Will produce a FeatureUnion of up to max_estimators number of steps. The output of the FeatureUnion will the all of the steps concatenated together.</p> Source code in <code>tpot/search_spaces/pipelines/dynamicunion.py</code> <pre><code>class DynamicUnionPipelineIndividual(SklearnIndividual):\n    \"\"\"\n    Takes in one search space.\n    Will produce a FeatureUnion of up to max_estimators number of steps.\n    The output of the FeatureUnion will the all of the steps concatenated together.\n\n    \"\"\"\n\n    def __init__(self, search_space : SearchSpace, max_estimators=None, allow_repeats=False, rng=None) -&gt; None:\n        super().__init__()\n        self.search_space = search_space\n\n        if max_estimators is None:\n            self.max_estimators = np.inf\n        else:\n            self.max_estimators = max_estimators\n\n        self.allow_repeats = allow_repeats\n\n        self.union_dict = {}\n\n        if self.max_estimators == np.inf:\n            init_max = 3\n        else:\n            init_max = self.max_estimators\n\n        rng = np.random.default_rng(rng)\n\n        for _ in range(rng.integers(1, init_max)):\n            self._mutate_add_step(rng)\n\n\n    def mutate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        mutation_funcs = [self._mutate_add_step, self._mutate_remove_step, self._mutate_replace_step, self._mutate_note]\n        rng.shuffle(mutation_funcs)\n        for mutation_func in mutation_funcs:\n            if mutation_func(rng):\n                return True\n\n    def _mutate_add_step(self, rng):\n        rng = np.random.default_rng(rng)\n        max_attempts = 10\n        if len(self.union_dict) &lt; self.max_estimators:\n            for _ in range(max_attempts):\n                new_step = self.search_space.generate(rng)\n                if new_step.unique_id() not in self.union_dict:\n                    self.union_dict[new_step.unique_id()] = new_step\n                    return True\n        return False\n\n    def _mutate_remove_step(self, rng):\n        rng = np.random.default_rng(rng)\n        if len(self.union_dict) &gt; 1:\n            self.union_dict.pop( rng.choice(list(self.union_dict.keys())))  \n            return True\n        return False\n\n    def _mutate_replace_step(self, rng):\n        rng = np.random.default_rng(rng)        \n        changed = self._mutate_remove_step(rng) or self._mutate_add_step(rng)\n        return changed\n\n    #TODO mutate one step or multiple?\n    def _mutate_note(self, rng):\n        rng = np.random.default_rng(rng)\n        changed = False\n        values = list(self.union_dict.values())\n        for step in values:\n            if rng.random() &lt; 0.5:\n                changed = step.mutate(rng) or changed\n\n        self.union_dict = {step.unique_id(): step for step in values}\n\n        return changed\n\n\n    def crossover(self, other, rng=None):\n        rng = np.random.default_rng(rng)\n\n        cx_funcs = [self._crossover_swap_multiple_nodes, self._crossover_node]\n        rng.shuffle(cx_funcs)\n        for cx_func in cx_funcs:\n            if cx_func(other, rng):\n                return True\n\n        return False\n\n\n    def _crossover_swap_multiple_nodes(self, other, rng):\n        rng = np.random.default_rng(rng)\n        self_values = list(self.union_dict.values())\n        other_values = list(other.union_dict.values())\n\n        rng.shuffle(self_values)\n        rng.shuffle(other_values)\n\n        self_idx = rng.integers(0,len(self_values))\n        other_idx = rng.integers(0,len(other_values))\n\n        #Note that this is not one-point-crossover since the sequence doesn't matter. this is just a quick way to swap multiple random items\n        self_values[:self_idx], other_values[:other_idx] = other_values[:other_idx], self_values[:self_idx]\n\n        self.union_dict = {step.unique_id(): step for step in self_values}\n        other.union_dict = {step.unique_id(): step for step in other_values}\n\n        return True\n\n\n    def _crossover_node(self, other, rng):\n        rng = np.random.default_rng(rng)\n\n        changed = False\n        self_values = list(self.union_dict.values())\n        other_values = list(other.union_dict.values())\n\n        rng.shuffle(self_values)\n        rng.shuffle(other_values)\n\n        for self_step, other_step in zip(self_values, other_values):\n            if rng.random() &lt; 0.5:\n                changed = self_step.crossover(other_step, rng) or changed\n\n        self.union_dict = {step.unique_id(): step for step in self_values}\n        other.union_dict = {step.unique_id(): step for step in other_values}\n\n        return changed\n\n    def export_pipeline(self, **kwargs):\n        values = list(self.union_dict.values())\n        return sklearn.pipeline.make_union(*[step.export_pipeline(**kwargs) for step in values])\n\n    def unique_id(self):\n        values = list(self.union_dict.values())\n        l = [step.unique_id() for step in values]\n        # if all items are strings, then sort them\n        if all([isinstance(x, str) for x in l]):\n            l.sort()\n        l = [\"FeatureUnion\"] + l\n        return TupleIndex(frozenset(l))\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.EstimatorNodeIndividual","title":"<code>EstimatorNodeIndividual</code>","text":"<p>               Bases: <code>SklearnIndividual</code></p> <p>Note that ConfigurationSpace does not support None as a parameter. Instead, use the special string \"\". TPOT will automatically replace instances of this string with the Python None.  <p>Parameters:</p> Name Type Description Default <code>method</code> <code>type</code> <p>The class of the estimator to be used</p> required <code>space</code> <code>ConfigurationSpace | dict</code> <p>The hyperparameter space to be used. If a dict is passed, hyperparameters are fixed and not learned.</p> required Source code in <code>tpot/search_spaces/nodes/estimator_node.py</code> <pre><code>class EstimatorNodeIndividual(SklearnIndividual):\n    \"\"\"\n    Note that ConfigurationSpace does not support None as a parameter. Instead, use the special string \"&lt;NONE&gt;\". TPOT will automatically replace instances of this string with the Python None. \n\n    Parameters\n    ----------\n    method : type\n        The class of the estimator to be used\n\n    space : ConfigurationSpace|dict\n        The hyperparameter space to be used. If a dict is passed, hyperparameters are fixed and not learned.\n\n    \"\"\"\n    def __init__(self, method: type, \n                        space: ConfigurationSpace|dict, #TODO If a dict is passed, hyperparameters are fixed and not learned. Is this confusing? Should we make a second node type?\n                        hyperparameter_parser: callable = None,\n                        rng=None) -&gt; None:\n        super().__init__()\n        self.method = method\n        self.space = space\n\n        if hyperparameter_parser is None:\n            self.hyperparameter_parser = default_hyperparameter_parser\n        else:\n            self.hyperparameter_parser = hyperparameter_parser\n\n        if isinstance(space, dict):\n            self.hyperparameters = space\n        else:\n            rng = np.random.default_rng(rng)\n            self.space.seed(rng.integers(0, 2**32))\n            self.hyperparameters = dict(self.space.sample_configuration())\n\n    def mutate(self, rng=None):\n        if isinstance(self.space, dict): \n            return False\n\n        rng = np.random.default_rng(rng)\n        self.space.seed(rng.integers(0, 2**32))\n        self.hyperparameters = dict(self.space.sample_configuration())\n        return True\n\n    def crossover(self, other, rng=None):\n        if isinstance(self.space, dict):\n            return False\n\n        rng = np.random.default_rng(rng)\n        if self.method != other.method:\n            return False\n\n        #loop through hyperparameters, randomly swap items in self.hyperparameters with items in other.hyperparameters\n        for hyperparameter in self.space:\n            if rng.choice([True, False]):\n                if hyperparameter in other.hyperparameters:\n                    self.hyperparameters[hyperparameter] = other.hyperparameters[hyperparameter]\n\n        return True\n\n\n\n    @final #this method should not be overridden, instead override hyperparameter_parser\n    def export_pipeline(self, **kwargs):\n        return self.method(**self.hyperparameter_parser(self.hyperparameters))\n\n    def unique_id(self):\n        #return a dictionary of the method and the hyperparameters\n        method_str = self.method.__name__\n        params = list(self.hyperparameters.keys())\n        params = sorted(params)\n\n        id_str = f\"{method_str}({', '.join([f'{param}={self.hyperparameters[param]}' for param in params])})\"\n\n        return id_str\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.FSSIndividual","title":"<code>FSSIndividual</code>","text":"<p>               Bases: <code>SklearnIndividual</code></p> Source code in <code>tpot/search_spaces/nodes/fss_node.py</code> <pre><code>class FSSIndividual(SklearnIndividual):\n    def __init__(   self,\n                    subsets,\n                    rng=None,\n                ):\n\n        \"\"\"\n        An individual for representing a specific FeatureSetSelector. \n        The FeatureSetSelector selects a feature list of list of predefined feature subsets.\n\n        This instance will select one set initially. Mutation and crossover can swap the selected subset with another.\n\n        Parameters\n        ----------\n        subsets : str or list, default=None\n            Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries. \n            Features are defined by column names if using a Pandas data frame, or ints corresponding to indexes if using numpy arrays.\n            - str : If a string, it is assumed to be a path to a csv file with the subsets. \n                The first column is assumed to be the name of the subset and the remaining columns are the features in the subset.\n            - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets (i.e a list of lists).\n            - dict : A dictionary where keys are the names of the subsets and the values are the list of features.\n            - int : If an int, it is assumed to be the number of subsets to generate. Each subset will contain one feature.\n            - None : If None, each column will be treated as a subset. One column will be selected per subset.\n        rng : int, np.random.Generator, optional\n            The random number generator. The default is None.\n            Only used to select the first subset.\n\n        Returns\n        -------\n        None    \n        \"\"\"\n\n        subsets = subsets\n        rng = np.random.default_rng(rng)\n\n        if isinstance(subsets, str):\n            df = pd.read_csv(subsets,header=None,index_col=0)\n            df['features'] = df.apply(lambda x: list([x[c] for c in df.columns]),axis=1)\n            self.subset_dict = {}\n            for row in df.index:\n                self.subset_dict[row] = df.loc[row]['features']\n        elif isinstance(subsets, dict):\n            self.subset_dict = subsets\n        elif isinstance(subsets, list) or isinstance(subsets, np.ndarray):\n            self.subset_dict = {str(i):subsets[i] for i in range(len(subsets))}\n        elif isinstance(subsets, int):\n            self.subset_dict = {\"{0}\".format(i):i for i in range(subsets)}\n        else:\n            raise ValueError(\"Subsets must be a string, dictionary, list, int, or numpy array\")\n\n        self.names_list = list(self.subset_dict.keys())\n\n\n        self.selected_subset_name = rng.choice(self.names_list)\n        self.sel_subset = self.subset_dict[self.selected_subset_name]\n\n\n    def mutate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        #get list of names not including the current one\n        names = [name for name in self.names_list if name != self.selected_subset_name]\n        self.selected_subset_name = rng.choice(names)\n        self.sel_subset = self.subset_dict[self.selected_subset_name]\n\n\n    def crossover(self, other, rng=None):\n        self.selected_subset_name = other.selected_subset_name\n        self.sel_subset = other.sel_subset\n\n    def export_pipeline(self, **kwargs):\n        return FeatureSetSelector(sel_subset=self.sel_subset, name=self.selected_subset_name)\n\n\n    def unique_id(self):\n        id_str = \"FeatureSetSelector({0})\".format(self.selected_subset_name)\n        return id_str\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.FSSIndividual.__init__","title":"<code>__init__(subsets, rng=None)</code>","text":"<p>An individual for representing a specific FeatureSetSelector.  The FeatureSetSelector selects a feature list of list of predefined feature subsets.</p> <p>This instance will select one set initially. Mutation and crossover can swap the selected subset with another.</p> <p>Parameters:</p> Name Type Description Default <code>subsets</code> <code>str or list</code> <p>Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries.  Features are defined by column names if using a Pandas data frame, or ints corresponding to indexes if using numpy arrays. - str : If a string, it is assumed to be a path to a csv file with the subsets.      The first column is assumed to be the name of the subset and the remaining columns are the features in the subset. - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets (i.e a list of lists). - dict : A dictionary where keys are the names of the subsets and the values are the list of features. - int : If an int, it is assumed to be the number of subsets to generate. Each subset will contain one feature. - None : If None, each column will be treated as a subset. One column will be selected per subset.</p> <code>None</code> <code>rng</code> <code>(int, Generator)</code> <p>The random number generator. The default is None. Only used to select the first subset.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>tpot/search_spaces/nodes/fss_node.py</code> <pre><code>def __init__(   self,\n                subsets,\n                rng=None,\n            ):\n\n    \"\"\"\n    An individual for representing a specific FeatureSetSelector. \n    The FeatureSetSelector selects a feature list of list of predefined feature subsets.\n\n    This instance will select one set initially. Mutation and crossover can swap the selected subset with another.\n\n    Parameters\n    ----------\n    subsets : str or list, default=None\n        Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries. \n        Features are defined by column names if using a Pandas data frame, or ints corresponding to indexes if using numpy arrays.\n        - str : If a string, it is assumed to be a path to a csv file with the subsets. \n            The first column is assumed to be the name of the subset and the remaining columns are the features in the subset.\n        - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets (i.e a list of lists).\n        - dict : A dictionary where keys are the names of the subsets and the values are the list of features.\n        - int : If an int, it is assumed to be the number of subsets to generate. Each subset will contain one feature.\n        - None : If None, each column will be treated as a subset. One column will be selected per subset.\n    rng : int, np.random.Generator, optional\n        The random number generator. The default is None.\n        Only used to select the first subset.\n\n    Returns\n    -------\n    None    \n    \"\"\"\n\n    subsets = subsets\n    rng = np.random.default_rng(rng)\n\n    if isinstance(subsets, str):\n        df = pd.read_csv(subsets,header=None,index_col=0)\n        df['features'] = df.apply(lambda x: list([x[c] for c in df.columns]),axis=1)\n        self.subset_dict = {}\n        for row in df.index:\n            self.subset_dict[row] = df.loc[row]['features']\n    elif isinstance(subsets, dict):\n        self.subset_dict = subsets\n    elif isinstance(subsets, list) or isinstance(subsets, np.ndarray):\n        self.subset_dict = {str(i):subsets[i] for i in range(len(subsets))}\n    elif isinstance(subsets, int):\n        self.subset_dict = {\"{0}\".format(i):i for i in range(subsets)}\n    else:\n        raise ValueError(\"Subsets must be a string, dictionary, list, int, or numpy array\")\n\n    self.names_list = list(self.subset_dict.keys())\n\n\n    self.selected_subset_name = rng.choice(self.names_list)\n    self.sel_subset = self.subset_dict[self.selected_subset_name]\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.FSSNode","title":"<code>FSSNode</code>","text":"<p>               Bases: <code>SearchSpace</code></p> Source code in <code>tpot/search_spaces/nodes/fss_node.py</code> <pre><code>class FSSNode(SearchSpace):\n    def __init__(self,                     \n                    subsets,\n                ):\n        \"\"\"\n        A search space for a FeatureSetSelector. \n        The FeatureSetSelector selects a feature list of list of predefined feature subsets.\n\n        Parameters\n        ----------\n        subsets : str or list, default=None\n            Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries. \n            Features are defined by column names if using a Pandas data frame, or ints corresponding to indexes if using numpy arrays.\n            - str : If a string, it is assumed to be a path to a csv file with the subsets. \n                The first column is assumed to be the name of the subset and the remaining columns are the features in the subset.\n            - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets (i.e a list of lists).\n            - dict : A dictionary where keys are the names of the subsets and the values are the list of features.\n            - int : If an int, it is assumed to be the number of subsets to generate. Each subset will contain one feature.\n            - None : If None, each column will be treated as a subset. One column will be selected per subset.\n\n        Returns\n        -------\n        None    \n\n        \"\"\"\n\n        self.subsets = subsets\n\n    def generate(self, rng=None) -&gt; SklearnIndividual:\n        return FSSIndividual(   \n            subsets=self.subsets,\n            rng=rng,\n            )\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.FSSNode.__init__","title":"<code>__init__(subsets)</code>","text":"<p>A search space for a FeatureSetSelector.  The FeatureSetSelector selects a feature list of list of predefined feature subsets.</p> <p>Parameters:</p> Name Type Description Default <code>subsets</code> <code>str or list</code> <p>Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries.  Features are defined by column names if using a Pandas data frame, or ints corresponding to indexes if using numpy arrays. - str : If a string, it is assumed to be a path to a csv file with the subsets.      The first column is assumed to be the name of the subset and the remaining columns are the features in the subset. - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets (i.e a list of lists). - dict : A dictionary where keys are the names of the subsets and the values are the list of features. - int : If an int, it is assumed to be the number of subsets to generate. Each subset will contain one feature. - None : If None, each column will be treated as a subset. One column will be selected per subset.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>tpot/search_spaces/nodes/fss_node.py</code> <pre><code>def __init__(self,                     \n                subsets,\n            ):\n    \"\"\"\n    A search space for a FeatureSetSelector. \n    The FeatureSetSelector selects a feature list of list of predefined feature subsets.\n\n    Parameters\n    ----------\n    subsets : str or list, default=None\n        Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries. \n        Features are defined by column names if using a Pandas data frame, or ints corresponding to indexes if using numpy arrays.\n        - str : If a string, it is assumed to be a path to a csv file with the subsets. \n            The first column is assumed to be the name of the subset and the remaining columns are the features in the subset.\n        - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets (i.e a list of lists).\n        - dict : A dictionary where keys are the names of the subsets and the values are the list of features.\n        - int : If an int, it is assumed to be the number of subsets to generate. Each subset will contain one feature.\n        - None : If None, each column will be treated as a subset. One column will be selected per subset.\n\n    Returns\n    -------\n    None    \n\n    \"\"\"\n\n    self.subsets = subsets\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.FeatureSetSelector","title":"<code>FeatureSetSelector</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>SelectorMixin</code></p> <p>Select predefined feature subsets.</p> Source code in <code>tpot/builtin_modules/feature_set_selector.py</code> <pre><code>class FeatureSetSelector(BaseEstimator, SelectorMixin):\n    \"\"\"\n    Select predefined feature subsets.\n\n\n    \"\"\"\n\n    def __init__(self, sel_subset=None, name=None):\n        \"\"\"Create a FeatureSetSelector object.\n\n        Parameters\n        ----------\n        sel_subset: list or int\n            If X is a dataframe, items in sel_subset list must correspond to column names\n            If X is a numpy array, items in sel_subset list must correspond to column indexes\n            int: index of a single column\n        Returns\n        -------\n        None\n\n        \"\"\"\n        self.name = name\n        self.sel_subset = sel_subset\n\n\n    def fit(self, X, y=None):\n        \"\"\"Fit FeatureSetSelector for feature selection\n\n        Parameters\n        ----------\n        X: array-like of shape (n_samples, n_features)\n            The training input samples.\n        y: array-like, shape (n_samples,)\n            The target values (integers that correspond to classes in classification, real numbers in regression).\n\n        Returns\n        -------\n        self: object\n            Returns a copy of the estimator\n        \"\"\"\n        if isinstance(self.sel_subset, int) or isinstance(self.sel_subset, str):\n            self.sel_subset = [self.sel_subset]\n\n        #generate  self.feat_list_idx\n        if isinstance(X, pd.DataFrame):\n            self.feature_names_in_ = X.columns.tolist()\n            self.feat_list_idx = sorted([self.feature_names_in_.index(feat) for feat in self.sel_subset])\n\n\n        elif isinstance(X, np.ndarray):\n            self.feature_names_in_ = None#list(range(X.shape[1]))\n\n            self.feat_list_idx = sorted(self.sel_subset)\n\n        n_features = X.shape[1]\n        self.mask = np.zeros(n_features, dtype=bool)\n        self.mask[np.asarray(self.feat_list_idx)] = True\n\n        return self\n\n    #TODO keep returned as dataframe if input is dataframe? may not be consistent with sklearn\n\n    # def transform(self, X):\n\n    def _get_tags(self):\n        tags = {\"allow_nan\": True, \"requires_y\": False}\n        return tags\n\n    def _get_support_mask(self):\n        \"\"\"\n        Get the boolean mask indicating which features are selected\n        Returns\n        -------\n        support : boolean array of shape [# input features]\n            An element is True iff its corresponding feature is selected for\n            retention.\n        \"\"\"\n        return self.mask\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.FeatureSetSelector.__init__","title":"<code>__init__(sel_subset=None, name=None)</code>","text":"<p>Create a FeatureSetSelector object.</p> <p>Parameters:</p> Name Type Description Default <code>sel_subset</code> <p>If X is a dataframe, items in sel_subset list must correspond to column names If X is a numpy array, items in sel_subset list must correspond to column indexes int: index of a single column</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>tpot/builtin_modules/feature_set_selector.py</code> <pre><code>def __init__(self, sel_subset=None, name=None):\n    \"\"\"Create a FeatureSetSelector object.\n\n    Parameters\n    ----------\n    sel_subset: list or int\n        If X is a dataframe, items in sel_subset list must correspond to column names\n        If X is a numpy array, items in sel_subset list must correspond to column indexes\n        int: index of a single column\n    Returns\n    -------\n    None\n\n    \"\"\"\n    self.name = name\n    self.sel_subset = sel_subset\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.FeatureSetSelector.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit FeatureSetSelector for feature selection</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>The training input samples.</p> required <code>y</code> <p>The target values (integers that correspond to classes in classification, real numbers in regression).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Returns a copy of the estimator</p> Source code in <code>tpot/builtin_modules/feature_set_selector.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Fit FeatureSetSelector for feature selection\n\n    Parameters\n    ----------\n    X: array-like of shape (n_samples, n_features)\n        The training input samples.\n    y: array-like, shape (n_samples,)\n        The target values (integers that correspond to classes in classification, real numbers in regression).\n\n    Returns\n    -------\n    self: object\n        Returns a copy of the estimator\n    \"\"\"\n    if isinstance(self.sel_subset, int) or isinstance(self.sel_subset, str):\n        self.sel_subset = [self.sel_subset]\n\n    #generate  self.feat_list_idx\n    if isinstance(X, pd.DataFrame):\n        self.feature_names_in_ = X.columns.tolist()\n        self.feat_list_idx = sorted([self.feature_names_in_.index(feat) for feat in self.sel_subset])\n\n\n    elif isinstance(X, np.ndarray):\n        self.feature_names_in_ = None#list(range(X.shape[1]))\n\n        self.feat_list_idx = sorted(self.sel_subset)\n\n    n_features = X.shape[1]\n    self.mask = np.zeros(n_features, dtype=bool)\n    self.mask[np.asarray(self.feat_list_idx)] = True\n\n    return self\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.GeneticFeatureSelectorNode","title":"<code>GeneticFeatureSelectorNode</code>","text":"<p>               Bases: <code>SearchSpace</code></p> Source code in <code>tpot/search_spaces/nodes/genetic_feature_selection.py</code> <pre><code>class GeneticFeatureSelectorNode(SearchSpace):\n    def __init__(self,                     \n                    n_features,\n                    start_p=0.2,\n                    mutation_rate = 0.1,\n                    crossover_rate = 0.1,\n                    mutation_rate_rate = 0, # These are still experimental but seem to help. Theory is that it takes slower steps as it gets closer to the optimal solution.\n                    crossover_rate_rate = 0,# Otherwise is mutation_rate is too small, it takes forever, and if its too large, it never converges.\n                    ):\n        \"\"\"\n        A node that generates a GeneticFeatureSelectorIndividual. Uses genetic algorithm to select novel subsets of features.\n\n        Parameters\n        ----------\n        n_features : int\n            Number of features in the dataset.\n        start_p : float\n            Probability of selecting a given feature for the initial subset of features.\n        mutation_rate : float\n            Probability of adding/removing a feature from the subset of features.\n        crossover_rate : float\n            Probability of swapping a feature between two subsets of features.\n        mutation_rate_rate : float\n            Probability of changing the mutation rate. (experimental)\n        crossover_rate_rate : float\n            Probability of changing the crossover rate. (experimental)\n\n        \"\"\"\n\n        self.n_features = n_features\n        self.start_p = start_p\n        self.mutation_rate = mutation_rate\n        self.crossover_rate = crossover_rate\n        self.mutation_rate_rate = mutation_rate_rate\n        self.crossover_rate_rate = crossover_rate_rate\n\n\n    def generate(self, rng=None) -&gt; SklearnIndividual:\n        return GeneticFeatureSelectorIndividual(   mask=self.n_features,\n                                                    start_p=self.start_p,\n                                                    mutation_rate=self.mutation_rate,\n                                                    crossover_rate=self.crossover_rate,\n                                                    mutation_rate_rate=self.mutation_rate_rate,\n                                                    crossover_rate_rate=self.crossover_rate_rate,\n                                                    rng=rng\n                                                )\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.GeneticFeatureSelectorNode.__init__","title":"<code>__init__(n_features, start_p=0.2, mutation_rate=0.1, crossover_rate=0.1, mutation_rate_rate=0, crossover_rate_rate=0)</code>","text":"<p>A node that generates a GeneticFeatureSelectorIndividual. Uses genetic algorithm to select novel subsets of features.</p> <p>Parameters:</p> Name Type Description Default <code>n_features</code> <code>int</code> <p>Number of features in the dataset.</p> required <code>start_p</code> <code>float</code> <p>Probability of selecting a given feature for the initial subset of features.</p> <code>0.2</code> <code>mutation_rate</code> <code>float</code> <p>Probability of adding/removing a feature from the subset of features.</p> <code>0.1</code> <code>crossover_rate</code> <code>float</code> <p>Probability of swapping a feature between two subsets of features.</p> <code>0.1</code> <code>mutation_rate_rate</code> <code>float</code> <p>Probability of changing the mutation rate. (experimental)</p> <code>0</code> <code>crossover_rate_rate</code> <code>float</code> <p>Probability of changing the crossover rate. (experimental)</p> <code>0</code> Source code in <code>tpot/search_spaces/nodes/genetic_feature_selection.py</code> <pre><code>def __init__(self,                     \n                n_features,\n                start_p=0.2,\n                mutation_rate = 0.1,\n                crossover_rate = 0.1,\n                mutation_rate_rate = 0, # These are still experimental but seem to help. Theory is that it takes slower steps as it gets closer to the optimal solution.\n                crossover_rate_rate = 0,# Otherwise is mutation_rate is too small, it takes forever, and if its too large, it never converges.\n                ):\n    \"\"\"\n    A node that generates a GeneticFeatureSelectorIndividual. Uses genetic algorithm to select novel subsets of features.\n\n    Parameters\n    ----------\n    n_features : int\n        Number of features in the dataset.\n    start_p : float\n        Probability of selecting a given feature for the initial subset of features.\n    mutation_rate : float\n        Probability of adding/removing a feature from the subset of features.\n    crossover_rate : float\n        Probability of swapping a feature between two subsets of features.\n    mutation_rate_rate : float\n        Probability of changing the mutation rate. (experimental)\n    crossover_rate_rate : float\n        Probability of changing the crossover rate. (experimental)\n\n    \"\"\"\n\n    self.n_features = n_features\n    self.start_p = start_p\n    self.mutation_rate = mutation_rate\n    self.crossover_rate = crossover_rate\n    self.mutation_rate_rate = mutation_rate_rate\n    self.crossover_rate_rate = crossover_rate_rate\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.GraphKey","title":"<code>GraphKey</code>","text":"<p>A class that can be used as a key for a graph.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>Graph</code> <p>The graph to use as a key. Node Attributes are used for the hash.</p> required <code>matched_label</code> <code>str</code> <p>The node attribute to consider for the hash.</p> <code>'label'</code> Source code in <code>tpot/search_spaces/pipelines/graph.py</code> <pre><code>class GraphKey():\n    '''\n    A class that can be used as a key for a graph.\n\n    Parameters\n    ----------\n    graph : (nx.Graph)\n        The graph to use as a key. Node Attributes are used for the hash.\n    matched_label : (str)\n        The node attribute to consider for the hash.\n    '''\n\n    def __init__(self, graph, matched_label='label') -&gt; None:#['hyperparameters', 'method_class']) -&gt; None:\n\n\n        self.graph = graph\n        self.matched_label = matched_label\n        self.node_match = partial(node_match, matched_labels=[matched_label])\n        self.key = int(nx.weisfeiler_lehman_graph_hash(self.graph, node_attr=self.matched_label),16) #hash(tuple(sorted([val for (node, val) in self.graph.degree()])))\n\n\n    #If hash is different, node is definitely different\n    # https://arxiv.org/pdf/2002.06653.pdf\n    def __hash__(self) -&gt; int:\n\n        return self.key\n\n    #If hash is same, use __eq__ to know if they are actually different\n    def __eq__(self, other):\n        return nx.is_isomorphic(self.graph, other.graph, node_match=self.node_match)\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.GraphPipelineIndividual","title":"<code>GraphPipelineIndividual</code>","text":"<p>               Bases: <code>SklearnIndividual</code></p> <p>Defines a search space of pipelines in the shape of a Directed Acyclic Graphs. The search spaces for root, leaf, and inner nodes can be defined separately if desired. Each graph will have a single root serving as the final estimator which is drawn from the <code>root_search_space</code>. If the <code>leaf_search_space</code> is defined, all leaves  in the pipeline will be drawn from that search space. If the <code>leaf_search_space</code> is not defined, all leaves will be drawn from the <code>inner_search_space</code>. Nodes that are not leaves or roots will be drawn from the <code>inner_search_space</code>. If the <code>inner_search_space</code> is not defined, there will be no inner nodes.</p> <p><code>cross_val_predict_cv</code>, <code>method</code>, <code>memory</code>, and <code>use_label_encoder</code> are passed to the GraphPipeline object when the pipeline is exported and not directly used in the search space.</p> <p>Exports to a GraphPipeline object.</p> <p>Parameters:</p> Name Type Description Default <code>root_search_space</code> <code>SearchSpace</code> <p>The search space for the root node of the graph. This node will be the final estimator in the pipeline.</p> required <code>inner_search_space</code> <code>SearchSpace</code> <p>The search space for the inner nodes of the graph. If not defined, there will be no inner nodes.</p> <code>None</code> <code>leaf_search_space</code> <code>SearchSpace</code> <p>The search space for the leaf nodes of the graph. If not defined, the leaf nodes will be drawn from the inner_search_space.</p> <code>None</code> <code>crossover_same_depth</code> <code>bool</code> <p>If True, crossover will only occur between nodes at the same depth in the graph. If False, crossover will occur between nodes at any depth.</p> <code>False</code> <code>cross_val_predict_cv</code> <code>Union[int, Callable]</code> <p>Determines the cross-validation splitting strategy used in inner classifiers or regressors</p> <code>0</code> <code>method</code> <code>str</code> <p>The prediction method to use for the inner classifiers or regressors. If 'auto', it will try to use predict_proba, decision_function, or predict in that order.</p> <code>'auto'</code> <code>memory</code> <p>Used to cache the input and outputs of nodes to prevent refitting or computationally heavy transformations. By default, no caching is performed. If a string is given, it is the path to the caching directory.</p> required <code>use_label_encoder</code> <code>bool</code> <p>If True, the label encoder is used to encode the labels to be 0 to N. If False, the label encoder is not used. Mainly useful for classifiers (XGBoost) that require labels to be ints from 0 to N. Can also be a sklearn.preprocessing.LabelEncoder object. If so, that label encoder is used.</p> <code>False</code> <code>rng</code> <p>Seed for sampling the first graph instance.</p> <code>None</code> Source code in <code>tpot/search_spaces/pipelines/graph.py</code> <pre><code>class GraphPipelineIndividual(SklearnIndividual):\n    \"\"\"\n        Defines a search space of pipelines in the shape of a Directed Acyclic Graphs. The search spaces for root, leaf, and inner nodes can be defined separately if desired.\n        Each graph will have a single root serving as the final estimator which is drawn from the `root_search_space`. If the `leaf_search_space` is defined, all leaves \n        in the pipeline will be drawn from that search space. If the `leaf_search_space` is not defined, all leaves will be drawn from the `inner_search_space`.\n        Nodes that are not leaves or roots will be drawn from the `inner_search_space`. If the `inner_search_space` is not defined, there will be no inner nodes.\n\n        `cross_val_predict_cv`, `method`, `memory`, and `use_label_encoder` are passed to the GraphPipeline object when the pipeline is exported and not directly used in the search space.\n\n        Exports to a GraphPipeline object.\n\n        Parameters\n        ----------\n\n        root_search_space: SearchSpace\n            The search space for the root node of the graph. This node will be the final estimator in the pipeline.\n\n        inner_search_space: SearchSpace, optional\n            The search space for the inner nodes of the graph. If not defined, there will be no inner nodes.\n\n        leaf_search_space: SearchSpace, optional\n            The search space for the leaf nodes of the graph. If not defined, the leaf nodes will be drawn from the inner_search_space.\n\n        crossover_same_depth: bool, optional\n            If True, crossover will only occur between nodes at the same depth in the graph. If False, crossover will occur between nodes at any depth.\n\n        cross_val_predict_cv: int, cross-validation generator or an iterable, optional\n            Determines the cross-validation splitting strategy used in inner classifiers or regressors\n\n        method: str, optional\n            The prediction method to use for the inner classifiers or regressors. If 'auto', it will try to use predict_proba, decision_function, or predict in that order.\n\n        memory: str or object with the joblib.Memory interface, optional\n            Used to cache the input and outputs of nodes to prevent refitting or computationally heavy transformations. By default, no caching is performed. If a string is given, it is the path to the caching directory.\n\n        use_label_encoder: bool, optional\n            If True, the label encoder is used to encode the labels to be 0 to N. If False, the label encoder is not used.\n            Mainly useful for classifiers (XGBoost) that require labels to be ints from 0 to N.\n            Can also be a sklearn.preprocessing.LabelEncoder object. If so, that label encoder is used.\n\n        rng: int, RandomState instance or None, optional\n            Seed for sampling the first graph instance. \n\n        \"\"\"\n\n    def __init__(\n            self,  \n            root_search_space: SearchSpace, \n            leaf_search_space: SearchSpace = None, \n            inner_search_space: SearchSpace = None, \n            max_size: int = np.inf,\n            crossover_same_depth: bool = False,\n            cross_val_predict_cv: Union[int, Callable] = 0, #signature function(estimator, X, y=none)\n            method: str = 'auto',\n            use_label_encoder: bool = False,\n            rng=None):\n\n        super().__init__()\n\n        self.__debug = False\n\n        rng = np.random.default_rng(rng)\n\n        self.root_search_space = root_search_space\n        self.leaf_search_space = leaf_search_space\n        self.inner_search_space = inner_search_space\n        self.max_size = max_size\n        self.crossover_same_depth = crossover_same_depth\n\n        self.cross_val_predict_cv = cross_val_predict_cv\n        self.method = method\n        self.use_label_encoder = use_label_encoder\n\n        self.root = self.root_search_space.generate(rng)\n        self.graph = nx.DiGraph()\n        self.graph.add_node(self.root)\n\n        if self.leaf_search_space is not None:\n            self.leaf = self.leaf_search_space.generate(rng)\n            self.graph.add_node(self.leaf)\n            self.graph.add_edge(self.root, self.leaf)\n\n        if self.inner_search_space is None and self.leaf_search_space is None:\n            self.mutate_methods_list = [self._mutate_node]\n            self.crossover_methods_list = [self._crossover_swap_branch,]#[self._crossover_swap_branch, self._crossover_swap_node, self._crossover_take_branch]  #TODO self._crossover_nodes, \n\n        else:\n            self.mutate_methods_list = [self._mutate_insert_leaf, self._mutate_insert_inner_node, self._mutate_remove_node, self._mutate_node, self._mutate_insert_bypass_node]\n            self.crossover_methods_list = [self._crossover_swap_branch, self._crossover_nodes, self._crossover_take_branch ]#[self._crossover_swap_branch, self._crossover_swap_node, self._crossover_take_branch]  #TODO self._crossover_nodes, \n\n        self.merge_duplicated_nodes_toggle = True\n\n        self.graphkey = None\n\n\n    def mutate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        rng.shuffle(self.mutate_methods_list)\n        for mutate_method in self.mutate_methods_list:\n            if mutate_method(rng=rng):\n\n                if self.merge_duplicated_nodes_toggle:\n                    self._merge_duplicated_nodes()\n\n                if self.__debug:\n                    print(mutate_method)\n\n                    if self.root not in self.graph.nodes:\n                        print('lost root something went wrong with ', mutate_method)\n\n                    if len(self.graph.predecessors(self.root)) &gt; 0:\n                        print('root has parents ', mutate_method)\n\n                    if any([n in nx.ancestors(self.graph,n) for n in self.graph.nodes]):\n                        print('a node is connecting to itself...')\n\n                    if self.__debug:\n                        try:\n                            nx.find_cycle(self.graph)\n                            print('something went wrong with ', mutate_method)\n                        except:\n                            pass\n\n                self.graphkey = None\n\n        return False\n\n\n\n\n    def _mutate_insert_leaf(self, rng=None):\n        rng = np.random.default_rng(rng)\n        if self.max_size &gt; self.graph.number_of_nodes():\n            sorted_nodes_list = list(self.graph.nodes)\n            rng.shuffle(sorted_nodes_list) #TODO: sort by number of children and/or parents? bias model one way or another\n            for node in sorted_nodes_list:\n                #if leafs are protected, check if node is a leaf\n                #if node is a leaf, skip because we don't want to add node on top of node\n                if (self.leaf_search_space is not None #if leafs are protected\n                    and   len(list(self.graph.successors(node))) == 0 #if node is leaf\n                    and  len(list(self.graph.predecessors(node))) &gt; 0 #except if node is root, in which case we want to add a leaf even if it happens to be a leaf too\n                    ):\n\n                    continue\n\n                #If node *is* the root or is not a leaf, add leaf node. (dont want to add leaf on top of leaf)\n                if self.leaf_search_space is not None:\n                    new_node = self.leaf_search_space.generate(rng)\n                else:\n                    new_node = self.inner_search_space.generate(rng)\n\n                self.graph.add_node(new_node)\n                self.graph.add_edge(node, new_node)\n                return True\n\n        return False\n\n    def _mutate_insert_inner_node(self, rng=None):\n        \"\"\"\n        Finds an edge in the graph and inserts a new node between the two nodes. Removes the edge between the two nodes.\n        \"\"\"\n        rng = np.random.default_rng(rng)\n        if self.max_size &gt; self.graph.number_of_nodes():\n            sorted_nodes_list = list(self.graph.nodes)\n            sorted_nodes_list2 = list(self.graph.nodes)\n            rng.shuffle(sorted_nodes_list) #TODO: sort by number of children and/or parents? bias model one way or another\n            rng.shuffle(sorted_nodes_list2)\n            for node in sorted_nodes_list:\n                #loop through children of node\n                for child_node in list(self.graph.successors(node)):\n\n                    if child_node is not node and child_node not in nx.ancestors(self.graph, node):\n                        if self.leaf_search_space is not None:\n                            #If if we are protecting leafs, dont add connection into a leaf\n                            if len(list(nx.descendants(self.graph,node))) ==0 :\n                                continue\n\n                        new_node = self.inner_search_space.generate(rng)\n\n                        self.graph.add_node(new_node)\n                        self.graph.add_edges_from([(node, new_node), (new_node, child_node)])\n                        self.graph.remove_edge(node, child_node)\n                        return True\n\n        return False\n\n\n    def _mutate_remove_node(self, rng=None):\n        '''\n        Removes a randomly chosen node and connects its parents to its children.\n        If the node is the only leaf for an inner node and 'leaf_search_space' is not none, we do not remove it.\n        '''\n        rng = np.random.default_rng(rng)\n        nodes_list = list(self.graph.nodes)\n        nodes_list.remove(self.root)\n        leaves = get_leaves(self.graph)\n\n        while len(nodes_list) &gt; 0:\n            node = rng.choice(nodes_list)\n            nodes_list.remove(node)\n\n            if self.leaf_search_space is not None and len(list(nx.descendants(self.graph,node))) == 0 : #if the node is a leaf\n                if len(leaves) &lt;= 1:\n                    continue #dont remove the last leaf\n                leaf_parents = self.graph.predecessors(node)\n\n                # if any of the parents of the node has one one child, continue\n                if any([len(list(self.graph.successors(lp))) &lt; 2 for lp in leaf_parents]): #dont remove a leaf if it is the only input into another node.\n                    continue\n\n                remove_and_stitch(self.graph, node)\n                remove_nodes_disconnected_from_node(self.graph, self.root)\n                return True\n\n            else:\n                remove_and_stitch(self.graph, node)\n                remove_nodes_disconnected_from_node(self.graph, self.root)\n                return True\n\n        return False\n\n\n\n    def _mutate_node(self, rng=None):\n        '''\n        Mutates the hyperparameters for a randomly chosen node in the graph.\n        '''\n        rng = np.random.default_rng(rng)\n        sorted_nodes_list = list(self.graph.nodes)\n        rng.shuffle(sorted_nodes_list)\n        completed_one = False\n        for node in sorted_nodes_list:\n            if node.mutate(rng):\n                return True\n        return False\n\n    def _mutate_remove_edge(self, rng=None):\n        '''\n        Deletes an edge as long as deleting that edge does not make the graph disconnected.\n        '''\n        rng = np.random.default_rng(rng)\n        sorted_nodes_list = list(self.graph.nodes)\n        rng.shuffle(sorted_nodes_list)\n        for child_node in sorted_nodes_list:\n            parents = list(self.graph.predecessors(child_node))\n            if len(parents) &gt; 1: # if it has more than one parent, you can remove an edge (if this is the only child of a node, it will become a leaf)\n\n                for parent_node in parents:\n                    # if removing the egde will make the parent_node a leaf node, skip\n                    if self.leaf_search_space is not None and len(list(self.graph.successors(parent_node))) &lt; 2:\n                        continue\n\n                    self.graph.remove_edge(parent_node, child_node)\n                    return True\n        return False   \n\n    def _mutate_add_edge(self, rng=None):\n        '''\n        Randomly add an edge from a node to another node that is not an ancestor of the first node.\n        '''\n        rng = np.random.default_rng(rng)\n        sorted_nodes_list = list(self.graph.nodes)\n        rng.shuffle(sorted_nodes_list)\n        for child_node in sorted_nodes_list:\n            for parent_node in sorted_nodes_list:\n                if self.leaf_search_space is not None:\n                    if len(list(self.graph.successors(parent_node))) == 0:\n                        continue\n\n                # skip if\n                # - parent and child are the same node\n                # - edge already exists\n                # - child is an ancestor of parent\n                if  (child_node is not parent_node) and not self.graph.has_edge(parent_node,child_node) and (child_node not in nx.ancestors(self.graph, parent_node)):\n                    self.graph.add_edge(parent_node,child_node)\n                    return True\n\n        return False\n\n    def _mutate_insert_bypass_node(self, rng=None):\n        \"\"\"\n        Pick two nodes (doesn't necessarily need to be connected). Create a new node. connect one node to the new node and the new node to the other node.\n        Does not remove any edges.\n        \"\"\"\n        rng = np.random.default_rng(rng)\n        if self.max_size &gt; self.graph.number_of_nodes():\n            sorted_nodes_list = list(self.graph.nodes)\n            sorted_nodes_list2 = list(self.graph.nodes)\n            rng.shuffle(sorted_nodes_list) #TODO: sort by number of children and/or parents? bias model one way or another\n            rng.shuffle(sorted_nodes_list2)\n            for node in sorted_nodes_list:\n                for child_node in sorted_nodes_list2:\n                    if child_node is not node and child_node not in nx.ancestors(self.graph, node):\n                        if self.leaf_search_space is not None:\n                            #If if we are protecting leafs, dont add connection into a leaf\n                            if len(list(nx.descendants(self.graph,node))) ==0 :\n                                continue\n\n                        new_node = self.inner_search_space.generate(rng)\n\n                        self.graph.add_node(new_node)\n                        self.graph.add_edges_from([(node, new_node), (new_node, child_node)])\n                        return True\n\n        return False\n\n\n    def crossover(self, ind2, rng=None):\n        '''\n        self is the first individual, ind2 is the second individual\n        If crossover_same_depth, it will select graphindividuals at the same recursive depth.\n        Otherwise, it will select graphindividuals randomly from the entire graph and its subgraphs.\n\n        This does not impact graphs without subgraphs. And it does not impacts nodes that are not graphindividuals. Cros\n        '''\n\n        rng = np.random.default_rng(rng)\n\n        rng.shuffle(self.crossover_methods_list)\n\n        finished = False\n\n        for crossover_method in self.crossover_methods_list:\n            if crossover_method(ind2, rng=rng):\n                self._merge_duplicated_nodes()\n                finished = True\n                break\n\n        if self.__debug:\n            try:\n                nx.find_cycle(self.graph)\n                print('something went wrong with ', crossover_method)\n            except:\n                pass\n\n        if finished:\n            self.graphkey = None\n\n        return finished\n\n\n    def _crossover_swap_branch(self, G2, rng=None):\n        '''\n        swaps a branch from parent1 with a branch from parent2. does not modify parent2\n        '''\n        rng = np.random.default_rng(rng)\n\n        if self.crossover_same_depth:\n            pair_gen = select_nodes_same_depth(self.graph, self.root, G2.graph, G2.root, rng=rng)\n        else:\n            pair_gen = select_nodes_randomly(self.graph, G2.graph, rng=rng)\n\n        for node1, node2 in pair_gen:\n            #TODO: if root is in inner_search_space, then do use it?\n            if node1 is self.root or node2 is G2.root: #dont want to add root as inner node\n                continue\n\n            #check if node1 is a leaf and leafs are protected, don't add an input to the leave\n            if self.leaf_search_space is not None: #if we are protecting leaves,\n                node1_is_leaf = len(list(self.graph.successors(node1))) == 0\n                node2_is_leaf = len(list(G2.graph.successors(node2))) == 0\n                #if not ((node1_is_leaf and node1_is_leaf) or (not node1_is_leaf and not node2_is_leaf)): #if node1 is a leaf\n                #if (node1_is_leaf and (not node2_is_leaf)) or ( (not node1_is_leaf) and node2_is_leaf):\n                if not node1_is_leaf:\n                    #only continue if node1 and node2 are both leaves or both not leaves\n                    continue\n\n            temp_graph_1 = self.graph.copy()\n            temp_graph_1.remove_node(node1)\n            remove_nodes_disconnected_from_node(temp_graph_1, self.root)\n\n            #isolating the branch\n            branch2 = G2.graph.copy()\n            n2_descendants = nx.descendants(branch2,node2)\n            for n in list(branch2.nodes):\n                if n not in n2_descendants and n is not node2: #removes all nodes not in the branch\n                    branch2.remove_node(n)\n\n            branch2 = copy.deepcopy(branch2)\n            branch2_root = get_roots(branch2)[0]\n            temp_graph_1.add_edges_from(branch2.edges)\n            for p in list(self.graph.predecessors(node1)):\n                temp_graph_1.add_edge(p,branch2_root)\n\n            if temp_graph_1.number_of_nodes() &gt; self.max_size:\n                continue\n\n            self.graph = temp_graph_1\n\n            return True\n        return False\n\n\n    def _crossover_take_branch(self, G2, rng=None):\n        '''\n        Takes a subgraph from Parent2 and add it to a randomly chosen node in Parent1.\n        '''\n        rng = np.random.default_rng(rng)\n\n        if self.crossover_same_depth:\n            pair_gen = select_nodes_same_depth(self.graph, self.root, G2.graph, G2.root, rng=rng)\n        else:\n            pair_gen = select_nodes_randomly(self.graph, G2.graph, rng=rng)\n\n        for node1, node2 in pair_gen:\n            #TODO: if root is in inner_search_space, then do use it?\n            if node2 is G2.root: #dont want to add root as inner node\n                continue\n\n\n            #check if node1 is a leaf and leafs are protected, don't add an input to the leave\n            if self.leaf_search_space is not None and len(list(self.graph.successors(node1))) == 0:\n                continue\n\n            #icheck if node2 is graph individual\n            # if isinstance(node2,GraphIndividual):\n            #     if not ((isinstance(node2,GraphIndividual) and (\"Recursive\" in self.inner_search_space or \"Recursive\" in self.leaf_search_space))):\n            #         continue\n\n            #isolating the branch\n            branch2 = G2.graph.copy()\n            n2_descendants = nx.descendants(branch2,node2)\n            for n in list(branch2.nodes):\n                if n not in n2_descendants and n is not node2: #removes all nodes not in the branch\n                    branch2.remove_node(n)\n\n            #if node1 plus node2 branch has more than max_children, skip\n            if branch2.number_of_nodes() + self.graph.number_of_nodes() &gt; self.max_size:\n                continue\n\n            branch2 = copy.deepcopy(branch2)\n            branch2_root = get_roots(branch2)[0]\n            self.graph.add_edges_from(branch2.edges)\n            self.graph.add_edge(node1,branch2_root)\n\n            return True\n        return False\n\n\n\n    def _crossover_nodes(self, G2, rng=None):\n        '''\n        Swaps the hyperparamters of one randomly chosen node in Parent1 with the hyperparameters of randomly chosen node in Parent2.\n        '''\n        rng = np.random.default_rng(rng)\n\n        if self.crossover_same_depth:\n            pair_gen = select_nodes_same_depth(self.graph, self.root, G2.graph, G2.root, rng=rng)\n        else:\n            pair_gen = select_nodes_randomly(self.graph, G2.graph, rng=rng)\n\n        for node1, node2 in pair_gen:\n\n            #if both nodes are leaves\n            if len(list(self.graph.successors(node1)))==0 and len(list(G2.graph.successors(node2)))==0:\n                if node1.crossover(node2):\n                    return True\n\n\n            #if both nodes are inner nodes\n            if len(list(self.graph.successors(node1)))&gt;0 and len(list(G2.graph.successors(node2)))&gt;0:\n                if len(list(self.graph.predecessors(node1)))&gt;0 and len(list(G2.graph.predecessors(node2)))&gt;0:\n                    if node1.crossover(node2):\n                        return True\n\n            #if both nodes are root nodes\n            if node1 is self.root and node2 is G2.root:\n                if node1.crossover(node2):\n                    return True\n\n\n        return False\n\n    #not including the nodes, just their children\n    #Finds leaves attached to nodes and swaps them\n    def _crossover_swap_leaf_at_node(self, G2, rng=None):\n        rng = np.random.default_rng(rng)\n\n        if self.crossover_same_depth:\n            pair_gen = select_nodes_same_depth(self.graph, self.root, G2.graph, G2.root, rng=rng)\n        else:\n            pair_gen = select_nodes_randomly(self.graph, G2.graph, rng=rng)\n\n        success = False\n        for node1, node2 in pair_gen:\n            # if leaves are protected node1 and node2 must both be leaves or both be inner nodes\n            if self.leaf_search_space is not None and not (len(list(self.graph.successors(node1)))==0 ^ len(list(G2.graph.successors(node2)))==0):\n                continue\n            #self_leafs = [c for c in nx.descendants(self.graph,node1) if len(list(self.graph.successors(c)))==0 and c is not node1]\n            node_leafs = [c for c in nx.descendants(G2.graph,node2) if len(list(G2.graph.successors(c)))==0 and c is not node2]\n\n            # if len(self_leafs) &gt;0:\n            #     for c in self_leafs:\n            #         if random.choice([True,False]):\n            #             self.graph.remove_node(c)\n            #             G2.graph.add_edge(node2, c)\n            #             success = True\n\n            if len(node_leafs) &gt;0:\n                for c in node_leafs:\n                    if rng.choice([True,False]):\n                        G2.graph.remove_node(c)\n                        self.graph.add_edge(node1, c)\n                        success = True\n\n        return success\n\n\n\n    #TODO edit so that G2 is not modified\n    def _crossover_swap_node(self, G2, rng=None):\n        '''\n        Swaps randomly chosen node from Parent1 with a randomly chosen node from Parent2.\n        '''\n        rng = np.random.default_rng(rng)\n\n        if self.crossover_same_depth:\n            pair_gen = select_nodes_same_depth(self.graph, self.root, G2.graph, G2.root, rng=rng)\n        else:\n            pair_gen = select_nodes_randomly(self.graph, G2.graph, rng=rng)\n\n        for node1, node2 in pair_gen:\n            if node1 is self.root or node2 is G2.root: #TODO: allow root\n                continue\n\n            #if leaves are protected\n            if self.leaf_search_space is not None:\n                #if one node is a leaf, the other must be a leaf\n                if not((len(list(self.graph.successors(node1)))==0) ^ (len(list(G2.graph.successors(node2)))==0)):\n                    continue #only continue if both are leaves, or both are not leaves\n\n\n            n1_s = self.graph.successors(node1)\n            n1_p = self.graph.predecessors(node1)\n\n            n2_s = G2.graph.successors(node2)\n            n2_p = G2.graph.predecessors(node2)\n\n            self.graph.remove_node(node1)\n            G2.graph.remove_node(node2)\n\n            self.graph.add_node(node2)\n\n            self.graph.add_edges_from([ (node2, n) for n in n1_s])\n            G2.graph.add_edges_from([ (node1, n) for n in n2_s])\n\n            self.graph.add_edges_from([ (n, node2) for n in n1_p])\n            G2.graph.add_edges_from([ (n, node1) for n in n2_p])\n\n            return True\n\n        return False\n\n\n    def _merge_duplicated_nodes(self):\n\n        graph_changed = False\n        merged = False\n        while(not merged):\n            node_list = list(self.graph.nodes)\n            merged = True\n            for node, other_node in itertools.product(node_list, node_list):\n                if node is other_node:\n                    continue\n\n                #If nodes are same class/hyperparameters\n                if node.unique_id() == other_node.unique_id():\n                    node_children = set(self.graph.successors(node))\n                    other_node_children = set(self.graph.successors(other_node))\n                    #if nodes have identical children, they can be merged\n                    if node_children == other_node_children:\n                        for other_node_parent in list(self.graph.predecessors(other_node)):\n                            if other_node_parent not in self.graph.predecessors(node):\n                                self.graph.add_edge(other_node_parent,node)\n\n                        self.graph.remove_node(other_node)\n                        merged=False\n                        graph_changed = True\n                        break\n\n        return graph_changed\n\n\n    def export_pipeline(self, memory=None, **kwargs):\n        estimator_graph = self.graph.copy()\n\n        #mapping = {node:node.method_class(**node.hyperparameters) for node in estimator_graph}\n        label_remapping = {}\n        label_to_instance = {}\n\n        for node in estimator_graph:\n            this_pipeline_node = node.export_pipeline(memory=memory, **kwargs)\n            found_unique_label = False\n            i=1\n            while not found_unique_label:\n                label = \"{0}_{1}\".format(this_pipeline_node.__class__.__name__, i)\n                if label not in label_to_instance:\n                    found_unique_label = True\n                else:\n                    i+=1\n\n            label_remapping[node] = label\n            label_to_instance[label] = this_pipeline_node\n\n        estimator_graph = nx.relabel_nodes(estimator_graph, label_remapping)\n\n        for label, instance in label_to_instance.items():\n            estimator_graph.nodes[label][\"instance\"] = instance\n\n        return tpot.GraphPipeline(graph=estimator_graph, memory=memory, use_label_encoder=self.use_label_encoder, method=self.method, cross_val_predict_cv=self.cross_val_predict_cv)\n\n\n    def plot(self):\n        G = self.graph.reverse()\n        #TODO clean this up\n        try:\n            pos = nx.planar_layout(G)  # positions for all nodes\n        except:\n            pos = nx.shell_layout(G)\n        # nodes\n        options = {'edgecolors': 'tab:gray', 'node_size': 800, 'alpha': 0.9}\n        nodelist = list(G.nodes)\n        node_color = [plt.cm.Set1(G.nodes[n]['recursive depth']) for n in G]\n\n        fig, ax = plt.subplots()\n\n        nx.draw(G, pos, nodelist=nodelist, node_color=node_color, ax=ax,  **options)\n\n\n        '''edgelist = []\n        for n in n1.node_set:\n            for child in n.children:\n                edgelist.append((n,child))'''\n\n        # edges\n        #nx.draw_networkx_edges(G, pos, width=3.0, arrows=True)\n        '''nx.draw_networkx_edges(\n            G,\n            pos,\n            edgelist=[edgelist],\n            width=8,\n            alpha=0.5,\n            edge_color='tab:red',\n        )'''\n\n\n\n        # some math labels\n        labels = {}\n        for i, n in enumerate(G.nodes):\n            labels[n] = n.method_class.__name__ + \"\\n\" + str(n.hyperparameters)\n\n\n        nx.draw_networkx_labels(G, pos, labels,ax=ax, font_size=7, font_color='black')\n\n        plt.tight_layout()\n        plt.axis('off')\n        plt.show()\n\n\n    def unique_id(self):\n        if self.graphkey is None:\n            #copy self.graph\n            new_graph = self.graph.copy()\n            for n in new_graph.nodes:\n                new_graph.nodes[n]['label'] = n.unique_id()\n\n            new_graph = nx.convert_node_labels_to_integers(new_graph)\n            self.graphkey = GraphKey(new_graph)\n\n        return self.graphkey\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.GraphPipelineIndividual.crossover","title":"<code>crossover(ind2, rng=None)</code>","text":"<p>self is the first individual, ind2 is the second individual If crossover_same_depth, it will select graphindividuals at the same recursive depth. Otherwise, it will select graphindividuals randomly from the entire graph and its subgraphs.</p> <p>This does not impact graphs without subgraphs. And it does not impacts nodes that are not graphindividuals. Cros</p> Source code in <code>tpot/search_spaces/pipelines/graph.py</code> <pre><code>def crossover(self, ind2, rng=None):\n    '''\n    self is the first individual, ind2 is the second individual\n    If crossover_same_depth, it will select graphindividuals at the same recursive depth.\n    Otherwise, it will select graphindividuals randomly from the entire graph and its subgraphs.\n\n    This does not impact graphs without subgraphs. And it does not impacts nodes that are not graphindividuals. Cros\n    '''\n\n    rng = np.random.default_rng(rng)\n\n    rng.shuffle(self.crossover_methods_list)\n\n    finished = False\n\n    for crossover_method in self.crossover_methods_list:\n        if crossover_method(ind2, rng=rng):\n            self._merge_duplicated_nodes()\n            finished = True\n            break\n\n    if self.__debug:\n        try:\n            nx.find_cycle(self.graph)\n            print('something went wrong with ', crossover_method)\n        except:\n            pass\n\n    if finished:\n        self.graphkey = None\n\n    return finished\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.GraphSearchPipeline","title":"<code>GraphSearchPipeline</code>","text":"<p>               Bases: <code>SearchSpace</code></p> Source code in <code>tpot/search_spaces/pipelines/graph.py</code> <pre><code>class GraphSearchPipeline(SearchSpace):\n    def __init__(self, \n        root_search_space: SearchSpace, \n        leaf_search_space: SearchSpace = None, \n        inner_search_space: SearchSpace = None, \n        max_size: int = np.inf,\n        crossover_same_depth: bool = False,\n        cross_val_predict_cv: Union[int, Callable] = 0, #signature function(estimator, X, y=none)\n        method: str = 'auto',\n        use_label_encoder: bool = False):\n\n        \"\"\"\n        Defines a search space of pipelines in the shape of a Directed Acyclic Graphs. The search spaces for root, leaf, and inner nodes can be defined separately if desired.\n        Each graph will have a single root serving as the final estimator which is drawn from the `root_search_space`. If the `leaf_search_space` is defined, all leaves \n        in the pipeline will be drawn from that search space. If the `leaf_search_space` is not defined, all leaves will be drawn from the `inner_search_space`.\n        Nodes that are not leaves or roots will be drawn from the `inner_search_space`. If the `inner_search_space` is not defined, there will be no inner nodes.\n\n        `cross_val_predict_cv`, `method`, `memory`, and `use_label_encoder` are passed to the GraphPipeline object when the pipeline is exported and not directly used in the search space.\n\n        Exports to a GraphPipeline object.\n\n        Parameters\n        ----------\n\n        root_search_space: SearchSpace\n            The search space for the root node of the graph. This node will be the final estimator in the pipeline.\n\n        inner_search_space: SearchSpace, optional\n            The search space for the inner nodes of the graph. If not defined, there will be no inner nodes.\n\n        leaf_search_space: SearchSpace, optional\n            The search space for the leaf nodes of the graph. If not defined, the leaf nodes will be drawn from the inner_search_space.\n\n        crossover_same_depth: bool, optional\n            If True, crossover will only occur between nodes at the same depth in the graph. If False, crossover will occur between nodes at any depth.\n\n        cross_val_predict_cv : int, default=0\n            Number of folds to use for the cross_val_predict function for inner classifiers and regressors. Estimators will still be fit on the full dataset, but the following node will get the outputs from cross_val_predict.\n\n            - 0-1 : When set to 0 or 1, the cross_val_predict function will not be used. The next layer will get the outputs from fitting and transforming the full dataset.\n            - &gt;=2 : When fitting pipelines with inner classifiers or regressors, they will still be fit on the full dataset.\n                    However, the output to the next node will come from cross_val_predict with the specified number of folds.\n\n        method: str, optional\n            The prediction method to use for the inner classifiers or regressors. If 'auto', it will try to use predict_proba, decision_function, or predict in that order.\n\n        memory: str or object with the joblib.Memory interface, optional\n            Used to cache the input and outputs of nodes to prevent refitting or computationally heavy transformations. By default, no caching is performed. If a string is given, it is the path to the caching directory.\n\n        use_label_encoder: bool, optional\n            If True, the label encoder is used to encode the labels to be 0 to N. If False, the label encoder is not used.\n            Mainly useful for classifiers (XGBoost) that require labels to be ints from 0 to N.\n            Can also be a sklearn.preprocessing.LabelEncoder object. If so, that label encoder is used.\n\n        \"\"\"\n\n\n        self.root_search_space = root_search_space\n        self.leaf_search_space = leaf_search_space\n        self.inner_search_space = inner_search_space\n        self.max_size = max_size\n        self.crossover_same_depth = crossover_same_depth\n\n        self.cross_val_predict_cv = cross_val_predict_cv\n        self.method = method\n        self.use_label_encoder = use_label_encoder\n\n    def generate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        ind =  GraphPipelineIndividual(self.root_search_space, self.leaf_search_space, self.inner_search_space, self.max_size, self.crossover_same_depth, \n                                       self.cross_val_predict_cv, self.method, self.use_label_encoder, rng=rng)  \n            # if user specified limit, grab a random number between that limit\n\n        if self.max_size is None or self.max_size == np.inf:\n            n_nodes = rng.integers(1, 5)\n        else:\n            n_nodes = min(rng.integers(1, self.max_size), 5)\n\n        starting_ops = []\n        if self.inner_search_space is not None:\n            starting_ops.append(ind._mutate_insert_inner_node)\n        if self.leaf_search_space is not None or self.inner_search_space is not None:\n            starting_ops.append(ind._mutate_insert_leaf)\n            n_nodes -= 1\n\n        if len(starting_ops) &gt; 0:\n            for _ in range(n_nodes-1):\n                func = rng.choice(starting_ops)\n                func(rng=rng)\n\n        ind._merge_duplicated_nodes()\n\n        return ind\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.GraphSearchPipeline.__init__","title":"<code>__init__(root_search_space, leaf_search_space=None, inner_search_space=None, max_size=np.inf, crossover_same_depth=False, cross_val_predict_cv=0, method='auto', use_label_encoder=False)</code>","text":"<p>Defines a search space of pipelines in the shape of a Directed Acyclic Graphs. The search spaces for root, leaf, and inner nodes can be defined separately if desired. Each graph will have a single root serving as the final estimator which is drawn from the <code>root_search_space</code>. If the <code>leaf_search_space</code> is defined, all leaves  in the pipeline will be drawn from that search space. If the <code>leaf_search_space</code> is not defined, all leaves will be drawn from the <code>inner_search_space</code>. Nodes that are not leaves or roots will be drawn from the <code>inner_search_space</code>. If the <code>inner_search_space</code> is not defined, there will be no inner nodes.</p> <p><code>cross_val_predict_cv</code>, <code>method</code>, <code>memory</code>, and <code>use_label_encoder</code> are passed to the GraphPipeline object when the pipeline is exported and not directly used in the search space.</p> <p>Exports to a GraphPipeline object.</p> <p>Parameters:</p> Name Type Description Default <code>root_search_space</code> <code>SearchSpace</code> <p>The search space for the root node of the graph. This node will be the final estimator in the pipeline.</p> required <code>inner_search_space</code> <code>SearchSpace</code> <p>The search space for the inner nodes of the graph. If not defined, there will be no inner nodes.</p> <code>None</code> <code>leaf_search_space</code> <code>SearchSpace</code> <p>The search space for the leaf nodes of the graph. If not defined, the leaf nodes will be drawn from the inner_search_space.</p> <code>None</code> <code>crossover_same_depth</code> <code>bool</code> <p>If True, crossover will only occur between nodes at the same depth in the graph. If False, crossover will occur between nodes at any depth.</p> <code>False</code> <code>cross_val_predict_cv</code> <code>int</code> <p>Number of folds to use for the cross_val_predict function for inner classifiers and regressors. Estimators will still be fit on the full dataset, but the following node will get the outputs from cross_val_predict.</p> <ul> <li>0-1 : When set to 0 or 1, the cross_val_predict function will not be used. The next layer will get the outputs from fitting and transforming the full dataset.</li> <li> <p>=2 : When fitting pipelines with inner classifiers or regressors, they will still be fit on the full dataset.         However, the output to the next node will come from cross_val_predict with the specified number of folds.</p> </li> </ul> <code>0</code> <code>method</code> <code>str</code> <p>The prediction method to use for the inner classifiers or regressors. If 'auto', it will try to use predict_proba, decision_function, or predict in that order.</p> <code>'auto'</code> <code>memory</code> <p>Used to cache the input and outputs of nodes to prevent refitting or computationally heavy transformations. By default, no caching is performed. If a string is given, it is the path to the caching directory.</p> required <code>use_label_encoder</code> <code>bool</code> <p>If True, the label encoder is used to encode the labels to be 0 to N. If False, the label encoder is not used. Mainly useful for classifiers (XGBoost) that require labels to be ints from 0 to N. Can also be a sklearn.preprocessing.LabelEncoder object. If so, that label encoder is used.</p> <code>False</code> Source code in <code>tpot/search_spaces/pipelines/graph.py</code> <pre><code>def __init__(self, \n    root_search_space: SearchSpace, \n    leaf_search_space: SearchSpace = None, \n    inner_search_space: SearchSpace = None, \n    max_size: int = np.inf,\n    crossover_same_depth: bool = False,\n    cross_val_predict_cv: Union[int, Callable] = 0, #signature function(estimator, X, y=none)\n    method: str = 'auto',\n    use_label_encoder: bool = False):\n\n    \"\"\"\n    Defines a search space of pipelines in the shape of a Directed Acyclic Graphs. The search spaces for root, leaf, and inner nodes can be defined separately if desired.\n    Each graph will have a single root serving as the final estimator which is drawn from the `root_search_space`. If the `leaf_search_space` is defined, all leaves \n    in the pipeline will be drawn from that search space. If the `leaf_search_space` is not defined, all leaves will be drawn from the `inner_search_space`.\n    Nodes that are not leaves or roots will be drawn from the `inner_search_space`. If the `inner_search_space` is not defined, there will be no inner nodes.\n\n    `cross_val_predict_cv`, `method`, `memory`, and `use_label_encoder` are passed to the GraphPipeline object when the pipeline is exported and not directly used in the search space.\n\n    Exports to a GraphPipeline object.\n\n    Parameters\n    ----------\n\n    root_search_space: SearchSpace\n        The search space for the root node of the graph. This node will be the final estimator in the pipeline.\n\n    inner_search_space: SearchSpace, optional\n        The search space for the inner nodes of the graph. If not defined, there will be no inner nodes.\n\n    leaf_search_space: SearchSpace, optional\n        The search space for the leaf nodes of the graph. If not defined, the leaf nodes will be drawn from the inner_search_space.\n\n    crossover_same_depth: bool, optional\n        If True, crossover will only occur between nodes at the same depth in the graph. If False, crossover will occur between nodes at any depth.\n\n    cross_val_predict_cv : int, default=0\n        Number of folds to use for the cross_val_predict function for inner classifiers and regressors. Estimators will still be fit on the full dataset, but the following node will get the outputs from cross_val_predict.\n\n        - 0-1 : When set to 0 or 1, the cross_val_predict function will not be used. The next layer will get the outputs from fitting and transforming the full dataset.\n        - &gt;=2 : When fitting pipelines with inner classifiers or regressors, they will still be fit on the full dataset.\n                However, the output to the next node will come from cross_val_predict with the specified number of folds.\n\n    method: str, optional\n        The prediction method to use for the inner classifiers or regressors. If 'auto', it will try to use predict_proba, decision_function, or predict in that order.\n\n    memory: str or object with the joblib.Memory interface, optional\n        Used to cache the input and outputs of nodes to prevent refitting or computationally heavy transformations. By default, no caching is performed. If a string is given, it is the path to the caching directory.\n\n    use_label_encoder: bool, optional\n        If True, the label encoder is used to encode the labels to be 0 to N. If False, the label encoder is not used.\n        Mainly useful for classifiers (XGBoost) that require labels to be ints from 0 to N.\n        Can also be a sklearn.preprocessing.LabelEncoder object. If so, that label encoder is used.\n\n    \"\"\"\n\n\n    self.root_search_space = root_search_space\n    self.leaf_search_space = leaf_search_space\n    self.inner_search_space = inner_search_space\n    self.max_size = max_size\n    self.crossover_same_depth = crossover_same_depth\n\n    self.cross_val_predict_cv = cross_val_predict_cv\n    self.method = method\n    self.use_label_encoder = use_label_encoder\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.MaskSelector","title":"<code>MaskSelector</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>SelectorMixin</code></p> <p>Select predefined feature subsets.</p> Source code in <code>tpot/search_spaces/nodes/genetic_feature_selection.py</code> <pre><code>class MaskSelector(BaseEstimator, SelectorMixin):\n    \"\"\"Select predefined feature subsets.\"\"\"\n\n    def __init__(self, mask, set_output_transform=None):\n        self.mask = mask\n        self.set_output_transform = set_output_transform\n        if set_output_transform is not None:\n            self.set_output(transform=set_output_transform)\n\n    def fit(self, X, y=None):\n        self.n_features_in_ = X.shape[1]\n        if isinstance(X, pd.DataFrame):\n            self.feature_names_in_ = X.columns\n        #     self.set_output(transform=\"pandas\")\n        self.is_fitted_ = True #so sklearn knows it's fitted\n        return self\n\n    def _get_tags(self):\n        tags = {\"allow_nan\": True, \"requires_y\": False}\n        return tags\n\n    def _get_support_mask(self):\n        return np.array(self.mask)\n\n    def get_feature_names_out(self, input_features=None):\n        return self.feature_names_in_[self.get_support()]\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.SequentialPipeline","title":"<code>SequentialPipeline</code>","text":"<p>               Bases: <code>SearchSpace</code></p> Source code in <code>tpot/search_spaces/pipelines/sequential.py</code> <pre><code>class SequentialPipeline(SearchSpace):\n    def __init__(self, search_spaces : List[SearchSpace] ) -&gt; None:\n        \"\"\"\n        Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.\n        \"\"\"\n\n        self.search_spaces = search_spaces\n\n    def generate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        return SequentialPipelineIndividual(self.search_spaces, rng=rng)\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.SequentialPipeline.__init__","title":"<code>__init__(search_spaces)</code>","text":"<p>Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.</p> Source code in <code>tpot/search_spaces/pipelines/sequential.py</code> <pre><code>def __init__(self, search_spaces : List[SearchSpace] ) -&gt; None:\n    \"\"\"\n    Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.\n    \"\"\"\n\n    self.search_spaces = search_spaces\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.SklearnIndividual","title":"<code>SklearnIndividual</code>","text":"<p>               Bases: <code>BaseIndividual</code></p> Source code in <code>tpot/search_spaces/base.py</code> <pre><code>class SklearnIndividual(tpot.BaseIndividual):\n\n    def __init_subclass__(cls):\n        cls.crossover = cls.validate_same_type(cls.crossover)\n\n\n    def __init__(self,) -&gt; None:\n        super().__init__()\n\n    def mutate(self, rng=None):\n        return\n\n    def crossover(self, other, rng=None, **kwargs):\n        return \n\n    @final\n    def validate_same_type(func):\n\n        def wrapper(self, other, rng=None, **kwargs):\n            if not isinstance(other, type(self)):\n                return False\n            return func(self, other, rng=rng, **kwargs)\n\n        return wrapper\n\n    def export_pipeline(self, **kwargs) -&gt; BaseEstimator:\n        return\n\n    def unique_id(self):\n        \"\"\"\n        Returns a unique identifier for the individual. Used for preventing duplicate individuals from being evaluated.\n        \"\"\"\n        return self\n\n    #TODO currently TPOT population class manually uses the unique_id to generate the index for the population data frame.\n    #alternatively, the index could be the individual itself, with the __eq__ and __hash__ methods implemented.\n\n    # Though this breaks the graphpipeline. When a mutation is called, it changes the __eq__ and __hash__ outputs.\n    # Since networkx uses the hash and eq to determine if a node is already in the graph, this causes the graph thing that \n    # This is a new node not in the graph. But this could be changed if when the graphpipeline mutates nodes, \n    # it \"replaces\" the existing node with the mutated node. This would require a change in the graphpipeline class.\n\n    # def __eq__(self, other):\n    #     return self.unique_id() == other.unique_id()\n\n    # def __hash__(self):\n    #     return hash(self.unique_id())\n\n    #number of components in the pipeline\n    def get_size(self):\n        return 1\n\n    @final\n    def export_flattened_graphpipeline(self, **graphpipeline_kwargs) -&gt; tpot.GraphPipeline:\n        return flatten_to_graphpipeline(self.export_pipeline(), **graphpipeline_kwargs)\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.SklearnIndividual.unique_id","title":"<code>unique_id()</code>","text":"<p>Returns a unique identifier for the individual. Used for preventing duplicate individuals from being evaluated.</p> Source code in <code>tpot/search_spaces/base.py</code> <pre><code>def unique_id(self):\n    \"\"\"\n    Returns a unique identifier for the individual. Used for preventing duplicate individuals from being evaluated.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.TreePipeline","title":"<code>TreePipeline</code>","text":"<p>               Bases: <code>SearchSpace</code></p> Source code in <code>tpot/search_spaces/pipelines/tree.py</code> <pre><code>class TreePipeline(SearchSpace):\n    def __init__(self, root_search_space : SearchSpace, \n                        leaf_search_space : SearchSpace = None, \n                        inner_search_space : SearchSpace =None, \n                        min_size: int = 2, \n                        max_size: int = 10,\n                        crossover_same_depth=False) -&gt; None:\n\n        \"\"\"\n        Generates a pipeline of variable length. Pipeline will have a tree structure similar to TPOT1.\n\n        \"\"\"\n\n        self.search_space = root_search_space\n        self.leaf_search_space = leaf_search_space\n        self.inner_search_space = inner_search_space\n        self.min_size = min_size\n        self.max_size = max_size\n        self.crossover_same_depth = crossover_same_depth\n\n    def generate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        return TreePipelineIndividual(self.search_space, self.leaf_search_space, self.inner_search_space, self.min_size, self.max_size, self.crossover_same_depth, rng=rng) \n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.TreePipeline.__init__","title":"<code>__init__(root_search_space, leaf_search_space=None, inner_search_space=None, min_size=2, max_size=10, crossover_same_depth=False)</code>","text":"<p>Generates a pipeline of variable length. Pipeline will have a tree structure similar to TPOT1.</p> Source code in <code>tpot/search_spaces/pipelines/tree.py</code> <pre><code>def __init__(self, root_search_space : SearchSpace, \n                    leaf_search_space : SearchSpace = None, \n                    inner_search_space : SearchSpace =None, \n                    min_size: int = 2, \n                    max_size: int = 10,\n                    crossover_same_depth=False) -&gt; None:\n\n    \"\"\"\n    Generates a pipeline of variable length. Pipeline will have a tree structure similar to TPOT1.\n\n    \"\"\"\n\n    self.search_space = root_search_space\n    self.leaf_search_space = leaf_search_space\n    self.inner_search_space = inner_search_space\n    self.min_size = min_size\n    self.max_size = max_size\n    self.crossover_same_depth = crossover_same_depth\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.TupleIndex","title":"<code>TupleIndex</code>","text":"<p>TPOT uses tuples to create a unique id for some pipeline search spaces. However, tuples sometimes don't interact correctly with pandas indexes. This class is a wrapper around a tuple that allows it to be used as a key in a dictionary, without it being an itereable.</p> <p>An alternative could be to make unique id return a string, but this would not work with graphpipelines, which require a special object. This class allows linear pipelines to contain graph pipelines while still being able to be used as a key in a dictionary.</p> Source code in <code>tpot/search_spaces/tuple_index.py</code> <pre><code>class TupleIndex():\n    \"\"\"\n    TPOT uses tuples to create a unique id for some pipeline search spaces. However, tuples sometimes don't interact correctly with pandas indexes.\n    This class is a wrapper around a tuple that allows it to be used as a key in a dictionary, without it being an itereable.\n\n    An alternative could be to make unique id return a string, but this would not work with graphpipelines, which require a special object.\n    This class allows linear pipelines to contain graph pipelines while still being able to be used as a key in a dictionary.\n\n    \"\"\"\n    def __init__(self, tup):\n        self.tup = tup\n\n    def __eq__(self,other) -&gt; bool:\n        return self.tup == other\n\n    def __hash__(self) -&gt; int:\n        return self.tup.__hash__()\n\n    def __str__(self) -&gt; str:\n        return self.tup.__str__()\n\n    def __repr__(self) -&gt; str:\n        return self.tup.__repr__()\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.UnionPipeline","title":"<code>UnionPipeline</code>","text":"<p>               Bases: <code>SearchSpace</code></p> Source code in <code>tpot/search_spaces/pipelines/union.py</code> <pre><code>class UnionPipeline(SearchSpace):\n    def __init__(self, search_spaces : List[SearchSpace] ) -&gt; None:\n        \"\"\"\n        Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.\n        \"\"\"\n\n        self.search_spaces = search_spaces\n\n    def generate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        return UnionPipelineIndividual(self.search_spaces, rng=rng)\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.UnionPipeline.__init__","title":"<code>__init__(search_spaces)</code>","text":"<p>Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.</p> Source code in <code>tpot/search_spaces/pipelines/union.py</code> <pre><code>def __init__(self, search_spaces : List[SearchSpace] ) -&gt; None:\n    \"\"\"\n    Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.\n    \"\"\"\n\n    self.search_spaces = search_spaces\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.UnionPipelineIndividual","title":"<code>UnionPipelineIndividual</code>","text":"<p>               Bases: <code>SklearnIndividual</code></p> <p>Takes in a list of search spaces. each space is a list of SearchSpaces. Will produce a FeatureUnion pipeline. Each step in the pipeline will correspond to the the search space provided in the same index. The resulting pipeline will be a FeatureUnion of the steps in the pipeline.</p> Source code in <code>tpot/search_spaces/pipelines/union.py</code> <pre><code>class UnionPipelineIndividual(SklearnIndividual):\n    \"\"\"\n    Takes in a list of search spaces. each space is a list of SearchSpaces.\n    Will produce a FeatureUnion pipeline. Each step in the pipeline will correspond to the the search space provided in the same index.\n    The resulting pipeline will be a FeatureUnion of the steps in the pipeline.\n\n    \"\"\"\n\n    def __init__(self, search_spaces : List[SearchSpace], rng=None) -&gt; None:\n        super().__init__()\n        self.search_spaces = search_spaces\n\n        self.pipeline = []\n        for space in self.search_spaces:\n            self.pipeline.append(space.generate(rng))\n\n    def mutate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        step = rng.choice(self.pipeline)\n        return step.mutate(rng)\n\n\n    def crossover(self, other, rng=None):\n        #swap a random step in the pipeline with the corresponding step in the other pipeline\n        rng = np.random.default_rng(rng)\n\n        cx_funcs = [self._crossover_node, self._crossover_swap_node]\n        rng.shuffle(cx_funcs)\n        for cx_func in cx_funcs:\n            if cx_func(other, rng):\n                return True\n\n        return False\n\n    def _crossover_swap_node(self, other, rng):\n        rng = np.random.default_rng(rng)\n        idx = rng.integers(1,len(self.pipeline))\n\n        self.pipeline[idx], other.pipeline[idx] = other.pipeline[idx], self.pipeline[idx]\n        return True\n\n    def _crossover_node(self, other, rng):\n        rng = np.random.default_rng(rng)\n\n        crossover_success = False\n        for idx in range(len(self.pipeline)):\n            if rng.random() &lt; 0.5:\n                if self.pipeline[idx].crossover(other.pipeline[idx], rng):\n                    crossover_success = True\n\n        return crossover_success\n\n    def export_pipeline(self, **kwargs):\n        return sklearn.pipeline.make_union(*[step.export_pipeline(**kwargs) for step in self.pipeline])\n\n    def unique_id(self):\n        l = [step.unique_id() for step in self.pipeline]\n        l = [\"FeatureUnion\"] + l\n        return TupleIndex(tuple(l))\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.WrapperPipeline","title":"<code>WrapperPipeline</code>","text":"<p>               Bases: <code>SearchSpace</code></p> Source code in <code>tpot/search_spaces/pipelines/wrapper.py</code> <pre><code>class WrapperPipeline(SearchSpace):\n    def __init__(\n            self, \n            method: type, \n            space: ConfigurationSpace,\n            estimator_search_space: SearchSpace,\n            hyperparameter_parser: callable = None, \n            wrapped_param_name: str = None\n            ) -&gt; None:\n\n        \"\"\"\n        This search space is for wrapping a sklearn estimator with a method that takes another estimator and hyperparameters as arguments.\n        For example, this can be used with sklearn.ensemble.BaggingClassifier or sklearn.ensemble.AdaBoostClassifier.\n\n        \"\"\"\n\n\n        self.estimator_search_space = estimator_search_space\n        self.method = method\n        self.space = space\n        self.hyperparameter_parser=hyperparameter_parser\n        self.wrapped_param_name = wrapped_param_name\n\n    def generate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        return WrapperPipelineIndividual(method=self.method, space=self.space, estimator_search_space=self.estimator_search_space, hyperparameter_parser=self.hyperparameter_parser, wrapped_param_name=self.wrapped_param_name,  rng=rng)\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.WrapperPipeline.__init__","title":"<code>__init__(method, space, estimator_search_space, hyperparameter_parser=None, wrapped_param_name=None)</code>","text":"<p>This search space is for wrapping a sklearn estimator with a method that takes another estimator and hyperparameters as arguments. For example, this can be used with sklearn.ensemble.BaggingClassifier or sklearn.ensemble.AdaBoostClassifier.</p> Source code in <code>tpot/search_spaces/pipelines/wrapper.py</code> <pre><code>def __init__(\n        self, \n        method: type, \n        space: ConfigurationSpace,\n        estimator_search_space: SearchSpace,\n        hyperparameter_parser: callable = None, \n        wrapped_param_name: str = None\n        ) -&gt; None:\n\n    \"\"\"\n    This search space is for wrapping a sklearn estimator with a method that takes another estimator and hyperparameters as arguments.\n    For example, this can be used with sklearn.ensemble.BaggingClassifier or sklearn.ensemble.AdaBoostClassifier.\n\n    \"\"\"\n\n\n    self.estimator_search_space = estimator_search_space\n    self.method = method\n    self.space = space\n    self.hyperparameter_parser=hyperparameter_parser\n    self.wrapped_param_name = wrapped_param_name\n</code></pre>"},{"location":"documentation/tpot/config/template_search_spaces/#tpot.config.template_search_spaces.get_template_search_spaces","title":"<code>get_template_search_spaces(search_space, classification=True, inner_predictors=None, cross_val_predict_cv=None, **get_search_space_params)</code>","text":"<p>Returns a search space which can be optimized by TPOT.</p> <p>Parameters:</p> Name Type Description Default <code>search_space</code> <p>The default search space to use. If a string, it should be one of the following:     - 'linear': A search space for linear pipelines     - 'linear-light': A search space for linear pipelines with a smaller, faster search space     - 'graph': A search space for graph pipelines     - 'graph-light': A search space for graph pipelines with a smaller, faster search space     - 'mdr': A search space for MDR pipelines If a SearchSpace object, it should be a valid search space object for TPOT.</p> required <code>classification</code> <p>Whether the problem is a classification problem or a regression problem.</p> <code>True</code> <code>inner_predictors</code> <p>Whether to include additional classifiers/regressors before the final classifier/regressor (allowing for ensembles).  Defaults to False for 'linear-light' and 'graph-light' search spaces, and True otherwise. (Not used for 'mdr' search space)</p> <code>None</code> <code>cross_val_predict_cv</code> <p>The number of folds to use for cross_val_predict.  Defaults to 0 for 'linear-light' and 'graph-light' search spaces, and 5 otherwise. (Not used for 'mdr' search space)</p> <code>None</code> <code>get_search_space_params</code> <p>Additional parameters to pass to the get_search_space function.</p> <code>{}</code> Source code in <code>tpot/config/template_search_spaces.py</code> <pre><code>def get_template_search_spaces(search_space, classification=True, inner_predictors=None, cross_val_predict_cv=None, **get_search_space_params):\n    \"\"\"\n    Returns a search space which can be optimized by TPOT.\n\n    Parameters\n    ----------\n    search_space: str or SearchSpace\n        The default search space to use. If a string, it should be one of the following:\n            - 'linear': A search space for linear pipelines\n            - 'linear-light': A search space for linear pipelines with a smaller, faster search space\n            - 'graph': A search space for graph pipelines\n            - 'graph-light': A search space for graph pipelines with a smaller, faster search space\n            - 'mdr': A search space for MDR pipelines\n        If a SearchSpace object, it should be a valid search space object for TPOT.\n\n    classification: bool, default=True\n        Whether the problem is a classification problem or a regression problem.\n\n    inner_predictors: bool, default=None\n        Whether to include additional classifiers/regressors before the final classifier/regressor (allowing for ensembles). \n        Defaults to False for 'linear-light' and 'graph-light' search spaces, and True otherwise. (Not used for 'mdr' search space)\n\n    cross_val_predict_cv: int, default=None\n        The number of folds to use for cross_val_predict. \n        Defaults to 0 for 'linear-light' and 'graph-light' search spaces, and 5 otherwise. (Not used for 'mdr' search space)\n\n    get_search_space_params: dict\n        Additional parameters to pass to the get_search_space function.\n\n    \"\"\"\n    if inner_predictors is None:\n        if search_space == \"light\" or search_space == \"graph_light\":\n            inner_predictors = False\n        else:\n            inner_predictors = True\n\n    if cross_val_predict_cv is None:\n        if search_space == \"light\" or search_space == \"graph_light\":\n            cross_val_predict_cv = 0\n        else:\n            if classification:\n                cross_val_predict_cv = sklearn.model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n            else:\n                cross_val_predict_cv = sklearn.model_selection.KFold(n_splits=5, shuffle=True, random_state=42)\n\n    if isinstance(search_space, str):\n        if search_space == \"linear\":\n            return get_linear_search_space(classification, inner_predictors, cross_val_predict_cv=cross_val_predict_cv, **get_search_space_params)\n        elif search_space == \"graph\":\n            return get_graph_search_space(classification, inner_predictors, cross_val_predict_cv=cross_val_predict_cv, **get_search_space_params)\n        elif search_space == \"graph-light\":\n            return get_graph_search_space_light(classification, inner_predictors, cross_val_predict_cv=cross_val_predict_cv, **get_search_space_params)\n        elif search_space == \"linear-light\":\n            return get_light_search_space(classification, inner_predictors, cross_val_predict_cv=cross_val_predict_cv, **get_search_space_params)\n        elif search_space == \"mdr\":\n            return get_mdr_search_space(classification, **get_search_space_params)\n        else:\n            raise ValueError(\"Invalid search space\")\n    else:\n        return search_space\n</code></pre>"},{"location":"documentation/tpot/config/transformers/","title":"Transformers","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/evolvers/base_evolver/","title":"Base evolver","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/evolvers/base_evolver/#tpot.evolvers.base_evolver.BaseEvolver","title":"<code>BaseEvolver</code>","text":"Source code in <code>tpot/evolvers/base_evolver.py</code> <pre><code>class BaseEvolver():\n    def __init__(   self,\n                    individual_generator ,\n\n                    objective_functions,\n                    objective_function_weights,\n                    objective_names = None,\n                    objective_kwargs = None,\n                    bigger_is_better = True,\n\n                    population_size = 50,\n                    initial_population_size = None,\n                    population_scaling = .5,\n                    generations_until_end_population = 1,\n                    generations = 50,\n                    early_stop = None,\n                    early_stop_tol = 0.001,\n\n\n                    max_time_mins=float(\"inf\"),\n                    max_eval_time_mins=5,\n\n                    n_jobs=1,\n                    memory_limit=\"4GB\",\n                    client=None,\n\n                    survival_percentage = 1,\n                    crossover_probability=.2,\n                    mutate_probability=.7,\n                    mutate_then_crossover_probability=.05,\n                    crossover_then_mutate_probability=.05,\n\n                    mutation_functions = [ind_mutate],\n                    crossover_functions = [ind_crossover],\n\n                    mutation_function_weights = None,\n                    crossover_function_weights = None,\n\n                    n_parents=2,\n\n                    survival_selector = survival_select_NSGA2,\n                    parent_selector = tournament_selection_dominated,\n\n                    budget_range = None,\n                    budget_scaling = .5,\n                    generations_until_end_budget = 1,\n                    stepwise_steps = 5,\n\n                    threshold_evaluation_pruning = None,\n                    threshold_evaluation_scaling = .5,\n                    min_history_threshold = 20,\n                    selection_evaluation_pruning = None,\n                    selection_evaluation_scaling = .5,\n                    evaluation_early_stop_steps = None,\n                    final_score_strategy = \"mean\",\n\n                    verbose = 0,\n                    periodic_checkpoint_folder = None,\n                    callback = None,\n                    rng=None,\n\n                    ) -&gt; None:\n        \"\"\"\n        Uses mutation, crossover, and optimization functions to evolve a population of individuals towards the given objective functions.\n\n        Parameters\n        ----------\n        individual_generator : generator\n            Generator that yields new base individuals. Used to generate initial population.\n        objective_functions : list of callables\n            list of functions that get applied to the individual and return a float or list of floats\n            If an objective function returns multiple values, they are all concatenated in order\n            with respect to objective_function_weights and early_stop_tol.\n        objective_function_weights : list of floats\n            list of weights for each objective function. Sign flips whether bigger is better or not\n        objective_names : list of strings, default=None\n            Names of the objectives. If None, objective0, objective1, etc. will be used\n        objective_kwargs : dict, default=None\n            Dictionary of keyword arguments to pass to the objective function\n        bigger_is_better : bool, default=True\n            If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n        population_size : int, default=50\n            Size of the population\n        initial_population_size : int, default=None\n            Size of the initial population. If None, population_size will be used.\n        population_scaling : int, default=0.5\n            Scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n        generations_until_end_population : int, default=1\n            Number of generations until the population size reaches population_size\n        generations : int, default=50\n            Number of generations to run\n        early_stop : int, default=None\n            Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered. In general a value of around 5-20 is good.\n        early_stop_tol : float, list of floats, or None, default=0.001\n            -list of floats\n                list of tolerances for each objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n                If an index of the list is None, that item will not be used for early stopping\n            -int\n                If an int is given, it will be used as the tolerance for all objectives\n        max_time_mins : float, default=float(\"inf\")\n            Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n        max_eval_time_mins : float, default=10\n            Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n        n_jobs : int, default=1\n            Number of processes to run in parallel.\n        memory_limit : str, default=None\n            Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n        client : dask.distributed.Client, default=None\n            A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.\n        survival_percentage : float, default=1\n            Percentage of the population size to utilize for mutation and crossover at the beginning of the generation. The rest are discarded. Individuals are selected with the selector passed into survival_selector. The value of this parameter must be between 0 and 1, inclusive.\n            For example, if the population size is 100 and the survival percentage is .5, 50 individuals will be selected with NSGA2 from the existing population. These will be used for mutation and crossover to generate the next 100 individuals for the next generation. The remainder are discarded from the live population. In the next generation, there will now be the 50 parents + the 100 individuals for a total of 150. Surivival percentage is based of the population size parameter and not the existing population size (current population size when using successive halving). Therefore, in the next generation we will still select 50 individuals from the currently existing 150.\n        crossover_probability : float, default=.2\n            Probability of generating a new individual by crossover between two individuals.\n        mutate_probability : float, default=.7\n            Probability of generating a new individual by crossover between one individuals.\n        mutate_then_crossover_probability : float, default=.05\n            Probability of generating a new individual by mutating two individuals followed by crossover.\n        crossover_then_mutate_probability : float, default=.05\n            Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.\n        n_parents : int, default=2\n            Number of parents to use for crossover. Must be greater than 1.\n        survival_selector : function, default=survival_select_NSGA2\n            Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes.\n            Used to selected population_size * survival_percentage individuals at the start of each generation to use for mutation and crossover.\n        parent_selector : function, default=parent_select_NSGA2\n            Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.\n        budget_range : list [start, end], default=None\n            This parameter is used for the successive halving algorithm.\n            A starting and ending budget to use for the budget scaling. The evolver will interpolate between these values over the generations_until_end_budget.\n            Use is dependent on the objective functions. (In TPOTEstimator this corresponds to the percentage of the data to sample.)\n        budget_scaling float : [0,1], default=0.5\n            A scaling factor to use when determining how fast we move the budget from the start to end budget.\n        generations_until_end_budget : int, default=1\n            The number of generations to run before reaching the max budget.\n        stepwise_steps : int, default=1\n            The number of staircase steps to take when interpolating the budget and population size.\n        threshold_evaluation_pruning : list [start, end], default=None\n            Starting and ending percentile to use as a threshold for the evaluation early stopping. The evolver will interpolate between these values over the evaluation_early_stop_steps.\n            Values between 0 and 100.\n            At each step of the evaluation, a threshold is calculated based on the previous evaluations. All individuals that are below the performance threshold are not evaluated for further steps.\n            For example, if the threshold is set to the 90th percentile of the previous evaluations, all individuals that are below the 90th percentile are not evaluated further. This can save computation by not evaluating all individuals for all steps of cross validation.\n        threshold_evaluation_scaling : float [0,inf), default=0.5\n            A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n            Must be greater than zero. Higher numbers will move the threshold to the end faster.\n        min_history_threshold : int, default=0\n            The minimum number of previous scores needed before using threshold early stopping.\n        selection_evaluation_pruning : list, default=None\n            A lower and upper percent of the population size to select each round of CV.\n            Values between 0 and 1.\n            Selects a percentage of the population to evaluate at each step of the evaluation. \n            For example, one strategy is to evaluate different steps of cross validation one at a time, and only select the best N individuals for subsequent steps. \n            This can save computation by not evaluating all individuals for all steps of cross validation. By default this selection is done with the NSGA2 selector.\n        selection_evaluation_scaling : float, default=0.5\n            A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n            Must be greater than zero. Higher numbers will move the threshold to the end faster.\n        evaluation_early_stop_steps : int, default=1\n            The number of steps that will be taken from the objective function. (e.g., the number of CV folds to evaluate)\n        final_score_strategy : str, default=\"mean\"\n            The strategy to use when determining the final score for an individual.\n            \"mean\": The mean of all objective scores\n            \"last\": The score returned by the last call. Currently each objective is evaluated with a clone of the individual.\n        verbose : int, default=0\n            How much information to print during the optimization process. Higher values include the information from lower values.\n            0. nothing\n            1. progress bar\n            2. evaluations progress bar\n            3. best individual\n            4. warnings\n            &gt;=5. full warnings trace\n        periodic_checkpoint_folder : str, default=None\n            Folder to save the population to periodically. If None, no periodic saving will be done.\n            If provided, training will resume from this checkpoint.\n        callback : tpot.CallBackInterface, default=None\n            Callback object. Not implemented\n        rng : Numpy.Random.Generator, None, default=None\n            An object for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes\n\n            - Numpy.Random.Generator\n                Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'. Note this will be the same Generator passed in.\n            - None\n                Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS\n\n        Attributes\n        ----------\n        population : tpot.Population\n            The population of individuals.\n            Use population.population to access the individuals in the current population.\n            Use population.evaluated_individuals to access a data frame of all individuals that have been explored.\n\n        \"\"\"\n\n        self.rng = np.random.default_rng(rng)\n\n        if threshold_evaluation_pruning is not None or selection_evaluation_pruning is not None:\n            if evaluation_early_stop_steps is None:\n                raise ValueError(\"evaluation_early_stop_steps must be set when using threshold_evaluation_pruning or selection_evaluation_pruning\")\n\n        self.individual_generator = individual_generator\n        self.population_size = population_size\n        self.objective_functions = objective_functions\n        self.objective_function_weights = np.array(objective_function_weights)\n        self.bigger_is_better = bigger_is_better\n        if not bigger_is_better:\n            self.objective_function_weights = np.array(self.objective_function_weights)*-1\n\n        self.initial_population_size = initial_population_size\n        if self.initial_population_size is None:\n            self.cur_population_size = population_size\n        else:\n            self.cur_population_size = initial_population_size\n\n        self.population_scaling = population_scaling\n        self.generations_until_end_population = generations_until_end_population\n\n        self.population_size_list = None\n\n\n        self.periodic_checkpoint_folder = periodic_checkpoint_folder\n        self.verbose  = verbose\n        self.callback = callback\n        self.generations = generations\n        self.n_jobs = n_jobs\n\n\n\n        if max_time_mins is None:\n            self.max_time_mins = float(\"inf\")\n        else:\n            self.max_time_mins = max_time_mins\n\n        #functools requires none for infinite time, doesn't support inf\n        if max_eval_time_mins is not None and math.isinf(max_eval_time_mins ):\n            self.max_eval_time_mins = None\n        else:\n            self.max_eval_time_mins = max_eval_time_mins\n\n\n\n\n        self.generation = 0\n\n\n        self.threshold_evaluation_pruning =threshold_evaluation_pruning\n        self.threshold_evaluation_scaling =  max(0.00001,threshold_evaluation_scaling )\n        self.min_history_threshold = min_history_threshold\n\n        self.selection_evaluation_pruning = selection_evaluation_pruning\n        self.selection_evaluation_scaling =  max(0.00001,selection_evaluation_scaling )\n        self.evaluation_early_stop_steps = evaluation_early_stop_steps\n        self.final_score_strategy = final_score_strategy\n\n        self.budget_range = budget_range\n        self.budget_scaling = budget_scaling\n        self.generations_until_end_budget = generations_until_end_budget\n        self.stepwise_steps = stepwise_steps\n\n        self.memory_limit = memory_limit\n\n        self.client = client\n\n\n        self.survival_selector=survival_selector\n        self.parent_selector=parent_selector\n        self.survival_percentage = survival_percentage\n\n        total_var_p = crossover_probability + mutate_probability + mutate_then_crossover_probability + crossover_then_mutate_probability\n        self.crossover_probability = crossover_probability / total_var_p\n        self.mutate_probability = mutate_probability  / total_var_p\n        self.mutate_then_crossover_probability= mutate_then_crossover_probability / total_var_p\n        self.crossover_then_mutate_probability= crossover_then_mutate_probability / total_var_p\n\n\n        self.mutation_functions = mutation_functions\n        self.crossover_functions = crossover_functions\n\n        if mutation_function_weights is None:\n            self.mutation_function_weights = [1 for _ in range(len(mutation_functions))]\n        else:\n            self.mutation_function_weights = mutation_function_weights\n\n        if mutation_function_weights is None:\n            self.crossover_function_weights = [1 for _ in range(len(mutation_functions))]\n        else:\n            self.crossover_function_weights = crossover_function_weights\n\n        self.n_parents = n_parents\n\n        if objective_kwargs is None:\n            self.objective_kwargs = {}\n        else:\n            self.objective_kwargs = objective_kwargs\n\n        # if objective_kwargs is None:\n        #     self.objective_kwargs = [{}] * len(self.objective_functions)\n        # elif isinstance(objective_kwargs, dict):\n        #     self.objective_kwargs = [objective_kwargs] * len(self.objective_functions)\n        # else:\n        #     self.objective_kwargs = objective_kwargs\n\n        ###########\n\n        if self.initial_population_size != self.population_size:\n            self.population_size_list = beta_interpolation(start=self.cur_population_size, end=self.population_size, scale=self.population_scaling, n=generations_until_end_population, n_steps=self.stepwise_steps)\n            self.population_size_list = np.round(self.population_size_list).astype(int)\n\n        if self.budget_range is None:\n            self.budget_list = None\n        else:\n            self.budget_list = beta_interpolation(start=self.budget_range[0], end=self.budget_range[1], n=self.generations_until_end_budget, scale=self.budget_scaling, n_steps=self.stepwise_steps)\n\n        if objective_names is None:\n            self.objective_names = [\"objective\"+str(i) for i in range(len(objective_function_weights))]\n        else:\n            self.objective_names = objective_names\n\n        if self.budget_list is not None:\n            if len(self.budget_list) &lt;= self.generation:\n                self.budget = self.budget_list[-1]\n            else:\n                self.budget = self.budget_list[self.generation]\n        else:\n            self.budget = None\n\n\n        self.early_stop_tol = early_stop_tol\n        self.early_stop = early_stop\n\n        if isinstance(self.early_stop_tol, float):\n            self.early_stop_tol = [self.early_stop_tol for _ in range(len(self.objective_names))]\n\n        self.early_stop_tol = [np.inf if tol is None else tol for tol in self.early_stop_tol]\n\n        self.population = None\n        self.population_file = None\n        if self.periodic_checkpoint_folder is not None:\n            self.population_file = os.path.join(self.periodic_checkpoint_folder, \"population.pkl\")\n            if not os.path.exists(self.periodic_checkpoint_folder):\n                os.makedirs(self.periodic_checkpoint_folder)\n            if os.path.exists(self.population_file):\n                self.population = pickle.load(open(self.population_file, \"rb\"))\n\n                if len(self.population.evaluated_individuals)&gt;0 and \"Generation\" in self.population.evaluated_individuals.columns:\n                    self.generation = self.population.evaluated_individuals['Generation'].max() + 1 #TODO check if this is empty?\n\n        init_names = self.objective_names\n        if self.budget_range is not None:\n            init_names = init_names + [\"Budget\"]\n        if self.population is None:\n            self.population = tpot.Population(column_names=init_names)\n            initial_population = [next(self.individual_generator) for _ in range(self.cur_population_size)]\n            self.population.add_to_population(initial_population, self.rng)\n            self.population.update_column(self.population.population, column_names=\"Generation\", data=self.generation)\n\n\n    def optimize(self, generations=None):\n        \"\"\"\n        Creates an initial population and runs the evolutionary algorithm for the given number of generations. \n        If generations is None, will use self.generations.\n\n        Parameters\n        ----------\n        generations : int, default=None\n            Number of generations to run. If None, will use self.generations.\n\n        \"\"\"\n\n        if self.client is not None: #If user passed in a client manually\n           self._client = self.client\n        else:\n\n            if self.verbose &gt;= 4:\n                silence_logs = 30\n            elif self.verbose &gt;=5:\n                silence_logs = 40\n            else:\n                silence_logs = 50\n            self._cluster = LocalCluster(n_workers=self.n_jobs, #if no client is passed in and no global client exists, create our own\n                    threads_per_worker=1,\n                    silence_logs=silence_logs,\n                    processes=True,\n                    memory_limit=self.memory_limit)\n            self._client = Client(self._cluster)\n\n\n\n        if generations is None:\n            generations = self.generations\n\n        start_time = time.time()\n\n        generations_without_improvement = np.array([0 for _ in range(len(self.objective_function_weights))])\n        best_scores = [-np.inf for _ in range(len(self.objective_function_weights))]\n\n\n        self.scheduled_timeout_time = time.time() + self.max_time_mins*60\n\n\n        try:\n            #for gen in tnrange(generations,desc=\"Generation\", disable=self.verbose&lt;1):\n            done = False\n            gen = 0\n            if self.verbose &gt;= 1:\n                if generations is None or np.isinf(generations):\n                    pbar = tqdm.tqdm(total=0)\n                else:\n                    pbar = tqdm.tqdm(total=generations)\n                pbar.set_description(\"Generation\")\n            while not done:\n                # Generation 0 is the initial population\n                if self.generation == 0:\n                    if self.population_file is not None:\n                        pickle.dump(self.population, open(self.population_file, \"wb\"))\n                    self.evaluate_population()\n                    if self.population_file is not None:\n                        pickle.dump(self.population, open(self.population_file, \"wb\"))\n\n                    attempts = 2\n                    while len(self.population.population) == 0 and attempts &gt; 0:\n                        new_initial_population = [next(self.individual_generator) for _ in range(self.cur_population_size)]\n                        self.population.add_to_population(new_initial_population, rng=self.rng)\n                        attempts -= 1\n                        self.evaluate_population()\n\n                    if len(self.population.population) == 0:\n                        raise Exception(\"No individuals could be evaluated in the initial population. This may indicate a bug in the configuration, included models, or objective functions. Set verbose&gt;=4 to see the errors that caused individuals to fail.\")\n\n                    self.generation += 1\n                # Generation 1 is the first generation after the initial population\n                else:\n                    if time.time() - start_time &gt; self.max_time_mins*60:\n                        if self.population.evaluated_individuals[self.objective_names].isnull().all().iloc[0]:\n                            raise Exception(\"No individuals could be evaluated in the initial population as the max_eval_mins time limit was reached before any individuals could be evaluated.\")\n                        break\n                    self.step()\n\n                if self.verbose &gt;= 3:\n                    sign = np.sign(self.objective_function_weights)\n                    valid_df = self.population.evaluated_individuals[~self.population.evaluated_individuals[self.objective_names].isin([\"TIMEOUT\",\"INVALID\"]).any(axis=1)][self.objective_names]*sign\n                    cur_best_scores = valid_df.max(axis=0)*sign\n                    cur_best_scores = cur_best_scores.to_numpy()\n                    print(\"Generation: \", self.generation)\n                    for i, obj in enumerate(self.objective_names):\n                        print(f\"Best {obj} score: {cur_best_scores[i]}\")\n\n\n                if self.early_stop:\n                    if self.budget is None or self.budget&gt;=self.budget_range[-1]: #self.budget&gt;=1:\n                        #get sign of objective_function_weights\n                        sign = np.sign(self.objective_function_weights)\n                        #get best score for each objective\n                        valid_df = self.population.evaluated_individuals[~self.population.evaluated_individuals[self.objective_names].isin([\"TIMEOUT\",\"INVALID\"]).any(axis=1)][self.objective_names]*sign\n                        cur_best_scores = valid_df.max(axis=0)\n                        cur_best_scores = cur_best_scores.to_numpy()\n                        #cur_best_scores =  self.population.get_column(self.population.population, column_names=self.objective_names).max(axis=0)*sign #TODO this assumes the current population is the best\n\n                        improved = ( np.array(cur_best_scores) - np.array(best_scores) &gt;= np.array(self.early_stop_tol) )\n                        not_improved = np.logical_not(improved)\n                        generations_without_improvement = generations_without_improvement * not_improved + not_improved #set to zero if not improved, else increment\n                        pass\n                        #update best score\n                        best_scores = [max(best_scores[i], cur_best_scores[i]) for i in range(len(self.objective_names))]\n\n                        if all(generations_without_improvement&gt;self.early_stop):\n                            if self.verbose &gt;= 3:\n                                print(\"Early stop\")\n                            break\n\n                #save population\n                if self.population_file is not None: # and time.time() - last_save_time &gt; 60*10:\n                    pickle.dump(self.population, open(self.population_file, \"wb\"))\n\n                gen += 1\n                if self.verbose &gt;= 1:\n                    pbar.update(1)\n\n                if generations is not None and gen &gt;= generations:\n                    done = True\n\n        except KeyboardInterrupt:\n            if self.verbose &gt;= 3:\n                print(\"KeyboardInterrupt\")\n            self.population.remove_invalid_from_population(column_names=self.objective_names, invalid_value=\"INVALID\")\n            self.population.remove_invalid_from_population(column_names=self.objective_names, invalid_value=\"TIMEOUT\")\n            self.population.remove_invalid_from_population(column_names=\"Eval Error\", invalid_value=\"INVALID\")\n            self.population.remove_invalid_from_population(column_names=\"Eval Error\", invalid_value=\"TIMEOUT\")\n\n\n\n\n        if self.population_file is not None:\n            pickle.dump(self.population, open(self.population_file, \"wb\"))\n\n        if self.client is None: #If we created our own client, close it\n            self._client.close()\n            self._cluster.close()\n\n        tpot.utils.get_pareto_frontier(self.population.evaluated_individuals, column_names=self.objective_names, weights=self.objective_function_weights)\n\n    def step(self,):\n        \"\"\"\n        Runs a single generation of the evolutionary algorithm. This includes selecting individuals for survival, generating offspring, and evaluating the offspring.\n\n        \"\"\"\n\n\n        if self.population_size_list is not None:\n            if self.generation &lt; len(self.population_size_list):\n                self.cur_population_size = self.population_size_list[self.generation]\n            else:\n                self.cur_population_size = self.population_size\n\n        if self.budget_list is not None:\n            if len(self.budget_list) &lt;= self.generation:\n                self.budget = self.budget_range[-1]\n            else:\n                self.budget = self.budget_list[self.generation]\n        else:\n            self.budget = None\n\n        if self.survival_selector is not None:\n            n_survivors = max(1,int(self.cur_population_size*self.survival_percentage)) #always keep at least one individual\n            self.population.survival_select(    selector=self.survival_selector,\n                                                weights=self.objective_function_weights,\n                                                columns_names=self.objective_names,\n                                                n_survivors=n_survivors,\n                                                inplace=True,\n                                                rng=self.rng,)\n\n        self.generate_offspring()\n        self.evaluate_population()\n\n        self.generation += 1\n\n    def generate_offspring(self, ): #your EA Algorithm goes here\n        \"\"\"\n        Create population_size new individuals from the current population. \n        This includes selecting parents, applying mutation and crossover, and adding the new individuals to the population.\n        \"\"\"\n        parents = self.population.parent_select(selector=self.parent_selector, weights=self.objective_function_weights, columns_names=self.objective_names, k=self.cur_population_size, n_parents=2, rng=self.rng)\n        p = np.array([self.crossover_probability, self.mutate_then_crossover_probability, self.crossover_then_mutate_probability, self.mutate_probability])\n        p = p / p.sum()\n        var_op_list = self.rng.choice([\"crossover\", \"mutate_then_crossover\", \"crossover_then_mutate\", \"mutate\"], size=self.cur_population_size, p=p)\n\n        for i, op in enumerate(var_op_list):\n            if op == \"mutate\":\n                parents[i] = parents[i][0] #mutations take a single individual\n\n        offspring = self.population.create_offspring2(parents, var_op_list, self.mutation_functions, self.mutation_function_weights, self.crossover_functions, self.crossover_function_weights, add_to_population=True, keep_repeats=False, mutate_until_unique=True, rng=self.rng)\n\n        self.population.update_column(offspring, column_names=\"Generation\", data=self.generation, )\n\n    # Gets a list of unevaluated individuals in the livepopulation, evaluates them, and removes failed attempts\n    # TODO This could probably be an independent function?\n    def evaluate_population(self,):\n        \"\"\"\n        Evaluates the individuals in the population that have not been evaluated yet.\n        \"\"\"\n        #Update the sliding scales and thresholds\n        # Save population, TODO remove some of these\n        if self.population_file is not None: # and time.time() - last_save_time &gt; 60*10:\n            pickle.dump(self.population, open(self.population_file, \"wb\"))\n            last_save_time = time.time()\n\n\n        #Get the current thresholds per step\n        self.thresholds = None\n        if self.threshold_evaluation_pruning is not None:\n            old_data = self.population.evaluated_individuals[self.objective_names]\n            old_data = old_data[old_data[self.objective_names].notnull().all(axis=1)]\n            if len(old_data) &gt;= self.min_history_threshold:\n                self.thresholds = np.array([get_thresholds(old_data[obj_name],\n                                                            start=self.threshold_evaluation_pruning[0],\n                                                            end=self.threshold_evaluation_pruning[1],\n                                                            scale=self.threshold_evaluation_scaling,\n                                                            n=self.evaluation_early_stop_steps)\n                                        for obj_name in self.objective_names]).T\n\n        #Get the selectors survival rates per step\n        if self.selection_evaluation_pruning is not None:\n            lower = self.cur_population_size*self.selection_evaluation_pruning[0]\n            upper = self.cur_population_size*self.selection_evaluation_pruning[1]\n            #survival_counts = self.cur_population_size*(scipy.special.betainc(1,self.selection_evaluation_scaling,np.linspace(0,1,self.evaluation_early_stop_steps))*(upper-lower)+lower)\n\n            survival_counts = np.array(beta_interpolation(start=lower, end=upper, scale=self.selection_evaluation_scaling, n=self.evaluation_early_stop_steps, n_steps=self.evaluation_early_stop_steps))\n            self.survival_counts = survival_counts.astype(int)\n        else:\n            self.survival_counts = None\n\n\n\n        if self.evaluation_early_stop_steps is not None:\n            if self.survival_counts is None:\n                #TODO if we are not using selection method for each step, we can create single threads that run all steps for an individual. No need to come back each step.\n                self.evaluate_population_selection_early_stop(survival_counts=self.survival_counts, thresholds=self.thresholds, budget=self.budget)\n            else:\n                #parallelize one step at a time. After each step, come together and select the next individuals to run the next step on.\n                self.evaluate_population_selection_early_stop(survival_counts=self.survival_counts, thresholds=self.thresholds, budget=self.budget)\n        else:\n            self.evaluate_population_full(budget=self.budget)\n\n\n        # Save population, TODO remove some of these\n        if self.population_file is not None: # and time.time() - last_save_time &gt; 60*10:\n            pickle.dump(self.population, open(self.population_file, \"wb\"))\n            last_save_time = time.time()\n\n    def evaluate_population_full(self, budget=None):\n        \"\"\"\n        Evaluates all individuals in the population that have not been evaluated yet.\n        This is the normal/default strategy for evaluating individuals without any early stopping of individual evaluation functions. (e.g., no threshold or selection early stopping). Early stopping by generation is still possible.\n        \"\"\"\n        individuals_to_evaluate = self.get_unevaluated_individuals(self.objective_names, budget=budget,)\n\n        #print(\"evaluating this many individuals: \", len(individuals_to_evaluate))\n\n        if len(individuals_to_evaluate) == 0:\n            if self.verbose &gt; 3:\n                print(\"No new individuals to evaluate\")\n            return\n\n        if self.max_eval_time_mins is not None:\n            theoretical_timeout = self.max_eval_time_mins * math.ceil(len(individuals_to_evaluate) / self.n_jobs)\n            theoretical_timeout = theoretical_timeout*2\n        else:\n            theoretical_timeout = np.inf\n        scheduled_timeout_time_left = self.scheduled_timeout_time - time.time()\n        parallel_timeout = min(theoretical_timeout, scheduled_timeout_time_left)\n        if parallel_timeout &lt; 0:\n            parallel_timeout = 10\n\n        scores, start_times, end_times, eval_errors = tpot.utils.eval_utils.parallel_eval_objective_list(individuals_to_evaluate, self.objective_functions, verbose=self.verbose, max_eval_time_mins=self.max_eval_time_mins, budget=budget, n_expected_columns=len(self.objective_names), client=self._client, scheduled_timeout_time=self.scheduled_timeout_time, **self.objective_kwargs)\n\n        self.population.update_column(individuals_to_evaluate, column_names=self.objective_names, data=scores)\n        if budget is not None:\n            self.population.update_column(individuals_to_evaluate, column_names=\"Budget\", data=budget)\n\n        self.population.update_column(individuals_to_evaluate, column_names=\"Submitted Timestamp\", data=start_times)\n        self.population.update_column(individuals_to_evaluate, column_names=\"Completed Timestamp\", data=end_times)\n        self.population.update_column(individuals_to_evaluate, column_names=\"Eval Error\", data=eval_errors)\n        self.population.remove_invalid_from_population(column_names=\"Eval Error\")\n        self.population.remove_invalid_from_population(column_names=\"Eval Error\", invalid_value=\"TIMEOUT\")\n\n    def get_unevaluated_individuals(self, column_names, budget=None, individual_list=None):\n        \"\"\"\n        This function is used to get a list of individuals in the current population that have not been evaluated yet.\n\n        Parameters\n        ----------\n        column_names : list of strings\n            Names of the columns to check for unevaluated individuals (generally objective functions).\n        budget : float, default=None\n            Budget to use when checking for unevaluated individuals. If None, will not check the budget column.\n            Finds individuals who have not been evaluated with the given budget on column names.\n        individual_list : list of individuals, default=None\n            List of individuals to check for unevaluated individuals. If None, will use the current population.\n        \"\"\"\n        if individual_list is not None:\n            cur_pop = np.array(individual_list)\n        else:\n            cur_pop = np.array(self.population.population)\n\n        if all([name_step in self.population.evaluated_individuals.columns for name_step in column_names]):\n            if budget is not None:\n                offspring_scores = self.population.get_column(cur_pop, column_names=column_names+[\"Budget\"], to_numpy=False)\n                #Individuals are unevaluated if we have a higher budget OR if any of the objectives are nan\n                unevaluated_filter = lambda i: any(offspring_scores.loc[offspring_scores.index[i]][column_names].isna()) or (offspring_scores.loc[offspring_scores.index[i]][\"Budget\"] &lt; budget)\n            else:\n                offspring_scores = self.population.get_column(cur_pop, column_names=column_names, to_numpy=False)\n                unevaluated_filter = lambda i: any(offspring_scores.loc[offspring_scores.index[i]][column_names].isna())\n            unevaluated_individuals_this_step = [i for i in range(len(cur_pop)) if unevaluated_filter(i)]\n            return cur_pop[unevaluated_individuals_this_step]\n\n        else: #if column names are not in the evaluated_individuals, then we have not evaluated any individuals yet\n            for name_step in column_names:\n                self.population.evaluated_individuals[name_step] = np.nan\n            return cur_pop\n\n    def evaluate_population_selection_early_stop(self,survival_counts, thresholds=None, budget=None):\n        \"\"\"\n        This function tries to save computation by partially evaluating the individuals and then selecting which individuals to evaluate further based on the results of the partial evaluation. \n\n        Two strategies are implemented:\n            1.  Selection early stopping: Selects a percentage of the population to evaluate at each step of the evaluation. \n                for example, one strategy is to evaluate different steps of cross validation one at a time, and only select the best N individuals for subsequent steps. \n                This can save computation by not evaluating all individuals for all steps of cross validation. By default this selection is done with the NSGA2 selector.\n            2.  Threshold early stopping: At each step of the evaluation, a threshold is calculated based on the previous evaluations. All individuals that are below the performance threshold are not evaluated for further steps.\n                For example, if the threshold is set to the 90th percentile of the previous evaluations, all individuals that are below the 90th percentile are not evaluated further. This can save computation by not evaluating all individuals for all steps of cross validation.\n\n        Both of these strategies can be used simultaneously. Individuals must pass both the selection and threshold criteria to be evaluated further.\n\n        Parameters\n        ----------\n        survival_counts : list of ints, default=None\n            Number of individuals to select for survival at each step of the evaluation. If None, will not use selection early stopping.\n            For example: [10, 5, 2] would select 10 individuals for the first step, 5 for the second, and 2 for the third.\n        thresholds : list of floats, default=None\n            Thresholds to use for early stopping at each step of the evaluation. If None, will not use threshold early stopping.\n        budget : float, default=None\n            Budget to use when evaluating individuals. Use is dependent on the objective functions. (In TPOTEstimator this corresponds to the percentage of the data to sample.)\n        \"\"\"\n\n        survival_selector = tpot.selectors.survival_select_NSGA2\n\n        ################\n\n        objective_function_signs = np.sign(self.objective_function_weights)\n\n\n        cur_individuals = self.population.population.copy()\n\n        all_step_names = []\n        for step in range(self.evaluation_early_stop_steps):\n            if budget is None:\n                this_step_names = [f\"{n}_step_{step}\" for n in self.objective_names]\n            else:\n                this_step_names = [f\"{n}_budget_{budget}_step_{step}\" for n in self.objective_names]\n\n            all_step_names.append(this_step_names)\n\n            unevaluated_individuals_this_step = self.get_unevaluated_individuals(this_step_names, budget=None, individual_list=cur_individuals)\n\n            if len(unevaluated_individuals_this_step) == 0:\n                if self.verbose &gt; 3:\n                    print(\"No new individuals to evaluate\")\n                continue\n\n            if self.max_eval_time_mins is not None:\n                theoretical_timeout = self.max_eval_time_mins * math.ceil(len(unevaluated_individuals_this_step) / self.n_jobs)*60\n                theoretical_timeout = theoretical_timeout*2\n            else:\n                theoretical_timeout = np.inf\n            scheduled_timeout_time_left = self.scheduled_timeout_time - time.time()\n            parallel_timeout = min(theoretical_timeout, scheduled_timeout_time_left)\n            if parallel_timeout &lt; 0:\n                parallel_timeout = 10\n\n            scores, start_times, end_times, eval_errors = tpot.utils.eval_utils.parallel_eval_objective_list(individual_list=unevaluated_individuals_this_step,\n                                    objective_list=self.objective_functions,\n                                    verbose=self.verbose,\n                                    max_eval_time_mins=self.max_eval_time_mins,\n                                    step=step,\n                                    budget = self.budget,\n                                    generation = self.generation,\n                                    n_expected_columns=len(self.objective_names),\n                                    client=self._client,\n                                    scheduled_timeout_time=self.scheduled_timeout_time,\n                                    **self.objective_kwargs,\n                                    )\n\n            self.population.update_column(unevaluated_individuals_this_step, column_names=this_step_names, data=scores)\n            self.population.update_column(unevaluated_individuals_this_step, column_names=\"Submitted Timestamp\", data=start_times)\n            self.population.update_column(unevaluated_individuals_this_step, column_names=\"Completed Timestamp\", data=end_times)\n            self.population.update_column(unevaluated_individuals_this_step, column_names=\"Eval Error\", data=eval_errors)\n\n            self.population.remove_invalid_from_population(column_names=\"Eval Error\")\n            self.population.remove_invalid_from_population(column_names=\"Eval Error\", invalid_value=\"TIMEOUT\")\n\n            #remove invalids:\n            invalids = []\n            #find indeces of invalids\n\n            for j in range(len(scores)):\n                if  any([s==\"INVALID\" for s in scores[j]]):\n                    invalids.append(j)\n\n            for j in range(len(scores)):\n                if  any([s==\"TIMEOUT\" for s in scores[j]]):\n                    invalids.append(j)\n\n\n            #already evaluated\n            already_evaluated = list(set(cur_individuals) - set(unevaluated_individuals_this_step))\n            #evaluated and valid\n            valid_evaluations_this_step = remove_items(unevaluated_individuals_this_step,invalids)\n            #update cur_individuals with current individuals with valid scores\n            cur_individuals = np.concatenate([already_evaluated, valid_evaluations_this_step])\n\n\n            #Get average scores\n\n            #array of shape (steps, individuals, objectives)\n            offspring_scores = [self.population.get_column(cur_individuals, column_names=step_names) for step_names in all_step_names]\n            offspring_scores = np.array(offspring_scores)\n            if self.final_score_strategy == 'mean':\n                offspring_scores  = offspring_scores.mean(axis=0)\n            elif self.final_score_strategy == 'last':\n                offspring_scores = offspring_scores[-1]\n\n            #remove individuals with nan scores\n            invalids = []\n            for i in range(len(offspring_scores)):\n                if any(np.isnan(offspring_scores[i])):\n                    invalids.append(i)\n\n            cur_individuals = remove_items(cur_individuals,invalids)\n            offspring_scores = remove_items(offspring_scores,invalids)\n\n            #if last step, add the final metrics\n            if step == self.evaluation_early_stop_steps-1:\n                self.population.update_column(cur_individuals, column_names=self.objective_names, data=offspring_scores)\n                if budget is not None:\n                    self.population.update_column(cur_individuals, column_names=\"Budget\", data=budget)\n                return\n\n            #If we have more threads than remaining individuals, we may as well evaluate the extras too\n            if self.n_jobs &lt; len(cur_individuals):\n                #Remove based on thresholds\n                if thresholds is not None:\n                    threshold = thresholds[step]\n                    invalids = []\n                    for i in range(len(offspring_scores)):\n\n                        if all([s*w&gt;t*w for s,t,w in zip(offspring_scores[i],threshold,objective_function_signs)  ]):\n                            invalids.append(i)\n\n                    if len(invalids) &gt; 0:\n\n                        max_to_remove = min(len(cur_individuals) - self.n_jobs, len(invalids))\n\n                        if max_to_remove &lt; len(invalids):\n                            # invalids = np.random.choice(invalids, max_to_remove, replace=False)\n                            invalids = self.rng.choice(invalids, max_to_remove, replace=False)\n\n                        cur_individuals = remove_items(cur_individuals,invalids)\n                        offspring_scores = remove_items(offspring_scores,invalids)\n\n                # Remove based on selection\n                if survival_counts is not None:\n                    if step &lt; self.evaluation_early_stop_steps - 1 and survival_counts[step]&gt;1: #don't do selection for the last loop since they are completed\n                        k = survival_counts[step] + len(invalids) #TODO can remove the min if the selections method can ignore k&gt;population size\n                        if len(cur_individuals)&gt; 1 and k &gt; self.n_jobs and k &lt; len(cur_individuals):\n                            weighted_scores = np.array([s * self.objective_function_weights for s in offspring_scores ])\n\n                            new_population_index = survival_selector(weighted_scores, k=k)\n                            cur_individuals = np.array(cur_individuals)[new_population_index]\n                            offspring_scores = offspring_scores[new_population_index]\n</code></pre>"},{"location":"documentation/tpot/evolvers/base_evolver/#tpot.evolvers.base_evolver.BaseEvolver.__init__","title":"<code>__init__(individual_generator, objective_functions, objective_function_weights, objective_names=None, objective_kwargs=None, bigger_is_better=True, population_size=50, initial_population_size=None, population_scaling=0.5, generations_until_end_population=1, generations=50, early_stop=None, early_stop_tol=0.001, max_time_mins=float('inf'), max_eval_time_mins=5, n_jobs=1, memory_limit='4GB', client=None, survival_percentage=1, crossover_probability=0.2, mutate_probability=0.7, mutate_then_crossover_probability=0.05, crossover_then_mutate_probability=0.05, mutation_functions=[ind_mutate], crossover_functions=[ind_crossover], mutation_function_weights=None, crossover_function_weights=None, n_parents=2, survival_selector=survival_select_NSGA2, parent_selector=tournament_selection_dominated, budget_range=None, budget_scaling=0.5, generations_until_end_budget=1, stepwise_steps=5, threshold_evaluation_pruning=None, threshold_evaluation_scaling=0.5, min_history_threshold=20, selection_evaluation_pruning=None, selection_evaluation_scaling=0.5, evaluation_early_stop_steps=None, final_score_strategy='mean', verbose=0, periodic_checkpoint_folder=None, callback=None, rng=None)</code>","text":"<p>Uses mutation, crossover, and optimization functions to evolve a population of individuals towards the given objective functions.</p> <p>Parameters:</p> Name Type Description Default <code>individual_generator</code> <code>generator</code> <p>Generator that yields new base individuals. Used to generate initial population.</p> required <code>objective_functions</code> <code>list of callables</code> <p>list of functions that get applied to the individual and return a float or list of floats If an objective function returns multiple values, they are all concatenated in order with respect to objective_function_weights and early_stop_tol.</p> required <code>objective_function_weights</code> <code>list of floats</code> <p>list of weights for each objective function. Sign flips whether bigger is better or not</p> required <code>objective_names</code> <code>list of strings</code> <p>Names of the objectives. If None, objective0, objective1, etc. will be used</p> <code>None</code> <code>objective_kwargs</code> <code>dict</code> <p>Dictionary of keyword arguments to pass to the objective function</p> <code>None</code> <code>bigger_is_better</code> <code>bool</code> <p>If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.</p> <code>True</code> <code>population_size</code> <code>int</code> <p>Size of the population</p> <code>50</code> <code>initial_population_size</code> <code>int</code> <p>Size of the initial population. If None, population_size will be used.</p> <code>None</code> <code>population_scaling</code> <code>int</code> <p>Scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.</p> <code>0.5</code> <code>generations_until_end_population</code> <code>int</code> <p>Number of generations until the population size reaches population_size</p> <code>1</code> <code>generations</code> <code>int</code> <p>Number of generations to run</p> <code>50</code> <code>early_stop</code> <code>int</code> <p>Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered. In general a value of around 5-20 is good.</p> <code>None</code> <code>early_stop_tol</code> <code>float, list of floats, or None</code> <p>-list of floats     list of tolerances for each objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged     If an index of the list is None, that item will not be used for early stopping -int     If an int is given, it will be used as the tolerance for all objectives</p> <code>0.001</code> <code>max_time_mins</code> <code>float</code> <p>Maximum time to run the optimization. If none or inf, will run until the end of the generations.</p> <code>float(\"inf\")</code> <code>max_eval_time_mins</code> <code>float</code> <p>Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.</p> <code>10</code> <code>n_jobs</code> <code>int</code> <p>Number of processes to run in parallel.</p> <code>1</code> <code>memory_limit</code> <code>str</code> <p>Memory limit for each job. See Dask LocalCluster documentation for more information.</p> <code>None</code> <code>client</code> <code>Client</code> <p>A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.</p> <code>None</code> <code>survival_percentage</code> <code>float</code> <p>Percentage of the population size to utilize for mutation and crossover at the beginning of the generation. The rest are discarded. Individuals are selected with the selector passed into survival_selector. The value of this parameter must be between 0 and 1, inclusive. For example, if the population size is 100 and the survival percentage is .5, 50 individuals will be selected with NSGA2 from the existing population. These will be used for mutation and crossover to generate the next 100 individuals for the next generation. The remainder are discarded from the live population. In the next generation, there will now be the 50 parents + the 100 individuals for a total of 150. Surivival percentage is based of the population size parameter and not the existing population size (current population size when using successive halving). Therefore, in the next generation we will still select 50 individuals from the currently existing 150.</p> <code>1</code> <code>crossover_probability</code> <code>float</code> <p>Probability of generating a new individual by crossover between two individuals.</p> <code>.2</code> <code>mutate_probability</code> <code>float</code> <p>Probability of generating a new individual by crossover between one individuals.</p> <code>.7</code> <code>mutate_then_crossover_probability</code> <code>float</code> <p>Probability of generating a new individual by mutating two individuals followed by crossover.</p> <code>.05</code> <code>crossover_then_mutate_probability</code> <code>float</code> <p>Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.</p> <code>.05</code> <code>n_parents</code> <code>int</code> <p>Number of parents to use for crossover. Must be greater than 1.</p> <code>2</code> <code>survival_selector</code> <code>function</code> <p>Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes. Used to selected population_size * survival_percentage individuals at the start of each generation to use for mutation and crossover.</p> <code>survival_select_NSGA2</code> <code>parent_selector</code> <code>function</code> <p>Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.</p> <code>parent_select_NSGA2</code> <code>budget_range</code> <code>list[start, end]</code> <p>This parameter is used for the successive halving algorithm. A starting and ending budget to use for the budget scaling. The evolver will interpolate between these values over the generations_until_end_budget. Use is dependent on the objective functions. (In TPOTEstimator this corresponds to the percentage of the data to sample.)</p> <code>None</code> <code>budget_scaling</code> <p>A scaling factor to use when determining how fast we move the budget from the start to end budget.</p> <code>0.5</code> <code>generations_until_end_budget</code> <code>int</code> <p>The number of generations to run before reaching the max budget.</p> <code>1</code> <code>stepwise_steps</code> <code>int</code> <p>The number of staircase steps to take when interpolating the budget and population size.</p> <code>1</code> <code>threshold_evaluation_pruning</code> <code>list[start, end]</code> <p>Starting and ending percentile to use as a threshold for the evaluation early stopping. The evolver will interpolate between these values over the evaluation_early_stop_steps. Values between 0 and 100. At each step of the evaluation, a threshold is calculated based on the previous evaluations. All individuals that are below the performance threshold are not evaluated for further steps. For example, if the threshold is set to the 90th percentile of the previous evaluations, all individuals that are below the 90th percentile are not evaluated further. This can save computation by not evaluating all individuals for all steps of cross validation.</p> <code>None</code> <code>threshold_evaluation_scaling</code> <code>float [0,inf)</code> <p>A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile. Must be greater than zero. Higher numbers will move the threshold to the end faster.</p> <code>0.5</code> <code>min_history_threshold</code> <code>int</code> <p>The minimum number of previous scores needed before using threshold early stopping.</p> <code>0</code> <code>selection_evaluation_pruning</code> <code>list</code> <p>A lower and upper percent of the population size to select each round of CV. Values between 0 and 1. Selects a percentage of the population to evaluate at each step of the evaluation.  For example, one strategy is to evaluate different steps of cross validation one at a time, and only select the best N individuals for subsequent steps.  This can save computation by not evaluating all individuals for all steps of cross validation. By default this selection is done with the NSGA2 selector.</p> <code>None</code> <code>selection_evaluation_scaling</code> <code>float</code> <p>A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile. Must be greater than zero. Higher numbers will move the threshold to the end faster.</p> <code>0.5</code> <code>evaluation_early_stop_steps</code> <code>int</code> <p>The number of steps that will be taken from the objective function. (e.g., the number of CV folds to evaluate)</p> <code>1</code> <code>final_score_strategy</code> <code>str</code> <p>The strategy to use when determining the final score for an individual. \"mean\": The mean of all objective scores \"last\": The score returned by the last call. Currently each objective is evaluated with a clone of the individual.</p> <code>\"mean\"</code> <code>verbose</code> <code>int</code> <p>How much information to print during the optimization process. Higher values include the information from lower values. 0. nothing 1. progress bar 2. evaluations progress bar 3. best individual 4. warnings</p> <p>=5. full warnings trace</p> <code>0</code> <code>periodic_checkpoint_folder</code> <code>str</code> <p>Folder to save the population to periodically. If None, no periodic saving will be done. If provided, training will resume from this checkpoint.</p> <code>None</code> <code>callback</code> <code>CallBackInterface</code> <p>Callback object. Not implemented</p> <code>None</code> <code>rng</code> <code>(Generator, None)</code> <p>An object for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes</p> <ul> <li>Numpy.Random.Generator     Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'. Note this will be the same Generator passed in.</li> <li>None     Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS</li> </ul> <code>None</code> <p>Attributes:</p> Name Type Description <code>population</code> <code>Population</code> <p>The population of individuals. Use population.population to access the individuals in the current population. Use population.evaluated_individuals to access a data frame of all individuals that have been explored.</p> Source code in <code>tpot/evolvers/base_evolver.py</code> <pre><code>def __init__(   self,\n                individual_generator ,\n\n                objective_functions,\n                objective_function_weights,\n                objective_names = None,\n                objective_kwargs = None,\n                bigger_is_better = True,\n\n                population_size = 50,\n                initial_population_size = None,\n                population_scaling = .5,\n                generations_until_end_population = 1,\n                generations = 50,\n                early_stop = None,\n                early_stop_tol = 0.001,\n\n\n                max_time_mins=float(\"inf\"),\n                max_eval_time_mins=5,\n\n                n_jobs=1,\n                memory_limit=\"4GB\",\n                client=None,\n\n                survival_percentage = 1,\n                crossover_probability=.2,\n                mutate_probability=.7,\n                mutate_then_crossover_probability=.05,\n                crossover_then_mutate_probability=.05,\n\n                mutation_functions = [ind_mutate],\n                crossover_functions = [ind_crossover],\n\n                mutation_function_weights = None,\n                crossover_function_weights = None,\n\n                n_parents=2,\n\n                survival_selector = survival_select_NSGA2,\n                parent_selector = tournament_selection_dominated,\n\n                budget_range = None,\n                budget_scaling = .5,\n                generations_until_end_budget = 1,\n                stepwise_steps = 5,\n\n                threshold_evaluation_pruning = None,\n                threshold_evaluation_scaling = .5,\n                min_history_threshold = 20,\n                selection_evaluation_pruning = None,\n                selection_evaluation_scaling = .5,\n                evaluation_early_stop_steps = None,\n                final_score_strategy = \"mean\",\n\n                verbose = 0,\n                periodic_checkpoint_folder = None,\n                callback = None,\n                rng=None,\n\n                ) -&gt; None:\n    \"\"\"\n    Uses mutation, crossover, and optimization functions to evolve a population of individuals towards the given objective functions.\n\n    Parameters\n    ----------\n    individual_generator : generator\n        Generator that yields new base individuals. Used to generate initial population.\n    objective_functions : list of callables\n        list of functions that get applied to the individual and return a float or list of floats\n        If an objective function returns multiple values, they are all concatenated in order\n        with respect to objective_function_weights and early_stop_tol.\n    objective_function_weights : list of floats\n        list of weights for each objective function. Sign flips whether bigger is better or not\n    objective_names : list of strings, default=None\n        Names of the objectives. If None, objective0, objective1, etc. will be used\n    objective_kwargs : dict, default=None\n        Dictionary of keyword arguments to pass to the objective function\n    bigger_is_better : bool, default=True\n        If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n    population_size : int, default=50\n        Size of the population\n    initial_population_size : int, default=None\n        Size of the initial population. If None, population_size will be used.\n    population_scaling : int, default=0.5\n        Scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n    generations_until_end_population : int, default=1\n        Number of generations until the population size reaches population_size\n    generations : int, default=50\n        Number of generations to run\n    early_stop : int, default=None\n        Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered. In general a value of around 5-20 is good.\n    early_stop_tol : float, list of floats, or None, default=0.001\n        -list of floats\n            list of tolerances for each objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n            If an index of the list is None, that item will not be used for early stopping\n        -int\n            If an int is given, it will be used as the tolerance for all objectives\n    max_time_mins : float, default=float(\"inf\")\n        Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n    max_eval_time_mins : float, default=10\n        Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n    n_jobs : int, default=1\n        Number of processes to run in parallel.\n    memory_limit : str, default=None\n        Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n    client : dask.distributed.Client, default=None\n        A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.\n    survival_percentage : float, default=1\n        Percentage of the population size to utilize for mutation and crossover at the beginning of the generation. The rest are discarded. Individuals are selected with the selector passed into survival_selector. The value of this parameter must be between 0 and 1, inclusive.\n        For example, if the population size is 100 and the survival percentage is .5, 50 individuals will be selected with NSGA2 from the existing population. These will be used for mutation and crossover to generate the next 100 individuals for the next generation. The remainder are discarded from the live population. In the next generation, there will now be the 50 parents + the 100 individuals for a total of 150. Surivival percentage is based of the population size parameter and not the existing population size (current population size when using successive halving). Therefore, in the next generation we will still select 50 individuals from the currently existing 150.\n    crossover_probability : float, default=.2\n        Probability of generating a new individual by crossover between two individuals.\n    mutate_probability : float, default=.7\n        Probability of generating a new individual by crossover between one individuals.\n    mutate_then_crossover_probability : float, default=.05\n        Probability of generating a new individual by mutating two individuals followed by crossover.\n    crossover_then_mutate_probability : float, default=.05\n        Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.\n    n_parents : int, default=2\n        Number of parents to use for crossover. Must be greater than 1.\n    survival_selector : function, default=survival_select_NSGA2\n        Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes.\n        Used to selected population_size * survival_percentage individuals at the start of each generation to use for mutation and crossover.\n    parent_selector : function, default=parent_select_NSGA2\n        Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.\n    budget_range : list [start, end], default=None\n        This parameter is used for the successive halving algorithm.\n        A starting and ending budget to use for the budget scaling. The evolver will interpolate between these values over the generations_until_end_budget.\n        Use is dependent on the objective functions. (In TPOTEstimator this corresponds to the percentage of the data to sample.)\n    budget_scaling float : [0,1], default=0.5\n        A scaling factor to use when determining how fast we move the budget from the start to end budget.\n    generations_until_end_budget : int, default=1\n        The number of generations to run before reaching the max budget.\n    stepwise_steps : int, default=1\n        The number of staircase steps to take when interpolating the budget and population size.\n    threshold_evaluation_pruning : list [start, end], default=None\n        Starting and ending percentile to use as a threshold for the evaluation early stopping. The evolver will interpolate between these values over the evaluation_early_stop_steps.\n        Values between 0 and 100.\n        At each step of the evaluation, a threshold is calculated based on the previous evaluations. All individuals that are below the performance threshold are not evaluated for further steps.\n        For example, if the threshold is set to the 90th percentile of the previous evaluations, all individuals that are below the 90th percentile are not evaluated further. This can save computation by not evaluating all individuals for all steps of cross validation.\n    threshold_evaluation_scaling : float [0,inf), default=0.5\n        A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n        Must be greater than zero. Higher numbers will move the threshold to the end faster.\n    min_history_threshold : int, default=0\n        The minimum number of previous scores needed before using threshold early stopping.\n    selection_evaluation_pruning : list, default=None\n        A lower and upper percent of the population size to select each round of CV.\n        Values between 0 and 1.\n        Selects a percentage of the population to evaluate at each step of the evaluation. \n        For example, one strategy is to evaluate different steps of cross validation one at a time, and only select the best N individuals for subsequent steps. \n        This can save computation by not evaluating all individuals for all steps of cross validation. By default this selection is done with the NSGA2 selector.\n    selection_evaluation_scaling : float, default=0.5\n        A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n        Must be greater than zero. Higher numbers will move the threshold to the end faster.\n    evaluation_early_stop_steps : int, default=1\n        The number of steps that will be taken from the objective function. (e.g., the number of CV folds to evaluate)\n    final_score_strategy : str, default=\"mean\"\n        The strategy to use when determining the final score for an individual.\n        \"mean\": The mean of all objective scores\n        \"last\": The score returned by the last call. Currently each objective is evaluated with a clone of the individual.\n    verbose : int, default=0\n        How much information to print during the optimization process. Higher values include the information from lower values.\n        0. nothing\n        1. progress bar\n        2. evaluations progress bar\n        3. best individual\n        4. warnings\n        &gt;=5. full warnings trace\n    periodic_checkpoint_folder : str, default=None\n        Folder to save the population to periodically. If None, no periodic saving will be done.\n        If provided, training will resume from this checkpoint.\n    callback : tpot.CallBackInterface, default=None\n        Callback object. Not implemented\n    rng : Numpy.Random.Generator, None, default=None\n        An object for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes\n\n        - Numpy.Random.Generator\n            Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'. Note this will be the same Generator passed in.\n        - None\n            Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS\n\n    Attributes\n    ----------\n    population : tpot.Population\n        The population of individuals.\n        Use population.population to access the individuals in the current population.\n        Use population.evaluated_individuals to access a data frame of all individuals that have been explored.\n\n    \"\"\"\n\n    self.rng = np.random.default_rng(rng)\n\n    if threshold_evaluation_pruning is not None or selection_evaluation_pruning is not None:\n        if evaluation_early_stop_steps is None:\n            raise ValueError(\"evaluation_early_stop_steps must be set when using threshold_evaluation_pruning or selection_evaluation_pruning\")\n\n    self.individual_generator = individual_generator\n    self.population_size = population_size\n    self.objective_functions = objective_functions\n    self.objective_function_weights = np.array(objective_function_weights)\n    self.bigger_is_better = bigger_is_better\n    if not bigger_is_better:\n        self.objective_function_weights = np.array(self.objective_function_weights)*-1\n\n    self.initial_population_size = initial_population_size\n    if self.initial_population_size is None:\n        self.cur_population_size = population_size\n    else:\n        self.cur_population_size = initial_population_size\n\n    self.population_scaling = population_scaling\n    self.generations_until_end_population = generations_until_end_population\n\n    self.population_size_list = None\n\n\n    self.periodic_checkpoint_folder = periodic_checkpoint_folder\n    self.verbose  = verbose\n    self.callback = callback\n    self.generations = generations\n    self.n_jobs = n_jobs\n\n\n\n    if max_time_mins is None:\n        self.max_time_mins = float(\"inf\")\n    else:\n        self.max_time_mins = max_time_mins\n\n    #functools requires none for infinite time, doesn't support inf\n    if max_eval_time_mins is not None and math.isinf(max_eval_time_mins ):\n        self.max_eval_time_mins = None\n    else:\n        self.max_eval_time_mins = max_eval_time_mins\n\n\n\n\n    self.generation = 0\n\n\n    self.threshold_evaluation_pruning =threshold_evaluation_pruning\n    self.threshold_evaluation_scaling =  max(0.00001,threshold_evaluation_scaling )\n    self.min_history_threshold = min_history_threshold\n\n    self.selection_evaluation_pruning = selection_evaluation_pruning\n    self.selection_evaluation_scaling =  max(0.00001,selection_evaluation_scaling )\n    self.evaluation_early_stop_steps = evaluation_early_stop_steps\n    self.final_score_strategy = final_score_strategy\n\n    self.budget_range = budget_range\n    self.budget_scaling = budget_scaling\n    self.generations_until_end_budget = generations_until_end_budget\n    self.stepwise_steps = stepwise_steps\n\n    self.memory_limit = memory_limit\n\n    self.client = client\n\n\n    self.survival_selector=survival_selector\n    self.parent_selector=parent_selector\n    self.survival_percentage = survival_percentage\n\n    total_var_p = crossover_probability + mutate_probability + mutate_then_crossover_probability + crossover_then_mutate_probability\n    self.crossover_probability = crossover_probability / total_var_p\n    self.mutate_probability = mutate_probability  / total_var_p\n    self.mutate_then_crossover_probability= mutate_then_crossover_probability / total_var_p\n    self.crossover_then_mutate_probability= crossover_then_mutate_probability / total_var_p\n\n\n    self.mutation_functions = mutation_functions\n    self.crossover_functions = crossover_functions\n\n    if mutation_function_weights is None:\n        self.mutation_function_weights = [1 for _ in range(len(mutation_functions))]\n    else:\n        self.mutation_function_weights = mutation_function_weights\n\n    if mutation_function_weights is None:\n        self.crossover_function_weights = [1 for _ in range(len(mutation_functions))]\n    else:\n        self.crossover_function_weights = crossover_function_weights\n\n    self.n_parents = n_parents\n\n    if objective_kwargs is None:\n        self.objective_kwargs = {}\n    else:\n        self.objective_kwargs = objective_kwargs\n\n    # if objective_kwargs is None:\n    #     self.objective_kwargs = [{}] * len(self.objective_functions)\n    # elif isinstance(objective_kwargs, dict):\n    #     self.objective_kwargs = [objective_kwargs] * len(self.objective_functions)\n    # else:\n    #     self.objective_kwargs = objective_kwargs\n\n    ###########\n\n    if self.initial_population_size != self.population_size:\n        self.population_size_list = beta_interpolation(start=self.cur_population_size, end=self.population_size, scale=self.population_scaling, n=generations_until_end_population, n_steps=self.stepwise_steps)\n        self.population_size_list = np.round(self.population_size_list).astype(int)\n\n    if self.budget_range is None:\n        self.budget_list = None\n    else:\n        self.budget_list = beta_interpolation(start=self.budget_range[0], end=self.budget_range[1], n=self.generations_until_end_budget, scale=self.budget_scaling, n_steps=self.stepwise_steps)\n\n    if objective_names is None:\n        self.objective_names = [\"objective\"+str(i) for i in range(len(objective_function_weights))]\n    else:\n        self.objective_names = objective_names\n\n    if self.budget_list is not None:\n        if len(self.budget_list) &lt;= self.generation:\n            self.budget = self.budget_list[-1]\n        else:\n            self.budget = self.budget_list[self.generation]\n    else:\n        self.budget = None\n\n\n    self.early_stop_tol = early_stop_tol\n    self.early_stop = early_stop\n\n    if isinstance(self.early_stop_tol, float):\n        self.early_stop_tol = [self.early_stop_tol for _ in range(len(self.objective_names))]\n\n    self.early_stop_tol = [np.inf if tol is None else tol for tol in self.early_stop_tol]\n\n    self.population = None\n    self.population_file = None\n    if self.periodic_checkpoint_folder is not None:\n        self.population_file = os.path.join(self.periodic_checkpoint_folder, \"population.pkl\")\n        if not os.path.exists(self.periodic_checkpoint_folder):\n            os.makedirs(self.periodic_checkpoint_folder)\n        if os.path.exists(self.population_file):\n            self.population = pickle.load(open(self.population_file, \"rb\"))\n\n            if len(self.population.evaluated_individuals)&gt;0 and \"Generation\" in self.population.evaluated_individuals.columns:\n                self.generation = self.population.evaluated_individuals['Generation'].max() + 1 #TODO check if this is empty?\n\n    init_names = self.objective_names\n    if self.budget_range is not None:\n        init_names = init_names + [\"Budget\"]\n    if self.population is None:\n        self.population = tpot.Population(column_names=init_names)\n        initial_population = [next(self.individual_generator) for _ in range(self.cur_population_size)]\n        self.population.add_to_population(initial_population, self.rng)\n        self.population.update_column(self.population.population, column_names=\"Generation\", data=self.generation)\n</code></pre>"},{"location":"documentation/tpot/evolvers/base_evolver/#tpot.evolvers.base_evolver.BaseEvolver.evaluate_population","title":"<code>evaluate_population()</code>","text":"<p>Evaluates the individuals in the population that have not been evaluated yet.</p> Source code in <code>tpot/evolvers/base_evolver.py</code> <pre><code>def evaluate_population(self,):\n    \"\"\"\n    Evaluates the individuals in the population that have not been evaluated yet.\n    \"\"\"\n    #Update the sliding scales and thresholds\n    # Save population, TODO remove some of these\n    if self.population_file is not None: # and time.time() - last_save_time &gt; 60*10:\n        pickle.dump(self.population, open(self.population_file, \"wb\"))\n        last_save_time = time.time()\n\n\n    #Get the current thresholds per step\n    self.thresholds = None\n    if self.threshold_evaluation_pruning is not None:\n        old_data = self.population.evaluated_individuals[self.objective_names]\n        old_data = old_data[old_data[self.objective_names].notnull().all(axis=1)]\n        if len(old_data) &gt;= self.min_history_threshold:\n            self.thresholds = np.array([get_thresholds(old_data[obj_name],\n                                                        start=self.threshold_evaluation_pruning[0],\n                                                        end=self.threshold_evaluation_pruning[1],\n                                                        scale=self.threshold_evaluation_scaling,\n                                                        n=self.evaluation_early_stop_steps)\n                                    for obj_name in self.objective_names]).T\n\n    #Get the selectors survival rates per step\n    if self.selection_evaluation_pruning is not None:\n        lower = self.cur_population_size*self.selection_evaluation_pruning[0]\n        upper = self.cur_population_size*self.selection_evaluation_pruning[1]\n        #survival_counts = self.cur_population_size*(scipy.special.betainc(1,self.selection_evaluation_scaling,np.linspace(0,1,self.evaluation_early_stop_steps))*(upper-lower)+lower)\n\n        survival_counts = np.array(beta_interpolation(start=lower, end=upper, scale=self.selection_evaluation_scaling, n=self.evaluation_early_stop_steps, n_steps=self.evaluation_early_stop_steps))\n        self.survival_counts = survival_counts.astype(int)\n    else:\n        self.survival_counts = None\n\n\n\n    if self.evaluation_early_stop_steps is not None:\n        if self.survival_counts is None:\n            #TODO if we are not using selection method for each step, we can create single threads that run all steps for an individual. No need to come back each step.\n            self.evaluate_population_selection_early_stop(survival_counts=self.survival_counts, thresholds=self.thresholds, budget=self.budget)\n        else:\n            #parallelize one step at a time. After each step, come together and select the next individuals to run the next step on.\n            self.evaluate_population_selection_early_stop(survival_counts=self.survival_counts, thresholds=self.thresholds, budget=self.budget)\n    else:\n        self.evaluate_population_full(budget=self.budget)\n\n\n    # Save population, TODO remove some of these\n    if self.population_file is not None: # and time.time() - last_save_time &gt; 60*10:\n        pickle.dump(self.population, open(self.population_file, \"wb\"))\n        last_save_time = time.time()\n</code></pre>"},{"location":"documentation/tpot/evolvers/base_evolver/#tpot.evolvers.base_evolver.BaseEvolver.evaluate_population_full","title":"<code>evaluate_population_full(budget=None)</code>","text":"<p>Evaluates all individuals in the population that have not been evaluated yet. This is the normal/default strategy for evaluating individuals without any early stopping of individual evaluation functions. (e.g., no threshold or selection early stopping). Early stopping by generation is still possible.</p> Source code in <code>tpot/evolvers/base_evolver.py</code> <pre><code>def evaluate_population_full(self, budget=None):\n    \"\"\"\n    Evaluates all individuals in the population that have not been evaluated yet.\n    This is the normal/default strategy for evaluating individuals without any early stopping of individual evaluation functions. (e.g., no threshold or selection early stopping). Early stopping by generation is still possible.\n    \"\"\"\n    individuals_to_evaluate = self.get_unevaluated_individuals(self.objective_names, budget=budget,)\n\n    #print(\"evaluating this many individuals: \", len(individuals_to_evaluate))\n\n    if len(individuals_to_evaluate) == 0:\n        if self.verbose &gt; 3:\n            print(\"No new individuals to evaluate\")\n        return\n\n    if self.max_eval_time_mins is not None:\n        theoretical_timeout = self.max_eval_time_mins * math.ceil(len(individuals_to_evaluate) / self.n_jobs)\n        theoretical_timeout = theoretical_timeout*2\n    else:\n        theoretical_timeout = np.inf\n    scheduled_timeout_time_left = self.scheduled_timeout_time - time.time()\n    parallel_timeout = min(theoretical_timeout, scheduled_timeout_time_left)\n    if parallel_timeout &lt; 0:\n        parallel_timeout = 10\n\n    scores, start_times, end_times, eval_errors = tpot.utils.eval_utils.parallel_eval_objective_list(individuals_to_evaluate, self.objective_functions, verbose=self.verbose, max_eval_time_mins=self.max_eval_time_mins, budget=budget, n_expected_columns=len(self.objective_names), client=self._client, scheduled_timeout_time=self.scheduled_timeout_time, **self.objective_kwargs)\n\n    self.population.update_column(individuals_to_evaluate, column_names=self.objective_names, data=scores)\n    if budget is not None:\n        self.population.update_column(individuals_to_evaluate, column_names=\"Budget\", data=budget)\n\n    self.population.update_column(individuals_to_evaluate, column_names=\"Submitted Timestamp\", data=start_times)\n    self.population.update_column(individuals_to_evaluate, column_names=\"Completed Timestamp\", data=end_times)\n    self.population.update_column(individuals_to_evaluate, column_names=\"Eval Error\", data=eval_errors)\n    self.population.remove_invalid_from_population(column_names=\"Eval Error\")\n    self.population.remove_invalid_from_population(column_names=\"Eval Error\", invalid_value=\"TIMEOUT\")\n</code></pre>"},{"location":"documentation/tpot/evolvers/base_evolver/#tpot.evolvers.base_evolver.BaseEvolver.evaluate_population_selection_early_stop","title":"<code>evaluate_population_selection_early_stop(survival_counts, thresholds=None, budget=None)</code>","text":"<p>This function tries to save computation by partially evaluating the individuals and then selecting which individuals to evaluate further based on the results of the partial evaluation. </p> <p>Two strategies are implemented:     1.  Selection early stopping: Selects a percentage of the population to evaluate at each step of the evaluation.          for example, one strategy is to evaluate different steps of cross validation one at a time, and only select the best N individuals for subsequent steps.          This can save computation by not evaluating all individuals for all steps of cross validation. By default this selection is done with the NSGA2 selector.     2.  Threshold early stopping: At each step of the evaluation, a threshold is calculated based on the previous evaluations. All individuals that are below the performance threshold are not evaluated for further steps.         For example, if the threshold is set to the 90th percentile of the previous evaluations, all individuals that are below the 90th percentile are not evaluated further. This can save computation by not evaluating all individuals for all steps of cross validation.</p> <p>Both of these strategies can be used simultaneously. Individuals must pass both the selection and threshold criteria to be evaluated further.</p> <p>Parameters:</p> Name Type Description Default <code>survival_counts</code> <code>list of ints</code> <p>Number of individuals to select for survival at each step of the evaluation. If None, will not use selection early stopping. For example: [10, 5, 2] would select 10 individuals for the first step, 5 for the second, and 2 for the third.</p> <code>None</code> <code>thresholds</code> <code>list of floats</code> <p>Thresholds to use for early stopping at each step of the evaluation. If None, will not use threshold early stopping.</p> <code>None</code> <code>budget</code> <code>float</code> <p>Budget to use when evaluating individuals. Use is dependent on the objective functions. (In TPOTEstimator this corresponds to the percentage of the data to sample.)</p> <code>None</code> Source code in <code>tpot/evolvers/base_evolver.py</code> <pre><code>def evaluate_population_selection_early_stop(self,survival_counts, thresholds=None, budget=None):\n    \"\"\"\n    This function tries to save computation by partially evaluating the individuals and then selecting which individuals to evaluate further based on the results of the partial evaluation. \n\n    Two strategies are implemented:\n        1.  Selection early stopping: Selects a percentage of the population to evaluate at each step of the evaluation. \n            for example, one strategy is to evaluate different steps of cross validation one at a time, and only select the best N individuals for subsequent steps. \n            This can save computation by not evaluating all individuals for all steps of cross validation. By default this selection is done with the NSGA2 selector.\n        2.  Threshold early stopping: At each step of the evaluation, a threshold is calculated based on the previous evaluations. All individuals that are below the performance threshold are not evaluated for further steps.\n            For example, if the threshold is set to the 90th percentile of the previous evaluations, all individuals that are below the 90th percentile are not evaluated further. This can save computation by not evaluating all individuals for all steps of cross validation.\n\n    Both of these strategies can be used simultaneously. Individuals must pass both the selection and threshold criteria to be evaluated further.\n\n    Parameters\n    ----------\n    survival_counts : list of ints, default=None\n        Number of individuals to select for survival at each step of the evaluation. If None, will not use selection early stopping.\n        For example: [10, 5, 2] would select 10 individuals for the first step, 5 for the second, and 2 for the third.\n    thresholds : list of floats, default=None\n        Thresholds to use for early stopping at each step of the evaluation. If None, will not use threshold early stopping.\n    budget : float, default=None\n        Budget to use when evaluating individuals. Use is dependent on the objective functions. (In TPOTEstimator this corresponds to the percentage of the data to sample.)\n    \"\"\"\n\n    survival_selector = tpot.selectors.survival_select_NSGA2\n\n    ################\n\n    objective_function_signs = np.sign(self.objective_function_weights)\n\n\n    cur_individuals = self.population.population.copy()\n\n    all_step_names = []\n    for step in range(self.evaluation_early_stop_steps):\n        if budget is None:\n            this_step_names = [f\"{n}_step_{step}\" for n in self.objective_names]\n        else:\n            this_step_names = [f\"{n}_budget_{budget}_step_{step}\" for n in self.objective_names]\n\n        all_step_names.append(this_step_names)\n\n        unevaluated_individuals_this_step = self.get_unevaluated_individuals(this_step_names, budget=None, individual_list=cur_individuals)\n\n        if len(unevaluated_individuals_this_step) == 0:\n            if self.verbose &gt; 3:\n                print(\"No new individuals to evaluate\")\n            continue\n\n        if self.max_eval_time_mins is not None:\n            theoretical_timeout = self.max_eval_time_mins * math.ceil(len(unevaluated_individuals_this_step) / self.n_jobs)*60\n            theoretical_timeout = theoretical_timeout*2\n        else:\n            theoretical_timeout = np.inf\n        scheduled_timeout_time_left = self.scheduled_timeout_time - time.time()\n        parallel_timeout = min(theoretical_timeout, scheduled_timeout_time_left)\n        if parallel_timeout &lt; 0:\n            parallel_timeout = 10\n\n        scores, start_times, end_times, eval_errors = tpot.utils.eval_utils.parallel_eval_objective_list(individual_list=unevaluated_individuals_this_step,\n                                objective_list=self.objective_functions,\n                                verbose=self.verbose,\n                                max_eval_time_mins=self.max_eval_time_mins,\n                                step=step,\n                                budget = self.budget,\n                                generation = self.generation,\n                                n_expected_columns=len(self.objective_names),\n                                client=self._client,\n                                scheduled_timeout_time=self.scheduled_timeout_time,\n                                **self.objective_kwargs,\n                                )\n\n        self.population.update_column(unevaluated_individuals_this_step, column_names=this_step_names, data=scores)\n        self.population.update_column(unevaluated_individuals_this_step, column_names=\"Submitted Timestamp\", data=start_times)\n        self.population.update_column(unevaluated_individuals_this_step, column_names=\"Completed Timestamp\", data=end_times)\n        self.population.update_column(unevaluated_individuals_this_step, column_names=\"Eval Error\", data=eval_errors)\n\n        self.population.remove_invalid_from_population(column_names=\"Eval Error\")\n        self.population.remove_invalid_from_population(column_names=\"Eval Error\", invalid_value=\"TIMEOUT\")\n\n        #remove invalids:\n        invalids = []\n        #find indeces of invalids\n\n        for j in range(len(scores)):\n            if  any([s==\"INVALID\" for s in scores[j]]):\n                invalids.append(j)\n\n        for j in range(len(scores)):\n            if  any([s==\"TIMEOUT\" for s in scores[j]]):\n                invalids.append(j)\n\n\n        #already evaluated\n        already_evaluated = list(set(cur_individuals) - set(unevaluated_individuals_this_step))\n        #evaluated and valid\n        valid_evaluations_this_step = remove_items(unevaluated_individuals_this_step,invalids)\n        #update cur_individuals with current individuals with valid scores\n        cur_individuals = np.concatenate([already_evaluated, valid_evaluations_this_step])\n\n\n        #Get average scores\n\n        #array of shape (steps, individuals, objectives)\n        offspring_scores = [self.population.get_column(cur_individuals, column_names=step_names) for step_names in all_step_names]\n        offspring_scores = np.array(offspring_scores)\n        if self.final_score_strategy == 'mean':\n            offspring_scores  = offspring_scores.mean(axis=0)\n        elif self.final_score_strategy == 'last':\n            offspring_scores = offspring_scores[-1]\n\n        #remove individuals with nan scores\n        invalids = []\n        for i in range(len(offspring_scores)):\n            if any(np.isnan(offspring_scores[i])):\n                invalids.append(i)\n\n        cur_individuals = remove_items(cur_individuals,invalids)\n        offspring_scores = remove_items(offspring_scores,invalids)\n\n        #if last step, add the final metrics\n        if step == self.evaluation_early_stop_steps-1:\n            self.population.update_column(cur_individuals, column_names=self.objective_names, data=offspring_scores)\n            if budget is not None:\n                self.population.update_column(cur_individuals, column_names=\"Budget\", data=budget)\n            return\n\n        #If we have more threads than remaining individuals, we may as well evaluate the extras too\n        if self.n_jobs &lt; len(cur_individuals):\n            #Remove based on thresholds\n            if thresholds is not None:\n                threshold = thresholds[step]\n                invalids = []\n                for i in range(len(offspring_scores)):\n\n                    if all([s*w&gt;t*w for s,t,w in zip(offspring_scores[i],threshold,objective_function_signs)  ]):\n                        invalids.append(i)\n\n                if len(invalids) &gt; 0:\n\n                    max_to_remove = min(len(cur_individuals) - self.n_jobs, len(invalids))\n\n                    if max_to_remove &lt; len(invalids):\n                        # invalids = np.random.choice(invalids, max_to_remove, replace=False)\n                        invalids = self.rng.choice(invalids, max_to_remove, replace=False)\n\n                    cur_individuals = remove_items(cur_individuals,invalids)\n                    offspring_scores = remove_items(offspring_scores,invalids)\n\n            # Remove based on selection\n            if survival_counts is not None:\n                if step &lt; self.evaluation_early_stop_steps - 1 and survival_counts[step]&gt;1: #don't do selection for the last loop since they are completed\n                    k = survival_counts[step] + len(invalids) #TODO can remove the min if the selections method can ignore k&gt;population size\n                    if len(cur_individuals)&gt; 1 and k &gt; self.n_jobs and k &lt; len(cur_individuals):\n                        weighted_scores = np.array([s * self.objective_function_weights for s in offspring_scores ])\n\n                        new_population_index = survival_selector(weighted_scores, k=k)\n                        cur_individuals = np.array(cur_individuals)[new_population_index]\n                        offspring_scores = offspring_scores[new_population_index]\n</code></pre>"},{"location":"documentation/tpot/evolvers/base_evolver/#tpot.evolvers.base_evolver.BaseEvolver.generate_offspring","title":"<code>generate_offspring()</code>","text":"<p>Create population_size new individuals from the current population.  This includes selecting parents, applying mutation and crossover, and adding the new individuals to the population.</p> Source code in <code>tpot/evolvers/base_evolver.py</code> <pre><code>def generate_offspring(self, ): #your EA Algorithm goes here\n    \"\"\"\n    Create population_size new individuals from the current population. \n    This includes selecting parents, applying mutation and crossover, and adding the new individuals to the population.\n    \"\"\"\n    parents = self.population.parent_select(selector=self.parent_selector, weights=self.objective_function_weights, columns_names=self.objective_names, k=self.cur_population_size, n_parents=2, rng=self.rng)\n    p = np.array([self.crossover_probability, self.mutate_then_crossover_probability, self.crossover_then_mutate_probability, self.mutate_probability])\n    p = p / p.sum()\n    var_op_list = self.rng.choice([\"crossover\", \"mutate_then_crossover\", \"crossover_then_mutate\", \"mutate\"], size=self.cur_population_size, p=p)\n\n    for i, op in enumerate(var_op_list):\n        if op == \"mutate\":\n            parents[i] = parents[i][0] #mutations take a single individual\n\n    offspring = self.population.create_offspring2(parents, var_op_list, self.mutation_functions, self.mutation_function_weights, self.crossover_functions, self.crossover_function_weights, add_to_population=True, keep_repeats=False, mutate_until_unique=True, rng=self.rng)\n\n    self.population.update_column(offspring, column_names=\"Generation\", data=self.generation, )\n</code></pre>"},{"location":"documentation/tpot/evolvers/base_evolver/#tpot.evolvers.base_evolver.BaseEvolver.get_unevaluated_individuals","title":"<code>get_unevaluated_individuals(column_names, budget=None, individual_list=None)</code>","text":"<p>This function is used to get a list of individuals in the current population that have not been evaluated yet.</p> <p>Parameters:</p> Name Type Description Default <code>column_names</code> <code>list of strings</code> <p>Names of the columns to check for unevaluated individuals (generally objective functions).</p> required <code>budget</code> <code>float</code> <p>Budget to use when checking for unevaluated individuals. If None, will not check the budget column. Finds individuals who have not been evaluated with the given budget on column names.</p> <code>None</code> <code>individual_list</code> <code>list of individuals</code> <p>List of individuals to check for unevaluated individuals. If None, will use the current population.</p> <code>None</code> Source code in <code>tpot/evolvers/base_evolver.py</code> <pre><code>def get_unevaluated_individuals(self, column_names, budget=None, individual_list=None):\n    \"\"\"\n    This function is used to get a list of individuals in the current population that have not been evaluated yet.\n\n    Parameters\n    ----------\n    column_names : list of strings\n        Names of the columns to check for unevaluated individuals (generally objective functions).\n    budget : float, default=None\n        Budget to use when checking for unevaluated individuals. If None, will not check the budget column.\n        Finds individuals who have not been evaluated with the given budget on column names.\n    individual_list : list of individuals, default=None\n        List of individuals to check for unevaluated individuals. If None, will use the current population.\n    \"\"\"\n    if individual_list is not None:\n        cur_pop = np.array(individual_list)\n    else:\n        cur_pop = np.array(self.population.population)\n\n    if all([name_step in self.population.evaluated_individuals.columns for name_step in column_names]):\n        if budget is not None:\n            offspring_scores = self.population.get_column(cur_pop, column_names=column_names+[\"Budget\"], to_numpy=False)\n            #Individuals are unevaluated if we have a higher budget OR if any of the objectives are nan\n            unevaluated_filter = lambda i: any(offspring_scores.loc[offspring_scores.index[i]][column_names].isna()) or (offspring_scores.loc[offspring_scores.index[i]][\"Budget\"] &lt; budget)\n        else:\n            offspring_scores = self.population.get_column(cur_pop, column_names=column_names, to_numpy=False)\n            unevaluated_filter = lambda i: any(offspring_scores.loc[offspring_scores.index[i]][column_names].isna())\n        unevaluated_individuals_this_step = [i for i in range(len(cur_pop)) if unevaluated_filter(i)]\n        return cur_pop[unevaluated_individuals_this_step]\n\n    else: #if column names are not in the evaluated_individuals, then we have not evaluated any individuals yet\n        for name_step in column_names:\n            self.population.evaluated_individuals[name_step] = np.nan\n        return cur_pop\n</code></pre>"},{"location":"documentation/tpot/evolvers/base_evolver/#tpot.evolvers.base_evolver.BaseEvolver.optimize","title":"<code>optimize(generations=None)</code>","text":"<p>Creates an initial population and runs the evolutionary algorithm for the given number of generations.  If generations is None, will use self.generations.</p> <p>Parameters:</p> Name Type Description Default <code>generations</code> <code>int</code> <p>Number of generations to run. If None, will use self.generations.</p> <code>None</code> Source code in <code>tpot/evolvers/base_evolver.py</code> <pre><code>def optimize(self, generations=None):\n    \"\"\"\n    Creates an initial population and runs the evolutionary algorithm for the given number of generations. \n    If generations is None, will use self.generations.\n\n    Parameters\n    ----------\n    generations : int, default=None\n        Number of generations to run. If None, will use self.generations.\n\n    \"\"\"\n\n    if self.client is not None: #If user passed in a client manually\n       self._client = self.client\n    else:\n\n        if self.verbose &gt;= 4:\n            silence_logs = 30\n        elif self.verbose &gt;=5:\n            silence_logs = 40\n        else:\n            silence_logs = 50\n        self._cluster = LocalCluster(n_workers=self.n_jobs, #if no client is passed in and no global client exists, create our own\n                threads_per_worker=1,\n                silence_logs=silence_logs,\n                processes=True,\n                memory_limit=self.memory_limit)\n        self._client = Client(self._cluster)\n\n\n\n    if generations is None:\n        generations = self.generations\n\n    start_time = time.time()\n\n    generations_without_improvement = np.array([0 for _ in range(len(self.objective_function_weights))])\n    best_scores = [-np.inf for _ in range(len(self.objective_function_weights))]\n\n\n    self.scheduled_timeout_time = time.time() + self.max_time_mins*60\n\n\n    try:\n        #for gen in tnrange(generations,desc=\"Generation\", disable=self.verbose&lt;1):\n        done = False\n        gen = 0\n        if self.verbose &gt;= 1:\n            if generations is None or np.isinf(generations):\n                pbar = tqdm.tqdm(total=0)\n            else:\n                pbar = tqdm.tqdm(total=generations)\n            pbar.set_description(\"Generation\")\n        while not done:\n            # Generation 0 is the initial population\n            if self.generation == 0:\n                if self.population_file is not None:\n                    pickle.dump(self.population, open(self.population_file, \"wb\"))\n                self.evaluate_population()\n                if self.population_file is not None:\n                    pickle.dump(self.population, open(self.population_file, \"wb\"))\n\n                attempts = 2\n                while len(self.population.population) == 0 and attempts &gt; 0:\n                    new_initial_population = [next(self.individual_generator) for _ in range(self.cur_population_size)]\n                    self.population.add_to_population(new_initial_population, rng=self.rng)\n                    attempts -= 1\n                    self.evaluate_population()\n\n                if len(self.population.population) == 0:\n                    raise Exception(\"No individuals could be evaluated in the initial population. This may indicate a bug in the configuration, included models, or objective functions. Set verbose&gt;=4 to see the errors that caused individuals to fail.\")\n\n                self.generation += 1\n            # Generation 1 is the first generation after the initial population\n            else:\n                if time.time() - start_time &gt; self.max_time_mins*60:\n                    if self.population.evaluated_individuals[self.objective_names].isnull().all().iloc[0]:\n                        raise Exception(\"No individuals could be evaluated in the initial population as the max_eval_mins time limit was reached before any individuals could be evaluated.\")\n                    break\n                self.step()\n\n            if self.verbose &gt;= 3:\n                sign = np.sign(self.objective_function_weights)\n                valid_df = self.population.evaluated_individuals[~self.population.evaluated_individuals[self.objective_names].isin([\"TIMEOUT\",\"INVALID\"]).any(axis=1)][self.objective_names]*sign\n                cur_best_scores = valid_df.max(axis=0)*sign\n                cur_best_scores = cur_best_scores.to_numpy()\n                print(\"Generation: \", self.generation)\n                for i, obj in enumerate(self.objective_names):\n                    print(f\"Best {obj} score: {cur_best_scores[i]}\")\n\n\n            if self.early_stop:\n                if self.budget is None or self.budget&gt;=self.budget_range[-1]: #self.budget&gt;=1:\n                    #get sign of objective_function_weights\n                    sign = np.sign(self.objective_function_weights)\n                    #get best score for each objective\n                    valid_df = self.population.evaluated_individuals[~self.population.evaluated_individuals[self.objective_names].isin([\"TIMEOUT\",\"INVALID\"]).any(axis=1)][self.objective_names]*sign\n                    cur_best_scores = valid_df.max(axis=0)\n                    cur_best_scores = cur_best_scores.to_numpy()\n                    #cur_best_scores =  self.population.get_column(self.population.population, column_names=self.objective_names).max(axis=0)*sign #TODO this assumes the current population is the best\n\n                    improved = ( np.array(cur_best_scores) - np.array(best_scores) &gt;= np.array(self.early_stop_tol) )\n                    not_improved = np.logical_not(improved)\n                    generations_without_improvement = generations_without_improvement * not_improved + not_improved #set to zero if not improved, else increment\n                    pass\n                    #update best score\n                    best_scores = [max(best_scores[i], cur_best_scores[i]) for i in range(len(self.objective_names))]\n\n                    if all(generations_without_improvement&gt;self.early_stop):\n                        if self.verbose &gt;= 3:\n                            print(\"Early stop\")\n                        break\n\n            #save population\n            if self.population_file is not None: # and time.time() - last_save_time &gt; 60*10:\n                pickle.dump(self.population, open(self.population_file, \"wb\"))\n\n            gen += 1\n            if self.verbose &gt;= 1:\n                pbar.update(1)\n\n            if generations is not None and gen &gt;= generations:\n                done = True\n\n    except KeyboardInterrupt:\n        if self.verbose &gt;= 3:\n            print(\"KeyboardInterrupt\")\n        self.population.remove_invalid_from_population(column_names=self.objective_names, invalid_value=\"INVALID\")\n        self.population.remove_invalid_from_population(column_names=self.objective_names, invalid_value=\"TIMEOUT\")\n        self.population.remove_invalid_from_population(column_names=\"Eval Error\", invalid_value=\"INVALID\")\n        self.population.remove_invalid_from_population(column_names=\"Eval Error\", invalid_value=\"TIMEOUT\")\n\n\n\n\n    if self.population_file is not None:\n        pickle.dump(self.population, open(self.population_file, \"wb\"))\n\n    if self.client is None: #If we created our own client, close it\n        self._client.close()\n        self._cluster.close()\n\n    tpot.utils.get_pareto_frontier(self.population.evaluated_individuals, column_names=self.objective_names, weights=self.objective_function_weights)\n</code></pre>"},{"location":"documentation/tpot/evolvers/base_evolver/#tpot.evolvers.base_evolver.BaseEvolver.step","title":"<code>step()</code>","text":"<p>Runs a single generation of the evolutionary algorithm. This includes selecting individuals for survival, generating offspring, and evaluating the offspring.</p> Source code in <code>tpot/evolvers/base_evolver.py</code> <pre><code>def step(self,):\n    \"\"\"\n    Runs a single generation of the evolutionary algorithm. This includes selecting individuals for survival, generating offspring, and evaluating the offspring.\n\n    \"\"\"\n\n\n    if self.population_size_list is not None:\n        if self.generation &lt; len(self.population_size_list):\n            self.cur_population_size = self.population_size_list[self.generation]\n        else:\n            self.cur_population_size = self.population_size\n\n    if self.budget_list is not None:\n        if len(self.budget_list) &lt;= self.generation:\n            self.budget = self.budget_range[-1]\n        else:\n            self.budget = self.budget_list[self.generation]\n    else:\n        self.budget = None\n\n    if self.survival_selector is not None:\n        n_survivors = max(1,int(self.cur_population_size*self.survival_percentage)) #always keep at least one individual\n        self.population.survival_select(    selector=self.survival_selector,\n                                            weights=self.objective_function_weights,\n                                            columns_names=self.objective_names,\n                                            n_survivors=n_survivors,\n                                            inplace=True,\n                                            rng=self.rng,)\n\n    self.generate_offspring()\n    self.evaluate_population()\n\n    self.generation += 1\n</code></pre>"},{"location":"documentation/tpot/evolvers/base_evolver/#tpot.evolvers.base_evolver.ind_crossover","title":"<code>ind_crossover(ind1, ind2, rng)</code>","text":"<p>Calls the ind1.crossover(ind2, rng=rng)</p> <p>Parameters:</p> Name Type Description Default <code>ind1</code> <code>BaseIndividual</code> required <code>ind2</code> <code>BaseIndividual</code> required <code>rng</code> <code>int or Generator</code> <p>A numpy random generator to use for reproducibility</p> required Source code in <code>tpot/evolvers/base_evolver.py</code> <pre><code>def ind_crossover(ind1, ind2, rng):\n    \"\"\"\n    Calls the ind1.crossover(ind2, rng=rng)\n    Parameters\n    ----------\n    ind1 : tpot.BaseIndividual\n    ind2 : tpot.BaseIndividual\n    rng : int or numpy.random.Generator\n        A numpy random generator to use for reproducibility\n    \"\"\"\n    rng = np.random.default_rng(rng)\n    return ind1.crossover(ind2, rng=rng)\n</code></pre>"},{"location":"documentation/tpot/evolvers/base_evolver/#tpot.evolvers.base_evolver.ind_mutate","title":"<code>ind_mutate(ind, rng)</code>","text":"<p>Calls the ind.mutate method on the individual</p> <p>Parameters:</p> Name Type Description Default <code>ind</code> <code>BaseIndividual</code> <p>The individual to mutate</p> required <code>rng</code> <code>int or Generator</code> <p>A numpy random generator to use for reproducibility</p> required Source code in <code>tpot/evolvers/base_evolver.py</code> <pre><code>def ind_mutate(ind, rng):\n    \"\"\"\n    Calls the ind.mutate method on the individual\n\n    Parameters\n    ----------\n    ind : tpot.BaseIndividual\n        The individual to mutate\n    rng : int or numpy.random.Generator\n        A numpy random generator to use for reproducibility\n    \"\"\"\n    rng = np.random.default_rng(rng)\n    return ind.mutate(rng=rng)\n</code></pre>"},{"location":"documentation/tpot/evolvers/steady_state_evolver/","title":"Steady state evolver","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/evolvers/steady_state_evolver/#tpot.evolvers.steady_state_evolver.SteadyStateEvolver","title":"<code>SteadyStateEvolver</code>","text":"Source code in <code>tpot/evolvers/steady_state_evolver.py</code> <pre><code>class SteadyStateEvolver():\n    def __init__(   self,\n                    individual_generator ,\n\n                    objective_functions,\n                    objective_function_weights,\n                    objective_names = None,\n                    objective_kwargs = None,\n                    bigger_is_better = True,\n\n                    initial_population_size = 50,\n                    population_size = 300,\n                    max_evaluated_individuals = None,\n                    early_stop = None,\n                    early_stop_mins = None,\n                    early_stop_tol = 0.001,\n\n\n                    max_time_mins=float(\"inf\"),\n                    max_eval_time_mins=10,\n\n                    n_jobs=1,\n                    memory_limit=\"4GB\",\n                    client=None,\n\n                    crossover_probability=.2,\n                    mutate_probability=.7,\n                    mutate_then_crossover_probability=.05,\n                    crossover_then_mutate_probability=.05,\n                    n_parents=2,\n\n                    survival_selector = survival_select_NSGA2,\n                    parent_selector = tournament_selection_dominated,\n\n                    budget_range = None,\n                    budget_scaling = .5,\n                    individuals_until_end_budget = 1,\n                    stepwise_steps = 5,\n\n                    verbose = 0,\n                    periodic_checkpoint_folder = None,\n                    callback = None,\n\n                    rng=None\n                    ) -&gt; None:\n        \"\"\"\n        Whereas the base_evolver uses a generational approach, the steady state evolver continuously generates individuals as resources become available.\n\n        This evolver will simultaneously evaluated n_jobs individuals. As soon as one individual is evaluated, the current population is updated with survival_selector, \n        a new individual is generated from parents selected with parent_selector, and the new individual is immediately submitted for evaluation.\n        In contrast, the base_evolver batches evaluations in generations, and only updates the population and creates new individuals after all individuals in the current generation are evaluated.\n\n        In practice, this means that steady state evolver is more likely to use all cores at all times, allowing for flexibility is duration of evaluations and number of evaluations. However, it \n        may also generate less diverse populations as a result.\n\n        Parameters\n        ----------\n        individual_generator : generator\n            Generator that yields new base individuals. Used to generate initial population.\n        objective_functions : list of callables\n            list of functions that get applied to the individual and return a float or list of floats\n            If an objective function returns multiple values, they are all concatenated in order\n            with respect to objective_function_weights and early_stop_tol.\n        objective_function_weights : list of floats\n            list of weights for each objective function. Sign flips whether bigger is better or not\n        objective_names : list of strings, default=None\n            Names of the objectives. If None, objective0, objective1, etc. will be used\n        objective_kwargs : dict, default=None\n            Dictionary of keyword arguments to pass to the objective function\n        bigger_is_better : bool, default=True\n            If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n\n        initial_population_size : int, default=50\n            Number of random individuals to generate in the initial population. These will all be randomly sampled, all other subsequent individuals will be generated from the population.\n        population_size : int, default=50\n            Note: This is different from the base_evolver. \n            In steady_state_evolver, the population_size is the number of individuals to keep in the live population. This is the total number of best individuals (as determined by survival_selector) to keep in the population.\n            New individuals are generated from this population size.\n            In base evolver, this is also the number of individuals to generate in each generation, however, here, we generate individuals as resources become available so there is no concept of a generation.\n            It is recommended to use a higher population_size to ensure diversity in the population.\n        max_evaluated_individuals : int, default=None\n            Maximum number of individuals to evaluate after which training is terminated. If None, will evaluate until time limit is reached.\n        early_stop : int, default=None\n            If the best individual has not improved in this many evaluations, stop training.\n            Note: Also different from base_evolver. In base evolver, this is the number of generations without improvement. Here, it is the number of individuals evaluated without improvement. Naturally, a higher value is recommended.\n        early_stop_mins : int, default=None\n            If the best individual has not improved in this many minutes, stop training.\n                early_stop_tol : float, list of floats, or None, default=0.001\n            -list of floats\n                list of tolerances for each objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n                If an index of the list is None, that item will not be used for early stopping\n            -int\n                If an int is given, it will be used as the tolerance for all objectives\n        max_time_mins : float, default=float(\"inf\")\n            Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n        max_eval_time_mins : float, default=10\n            Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n        n_jobs : int, default=1\n            Number of processes to run in parallel.\n        memory_limit : str, default=None\n            Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n        client : dask.distributed.Client, default=None\n            A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.\n        crossover_probability : float, default=.2\n            Probability of generating a new individual by crossover between two individuals.\n        mutate_probability : float, default=.7\n            Probability of generating a new individual by crossover between one individuals.\n        mutate_then_crossover_probability : float, default=.05\n            Probability of generating a new individual by mutating two individuals followed by crossover.\n        crossover_then_mutate_probability : float, default=.05\n            Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.\n        n_parents : int, default=2\n            Number of parents to use for crossover. Must be greater than 1.\n        survival_selector : function, default=survival_select_NSGA2\n            Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes.\n            Used to selected population_size * survival_percentage individuals at the start of each generation to use for mutation and crossover.\n        parent_selector : function, default=parent_select_NSGA2\n            Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.     \n\n        budget_range : list [start, end], default=None\n            This parameter is used for the successive halving algorithm.\n            A starting and ending budget to use for the budget scaling. The evolver will interpolate between these values over the generations_until_end_budget.\n            Use is dependent on the objective functions. (In TPOTEstimator this corresponds to the percentage of the data to sample.)\n        budget_scaling float : [0,1], default=0.5\n            A scaling factor to use when determining how fast we move the budget from the start to end budget.\n        evaluations_until_end_budget : int, default=1\n            The number of evaluations to run before reaching the max budget.\n        stepwise_steps : int, default=1\n            The number of staircase steps to take when interpolating the budget.\n        verbose : int, default=0\n            How much information to print during the optimization process. Higher values include the information from lower values.\n            0. nothing\n            1. progress bar\n            2. evaluations progress bar\n            3. best individual\n            4. warnings\n            &gt;=5. full warnings trace\n        periodic_checkpoint_folder : str, default=None\n            Folder to save the population to periodically. If None, no periodic saving will be done.\n            If provided, training will resume from this checkpoint.\n        callback : tpot.CallBackInterface, default=None\n            Callback object. Not implemented\n        rng : Numpy.Random.Generator, None, default=None\n            An object for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes\n\n            - Numpy.Random.Generator\n                Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'. Note this will be the same Generator passed in.\n            - None\n                Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS\n\n        Attributes\n        ----------\n        population : tpot.Population\n            The population of individuals.\n            Use population.population to access the individuals in the current population.\n            Use population.evaluated_individuals to access a data frame of all individuals that have been explored.\n\n        \"\"\"\n\n        self.rng = np.random.default_rng(rng)\n\n        self.max_evaluated_individuals = max_evaluated_individuals\n        self.individuals_until_end_budget = individuals_until_end_budget\n\n        self.individual_generator = individual_generator\n        self.population_size = population_size\n        self.objective_functions = objective_functions\n        self.objective_function_weights = np.array(objective_function_weights)\n        self.bigger_is_better = bigger_is_better\n        if not bigger_is_better:\n            self.objective_function_weights = np.array(self.objective_function_weights)*-1\n\n        self.population_size_list = None\n\n\n        self.periodic_checkpoint_folder = periodic_checkpoint_folder\n        self.verbose  = verbose\n        self.callback = callback\n        self.n_jobs = n_jobs\n\n        if max_time_mins is None:\n            self.max_time_mins = float(\"inf\")\n        else:\n            self.max_time_mins = max_time_mins\n\n        #functools requires none for infinite time, doesn't support inf\n        if max_eval_time_mins is not None and math.isinf(max_eval_time_mins ):\n            self.max_eval_time_mins = None\n        else:\n            self.max_eval_time_mins = max_eval_time_mins\n\n        self.initial_population_size = initial_population_size\n        self.budget_range = budget_range\n        self.budget_scaling = budget_scaling\n        self.stepwise_steps = stepwise_steps\n\n        self.memory_limit = memory_limit\n\n        self.client = client\n\n\n        self.survival_selector=survival_selector\n        self.parent_selector=parent_selector\n\n\n        total_var_p = crossover_probability + mutate_probability + mutate_then_crossover_probability + crossover_then_mutate_probability\n        self.crossover_probability = crossover_probability / total_var_p\n        self.mutate_probability = mutate_probability  / total_var_p\n        self.mutate_then_crossover_probability= mutate_then_crossover_probability / total_var_p\n        self.crossover_then_mutate_probability= crossover_then_mutate_probability / total_var_p\n\n        self.n_parents = n_parents\n\n        if objective_kwargs is None:\n            self.objective_kwargs = {}\n        else:\n            self.objective_kwargs = objective_kwargs\n\n        ###########\n\n\n        if self.budget_range is None:\n            self.budget_list = None\n        else:\n            self.budget_list = beta_interpolation(start=self.budget_range[0], end=self.budget_range[1], n=self.generations_until_end_budget, scale=self.budget_scaling, n_steps=self.stepwise_steps)\n\n        if objective_names is None:\n            self.objective_names = [\"objective\"+str(i) for i in range(len(objective_function_weights))]\n        else:\n            self.objective_names = objective_names\n\n        if self.budget_list is not None:\n            if len(self.budget_list) &lt;= self.generation:\n                self.budget = self.budget_list[-1]\n            else:\n                self.budget = self.budget_list[self.generation]\n        else:\n            self.budget = None\n\n\n        self.early_stop_tol = early_stop_tol\n        self.early_stop_mins = early_stop_mins\n        self.early_stop = early_stop\n\n        if isinstance(self.early_stop_tol, float):\n            self.early_stop_tol = [self.early_stop_tol for _ in range(len(self.objective_names))]\n\n        self.early_stop_tol = [np.inf if tol is None else tol for tol in self.early_stop_tol]\n\n        self.population = None\n        self.population_file = None\n        if self.periodic_checkpoint_folder is not None:\n            self.population_file = os.path.join(self.periodic_checkpoint_folder, \"population.pkl\")\n            if not os.path.exists(self.periodic_checkpoint_folder):\n                os.makedirs(self.periodic_checkpoint_folder)\n            if os.path.exists(self.population_file):\n                self.population = pickle.load(open(self.population_file, \"rb\"))\n\n\n        init_names = self.objective_names\n        if self.budget_range is not None:\n            init_names = init_names + [\"Budget\"]\n        if self.population is None:\n            self.population = tpot.Population(column_names=init_names)\n            initial_population = [next(self.individual_generator) for _ in range(self.initial_population_size)]\n            self.population.add_to_population(initial_population, rng=self.rng)\n\n\n    def optimize(self):\n        \"\"\"\n        Creates an initial population and runs the evolutionary algorithm for the given number of generations. \n        If generations is None, will use self.generations.\n        \"\"\"\n\n        #intialize the client\n        if self.client is not None: #If user passed in a client manually\n           self._client = self.client\n        else:\n\n            if self.verbose &gt;= 4:\n                silence_logs = 30\n            elif self.verbose &gt;=5:\n                silence_logs = 40\n            else:\n                silence_logs = 50\n            self._cluster = LocalCluster(n_workers=self.n_jobs, #if no client is passed in and no global client exists, create our own\n                    threads_per_worker=1,\n                    silence_logs=silence_logs,\n                    processes=False,\n                    memory_limit=self.memory_limit)\n            self._client = Client(self._cluster)\n\n\n        self.max_queue_size = len(self._client.cluster.workers)\n\n        #set up logging params\n        evaluated_count = 0\n        generations_without_improvement = np.array([0 for _ in range(len(self.objective_function_weights))])\n        timestamp_of_last_improvement = np.array([time.time() for _ in range(len(self.objective_function_weights))])\n        best_scores = [-np.inf for _ in range(len(self.objective_function_weights))]\n        scheduled_timeout_time = time.time() + self.max_time_mins*60\n        budget = None\n\n        submitted_futures = {}\n        submitted_inds = set()\n\n        start_time = time.time()\n\n        try:\n\n\n            if self.verbose &gt;= 1:\n                if self.max_evaluated_individuals is not None:\n                    pbar = tqdm.tqdm(total=self.max_evaluated_individuals, miniters=1)\n                else:\n                    pbar = tqdm.tqdm(total=0, miniters=1)\n                pbar.set_description(\"Evaluations\")\n\n            #submit initial population\n            individuals_to_evaluate = self.get_unevaluated_individuals(self.objective_names, budget=budget,)\n\n            for individual in individuals_to_evaluate:\n                if len(submitted_futures) &gt;= self.max_queue_size:\n                    break\n                future = self._client.submit(tpot.utils.eval_utils.eval_objective_list, individual,  self.objective_functions, verbose=self.verbose, timeout=self.max_eval_time_mins*60,**self.objective_kwargs)\n\n                submitted_futures[future] = {\"individual\": individual,\n                                            \"time\": time.time(),\n                                            \"budget\": budget,}\n                submitted_inds.add(individual.unique_id())\n                self.population.update_column(individual, column_names=\"Submitted Timestamp\", data=time.time())\n\n            done = False\n            start_time = time.time()\n\n            enough_parents_evaluated=False\n            while not done:\n\n                ###############################\n                # Step 1: Check for finished futures\n                ###############################\n\n                #wait for at least one future to finish or timeout\n                try:\n                    next(distributed.as_completed(submitted_futures, timeout=self.max_eval_time_mins*60))\n                except dask.distributed.TimeoutError:\n                    pass\n                except dask.distributed.CancelledError:\n                    pass\n\n                #Loop through all futures, collect completed and timeout futures.\n                for completed_future in list(submitted_futures.keys()):\n                    eval_error = None\n                    #get scores and update\n                    if completed_future.done(): #if future is done\n                        #If the future is done but threw and error, record the error\n                        if completed_future.exception() or completed_future.status == \"error\": #if the future is done and threw an error\n                            print(\"Exception in future\")\n                            print(completed_future.exception())\n                            scores = [np.nan for _ in range(len(self.objective_names))]\n                            eval_error = \"INVALID\"\n                        elif completed_future.cancelled(): #if the future is done and was cancelled\n                            print(\"Cancelled future (likely memory related)\")\n                            scores = [np.nan for _ in range(len(self.objective_names))]\n                            eval_error = \"INVALID\"\n                            client.run(gc.collect)\n                        else: #if the future is done and did not throw an error, get the scores\n                            try:\n                                scores = completed_future.result()\n\n                                #check if scores contain \"INVALID\" or \"TIMEOUT\"\n                                if \"INVALID\" in scores:\n                                    eval_error = \"INVALID\"\n                                    scores = [np.nan]\n                                elif \"TIMEOUT\" in scores:\n                                    eval_error = \"TIMEOUT\"\n                                    scores = [np.nan]\n\n                            except Exception as e:\n                                print(\"Exception in future, but not caught by dask\")\n                                print(e)\n                                print(completed_future.exception())\n                                print(completed_future)\n                                print(\"status\", completed_future.status)\n                                print(\"done\", completed_future.done())\n                                print(\"cancelld \", completed_future.cancelled())\n                                scores = [np.nan for _ in range(len(self.objective_names))]\n                                eval_error = \"INVALID\"\n                        completed_future.release() #release the future\n                    else: #if future is not done\n\n                        if self.max_eval_time_mins is not None:\n                            #check if the future has been running for too long, cancel the future\n                            if time.time() - submitted_futures[completed_future][\"time\"] &gt; self.max_eval_time_mins*1.25*60:\n                                completed_future.cancel()\n                                completed_future.release() #release the future\n                                if self.verbose &gt;= 4:\n                                    print(f'WARNING AN INDIVIDUAL TIMED OUT (Fallback): \\n {submitted_futures[completed_future]} \\n')\n\n                                scores = [np.nan for _ in range(len(self.objective_names))]\n                                eval_error = \"TIMEOUT\"\n                            else:\n                                continue #otherwise, continue to next future\n\n\n\n                    #update population\n                    this_individual = submitted_futures[completed_future][\"individual\"]\n                    this_budget = submitted_futures[completed_future][\"budget\"]\n                    this_time = submitted_futures[completed_future][\"time\"]\n\n                    if len(scores) &lt; len(self.objective_names):\n                        scores = [scores[0] for _ in range(len(self.objective_names))]\n                    self.population.update_column(this_individual, column_names=self.objective_names, data=scores)\n                    self.population.update_column(this_individual, column_names=\"Completed Timestamp\", data=time.time())\n                    self.population.update_column(this_individual, column_names=\"Eval Error\", data=eval_error)\n                    if budget is not None:\n                        self.population.update_column(this_individual, column_names=\"Budget\", data=this_budget)\n\n                    submitted_futures.pop(completed_future)\n                    submitted_inds.add(this_individual.unique_id())\n                    if self.verbose &gt;= 1:\n                        pbar.update(1)\n\n                #now we have a list of completed futures\n\n                self.population.remove_invalid_from_population(column_names=\"Eval Error\", invalid_value=\"INVALID\")\n                self.population.remove_invalid_from_population(column_names=\"Eval Error\", invalid_value=\"TIMEOUT\")\n\n                #I am not entirely sure if this is necessary. I believe that calling release on the futures should be enough to free up memory. If memory issues persist, this may be a good place to start.\n                #client.run(gc.collect) #run garbage collection to free up memory\n\n                ###############################\n                # Step 2: Early Stopping\n                ###############################\n                if self.verbose &gt;= 3:\n                    sign = np.sign(self.objective_function_weights)\n                    valid_df = self.population.evaluated_individuals[~self.population.evaluated_individuals[[\"Eval Error\"]].isin([\"TIMEOUT\",\"INVALID\"]).any(axis=1)][self.objective_names]*sign\n                    cur_best_scores = valid_df.max(axis=0)*sign\n                    cur_best_scores = cur_best_scores.to_numpy()\n                    for i, obj in enumerate(self.objective_names):\n                        print(f\"Best {obj} score: {cur_best_scores[i]}\")\n\n                if self.early_stop or self.early_stop_mins:\n                    if self.budget is None or self.budget&gt;=self.budget_range[-1]: #self.budget&gt;=1:\n                        #get sign of objective_function_weights\n                        sign = np.sign(self.objective_function_weights)\n                        #get best score for each objective\n                        valid_df = self.population.evaluated_individuals[~self.population.evaluated_individuals[[\"Eval Error\"]].isin([\"TIMEOUT\",\"INVALID\"]).any(axis=1)][self.objective_names]*sign\n                        cur_best_scores = valid_df.max(axis=0)\n                        cur_best_scores = cur_best_scores.to_numpy()\n                        #cur_best_scores =  self.population.get_column(self.population.population, column_names=self.objective_names).max(axis=0)*sign #TODO this assumes the current population is the best\n\n                        improved = ( np.array(cur_best_scores) - np.array(best_scores) &gt;= np.array(self.early_stop_tol) )\n                        not_improved = np.logical_not(improved)\n                        generations_without_improvement = generations_without_improvement * not_improved + not_improved #set to zero if not improved, else increment\n\n                        timestamp_of_last_improvement = timestamp_of_last_improvement * not_improved + time.time()*improved #set to current time if improved\n\n                        pass\n                        #update best score\n                        best_scores = [max(best_scores[i], cur_best_scores[i]) for i in range(len(self.objective_names))]\n\n                        if self.early_stop:\n                            if all(generations_without_improvement&gt;self.early_stop):\n                                if self.verbose &gt;= 3:\n                                    print(f\"Early stop ({self.early_stop} individuals evaluated without improvement)\")\n                                break\n\n                        if self.early_stop_mins:\n                            if any(time.time() - timestamp_of_last_improvement &gt; self.early_stop_mins*60):\n                                if self.verbose &gt;= 3:\n                                    print(f\"Early stop  ({self.early_stop_mins} seconds passed without improvement)\")\n                                break\n\n                #if we evaluated enough individuals or time is up, stop\n                if self.max_time_mins is not None and time.time() - start_time &gt; self.max_time_mins*60:\n                    if self.verbose &gt;= 3:\n                        print(\"Time limit reached\")\n                    done = True\n                    break\n\n                if self.max_evaluated_individuals is not None and len(self.population.evaluated_individuals.dropna(subset=self.objective_names)) &gt;= self.max_evaluated_individuals:\n                    print(\"Evaluated enough individuals\")\n                    done = True\n                    break\n\n                ###############################\n                # Step 3: Submit unevaluated individuals from the initial population\n                ###############################\n                individuals_to_evaluate = self.get_unevaluated_individuals(self.objective_names, budget=budget,)\n                individuals_to_evaluate = [ind for ind in individuals_to_evaluate if ind.unique_id() not in submitted_inds]\n                for individual in individuals_to_evaluate:\n                    if self.max_queue_size &gt; len(submitted_futures):\n                        future = self._client.submit(tpot.utils.eval_utils.eval_objective_list, individual,  self.objective_functions, verbose=self.verbose, timeout=self.max_eval_time_mins*60,**self.objective_kwargs)\n\n                        submitted_futures[future] = {\"individual\": individual,\n                                                    \"time\": time.time(),\n                                                    \"budget\": budget,}\n                        submitted_inds.add(individual.unique_id())\n\n                        self.population.update_column(individual, column_names=\"Submitted Timestamp\", data=time.time())\n\n\n                ###############################\n                # Step 4: Survival Selection\n                ###############################\n                if self.survival_selector is not None:\n                    parents_df = self.population.get_column(self.population.population, column_names=self.objective_names + [\"Individual\"], to_numpy=False)\n                    evaluated = parents_df[~parents_df[self.objective_names].isna().any(axis=1)]\n                    if len(evaluated) &gt; self.population_size:\n                        unevaluated = parents_df[parents_df[self.objective_names].isna().any(axis=1)]\n\n                        cur_evaluated_population = parents_df[\"Individual\"].to_numpy()\n                        if len(cur_evaluated_population) &gt; self.population_size:\n                            scores = evaluated[self.objective_names].to_numpy()\n                            weighted_scores = scores * self.objective_function_weights\n                            new_population_index = np.ravel(self.survival_selector(weighted_scores, k=self.population_size, rng=self.rng)) #TODO make it clear that we are concatenating scores...\n\n                            #set new population\n                            try:\n                                cur_evaluated_population = np.array(cur_evaluated_population)[new_population_index]\n                                cur_evaluated_population = np.concatenate([cur_evaluated_population, unevaluated[\"Individual\"].to_numpy()])\n                                self.population.set_population(cur_evaluated_population, rng=self.rng)\n                            except Exception as e:\n                                print(\"Exception in survival selection\")\n                                print(e)\n                                print(\"new_population_index\", new_population_index)\n                                print(\"cur_evaluated_population\", cur_evaluated_population)\n                                print(\"unevaluated\", unevaluated)\n                                print(\"evaluated\", evaluated)\n                                print(\"scores\", scores)\n                                print(\"weighted_scores\", weighted_scores)\n                                print(\"self.objective_function_weights\", self.objective_function_weights)\n                                print(\"self.population_size\", self.population_size)\n                                print(\"parents_df\", parents_df)\n\n                ###############################\n                # Step 5: Parent Selection and Variation\n                ###############################\n                n_individuals_to_submit = self.max_queue_size - len(submitted_futures)\n                if n_individuals_to_submit &gt; 0:\n                    #count non-nan values in the objective columns\n                    if not enough_parents_evaluated:\n                        parents_df = self.population.get_column(self.population.population, column_names=self.objective_names, to_numpy=False)\n                        scores = parents_df[self.objective_names[0]].to_numpy()\n                        #count non-nan values in the objective columns\n                        n_evaluated = np.count_nonzero(~np.isnan(scores))\n                        if n_evaluated &gt;0 :\n                            enough_parents_evaluated=True\n\n                    # parents_df = self.population.get_column(self.population.population, column_names=self.objective_names+ [\"Individual\"], to_numpy=False)\n                    # parents_df = parents_df[~parents_df[self.objective_names].isin([\"TIMEOUT\",\"INVALID\"]).any(axis=1)]\n                    # parents_df = parents_df[~parents_df[self.objective_names].isna().any(axis=1)]\n\n                    # cur_evaluated_population = parents_df[\"Individual\"].to_numpy()\n                    # if len(cur_evaluated_population) &gt; 0:\n                    #     scores = parents_df[self.objective_names].to_numpy()\n                    #     weighted_scores = scores * self.objective_function_weights\n                    #     #number of crossover pairs and mutation only parent to generate\n\n                    #     if len(parents_df) &lt; 2:\n                    #         var_ops = [\"mutate\" for _ in range(n_individuals_to_submit)]\n                    #     else:\n                    #         var_ops = [self.rng.choice([\"crossover\",\"mutate_then_crossover\",\"crossover_then_mutate\",'mutate'],p=[self.crossover_probability,self.mutate_then_crossover_probability, self.crossover_then_mutate_probability,self.mutate_probability]) for _ in range(n_individuals_to_submit)]\n\n                    #     parents = []\n                    #     for op in var_ops:\n                    #         if op == \"mutate\":\n                    #             parents.extend(np.array(cur_evaluated_population)[self.parent_selector(weighted_scores, k=1, n_parents=1, rng=self.rng)])\n                    #         else:\n                    #             parents.extend(np.array(cur_evaluated_population)[self.parent_selector(weighted_scores, k=1, n_parents=2, rng=self.rng)])\n\n                    #     #_offspring = self.population.create_offspring2(parents, var_ops, rng=self.rng, add_to_population=True)\n                    #     offspring = self.population.create_offspring2(parents, var_ops, [ind_mutate], None, [ind_crossover], None, add_to_population=True, keep_repeats=False, mutate_until_unique=True, rng=self.rng)\n\n                    if enough_parents_evaluated:\n\n                        parents = self.population.parent_select(selector=self.parent_selector, weights=self.objective_function_weights, columns_names=self.objective_names, k=n_individuals_to_submit, n_parents=2, rng=self.rng)\n                        p = np.array([self.crossover_probability, self.mutate_then_crossover_probability, self.crossover_then_mutate_probability, self.mutate_probability])\n                        p = p / p.sum()\n                        var_op_list = self.rng.choice([\"crossover\", \"mutate_then_crossover\", \"crossover_then_mutate\", \"mutate\"], size=n_individuals_to_submit, p=p)\n\n                        for i, op in enumerate(var_op_list):\n                            if op == \"mutate\":\n                                parents[i] = parents[i][0] #mutations take a single individual\n\n                        offspring = self.population.create_offspring2(parents, var_op_list, [ind_mutate], None, [ind_crossover], None, add_to_population=True, keep_repeats=False, mutate_until_unique=True, rng=self.rng)\n\n                    # If we don't have enough evaluated individuals to use as parents for variation, we create new individuals randomly\n                    # This can happen if the individuals in the initial population are invalid\n                    elif len(submitted_futures) &lt; self.max_queue_size:\n\n                        initial_population = self.population.evaluated_individuals.iloc[:self.initial_population_size*3]\n                        invalid_initial_population = initial_population[initial_population[[\"Eval Error\"]].isin([\"TIMEOUT\",\"INVALID\"]).any(axis=1)]\n                        if len(invalid_initial_population) &gt;= self.initial_population_size*3: #if all individuals in the 3*initial population are invalid\n                            raise Exception(\"No individuals could be evaluated in the initial population. This may indicate a bug in the configuration, included models, or objective functions. Set verbose&gt;=4 to see the errors that caused individuals to fail.\")\n\n                        n_individuals_to_create = self.max_queue_size - len(submitted_futures)\n                        initial_population = [next(self.individual_generator) for _ in range(n_individuals_to_create)]\n                        self.population.add_to_population(initial_population, rng=self.rng)\n\n\n\n\n                ###############################\n                # Step 6: Add Unevaluated Individuals Generated by Variation\n                ###############################\n                individuals_to_evaluate = self.get_unevaluated_individuals(self.objective_names, budget=budget,)\n                individuals_to_evaluate = [ind for ind in individuals_to_evaluate if ind.unique_id() not in submitted_inds]\n                for individual in individuals_to_evaluate:\n                    if self.max_queue_size &gt; len(submitted_futures):\n                        future = self._client.submit(tpot.utils.eval_utils.eval_objective_list, individual,  self.objective_functions, verbose=self.verbose, timeout=self.max_eval_time_mins*60,**self.objective_kwargs)\n\n                        submitted_futures[future] = {\"individual\": individual,\n                                                    \"time\": time.time(),\n                                                    \"budget\": budget,}\n                        submitted_inds.add(individual.unique_id())\n                        self.population.update_column(individual, column_names=\"Submitted Timestamp\", data=time.time())\n\n\n                #Checkpointing\n                if self.population_file is not None: # and time.time() - last_save_time &gt; 60*10:\n                    pickle.dump(self.population, open(self.population_file, \"wb\"))\n\n\n\n        except KeyboardInterrupt:\n            if self.verbose &gt;= 3:\n                print(\"KeyboardInterrupt\")\n\n        ###############################\n        # Step 7: Cleanup\n        ###############################\n\n        self.population.remove_invalid_from_population(column_names=\"Eval Error\", invalid_value=\"INVALID\")\n        self.population.remove_invalid_from_population(column_names=\"Eval Error\", invalid_value=\"TIMEOUT\")\n\n\n        #done, cleanup futures\n        for future in submitted_futures.keys():\n            future.cancel()\n            future.release() #release the future\n\n        #I am not entirely sure if this is necessary. I believe that calling release on the futures should be enough to free up memory. If memory issues persist, this may be a good place to start.\n        #client.run(gc.collect) #run garbage collection to free up memory\n\n        #checkpoint\n        if self.population_file is not None:\n            pickle.dump(self.population, open(self.population_file, \"wb\"))\n\n        if self.client is None: #If we created our own client, close it\n            self._client.close()\n            self._cluster.close()\n\n        tpot.utils.get_pareto_frontier(self.population.evaluated_individuals, column_names=self.objective_names, weights=self.objective_function_weights)\n\n\n    def get_unevaluated_individuals(self, column_names, budget=None, individual_list=None):\n        \"\"\"\n        This function is used to get a list of individuals in the current population that have not been evaluated yet.\n\n        Parameters\n        ----------\n        column_names : list of strings\n            Names of the columns to check for unevaluated individuals (generally objective functions).\n        budget : float, default=None\n            Budget to use when checking for unevaluated individuals. If None, will not check the budget column.\n            Finds individuals who have not been evaluated with the given budget on column names.\n        individual_list : list of individuals, default=None\n            List of individuals to check for unevaluated individuals. If None, will use the current population.\n        \"\"\"\n        if individual_list is not None:\n            cur_pop = np.array(individual_list)\n        else:\n            cur_pop = np.array(self.population.population)\n\n        if all([name_step in self.population.evaluated_individuals.columns for name_step in column_names]):\n            if budget is not None:\n                offspring_scores = self.population.get_column(cur_pop, column_names=column_names+[\"Budget\"], to_numpy=False)\n                #Individuals are unevaluated if we have a higher budget OR if any of the objectives are nan\n                unevaluated_filter = lambda i: any(offspring_scores.loc[offspring_scores.index[i]][column_names].isna()) or (offspring_scores.loc[offspring_scores.index[i]][\"Budget\"] &lt; budget)\n            else:\n                offspring_scores = self.population.get_column(cur_pop, column_names=column_names, to_numpy=False)\n                unevaluated_filter = lambda i: any(offspring_scores.loc[offspring_scores.index[i]][column_names].isna())\n            unevaluated_individuals_this_step = [i for i in range(len(cur_pop)) if unevaluated_filter(i)]\n            return cur_pop[unevaluated_individuals_this_step]\n\n        else: #if column names are not in the evaluated_individuals, then we have not evaluated any individuals yet\n            for name_step in column_names:\n                self.population.evaluated_individuals[name_step] = np.nan\n            return cur_pop\n</code></pre>"},{"location":"documentation/tpot/evolvers/steady_state_evolver/#tpot.evolvers.steady_state_evolver.SteadyStateEvolver.__init__","title":"<code>__init__(individual_generator, objective_functions, objective_function_weights, objective_names=None, objective_kwargs=None, bigger_is_better=True, initial_population_size=50, population_size=300, max_evaluated_individuals=None, early_stop=None, early_stop_mins=None, early_stop_tol=0.001, max_time_mins=float('inf'), max_eval_time_mins=10, n_jobs=1, memory_limit='4GB', client=None, crossover_probability=0.2, mutate_probability=0.7, mutate_then_crossover_probability=0.05, crossover_then_mutate_probability=0.05, n_parents=2, survival_selector=survival_select_NSGA2, parent_selector=tournament_selection_dominated, budget_range=None, budget_scaling=0.5, individuals_until_end_budget=1, stepwise_steps=5, verbose=0, periodic_checkpoint_folder=None, callback=None, rng=None)</code>","text":"<p>Whereas the base_evolver uses a generational approach, the steady state evolver continuously generates individuals as resources become available.</p> <p>This evolver will simultaneously evaluated n_jobs individuals. As soon as one individual is evaluated, the current population is updated with survival_selector,  a new individual is generated from parents selected with parent_selector, and the new individual is immediately submitted for evaluation. In contrast, the base_evolver batches evaluations in generations, and only updates the population and creates new individuals after all individuals in the current generation are evaluated.</p> <p>In practice, this means that steady state evolver is more likely to use all cores at all times, allowing for flexibility is duration of evaluations and number of evaluations. However, it  may also generate less diverse populations as a result.</p> <p>Parameters:</p> Name Type Description Default <code>individual_generator</code> <code>generator</code> <p>Generator that yields new base individuals. Used to generate initial population.</p> required <code>objective_functions</code> <code>list of callables</code> <p>list of functions that get applied to the individual and return a float or list of floats If an objective function returns multiple values, they are all concatenated in order with respect to objective_function_weights and early_stop_tol.</p> required <code>objective_function_weights</code> <code>list of floats</code> <p>list of weights for each objective function. Sign flips whether bigger is better or not</p> required <code>objective_names</code> <code>list of strings</code> <p>Names of the objectives. If None, objective0, objective1, etc. will be used</p> <code>None</code> <code>objective_kwargs</code> <code>dict</code> <p>Dictionary of keyword arguments to pass to the objective function</p> <code>None</code> <code>bigger_is_better</code> <code>bool</code> <p>If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.</p> <code>True</code> <code>initial_population_size</code> <code>int</code> <p>Number of random individuals to generate in the initial population. These will all be randomly sampled, all other subsequent individuals will be generated from the population.</p> <code>50</code> <code>population_size</code> <code>int</code> <p>Note: This is different from the base_evolver.  In steady_state_evolver, the population_size is the number of individuals to keep in the live population. This is the total number of best individuals (as determined by survival_selector) to keep in the population. New individuals are generated from this population size. In base evolver, this is also the number of individuals to generate in each generation, however, here, we generate individuals as resources become available so there is no concept of a generation. It is recommended to use a higher population_size to ensure diversity in the population.</p> <code>50</code> <code>max_evaluated_individuals</code> <code>int</code> <p>Maximum number of individuals to evaluate after which training is terminated. If None, will evaluate until time limit is reached.</p> <code>None</code> <code>early_stop</code> <code>int</code> <p>If the best individual has not improved in this many evaluations, stop training. Note: Also different from base_evolver. In base evolver, this is the number of generations without improvement. Here, it is the number of individuals evaluated without improvement. Naturally, a higher value is recommended.</p> <code>None</code> <code>early_stop_mins</code> <code>int</code> <p>If the best individual has not improved in this many minutes, stop training.     early_stop_tol : float, list of floats, or None, default=0.001 -list of floats     list of tolerances for each objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged     If an index of the list is None, that item will not be used for early stopping -int     If an int is given, it will be used as the tolerance for all objectives</p> <code>None</code> <code>max_time_mins</code> <code>float</code> <p>Maximum time to run the optimization. If none or inf, will run until the end of the generations.</p> <code>float(\"inf\")</code> <code>max_eval_time_mins</code> <code>float</code> <p>Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.</p> <code>10</code> <code>n_jobs</code> <code>int</code> <p>Number of processes to run in parallel.</p> <code>1</code> <code>memory_limit</code> <code>str</code> <p>Memory limit for each job. See Dask LocalCluster documentation for more information.</p> <code>None</code> <code>client</code> <code>Client</code> <p>A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.</p> <code>None</code> <code>crossover_probability</code> <code>float</code> <p>Probability of generating a new individual by crossover between two individuals.</p> <code>.2</code> <code>mutate_probability</code> <code>float</code> <p>Probability of generating a new individual by crossover between one individuals.</p> <code>.7</code> <code>mutate_then_crossover_probability</code> <code>float</code> <p>Probability of generating a new individual by mutating two individuals followed by crossover.</p> <code>.05</code> <code>crossover_then_mutate_probability</code> <code>float</code> <p>Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.</p> <code>.05</code> <code>n_parents</code> <code>int</code> <p>Number of parents to use for crossover. Must be greater than 1.</p> <code>2</code> <code>survival_selector</code> <code>function</code> <p>Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes. Used to selected population_size * survival_percentage individuals at the start of each generation to use for mutation and crossover.</p> <code>survival_select_NSGA2</code> <code>parent_selector</code> <code>function</code> <p>Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.</p> <code>parent_select_NSGA2</code> <code>budget_range</code> <code>list[start, end]</code> <p>This parameter is used for the successive halving algorithm. A starting and ending budget to use for the budget scaling. The evolver will interpolate between these values over the generations_until_end_budget. Use is dependent on the objective functions. (In TPOTEstimator this corresponds to the percentage of the data to sample.)</p> <code>None</code> <code>budget_scaling</code> <p>A scaling factor to use when determining how fast we move the budget from the start to end budget.</p> <code>0.5</code> <code>evaluations_until_end_budget</code> <code>int</code> <p>The number of evaluations to run before reaching the max budget.</p> <code>1</code> <code>stepwise_steps</code> <code>int</code> <p>The number of staircase steps to take when interpolating the budget.</p> <code>1</code> <code>verbose</code> <code>int</code> <p>How much information to print during the optimization process. Higher values include the information from lower values. 0. nothing 1. progress bar 2. evaluations progress bar 3. best individual 4. warnings</p> <p>=5. full warnings trace</p> <code>0</code> <code>periodic_checkpoint_folder</code> <code>str</code> <p>Folder to save the population to periodically. If None, no periodic saving will be done. If provided, training will resume from this checkpoint.</p> <code>None</code> <code>callback</code> <code>CallBackInterface</code> <p>Callback object. Not implemented</p> <code>None</code> <code>rng</code> <code>(Generator, None)</code> <p>An object for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes</p> <ul> <li>Numpy.Random.Generator     Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'. Note this will be the same Generator passed in.</li> <li>None     Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS</li> </ul> <code>None</code> <p>Attributes:</p> Name Type Description <code>population</code> <code>Population</code> <p>The population of individuals. Use population.population to access the individuals in the current population. Use population.evaluated_individuals to access a data frame of all individuals that have been explored.</p> Source code in <code>tpot/evolvers/steady_state_evolver.py</code> <pre><code>def __init__(   self,\n                individual_generator ,\n\n                objective_functions,\n                objective_function_weights,\n                objective_names = None,\n                objective_kwargs = None,\n                bigger_is_better = True,\n\n                initial_population_size = 50,\n                population_size = 300,\n                max_evaluated_individuals = None,\n                early_stop = None,\n                early_stop_mins = None,\n                early_stop_tol = 0.001,\n\n\n                max_time_mins=float(\"inf\"),\n                max_eval_time_mins=10,\n\n                n_jobs=1,\n                memory_limit=\"4GB\",\n                client=None,\n\n                crossover_probability=.2,\n                mutate_probability=.7,\n                mutate_then_crossover_probability=.05,\n                crossover_then_mutate_probability=.05,\n                n_parents=2,\n\n                survival_selector = survival_select_NSGA2,\n                parent_selector = tournament_selection_dominated,\n\n                budget_range = None,\n                budget_scaling = .5,\n                individuals_until_end_budget = 1,\n                stepwise_steps = 5,\n\n                verbose = 0,\n                periodic_checkpoint_folder = None,\n                callback = None,\n\n                rng=None\n                ) -&gt; None:\n    \"\"\"\n    Whereas the base_evolver uses a generational approach, the steady state evolver continuously generates individuals as resources become available.\n\n    This evolver will simultaneously evaluated n_jobs individuals. As soon as one individual is evaluated, the current population is updated with survival_selector, \n    a new individual is generated from parents selected with parent_selector, and the new individual is immediately submitted for evaluation.\n    In contrast, the base_evolver batches evaluations in generations, and only updates the population and creates new individuals after all individuals in the current generation are evaluated.\n\n    In practice, this means that steady state evolver is more likely to use all cores at all times, allowing for flexibility is duration of evaluations and number of evaluations. However, it \n    may also generate less diverse populations as a result.\n\n    Parameters\n    ----------\n    individual_generator : generator\n        Generator that yields new base individuals. Used to generate initial population.\n    objective_functions : list of callables\n        list of functions that get applied to the individual and return a float or list of floats\n        If an objective function returns multiple values, they are all concatenated in order\n        with respect to objective_function_weights and early_stop_tol.\n    objective_function_weights : list of floats\n        list of weights for each objective function. Sign flips whether bigger is better or not\n    objective_names : list of strings, default=None\n        Names of the objectives. If None, objective0, objective1, etc. will be used\n    objective_kwargs : dict, default=None\n        Dictionary of keyword arguments to pass to the objective function\n    bigger_is_better : bool, default=True\n        If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n\n    initial_population_size : int, default=50\n        Number of random individuals to generate in the initial population. These will all be randomly sampled, all other subsequent individuals will be generated from the population.\n    population_size : int, default=50\n        Note: This is different from the base_evolver. \n        In steady_state_evolver, the population_size is the number of individuals to keep in the live population. This is the total number of best individuals (as determined by survival_selector) to keep in the population.\n        New individuals are generated from this population size.\n        In base evolver, this is also the number of individuals to generate in each generation, however, here, we generate individuals as resources become available so there is no concept of a generation.\n        It is recommended to use a higher population_size to ensure diversity in the population.\n    max_evaluated_individuals : int, default=None\n        Maximum number of individuals to evaluate after which training is terminated. If None, will evaluate until time limit is reached.\n    early_stop : int, default=None\n        If the best individual has not improved in this many evaluations, stop training.\n        Note: Also different from base_evolver. In base evolver, this is the number of generations without improvement. Here, it is the number of individuals evaluated without improvement. Naturally, a higher value is recommended.\n    early_stop_mins : int, default=None\n        If the best individual has not improved in this many minutes, stop training.\n            early_stop_tol : float, list of floats, or None, default=0.001\n        -list of floats\n            list of tolerances for each objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n            If an index of the list is None, that item will not be used for early stopping\n        -int\n            If an int is given, it will be used as the tolerance for all objectives\n    max_time_mins : float, default=float(\"inf\")\n        Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n    max_eval_time_mins : float, default=10\n        Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n    n_jobs : int, default=1\n        Number of processes to run in parallel.\n    memory_limit : str, default=None\n        Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n    client : dask.distributed.Client, default=None\n        A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.\n    crossover_probability : float, default=.2\n        Probability of generating a new individual by crossover between two individuals.\n    mutate_probability : float, default=.7\n        Probability of generating a new individual by crossover between one individuals.\n    mutate_then_crossover_probability : float, default=.05\n        Probability of generating a new individual by mutating two individuals followed by crossover.\n    crossover_then_mutate_probability : float, default=.05\n        Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.\n    n_parents : int, default=2\n        Number of parents to use for crossover. Must be greater than 1.\n    survival_selector : function, default=survival_select_NSGA2\n        Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes.\n        Used to selected population_size * survival_percentage individuals at the start of each generation to use for mutation and crossover.\n    parent_selector : function, default=parent_select_NSGA2\n        Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.     \n\n    budget_range : list [start, end], default=None\n        This parameter is used for the successive halving algorithm.\n        A starting and ending budget to use for the budget scaling. The evolver will interpolate between these values over the generations_until_end_budget.\n        Use is dependent on the objective functions. (In TPOTEstimator this corresponds to the percentage of the data to sample.)\n    budget_scaling float : [0,1], default=0.5\n        A scaling factor to use when determining how fast we move the budget from the start to end budget.\n    evaluations_until_end_budget : int, default=1\n        The number of evaluations to run before reaching the max budget.\n    stepwise_steps : int, default=1\n        The number of staircase steps to take when interpolating the budget.\n    verbose : int, default=0\n        How much information to print during the optimization process. Higher values include the information from lower values.\n        0. nothing\n        1. progress bar\n        2. evaluations progress bar\n        3. best individual\n        4. warnings\n        &gt;=5. full warnings trace\n    periodic_checkpoint_folder : str, default=None\n        Folder to save the population to periodically. If None, no periodic saving will be done.\n        If provided, training will resume from this checkpoint.\n    callback : tpot.CallBackInterface, default=None\n        Callback object. Not implemented\n    rng : Numpy.Random.Generator, None, default=None\n        An object for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes\n\n        - Numpy.Random.Generator\n            Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'. Note this will be the same Generator passed in.\n        - None\n            Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS\n\n    Attributes\n    ----------\n    population : tpot.Population\n        The population of individuals.\n        Use population.population to access the individuals in the current population.\n        Use population.evaluated_individuals to access a data frame of all individuals that have been explored.\n\n    \"\"\"\n\n    self.rng = np.random.default_rng(rng)\n\n    self.max_evaluated_individuals = max_evaluated_individuals\n    self.individuals_until_end_budget = individuals_until_end_budget\n\n    self.individual_generator = individual_generator\n    self.population_size = population_size\n    self.objective_functions = objective_functions\n    self.objective_function_weights = np.array(objective_function_weights)\n    self.bigger_is_better = bigger_is_better\n    if not bigger_is_better:\n        self.objective_function_weights = np.array(self.objective_function_weights)*-1\n\n    self.population_size_list = None\n\n\n    self.periodic_checkpoint_folder = periodic_checkpoint_folder\n    self.verbose  = verbose\n    self.callback = callback\n    self.n_jobs = n_jobs\n\n    if max_time_mins is None:\n        self.max_time_mins = float(\"inf\")\n    else:\n        self.max_time_mins = max_time_mins\n\n    #functools requires none for infinite time, doesn't support inf\n    if max_eval_time_mins is not None and math.isinf(max_eval_time_mins ):\n        self.max_eval_time_mins = None\n    else:\n        self.max_eval_time_mins = max_eval_time_mins\n\n    self.initial_population_size = initial_population_size\n    self.budget_range = budget_range\n    self.budget_scaling = budget_scaling\n    self.stepwise_steps = stepwise_steps\n\n    self.memory_limit = memory_limit\n\n    self.client = client\n\n\n    self.survival_selector=survival_selector\n    self.parent_selector=parent_selector\n\n\n    total_var_p = crossover_probability + mutate_probability + mutate_then_crossover_probability + crossover_then_mutate_probability\n    self.crossover_probability = crossover_probability / total_var_p\n    self.mutate_probability = mutate_probability  / total_var_p\n    self.mutate_then_crossover_probability= mutate_then_crossover_probability / total_var_p\n    self.crossover_then_mutate_probability= crossover_then_mutate_probability / total_var_p\n\n    self.n_parents = n_parents\n\n    if objective_kwargs is None:\n        self.objective_kwargs = {}\n    else:\n        self.objective_kwargs = objective_kwargs\n\n    ###########\n\n\n    if self.budget_range is None:\n        self.budget_list = None\n    else:\n        self.budget_list = beta_interpolation(start=self.budget_range[0], end=self.budget_range[1], n=self.generations_until_end_budget, scale=self.budget_scaling, n_steps=self.stepwise_steps)\n\n    if objective_names is None:\n        self.objective_names = [\"objective\"+str(i) for i in range(len(objective_function_weights))]\n    else:\n        self.objective_names = objective_names\n\n    if self.budget_list is not None:\n        if len(self.budget_list) &lt;= self.generation:\n            self.budget = self.budget_list[-1]\n        else:\n            self.budget = self.budget_list[self.generation]\n    else:\n        self.budget = None\n\n\n    self.early_stop_tol = early_stop_tol\n    self.early_stop_mins = early_stop_mins\n    self.early_stop = early_stop\n\n    if isinstance(self.early_stop_tol, float):\n        self.early_stop_tol = [self.early_stop_tol for _ in range(len(self.objective_names))]\n\n    self.early_stop_tol = [np.inf if tol is None else tol for tol in self.early_stop_tol]\n\n    self.population = None\n    self.population_file = None\n    if self.periodic_checkpoint_folder is not None:\n        self.population_file = os.path.join(self.periodic_checkpoint_folder, \"population.pkl\")\n        if not os.path.exists(self.periodic_checkpoint_folder):\n            os.makedirs(self.periodic_checkpoint_folder)\n        if os.path.exists(self.population_file):\n            self.population = pickle.load(open(self.population_file, \"rb\"))\n\n\n    init_names = self.objective_names\n    if self.budget_range is not None:\n        init_names = init_names + [\"Budget\"]\n    if self.population is None:\n        self.population = tpot.Population(column_names=init_names)\n        initial_population = [next(self.individual_generator) for _ in range(self.initial_population_size)]\n        self.population.add_to_population(initial_population, rng=self.rng)\n</code></pre>"},{"location":"documentation/tpot/evolvers/steady_state_evolver/#tpot.evolvers.steady_state_evolver.SteadyStateEvolver.get_unevaluated_individuals","title":"<code>get_unevaluated_individuals(column_names, budget=None, individual_list=None)</code>","text":"<p>This function is used to get a list of individuals in the current population that have not been evaluated yet.</p> <p>Parameters:</p> Name Type Description Default <code>column_names</code> <code>list of strings</code> <p>Names of the columns to check for unevaluated individuals (generally objective functions).</p> required <code>budget</code> <code>float</code> <p>Budget to use when checking for unevaluated individuals. If None, will not check the budget column. Finds individuals who have not been evaluated with the given budget on column names.</p> <code>None</code> <code>individual_list</code> <code>list of individuals</code> <p>List of individuals to check for unevaluated individuals. If None, will use the current population.</p> <code>None</code> Source code in <code>tpot/evolvers/steady_state_evolver.py</code> <pre><code>def get_unevaluated_individuals(self, column_names, budget=None, individual_list=None):\n    \"\"\"\n    This function is used to get a list of individuals in the current population that have not been evaluated yet.\n\n    Parameters\n    ----------\n    column_names : list of strings\n        Names of the columns to check for unevaluated individuals (generally objective functions).\n    budget : float, default=None\n        Budget to use when checking for unevaluated individuals. If None, will not check the budget column.\n        Finds individuals who have not been evaluated with the given budget on column names.\n    individual_list : list of individuals, default=None\n        List of individuals to check for unevaluated individuals. If None, will use the current population.\n    \"\"\"\n    if individual_list is not None:\n        cur_pop = np.array(individual_list)\n    else:\n        cur_pop = np.array(self.population.population)\n\n    if all([name_step in self.population.evaluated_individuals.columns for name_step in column_names]):\n        if budget is not None:\n            offspring_scores = self.population.get_column(cur_pop, column_names=column_names+[\"Budget\"], to_numpy=False)\n            #Individuals are unevaluated if we have a higher budget OR if any of the objectives are nan\n            unevaluated_filter = lambda i: any(offspring_scores.loc[offspring_scores.index[i]][column_names].isna()) or (offspring_scores.loc[offspring_scores.index[i]][\"Budget\"] &lt; budget)\n        else:\n            offspring_scores = self.population.get_column(cur_pop, column_names=column_names, to_numpy=False)\n            unevaluated_filter = lambda i: any(offspring_scores.loc[offspring_scores.index[i]][column_names].isna())\n        unevaluated_individuals_this_step = [i for i in range(len(cur_pop)) if unevaluated_filter(i)]\n        return cur_pop[unevaluated_individuals_this_step]\n\n    else: #if column names are not in the evaluated_individuals, then we have not evaluated any individuals yet\n        for name_step in column_names:\n            self.population.evaluated_individuals[name_step] = np.nan\n        return cur_pop\n</code></pre>"},{"location":"documentation/tpot/evolvers/steady_state_evolver/#tpot.evolvers.steady_state_evolver.SteadyStateEvolver.optimize","title":"<code>optimize()</code>","text":"<p>Creates an initial population and runs the evolutionary algorithm for the given number of generations.  If generations is None, will use self.generations.</p> Source code in <code>tpot/evolvers/steady_state_evolver.py</code> <pre><code>def optimize(self):\n    \"\"\"\n    Creates an initial population and runs the evolutionary algorithm for the given number of generations. \n    If generations is None, will use self.generations.\n    \"\"\"\n\n    #intialize the client\n    if self.client is not None: #If user passed in a client manually\n       self._client = self.client\n    else:\n\n        if self.verbose &gt;= 4:\n            silence_logs = 30\n        elif self.verbose &gt;=5:\n            silence_logs = 40\n        else:\n            silence_logs = 50\n        self._cluster = LocalCluster(n_workers=self.n_jobs, #if no client is passed in and no global client exists, create our own\n                threads_per_worker=1,\n                silence_logs=silence_logs,\n                processes=False,\n                memory_limit=self.memory_limit)\n        self._client = Client(self._cluster)\n\n\n    self.max_queue_size = len(self._client.cluster.workers)\n\n    #set up logging params\n    evaluated_count = 0\n    generations_without_improvement = np.array([0 for _ in range(len(self.objective_function_weights))])\n    timestamp_of_last_improvement = np.array([time.time() for _ in range(len(self.objective_function_weights))])\n    best_scores = [-np.inf for _ in range(len(self.objective_function_weights))]\n    scheduled_timeout_time = time.time() + self.max_time_mins*60\n    budget = None\n\n    submitted_futures = {}\n    submitted_inds = set()\n\n    start_time = time.time()\n\n    try:\n\n\n        if self.verbose &gt;= 1:\n            if self.max_evaluated_individuals is not None:\n                pbar = tqdm.tqdm(total=self.max_evaluated_individuals, miniters=1)\n            else:\n                pbar = tqdm.tqdm(total=0, miniters=1)\n            pbar.set_description(\"Evaluations\")\n\n        #submit initial population\n        individuals_to_evaluate = self.get_unevaluated_individuals(self.objective_names, budget=budget,)\n\n        for individual in individuals_to_evaluate:\n            if len(submitted_futures) &gt;= self.max_queue_size:\n                break\n            future = self._client.submit(tpot.utils.eval_utils.eval_objective_list, individual,  self.objective_functions, verbose=self.verbose, timeout=self.max_eval_time_mins*60,**self.objective_kwargs)\n\n            submitted_futures[future] = {\"individual\": individual,\n                                        \"time\": time.time(),\n                                        \"budget\": budget,}\n            submitted_inds.add(individual.unique_id())\n            self.population.update_column(individual, column_names=\"Submitted Timestamp\", data=time.time())\n\n        done = False\n        start_time = time.time()\n\n        enough_parents_evaluated=False\n        while not done:\n\n            ###############################\n            # Step 1: Check for finished futures\n            ###############################\n\n            #wait for at least one future to finish or timeout\n            try:\n                next(distributed.as_completed(submitted_futures, timeout=self.max_eval_time_mins*60))\n            except dask.distributed.TimeoutError:\n                pass\n            except dask.distributed.CancelledError:\n                pass\n\n            #Loop through all futures, collect completed and timeout futures.\n            for completed_future in list(submitted_futures.keys()):\n                eval_error = None\n                #get scores and update\n                if completed_future.done(): #if future is done\n                    #If the future is done but threw and error, record the error\n                    if completed_future.exception() or completed_future.status == \"error\": #if the future is done and threw an error\n                        print(\"Exception in future\")\n                        print(completed_future.exception())\n                        scores = [np.nan for _ in range(len(self.objective_names))]\n                        eval_error = \"INVALID\"\n                    elif completed_future.cancelled(): #if the future is done and was cancelled\n                        print(\"Cancelled future (likely memory related)\")\n                        scores = [np.nan for _ in range(len(self.objective_names))]\n                        eval_error = \"INVALID\"\n                        client.run(gc.collect)\n                    else: #if the future is done and did not throw an error, get the scores\n                        try:\n                            scores = completed_future.result()\n\n                            #check if scores contain \"INVALID\" or \"TIMEOUT\"\n                            if \"INVALID\" in scores:\n                                eval_error = \"INVALID\"\n                                scores = [np.nan]\n                            elif \"TIMEOUT\" in scores:\n                                eval_error = \"TIMEOUT\"\n                                scores = [np.nan]\n\n                        except Exception as e:\n                            print(\"Exception in future, but not caught by dask\")\n                            print(e)\n                            print(completed_future.exception())\n                            print(completed_future)\n                            print(\"status\", completed_future.status)\n                            print(\"done\", completed_future.done())\n                            print(\"cancelld \", completed_future.cancelled())\n                            scores = [np.nan for _ in range(len(self.objective_names))]\n                            eval_error = \"INVALID\"\n                    completed_future.release() #release the future\n                else: #if future is not done\n\n                    if self.max_eval_time_mins is not None:\n                        #check if the future has been running for too long, cancel the future\n                        if time.time() - submitted_futures[completed_future][\"time\"] &gt; self.max_eval_time_mins*1.25*60:\n                            completed_future.cancel()\n                            completed_future.release() #release the future\n                            if self.verbose &gt;= 4:\n                                print(f'WARNING AN INDIVIDUAL TIMED OUT (Fallback): \\n {submitted_futures[completed_future]} \\n')\n\n                            scores = [np.nan for _ in range(len(self.objective_names))]\n                            eval_error = \"TIMEOUT\"\n                        else:\n                            continue #otherwise, continue to next future\n\n\n\n                #update population\n                this_individual = submitted_futures[completed_future][\"individual\"]\n                this_budget = submitted_futures[completed_future][\"budget\"]\n                this_time = submitted_futures[completed_future][\"time\"]\n\n                if len(scores) &lt; len(self.objective_names):\n                    scores = [scores[0] for _ in range(len(self.objective_names))]\n                self.population.update_column(this_individual, column_names=self.objective_names, data=scores)\n                self.population.update_column(this_individual, column_names=\"Completed Timestamp\", data=time.time())\n                self.population.update_column(this_individual, column_names=\"Eval Error\", data=eval_error)\n                if budget is not None:\n                    self.population.update_column(this_individual, column_names=\"Budget\", data=this_budget)\n\n                submitted_futures.pop(completed_future)\n                submitted_inds.add(this_individual.unique_id())\n                if self.verbose &gt;= 1:\n                    pbar.update(1)\n\n            #now we have a list of completed futures\n\n            self.population.remove_invalid_from_population(column_names=\"Eval Error\", invalid_value=\"INVALID\")\n            self.population.remove_invalid_from_population(column_names=\"Eval Error\", invalid_value=\"TIMEOUT\")\n\n            #I am not entirely sure if this is necessary. I believe that calling release on the futures should be enough to free up memory. If memory issues persist, this may be a good place to start.\n            #client.run(gc.collect) #run garbage collection to free up memory\n\n            ###############################\n            # Step 2: Early Stopping\n            ###############################\n            if self.verbose &gt;= 3:\n                sign = np.sign(self.objective_function_weights)\n                valid_df = self.population.evaluated_individuals[~self.population.evaluated_individuals[[\"Eval Error\"]].isin([\"TIMEOUT\",\"INVALID\"]).any(axis=1)][self.objective_names]*sign\n                cur_best_scores = valid_df.max(axis=0)*sign\n                cur_best_scores = cur_best_scores.to_numpy()\n                for i, obj in enumerate(self.objective_names):\n                    print(f\"Best {obj} score: {cur_best_scores[i]}\")\n\n            if self.early_stop or self.early_stop_mins:\n                if self.budget is None or self.budget&gt;=self.budget_range[-1]: #self.budget&gt;=1:\n                    #get sign of objective_function_weights\n                    sign = np.sign(self.objective_function_weights)\n                    #get best score for each objective\n                    valid_df = self.population.evaluated_individuals[~self.population.evaluated_individuals[[\"Eval Error\"]].isin([\"TIMEOUT\",\"INVALID\"]).any(axis=1)][self.objective_names]*sign\n                    cur_best_scores = valid_df.max(axis=0)\n                    cur_best_scores = cur_best_scores.to_numpy()\n                    #cur_best_scores =  self.population.get_column(self.population.population, column_names=self.objective_names).max(axis=0)*sign #TODO this assumes the current population is the best\n\n                    improved = ( np.array(cur_best_scores) - np.array(best_scores) &gt;= np.array(self.early_stop_tol) )\n                    not_improved = np.logical_not(improved)\n                    generations_without_improvement = generations_without_improvement * not_improved + not_improved #set to zero if not improved, else increment\n\n                    timestamp_of_last_improvement = timestamp_of_last_improvement * not_improved + time.time()*improved #set to current time if improved\n\n                    pass\n                    #update best score\n                    best_scores = [max(best_scores[i], cur_best_scores[i]) for i in range(len(self.objective_names))]\n\n                    if self.early_stop:\n                        if all(generations_without_improvement&gt;self.early_stop):\n                            if self.verbose &gt;= 3:\n                                print(f\"Early stop ({self.early_stop} individuals evaluated without improvement)\")\n                            break\n\n                    if self.early_stop_mins:\n                        if any(time.time() - timestamp_of_last_improvement &gt; self.early_stop_mins*60):\n                            if self.verbose &gt;= 3:\n                                print(f\"Early stop  ({self.early_stop_mins} seconds passed without improvement)\")\n                            break\n\n            #if we evaluated enough individuals or time is up, stop\n            if self.max_time_mins is not None and time.time() - start_time &gt; self.max_time_mins*60:\n                if self.verbose &gt;= 3:\n                    print(\"Time limit reached\")\n                done = True\n                break\n\n            if self.max_evaluated_individuals is not None and len(self.population.evaluated_individuals.dropna(subset=self.objective_names)) &gt;= self.max_evaluated_individuals:\n                print(\"Evaluated enough individuals\")\n                done = True\n                break\n\n            ###############################\n            # Step 3: Submit unevaluated individuals from the initial population\n            ###############################\n            individuals_to_evaluate = self.get_unevaluated_individuals(self.objective_names, budget=budget,)\n            individuals_to_evaluate = [ind for ind in individuals_to_evaluate if ind.unique_id() not in submitted_inds]\n            for individual in individuals_to_evaluate:\n                if self.max_queue_size &gt; len(submitted_futures):\n                    future = self._client.submit(tpot.utils.eval_utils.eval_objective_list, individual,  self.objective_functions, verbose=self.verbose, timeout=self.max_eval_time_mins*60,**self.objective_kwargs)\n\n                    submitted_futures[future] = {\"individual\": individual,\n                                                \"time\": time.time(),\n                                                \"budget\": budget,}\n                    submitted_inds.add(individual.unique_id())\n\n                    self.population.update_column(individual, column_names=\"Submitted Timestamp\", data=time.time())\n\n\n            ###############################\n            # Step 4: Survival Selection\n            ###############################\n            if self.survival_selector is not None:\n                parents_df = self.population.get_column(self.population.population, column_names=self.objective_names + [\"Individual\"], to_numpy=False)\n                evaluated = parents_df[~parents_df[self.objective_names].isna().any(axis=1)]\n                if len(evaluated) &gt; self.population_size:\n                    unevaluated = parents_df[parents_df[self.objective_names].isna().any(axis=1)]\n\n                    cur_evaluated_population = parents_df[\"Individual\"].to_numpy()\n                    if len(cur_evaluated_population) &gt; self.population_size:\n                        scores = evaluated[self.objective_names].to_numpy()\n                        weighted_scores = scores * self.objective_function_weights\n                        new_population_index = np.ravel(self.survival_selector(weighted_scores, k=self.population_size, rng=self.rng)) #TODO make it clear that we are concatenating scores...\n\n                        #set new population\n                        try:\n                            cur_evaluated_population = np.array(cur_evaluated_population)[new_population_index]\n                            cur_evaluated_population = np.concatenate([cur_evaluated_population, unevaluated[\"Individual\"].to_numpy()])\n                            self.population.set_population(cur_evaluated_population, rng=self.rng)\n                        except Exception as e:\n                            print(\"Exception in survival selection\")\n                            print(e)\n                            print(\"new_population_index\", new_population_index)\n                            print(\"cur_evaluated_population\", cur_evaluated_population)\n                            print(\"unevaluated\", unevaluated)\n                            print(\"evaluated\", evaluated)\n                            print(\"scores\", scores)\n                            print(\"weighted_scores\", weighted_scores)\n                            print(\"self.objective_function_weights\", self.objective_function_weights)\n                            print(\"self.population_size\", self.population_size)\n                            print(\"parents_df\", parents_df)\n\n            ###############################\n            # Step 5: Parent Selection and Variation\n            ###############################\n            n_individuals_to_submit = self.max_queue_size - len(submitted_futures)\n            if n_individuals_to_submit &gt; 0:\n                #count non-nan values in the objective columns\n                if not enough_parents_evaluated:\n                    parents_df = self.population.get_column(self.population.population, column_names=self.objective_names, to_numpy=False)\n                    scores = parents_df[self.objective_names[0]].to_numpy()\n                    #count non-nan values in the objective columns\n                    n_evaluated = np.count_nonzero(~np.isnan(scores))\n                    if n_evaluated &gt;0 :\n                        enough_parents_evaluated=True\n\n                # parents_df = self.population.get_column(self.population.population, column_names=self.objective_names+ [\"Individual\"], to_numpy=False)\n                # parents_df = parents_df[~parents_df[self.objective_names].isin([\"TIMEOUT\",\"INVALID\"]).any(axis=1)]\n                # parents_df = parents_df[~parents_df[self.objective_names].isna().any(axis=1)]\n\n                # cur_evaluated_population = parents_df[\"Individual\"].to_numpy()\n                # if len(cur_evaluated_population) &gt; 0:\n                #     scores = parents_df[self.objective_names].to_numpy()\n                #     weighted_scores = scores * self.objective_function_weights\n                #     #number of crossover pairs and mutation only parent to generate\n\n                #     if len(parents_df) &lt; 2:\n                #         var_ops = [\"mutate\" for _ in range(n_individuals_to_submit)]\n                #     else:\n                #         var_ops = [self.rng.choice([\"crossover\",\"mutate_then_crossover\",\"crossover_then_mutate\",'mutate'],p=[self.crossover_probability,self.mutate_then_crossover_probability, self.crossover_then_mutate_probability,self.mutate_probability]) for _ in range(n_individuals_to_submit)]\n\n                #     parents = []\n                #     for op in var_ops:\n                #         if op == \"mutate\":\n                #             parents.extend(np.array(cur_evaluated_population)[self.parent_selector(weighted_scores, k=1, n_parents=1, rng=self.rng)])\n                #         else:\n                #             parents.extend(np.array(cur_evaluated_population)[self.parent_selector(weighted_scores, k=1, n_parents=2, rng=self.rng)])\n\n                #     #_offspring = self.population.create_offspring2(parents, var_ops, rng=self.rng, add_to_population=True)\n                #     offspring = self.population.create_offspring2(parents, var_ops, [ind_mutate], None, [ind_crossover], None, add_to_population=True, keep_repeats=False, mutate_until_unique=True, rng=self.rng)\n\n                if enough_parents_evaluated:\n\n                    parents = self.population.parent_select(selector=self.parent_selector, weights=self.objective_function_weights, columns_names=self.objective_names, k=n_individuals_to_submit, n_parents=2, rng=self.rng)\n                    p = np.array([self.crossover_probability, self.mutate_then_crossover_probability, self.crossover_then_mutate_probability, self.mutate_probability])\n                    p = p / p.sum()\n                    var_op_list = self.rng.choice([\"crossover\", \"mutate_then_crossover\", \"crossover_then_mutate\", \"mutate\"], size=n_individuals_to_submit, p=p)\n\n                    for i, op in enumerate(var_op_list):\n                        if op == \"mutate\":\n                            parents[i] = parents[i][0] #mutations take a single individual\n\n                    offspring = self.population.create_offspring2(parents, var_op_list, [ind_mutate], None, [ind_crossover], None, add_to_population=True, keep_repeats=False, mutate_until_unique=True, rng=self.rng)\n\n                # If we don't have enough evaluated individuals to use as parents for variation, we create new individuals randomly\n                # This can happen if the individuals in the initial population are invalid\n                elif len(submitted_futures) &lt; self.max_queue_size:\n\n                    initial_population = self.population.evaluated_individuals.iloc[:self.initial_population_size*3]\n                    invalid_initial_population = initial_population[initial_population[[\"Eval Error\"]].isin([\"TIMEOUT\",\"INVALID\"]).any(axis=1)]\n                    if len(invalid_initial_population) &gt;= self.initial_population_size*3: #if all individuals in the 3*initial population are invalid\n                        raise Exception(\"No individuals could be evaluated in the initial population. This may indicate a bug in the configuration, included models, or objective functions. Set verbose&gt;=4 to see the errors that caused individuals to fail.\")\n\n                    n_individuals_to_create = self.max_queue_size - len(submitted_futures)\n                    initial_population = [next(self.individual_generator) for _ in range(n_individuals_to_create)]\n                    self.population.add_to_population(initial_population, rng=self.rng)\n\n\n\n\n            ###############################\n            # Step 6: Add Unevaluated Individuals Generated by Variation\n            ###############################\n            individuals_to_evaluate = self.get_unevaluated_individuals(self.objective_names, budget=budget,)\n            individuals_to_evaluate = [ind for ind in individuals_to_evaluate if ind.unique_id() not in submitted_inds]\n            for individual in individuals_to_evaluate:\n                if self.max_queue_size &gt; len(submitted_futures):\n                    future = self._client.submit(tpot.utils.eval_utils.eval_objective_list, individual,  self.objective_functions, verbose=self.verbose, timeout=self.max_eval_time_mins*60,**self.objective_kwargs)\n\n                    submitted_futures[future] = {\"individual\": individual,\n                                                \"time\": time.time(),\n                                                \"budget\": budget,}\n                    submitted_inds.add(individual.unique_id())\n                    self.population.update_column(individual, column_names=\"Submitted Timestamp\", data=time.time())\n\n\n            #Checkpointing\n            if self.population_file is not None: # and time.time() - last_save_time &gt; 60*10:\n                pickle.dump(self.population, open(self.population_file, \"wb\"))\n\n\n\n    except KeyboardInterrupt:\n        if self.verbose &gt;= 3:\n            print(\"KeyboardInterrupt\")\n\n    ###############################\n    # Step 7: Cleanup\n    ###############################\n\n    self.population.remove_invalid_from_population(column_names=\"Eval Error\", invalid_value=\"INVALID\")\n    self.population.remove_invalid_from_population(column_names=\"Eval Error\", invalid_value=\"TIMEOUT\")\n\n\n    #done, cleanup futures\n    for future in submitted_futures.keys():\n        future.cancel()\n        future.release() #release the future\n\n    #I am not entirely sure if this is necessary. I believe that calling release on the futures should be enough to free up memory. If memory issues persist, this may be a good place to start.\n    #client.run(gc.collect) #run garbage collection to free up memory\n\n    #checkpoint\n    if self.population_file is not None:\n        pickle.dump(self.population, open(self.population_file, \"wb\"))\n\n    if self.client is None: #If we created our own client, close it\n        self._client.close()\n        self._cluster.close()\n\n    tpot.utils.get_pareto_frontier(self.population.evaluated_individuals, column_names=self.objective_names, weights=self.objective_function_weights)\n</code></pre>"},{"location":"documentation/tpot/evolvers/steady_state_evolver/#tpot.evolvers.steady_state_evolver.ind_crossover","title":"<code>ind_crossover(ind1, ind2, rng)</code>","text":"<p>Calls the ind1.crossover(ind2, rng=rng)</p> <p>Parameters:</p> Name Type Description Default <code>ind1</code> <code>BaseIndividual</code> required <code>ind2</code> <code>BaseIndividual</code> required <code>rng</code> <code>int or Generator</code> <p>A numpy random generator to use for reproducibility</p> required Source code in <code>tpot/evolvers/steady_state_evolver.py</code> <pre><code>def ind_crossover(ind1, ind2, rng):\n    \"\"\"\n    Calls the ind1.crossover(ind2, rng=rng)\n    Parameters\n    ----------\n    ind1 : tpot.BaseIndividual\n    ind2 : tpot.BaseIndividual\n    rng : int or numpy.random.Generator\n        A numpy random generator to use for reproducibility\n    \"\"\"\n    rng = np.random.default_rng(rng)\n    return ind1.crossover(ind2, rng=rng)\n</code></pre>"},{"location":"documentation/tpot/evolvers/steady_state_evolver/#tpot.evolvers.steady_state_evolver.ind_mutate","title":"<code>ind_mutate(ind, rng)</code>","text":"<p>Calls the ind.mutate method on the individual</p> <p>Parameters:</p> Name Type Description Default <code>ind</code> <code>BaseIndividual</code> <p>The individual to mutate</p> required <code>rng</code> <code>int or Generator</code> <p>A numpy random generator to use for reproducibility</p> required Source code in <code>tpot/evolvers/steady_state_evolver.py</code> <pre><code>def ind_mutate(ind, rng):\n    \"\"\"\n    Calls the ind.mutate method on the individual\n\n    Parameters\n    ----------\n    ind : tpot.BaseIndividual\n        The individual to mutate\n    rng : int or numpy.random.Generator\n        A numpy random generator to use for reproducibility\n    \"\"\"\n    rng = np.random.default_rng(rng)\n    return ind.mutate(rng=rng)\n</code></pre>"},{"location":"documentation/tpot/objectives/average_path_length/","title":"Average path length","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/objectives/average_path_length/#tpot.objectives.average_path_length.average_path_length_objective","title":"<code>average_path_length_objective(graph_pipeline)</code>","text":"<p>Computes the average shortest path from all nodes to the root/final estimator (only supported for GraphPipeline)</p> <p>Parameters:</p> Name Type Description Default <code>graph_pipeline</code> <p>The pipeline to compute the average path length for</p> required Source code in <code>tpot/objectives/average_path_length.py</code> <pre><code>def average_path_length_objective(graph_pipeline):\n    \"\"\"\n    Computes the average shortest path from all nodes to the root/final estimator (only supported for GraphPipeline)\n\n    Parameters\n    ----------\n    graph_pipeline: GraphPipeline\n        The pipeline to compute the average path length for\n\n    \"\"\"\n\n    path_lengths =  nx.shortest_path_length(graph_pipeline.graph, source=graph_pipeline.root)\n    return np.mean(np.array(list(path_lengths.values())))+1\n</code></pre>"},{"location":"documentation/tpot/objectives/complexity/","title":"Complexity","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/objectives/complexity/#tpot.objectives.complexity.complexity_scorer","title":"<code>complexity_scorer(est, X=None, y=None)</code>","text":"<p>Estimates the number of learned parameters across all classifiers and regressors in the pipelines.  Additionally, currently transformers add 1 point and selectors add 0 points (since they don't affect the complexity of the \"final\" predictive pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>est</code> <p>The estimator or pipeline to compute the complexity for</p> required <code>X</code> <p>The input samples (unused)</p> <code>None</code> <code>y</code> <p>The target values (unused)</p> <code>None</code> Source code in <code>tpot/objectives/complexity.py</code> <pre><code>def complexity_scorer(est, X=None, y=None):\n    \"\"\"\n    Estimates the number of learned parameters across all classifiers and regressors in the pipelines. \n    Additionally, currently transformers add 1 point and selectors add 0 points (since they don't affect the complexity of the \"final\" predictive pipeline.\n\n    Parameters\n    ----------\n    est: sklearn.base.BaseEstimator\n        The estimator or pipeline to compute the complexity for\n    X: array-like\n        The input samples (unused)\n    y: array-like\n        The target values (unused)\n\n    \"\"\"\n    return calculate_model_complexity(est)\n</code></pre>"},{"location":"documentation/tpot/objectives/number_of_leaves/","title":"Number of leaves","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/objectives/number_of_leaves/#tpot.objectives.number_of_leaves.number_of_leaves_objective","title":"<code>number_of_leaves_objective(est)</code>","text":"<p>Calculates the number of leaves (input nodes) in a GraphPipeline</p> <p>Parameters:</p> Name Type Description Default <code>est</code> <p>The pipeline to compute the number of leaves for</p> required Source code in <code>tpot/objectives/number_of_leaves.py</code> <pre><code>def number_of_leaves_objective(est):\n    \"\"\"\n    Calculates the number of leaves (input nodes) in a GraphPipeline\n\n    Parameters\n    ----------\n    est: GraphPipeline\n        The pipeline to compute the number of leaves for\n    \"\"\"\n    return len([v for v, d in est.graph.out_degree() if d == 0])\n</code></pre>"},{"location":"documentation/tpot/objectives/number_of_nodes/","title":"Number of nodes","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/objectives/number_of_nodes/#tpot.objectives.number_of_nodes.number_of_nodes_objective","title":"<code>number_of_nodes_objective(est)</code>","text":"<p>Calculates the number of leaves (input nodes) in an sklearn pipeline</p> <p>Parameters:</p> Name Type Description Default <code>est</code> <p>The pipeline to compute the number of nodes from.</p> required Source code in <code>tpot/objectives/number_of_nodes.py</code> <pre><code>def number_of_nodes_objective(est):\n    \"\"\"\n    Calculates the number of leaves (input nodes) in an sklearn pipeline\n\n    Parameters\n    ----------\n    est: GraphPipeline | Pipeline | FeatureUnion | BaseEstimator\n        The pipeline to compute the number of nodes from.\n    \"\"\"\n\n    if isinstance(est, GraphPipeline):\n        return sum(number_of_nodes_objective(est.graph.nodes[node][\"instance\"]) for node in est.graph.nodes)\n    if isinstance(est, Pipeline):\n        return sum(number_of_nodes_objective(estimator) for _,estimator in est.steps)\n    if isinstance(est, sklearn.pipeline.FeatureUnion):\n        return sum(number_of_nodes_objective(estimator) for _,estimator in est.transformer_list)\n\n    return 1\n</code></pre>"},{"location":"documentation/tpot/old_config_utils/old_config_utils/","title":"Old config utils","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/old_config_utils/old_config_utils/#tpot.old_config_utils.old_config_utils.convert_config_dict_to_choicepipeline","title":"<code>convert_config_dict_to_choicepipeline(config_dict)</code>","text":"<p>Takes in a TPOT config dictionary and returns a ChoicePipeline search space that represents the config_dict. This space will sample from all included modules in the config_dict.</p> <p>Parameters:</p> Name Type Description Default <code>config_dict</code> <code>dict</code> <p>The dictionary representation of the TPOT config.</p> required <p>Returns:</p> Type Description <code>ChoicePipeline</code> <p>A ChoicePipeline search space that represents the config_dict.</p> Source code in <code>tpot/old_config_utils/old_config_utils.py</code> <pre><code>def convert_config_dict_to_choicepipeline(config_dict):\n    \"\"\"\n    Takes in a TPOT config dictionary and returns a ChoicePipeline search space that represents the config_dict.\n    This space will sample from all included modules in the config_dict.\n\n    Parameters\n    ----------\n    config_dict : dict\n        The dictionary representation of the TPOT config.\n\n    Returns\n    -------\n    ChoicePipeline\n        A ChoicePipeline search space that represents the config_dict.\n    \"\"\"\n    search_spaces = []\n    for key, value in config_dict.items():\n        search_spaces.append(get_node_space(key, value))\n    return ChoicePipeline(search_spaces)\n</code></pre>"},{"location":"documentation/tpot/old_config_utils/old_config_utils/#tpot.old_config_utils.old_config_utils.convert_config_dict_to_graphpipeline","title":"<code>convert_config_dict_to_graphpipeline(config_dict)</code>","text":"<p>Takes in a TPOT config dictionary and returns a GraphSearchPipeline search space that represents the config_dict. This space will sample from all included modules in the config_dict. It will also identify classifiers/regressors to set the search space for the root node.</p> <p>Note doesn't convert estimators so they passthrough inputs like in TPOT1</p> <p>Parameters:</p> Name Type Description Default <code>config_dict</code> <code>dict</code> <p>The dictionary representation of the TPOT config.</p> required <p>Returns:</p> Type Description <code>GraphSearchPipeline</code> <p>A GraphSearchPipeline search space that represents the config_dict.</p> Source code in <code>tpot/old_config_utils/old_config_utils.py</code> <pre><code>def convert_config_dict_to_graphpipeline(config_dict):\n    \"\"\"\n    Takes in a TPOT config dictionary and returns a GraphSearchPipeline search space that represents the config_dict.\n    This space will sample from all included modules in the config_dict. It will also identify classifiers/regressors to set the search space for the root node.\n\n    Note doesn't convert estimators so they passthrough inputs like in TPOT1\n    Parameters\n    ----------\n    config_dict : dict\n        The dictionary representation of the TPOT config.\n\n    Returns\n    -------\n    GraphSearchPipeline\n        A GraphSearchPipeline search space that represents the config_dict.\n    \"\"\"\n    root_search_spaces = []\n    inner_search_spaces = []\n\n    for key, value in config_dict.items():\n        #if root\n        if issubclass(load_get_module_from_string(key), sklearn.base.ClassifierMixin) or issubclass(load_get_module_from_string(key), sklearn.base.RegressorMixin):\n            root_search_spaces.append(get_node_space(key, value))\n        else:\n            inner_search_spaces.append(get_node_space(key, value))\n\n    if len(root_search_spaces) == 0:\n        Warning(\"No classifiers or regressors found, allowing any estimator to be the root node\")\n        root_search_spaces = inner_search_spaces\n\n    #merge inner and root search spaces\n\n    inner_space = np.concatenate([root_search_spaces,inner_search_spaces])\n\n    root_space = ChoicePipeline(root_search_spaces)\n    inner_space = ChoicePipeline(inner_search_spaces)\n\n    final_space = GraphSearchPipeline(root_search_space=root_space, inner_search_space=inner_space)\n    return final_space\n</code></pre>"},{"location":"documentation/tpot/old_config_utils/old_config_utils/#tpot.old_config_utils.old_config_utils.convert_config_dict_to_linearpipeline","title":"<code>convert_config_dict_to_linearpipeline(config_dict)</code>","text":"<p>Takes in a TPOT config dictionary and returns a GraphSearchPipeline search space that represents the config_dict. This space will sample from all included modules in the config_dict. It will also identify classifiers/regressors to set the search space for the root node.</p> <p>Note doesn't convert estimators so they passthrough inputs like in TPOT1</p> <p>Parameters:</p> Name Type Description Default <code>config_dict</code> <code>dict</code> <p>The dictionary representation of the TPOT config.</p> required <p>Returns:</p> Type Description <code>GraphSearchPipeline</code> <p>A GraphSearchPipeline search space that represents the config_dict.</p> Source code in <code>tpot/old_config_utils/old_config_utils.py</code> <pre><code>def convert_config_dict_to_linearpipeline(config_dict):\n    \"\"\"\n    Takes in a TPOT config dictionary and returns a GraphSearchPipeline search space that represents the config_dict.\n    This space will sample from all included modules in the config_dict. It will also identify classifiers/regressors to set the search space for the root node.\n\n    Note doesn't convert estimators so they passthrough inputs like in TPOT1\n    Parameters\n    ----------\n    config_dict : dict\n        The dictionary representation of the TPOT config.\n\n    Returns\n    -------\n    GraphSearchPipeline\n        A GraphSearchPipeline search space that represents the config_dict.\n    \"\"\"\n    root_search_spaces = []\n    inner_search_spaces = []\n\n    for key, value in config_dict.items():\n        #if root\n        if issubclass(load_get_module_from_string(key), sklearn.base.ClassifierMixin) or issubclass(load_get_module_from_string(key), sklearn.base.RegressorMixin):\n            root_search_spaces.append(get_node_space(key, value))\n        else:\n            inner_search_spaces.append(get_node_space(key, value))\n\n    if len(root_search_spaces) == 0:\n        Warning(\"No classifiers or regressors found, allowing any estimator to be the root node\")\n        root_search_spaces = inner_search_spaces\n\n    #merge inner and root search spaces\n\n    inner_space = np.concatenate([root_search_spaces,inner_search_spaces])\n\n    root_space = ChoicePipeline(root_search_spaces)\n    inner_space = ChoicePipeline(inner_search_spaces)\n\n    final_space = SequentialPipeline([\n        DynamicLinearPipeline(inner_space, 10),\n        root_space\n    ])\n    return final_space\n</code></pre>"},{"location":"documentation/tpot/old_config_utils/old_config_utils/#tpot.old_config_utils.old_config_utils.convert_config_dict_to_list","title":"<code>convert_config_dict_to_list(config_dict)</code>","text":"<p>Takes in a TPOT config dictionary and returns a list of search spaces (EstimatorNode, WrapperPipeline)</p> <p>Parameters:</p> Name Type Description Default <code>config_dict</code> <code>dict</code> <p>The dictionary representation of the TPOT config.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of search spaces (EstimatorNode, WrapperPipeline) that represent the config_dict.</p> Source code in <code>tpot/old_config_utils/old_config_utils.py</code> <pre><code>def convert_config_dict_to_list(config_dict):\n    \"\"\"\n    Takes in a TPOT config dictionary and returns a list of search spaces (EstimatorNode, WrapperPipeline)\n\n    Parameters\n    ----------\n    config_dict : dict\n        The dictionary representation of the TPOT config.\n\n    Returns\n    -------\n    list\n        A list of search spaces (EstimatorNode, WrapperPipeline) that represent the config_dict.\n    \"\"\"\n    search_spaces = []\n    for key, value in config_dict.items():\n        search_spaces.append(get_node_space(key, value))\n    return search_spaces\n</code></pre>"},{"location":"documentation/tpot/old_config_utils/old_config_utils/#tpot.old_config_utils.old_config_utils.get_node_space","title":"<code>get_node_space(module_string, params)</code>","text":"<p>Create the search space for a single node in the TPOT config.</p> <p>Parameters:</p> Name Type Description Default <code>module_string</code> <code>str</code> <p>The string representation of the module and class to load. E.g. 'sklearn.ensemble.RandomForestClassifier'</p> required <code>params</code> <code>dict</code> <p>The dictionary representation of the hyperparameter search space for the module_string.</p> required <p>Returns:</p> Type Description <code>EstimatorNode or WrapperPipeline</code> Source code in <code>tpot/old_config_utils/old_config_utils.py</code> <pre><code>def get_node_space(module_string, params):\n    \"\"\"\n    Create the search space for a single node in the TPOT config.\n\n    Parameters\n    ----------\n    module_string : str\n        The string representation of the module and class to load. E.g. 'sklearn.ensemble.RandomForestClassifier'\n    params : dict\n        The dictionary representation of the hyperparameter search space for the module_string.\n\n    Returns\n    -------\n    EstimatorNode or WrapperPipeline\n    \"\"\"\n    method = load_get_module_from_string(module_string)\n    config_space = ConfigurationSpace()\n    sub_space = None\n    sub_space_name = None\n\n    function_params_conversion_dict = {}\n\n    if params is None:\n        return EstimatorNode(method=method, space=config_space)\n\n    for param_name, param in params.items():\n        if param is None:\n            config_space.add(Categorical(param_name, [None]))\n\n        if isinstance(param, range):\n            param = list(param)\n\n        if isinstance(param, list) or isinstance(param, np.ndarray):\n            if len(param) == 1:\n                p = param[0]\n                config_space.add(ConfigSpace.hyperparameters.Constant(param_name, p))\n            else:\n                config_space.add(Categorical(param_name, param))\n            # if all(isinstance(i, int) for i in param):\n            #     config_space.add_hyperparameter(Integer(param_name, (min(param), max(param))))\n            # elif all(isinstance(i, float) for i in param):\n            #     config_space.add_hyperparameter(Float(param_name, (min(param), max(param))))\n            # else:\n            #     config_space.add_hyperparameter(Categorical(param_name, param))\n        elif isinstance(param, dict): #TPOT1 config dicts have dictionaries for values of hyperparameters that are either a function or an estimator\n            if len(param) &gt; 1:\n                    raise ValueError(f\"Multiple items in dictionary entry for {param_name}\")\n\n            key = list(param.keys())[0]\n\n            innermethod = load_get_module_from_string(key)\n\n            if inspect.isclass(innermethod) and issubclass(innermethod, sklearn.base.BaseEstimator): #is an estimator\n                if sub_space is None:\n                    sub_space_name = param_name\n                    sub_space = get_node_space(key, param[key])   \n                else:\n                    raise ValueError(\"Only multiple hyperparameters are estimators. Only one parameter \")\n\n            else: #assume the key is a function and ignore the value\n                function_params_conversion_dict[param_name] = innermethod\n\n        else:\n            # config_space.add_hyperparameter(Categorical(param_name, param))\n            config_space.add(ConfigSpace.hyperparameters.Constant(param_name, param))\n\n    parser=None\n    if len(function_params_conversion_dict) &gt; 0:\n        parser = partial(hyperparameter_parser, function_params_conversion_dict)\n\n\n    if sub_space is None:\n\n        if parser is not None:\n            return EstimatorNode(method=method, space=config_space, hyperparameter_parser=parser)\n        else:\n            return EstimatorNode(method=method, space=config_space)\n\n\n    else:\n        if parser is not None:\n            return WrapperPipeline(method=method, space=config_space, estimator_search_space=sub_space, wrapped_param_name=sub_space_name, hyperparameter_parser=parser)\n        else:\n            return WrapperPipeline(method=method, space=config_space, estimator_search_space=sub_space, wrapped_param_name=sub_space_name)\n</code></pre>"},{"location":"documentation/tpot/old_config_utils/old_config_utils/#tpot.old_config_utils.old_config_utils.load_get_module_from_string","title":"<code>load_get_module_from_string(module_string)</code>","text":"<p>Takes a string in the form of 'module.submodule.class' and returns the class.</p> <p>Parameters:</p> Name Type Description Default <code>module_string</code> <code>str</code> <p>The string representation of the module and class to load.</p> required <p>Returns:</p> Type Description <code>class</code> <p>The class that was loaded from the module string.</p> Source code in <code>tpot/old_config_utils/old_config_utils.py</code> <pre><code>def load_get_module_from_string(module_string):\n    \"\"\"\n    Takes a string in the form of 'module.submodule.class' and returns the class.\n\n    Parameters\n    ----------\n    module_string : str\n        The string representation of the module and class to load.\n\n    Returns\n    -------\n    class\n        The class that was loaded from the module string.\n    \"\"\"\n    module_name, class_name = module_string.rsplit('.', 1)\n    module = __import__(module_name, fromlist=[class_name])\n    return getattr(module, class_name)\n</code></pre>"},{"location":"documentation/tpot/search_spaces/base/","title":"Base","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/search_spaces/base/#tpot.search_spaces.base.SklearnIndividual","title":"<code>SklearnIndividual</code>","text":"<p>               Bases: <code>BaseIndividual</code></p> Source code in <code>tpot/search_spaces/base.py</code> <pre><code>class SklearnIndividual(tpot.BaseIndividual):\n\n    def __init_subclass__(cls):\n        cls.crossover = cls.validate_same_type(cls.crossover)\n\n\n    def __init__(self,) -&gt; None:\n        super().__init__()\n\n    def mutate(self, rng=None):\n        return\n\n    def crossover(self, other, rng=None, **kwargs):\n        return \n\n    @final\n    def validate_same_type(func):\n\n        def wrapper(self, other, rng=None, **kwargs):\n            if not isinstance(other, type(self)):\n                return False\n            return func(self, other, rng=rng, **kwargs)\n\n        return wrapper\n\n    def export_pipeline(self, **kwargs) -&gt; BaseEstimator:\n        return\n\n    def unique_id(self):\n        \"\"\"\n        Returns a unique identifier for the individual. Used for preventing duplicate individuals from being evaluated.\n        \"\"\"\n        return self\n\n    #TODO currently TPOT population class manually uses the unique_id to generate the index for the population data frame.\n    #alternatively, the index could be the individual itself, with the __eq__ and __hash__ methods implemented.\n\n    # Though this breaks the graphpipeline. When a mutation is called, it changes the __eq__ and __hash__ outputs.\n    # Since networkx uses the hash and eq to determine if a node is already in the graph, this causes the graph thing that \n    # This is a new node not in the graph. But this could be changed if when the graphpipeline mutates nodes, \n    # it \"replaces\" the existing node with the mutated node. This would require a change in the graphpipeline class.\n\n    # def __eq__(self, other):\n    #     return self.unique_id() == other.unique_id()\n\n    # def __hash__(self):\n    #     return hash(self.unique_id())\n\n    #number of components in the pipeline\n    def get_size(self):\n        return 1\n\n    @final\n    def export_flattened_graphpipeline(self, **graphpipeline_kwargs) -&gt; tpot.GraphPipeline:\n        return flatten_to_graphpipeline(self.export_pipeline(), **graphpipeline_kwargs)\n</code></pre>"},{"location":"documentation/tpot/search_spaces/base/#tpot.search_spaces.base.SklearnIndividual.unique_id","title":"<code>unique_id()</code>","text":"<p>Returns a unique identifier for the individual. Used for preventing duplicate individuals from being evaluated.</p> Source code in <code>tpot/search_spaces/base.py</code> <pre><code>def unique_id(self):\n    \"\"\"\n    Returns a unique identifier for the individual. Used for preventing duplicate individuals from being evaluated.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"documentation/tpot/search_spaces/tuple_index/","title":"Tuple index","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/search_spaces/tuple_index/#tpot.search_spaces.tuple_index.TupleIndex","title":"<code>TupleIndex</code>","text":"<p>TPOT uses tuples to create a unique id for some pipeline search spaces. However, tuples sometimes don't interact correctly with pandas indexes. This class is a wrapper around a tuple that allows it to be used as a key in a dictionary, without it being an itereable.</p> <p>An alternative could be to make unique id return a string, but this would not work with graphpipelines, which require a special object. This class allows linear pipelines to contain graph pipelines while still being able to be used as a key in a dictionary.</p> Source code in <code>tpot/search_spaces/tuple_index.py</code> <pre><code>class TupleIndex():\n    \"\"\"\n    TPOT uses tuples to create a unique id for some pipeline search spaces. However, tuples sometimes don't interact correctly with pandas indexes.\n    This class is a wrapper around a tuple that allows it to be used as a key in a dictionary, without it being an itereable.\n\n    An alternative could be to make unique id return a string, but this would not work with graphpipelines, which require a special object.\n    This class allows linear pipelines to contain graph pipelines while still being able to be used as a key in a dictionary.\n\n    \"\"\"\n    def __init__(self, tup):\n        self.tup = tup\n\n    def __eq__(self,other) -&gt; bool:\n        return self.tup == other\n\n    def __hash__(self) -&gt; int:\n        return self.tup.__hash__()\n\n    def __str__(self) -&gt; str:\n        return self.tup.__str__()\n\n    def __repr__(self) -&gt; str:\n        return self.tup.__repr__()\n</code></pre>"},{"location":"documentation/tpot/search_spaces/nodes/estimator_node/","title":"Estimator node","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/search_spaces/nodes/estimator_node/#tpot.search_spaces.nodes.estimator_node.EstimatorNodeIndividual","title":"<code>EstimatorNodeIndividual</code>","text":"<p>               Bases: <code>SklearnIndividual</code></p> <p>Note that ConfigurationSpace does not support None as a parameter. Instead, use the special string \"\". TPOT will automatically replace instances of this string with the Python None.  <p>Parameters:</p> Name Type Description Default <code>method</code> <code>type</code> <p>The class of the estimator to be used</p> required <code>space</code> <code>ConfigurationSpace | dict</code> <p>The hyperparameter space to be used. If a dict is passed, hyperparameters are fixed and not learned.</p> required Source code in <code>tpot/search_spaces/nodes/estimator_node.py</code> <pre><code>class EstimatorNodeIndividual(SklearnIndividual):\n    \"\"\"\n    Note that ConfigurationSpace does not support None as a parameter. Instead, use the special string \"&lt;NONE&gt;\". TPOT will automatically replace instances of this string with the Python None. \n\n    Parameters\n    ----------\n    method : type\n        The class of the estimator to be used\n\n    space : ConfigurationSpace|dict\n        The hyperparameter space to be used. If a dict is passed, hyperparameters are fixed and not learned.\n\n    \"\"\"\n    def __init__(self, method: type, \n                        space: ConfigurationSpace|dict, #TODO If a dict is passed, hyperparameters are fixed and not learned. Is this confusing? Should we make a second node type?\n                        hyperparameter_parser: callable = None,\n                        rng=None) -&gt; None:\n        super().__init__()\n        self.method = method\n        self.space = space\n\n        if hyperparameter_parser is None:\n            self.hyperparameter_parser = default_hyperparameter_parser\n        else:\n            self.hyperparameter_parser = hyperparameter_parser\n\n        if isinstance(space, dict):\n            self.hyperparameters = space\n        else:\n            rng = np.random.default_rng(rng)\n            self.space.seed(rng.integers(0, 2**32))\n            self.hyperparameters = dict(self.space.sample_configuration())\n\n    def mutate(self, rng=None):\n        if isinstance(self.space, dict): \n            return False\n\n        rng = np.random.default_rng(rng)\n        self.space.seed(rng.integers(0, 2**32))\n        self.hyperparameters = dict(self.space.sample_configuration())\n        return True\n\n    def crossover(self, other, rng=None):\n        if isinstance(self.space, dict):\n            return False\n\n        rng = np.random.default_rng(rng)\n        if self.method != other.method:\n            return False\n\n        #loop through hyperparameters, randomly swap items in self.hyperparameters with items in other.hyperparameters\n        for hyperparameter in self.space:\n            if rng.choice([True, False]):\n                if hyperparameter in other.hyperparameters:\n                    self.hyperparameters[hyperparameter] = other.hyperparameters[hyperparameter]\n\n        return True\n\n\n\n    @final #this method should not be overridden, instead override hyperparameter_parser\n    def export_pipeline(self, **kwargs):\n        return self.method(**self.hyperparameter_parser(self.hyperparameters))\n\n    def unique_id(self):\n        #return a dictionary of the method and the hyperparameters\n        method_str = self.method.__name__\n        params = list(self.hyperparameters.keys())\n        params = sorted(params)\n\n        id_str = f\"{method_str}({', '.join([f'{param}={self.hyperparameters[param]}' for param in params])})\"\n\n        return id_str\n</code></pre>"},{"location":"documentation/tpot/search_spaces/nodes/estimator_node_gradual/","title":"Estimator node gradual","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/search_spaces/nodes/estimator_node_gradual/#tpot.search_spaces.nodes.estimator_node_gradual.EstimatorNodeIndividual_gradual","title":"<code>EstimatorNodeIndividual_gradual</code>","text":"<p>               Bases: <code>SklearnIndividual</code></p> <p>Note that ConfigurationSpace does not support None as a parameter. Instead, use the special string \"\". TPOT will automatically replace instances of this string with the Python None.  <p>Parameters:</p> Name Type Description Default <code>method</code> <code>type</code> <p>The class of the estimator to be used</p> required <code>space</code> <code>ConfigurationSpace | dict</code> <p>The hyperparameter space to be used. If a dict is passed, hyperparameters are fixed and not learned.</p> required Source code in <code>tpot/search_spaces/nodes/estimator_node_gradual.py</code> <pre><code>class EstimatorNodeIndividual_gradual(SklearnIndividual):\n    \"\"\"\n    Note that ConfigurationSpace does not support None as a parameter. Instead, use the special string \"&lt;NONE&gt;\". TPOT will automatically replace instances of this string with the Python None. \n\n    Parameters\n    ----------\n    method : type\n        The class of the estimator to be used\n\n    space : ConfigurationSpace|dict\n        The hyperparameter space to be used. If a dict is passed, hyperparameters are fixed and not learned.\n\n    \"\"\"\n    def __init__(self, method: type, \n                        space: ConfigurationSpace|dict, #TODO If a dict is passed, hyperparameters are fixed and not learned. Is this confusing? Should we make a second node type?\n                        hyperparameter_parser: callable = None,\n                        rng=None) -&gt; None:\n        super().__init__()\n        self.method = method\n        self.space = space\n\n        if hyperparameter_parser is None:\n            self.hyperparameter_parser = default_hyperparameter_parser\n        else:\n            self.hyperparameter_parser = hyperparameter_parser\n\n        if isinstance(space, dict):\n            self.hyperparameters = space\n        else:\n            rng = np.random.default_rng(rng)\n            self.space.seed(rng.integers(0, 2**32))\n            self.hyperparameters = dict(self.space.sample_configuration())\n\n    def mutate(self, rng=None):\n        if isinstance(self.space, dict): \n            return False\n        self.hyperparameters = gradual_hyperparameter_update(params=self.hyperparameters, configspace=self.space, rng=rng)\n        return True\n\n    def crossover(self, other, rng=None):\n        if isinstance(self.space, dict):\n            return False\n\n        rng = np.random.default_rng(rng)\n        if self.method != other.method:\n            return False\n\n        #loop through hyperparameters, randomly swap items in self.hyperparameters with items in other.hyperparameters\n        for hyperparameter in self.space:\n            if rng.choice([True, False]):\n                if hyperparameter in other.hyperparameters:\n                    self.hyperparameters[hyperparameter] = other.hyperparameters[hyperparameter]\n\n        return True\n\n\n    @final #this method should not be overridden, instead override hyperparameter_parser\n    def export_pipeline(self, **kwargs):\n        return self.method(**self.hyperparameter_parser(self.hyperparameters))\n\n    def unique_id(self):\n        #return a dictionary of the method and the hyperparameters\n        method_str = self.method.__name__\n        params = list(self.hyperparameters.keys())\n        params = sorted(params)\n\n        id_str = f\"{method_str}({', '.join([f'{param}={self.hyperparameters[param]}' for param in params])})\"\n\n        return id_str\n</code></pre>"},{"location":"documentation/tpot/search_spaces/nodes/fss_node/","title":"Fss node","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/search_spaces/nodes/fss_node/#tpot.search_spaces.nodes.fss_node.FSSIndividual","title":"<code>FSSIndividual</code>","text":"<p>               Bases: <code>SklearnIndividual</code></p> Source code in <code>tpot/search_spaces/nodes/fss_node.py</code> <pre><code>class FSSIndividual(SklearnIndividual):\n    def __init__(   self,\n                    subsets,\n                    rng=None,\n                ):\n\n        \"\"\"\n        An individual for representing a specific FeatureSetSelector. \n        The FeatureSetSelector selects a feature list of list of predefined feature subsets.\n\n        This instance will select one set initially. Mutation and crossover can swap the selected subset with another.\n\n        Parameters\n        ----------\n        subsets : str or list, default=None\n            Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries. \n            Features are defined by column names if using a Pandas data frame, or ints corresponding to indexes if using numpy arrays.\n            - str : If a string, it is assumed to be a path to a csv file with the subsets. \n                The first column is assumed to be the name of the subset and the remaining columns are the features in the subset.\n            - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets (i.e a list of lists).\n            - dict : A dictionary where keys are the names of the subsets and the values are the list of features.\n            - int : If an int, it is assumed to be the number of subsets to generate. Each subset will contain one feature.\n            - None : If None, each column will be treated as a subset. One column will be selected per subset.\n        rng : int, np.random.Generator, optional\n            The random number generator. The default is None.\n            Only used to select the first subset.\n\n        Returns\n        -------\n        None    \n        \"\"\"\n\n        subsets = subsets\n        rng = np.random.default_rng(rng)\n\n        if isinstance(subsets, str):\n            df = pd.read_csv(subsets,header=None,index_col=0)\n            df['features'] = df.apply(lambda x: list([x[c] for c in df.columns]),axis=1)\n            self.subset_dict = {}\n            for row in df.index:\n                self.subset_dict[row] = df.loc[row]['features']\n        elif isinstance(subsets, dict):\n            self.subset_dict = subsets\n        elif isinstance(subsets, list) or isinstance(subsets, np.ndarray):\n            self.subset_dict = {str(i):subsets[i] for i in range(len(subsets))}\n        elif isinstance(subsets, int):\n            self.subset_dict = {\"{0}\".format(i):i for i in range(subsets)}\n        else:\n            raise ValueError(\"Subsets must be a string, dictionary, list, int, or numpy array\")\n\n        self.names_list = list(self.subset_dict.keys())\n\n\n        self.selected_subset_name = rng.choice(self.names_list)\n        self.sel_subset = self.subset_dict[self.selected_subset_name]\n\n\n    def mutate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        #get list of names not including the current one\n        names = [name for name in self.names_list if name != self.selected_subset_name]\n        self.selected_subset_name = rng.choice(names)\n        self.sel_subset = self.subset_dict[self.selected_subset_name]\n\n\n    def crossover(self, other, rng=None):\n        self.selected_subset_name = other.selected_subset_name\n        self.sel_subset = other.sel_subset\n\n    def export_pipeline(self, **kwargs):\n        return FeatureSetSelector(sel_subset=self.sel_subset, name=self.selected_subset_name)\n\n\n    def unique_id(self):\n        id_str = \"FeatureSetSelector({0})\".format(self.selected_subset_name)\n        return id_str\n</code></pre>"},{"location":"documentation/tpot/search_spaces/nodes/fss_node/#tpot.search_spaces.nodes.fss_node.FSSIndividual.__init__","title":"<code>__init__(subsets, rng=None)</code>","text":"<p>An individual for representing a specific FeatureSetSelector.  The FeatureSetSelector selects a feature list of list of predefined feature subsets.</p> <p>This instance will select one set initially. Mutation and crossover can swap the selected subset with another.</p> <p>Parameters:</p> Name Type Description Default <code>subsets</code> <code>str or list</code> <p>Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries.  Features are defined by column names if using a Pandas data frame, or ints corresponding to indexes if using numpy arrays. - str : If a string, it is assumed to be a path to a csv file with the subsets.      The first column is assumed to be the name of the subset and the remaining columns are the features in the subset. - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets (i.e a list of lists). - dict : A dictionary where keys are the names of the subsets and the values are the list of features. - int : If an int, it is assumed to be the number of subsets to generate. Each subset will contain one feature. - None : If None, each column will be treated as a subset. One column will be selected per subset.</p> <code>None</code> <code>rng</code> <code>(int, Generator)</code> <p>The random number generator. The default is None. Only used to select the first subset.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>tpot/search_spaces/nodes/fss_node.py</code> <pre><code>def __init__(   self,\n                subsets,\n                rng=None,\n            ):\n\n    \"\"\"\n    An individual for representing a specific FeatureSetSelector. \n    The FeatureSetSelector selects a feature list of list of predefined feature subsets.\n\n    This instance will select one set initially. Mutation and crossover can swap the selected subset with another.\n\n    Parameters\n    ----------\n    subsets : str or list, default=None\n        Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries. \n        Features are defined by column names if using a Pandas data frame, or ints corresponding to indexes if using numpy arrays.\n        - str : If a string, it is assumed to be a path to a csv file with the subsets. \n            The first column is assumed to be the name of the subset and the remaining columns are the features in the subset.\n        - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets (i.e a list of lists).\n        - dict : A dictionary where keys are the names of the subsets and the values are the list of features.\n        - int : If an int, it is assumed to be the number of subsets to generate. Each subset will contain one feature.\n        - None : If None, each column will be treated as a subset. One column will be selected per subset.\n    rng : int, np.random.Generator, optional\n        The random number generator. The default is None.\n        Only used to select the first subset.\n\n    Returns\n    -------\n    None    \n    \"\"\"\n\n    subsets = subsets\n    rng = np.random.default_rng(rng)\n\n    if isinstance(subsets, str):\n        df = pd.read_csv(subsets,header=None,index_col=0)\n        df['features'] = df.apply(lambda x: list([x[c] for c in df.columns]),axis=1)\n        self.subset_dict = {}\n        for row in df.index:\n            self.subset_dict[row] = df.loc[row]['features']\n    elif isinstance(subsets, dict):\n        self.subset_dict = subsets\n    elif isinstance(subsets, list) or isinstance(subsets, np.ndarray):\n        self.subset_dict = {str(i):subsets[i] for i in range(len(subsets))}\n    elif isinstance(subsets, int):\n        self.subset_dict = {\"{0}\".format(i):i for i in range(subsets)}\n    else:\n        raise ValueError(\"Subsets must be a string, dictionary, list, int, or numpy array\")\n\n    self.names_list = list(self.subset_dict.keys())\n\n\n    self.selected_subset_name = rng.choice(self.names_list)\n    self.sel_subset = self.subset_dict[self.selected_subset_name]\n</code></pre>"},{"location":"documentation/tpot/search_spaces/nodes/fss_node/#tpot.search_spaces.nodes.fss_node.FSSNode","title":"<code>FSSNode</code>","text":"<p>               Bases: <code>SearchSpace</code></p> Source code in <code>tpot/search_spaces/nodes/fss_node.py</code> <pre><code>class FSSNode(SearchSpace):\n    def __init__(self,                     \n                    subsets,\n                ):\n        \"\"\"\n        A search space for a FeatureSetSelector. \n        The FeatureSetSelector selects a feature list of list of predefined feature subsets.\n\n        Parameters\n        ----------\n        subsets : str or list, default=None\n            Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries. \n            Features are defined by column names if using a Pandas data frame, or ints corresponding to indexes if using numpy arrays.\n            - str : If a string, it is assumed to be a path to a csv file with the subsets. \n                The first column is assumed to be the name of the subset and the remaining columns are the features in the subset.\n            - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets (i.e a list of lists).\n            - dict : A dictionary where keys are the names of the subsets and the values are the list of features.\n            - int : If an int, it is assumed to be the number of subsets to generate. Each subset will contain one feature.\n            - None : If None, each column will be treated as a subset. One column will be selected per subset.\n\n        Returns\n        -------\n        None    \n\n        \"\"\"\n\n        self.subsets = subsets\n\n    def generate(self, rng=None) -&gt; SklearnIndividual:\n        return FSSIndividual(   \n            subsets=self.subsets,\n            rng=rng,\n            )\n</code></pre>"},{"location":"documentation/tpot/search_spaces/nodes/fss_node/#tpot.search_spaces.nodes.fss_node.FSSNode.__init__","title":"<code>__init__(subsets)</code>","text":"<p>A search space for a FeatureSetSelector.  The FeatureSetSelector selects a feature list of list of predefined feature subsets.</p> <p>Parameters:</p> Name Type Description Default <code>subsets</code> <code>str or list</code> <p>Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries.  Features are defined by column names if using a Pandas data frame, or ints corresponding to indexes if using numpy arrays. - str : If a string, it is assumed to be a path to a csv file with the subsets.      The first column is assumed to be the name of the subset and the remaining columns are the features in the subset. - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets (i.e a list of lists). - dict : A dictionary where keys are the names of the subsets and the values are the list of features. - int : If an int, it is assumed to be the number of subsets to generate. Each subset will contain one feature. - None : If None, each column will be treated as a subset. One column will be selected per subset.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>tpot/search_spaces/nodes/fss_node.py</code> <pre><code>def __init__(self,                     \n                subsets,\n            ):\n    \"\"\"\n    A search space for a FeatureSetSelector. \n    The FeatureSetSelector selects a feature list of list of predefined feature subsets.\n\n    Parameters\n    ----------\n    subsets : str or list, default=None\n        Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries. \n        Features are defined by column names if using a Pandas data frame, or ints corresponding to indexes if using numpy arrays.\n        - str : If a string, it is assumed to be a path to a csv file with the subsets. \n            The first column is assumed to be the name of the subset and the remaining columns are the features in the subset.\n        - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets (i.e a list of lists).\n        - dict : A dictionary where keys are the names of the subsets and the values are the list of features.\n        - int : If an int, it is assumed to be the number of subsets to generate. Each subset will contain one feature.\n        - None : If None, each column will be treated as a subset. One column will be selected per subset.\n\n    Returns\n    -------\n    None    \n\n    \"\"\"\n\n    self.subsets = subsets\n</code></pre>"},{"location":"documentation/tpot/search_spaces/nodes/genetic_feature_selection/","title":"Genetic feature selection","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/search_spaces/nodes/genetic_feature_selection/#tpot.search_spaces.nodes.genetic_feature_selection.GeneticFeatureSelectorNode","title":"<code>GeneticFeatureSelectorNode</code>","text":"<p>               Bases: <code>SearchSpace</code></p> Source code in <code>tpot/search_spaces/nodes/genetic_feature_selection.py</code> <pre><code>class GeneticFeatureSelectorNode(SearchSpace):\n    def __init__(self,                     \n                    n_features,\n                    start_p=0.2,\n                    mutation_rate = 0.1,\n                    crossover_rate = 0.1,\n                    mutation_rate_rate = 0, # These are still experimental but seem to help. Theory is that it takes slower steps as it gets closer to the optimal solution.\n                    crossover_rate_rate = 0,# Otherwise is mutation_rate is too small, it takes forever, and if its too large, it never converges.\n                    ):\n        \"\"\"\n        A node that generates a GeneticFeatureSelectorIndividual. Uses genetic algorithm to select novel subsets of features.\n\n        Parameters\n        ----------\n        n_features : int\n            Number of features in the dataset.\n        start_p : float\n            Probability of selecting a given feature for the initial subset of features.\n        mutation_rate : float\n            Probability of adding/removing a feature from the subset of features.\n        crossover_rate : float\n            Probability of swapping a feature between two subsets of features.\n        mutation_rate_rate : float\n            Probability of changing the mutation rate. (experimental)\n        crossover_rate_rate : float\n            Probability of changing the crossover rate. (experimental)\n\n        \"\"\"\n\n        self.n_features = n_features\n        self.start_p = start_p\n        self.mutation_rate = mutation_rate\n        self.crossover_rate = crossover_rate\n        self.mutation_rate_rate = mutation_rate_rate\n        self.crossover_rate_rate = crossover_rate_rate\n\n\n    def generate(self, rng=None) -&gt; SklearnIndividual:\n        return GeneticFeatureSelectorIndividual(   mask=self.n_features,\n                                                    start_p=self.start_p,\n                                                    mutation_rate=self.mutation_rate,\n                                                    crossover_rate=self.crossover_rate,\n                                                    mutation_rate_rate=self.mutation_rate_rate,\n                                                    crossover_rate_rate=self.crossover_rate_rate,\n                                                    rng=rng\n                                                )\n</code></pre>"},{"location":"documentation/tpot/search_spaces/nodes/genetic_feature_selection/#tpot.search_spaces.nodes.genetic_feature_selection.GeneticFeatureSelectorNode.__init__","title":"<code>__init__(n_features, start_p=0.2, mutation_rate=0.1, crossover_rate=0.1, mutation_rate_rate=0, crossover_rate_rate=0)</code>","text":"<p>A node that generates a GeneticFeatureSelectorIndividual. Uses genetic algorithm to select novel subsets of features.</p> <p>Parameters:</p> Name Type Description Default <code>n_features</code> <code>int</code> <p>Number of features in the dataset.</p> required <code>start_p</code> <code>float</code> <p>Probability of selecting a given feature for the initial subset of features.</p> <code>0.2</code> <code>mutation_rate</code> <code>float</code> <p>Probability of adding/removing a feature from the subset of features.</p> <code>0.1</code> <code>crossover_rate</code> <code>float</code> <p>Probability of swapping a feature between two subsets of features.</p> <code>0.1</code> <code>mutation_rate_rate</code> <code>float</code> <p>Probability of changing the mutation rate. (experimental)</p> <code>0</code> <code>crossover_rate_rate</code> <code>float</code> <p>Probability of changing the crossover rate. (experimental)</p> <code>0</code> Source code in <code>tpot/search_spaces/nodes/genetic_feature_selection.py</code> <pre><code>def __init__(self,                     \n                n_features,\n                start_p=0.2,\n                mutation_rate = 0.1,\n                crossover_rate = 0.1,\n                mutation_rate_rate = 0, # These are still experimental but seem to help. Theory is that it takes slower steps as it gets closer to the optimal solution.\n                crossover_rate_rate = 0,# Otherwise is mutation_rate is too small, it takes forever, and if its too large, it never converges.\n                ):\n    \"\"\"\n    A node that generates a GeneticFeatureSelectorIndividual. Uses genetic algorithm to select novel subsets of features.\n\n    Parameters\n    ----------\n    n_features : int\n        Number of features in the dataset.\n    start_p : float\n        Probability of selecting a given feature for the initial subset of features.\n    mutation_rate : float\n        Probability of adding/removing a feature from the subset of features.\n    crossover_rate : float\n        Probability of swapping a feature between two subsets of features.\n    mutation_rate_rate : float\n        Probability of changing the mutation rate. (experimental)\n    crossover_rate_rate : float\n        Probability of changing the crossover rate. (experimental)\n\n    \"\"\"\n\n    self.n_features = n_features\n    self.start_p = start_p\n    self.mutation_rate = mutation_rate\n    self.crossover_rate = crossover_rate\n    self.mutation_rate_rate = mutation_rate_rate\n    self.crossover_rate_rate = crossover_rate_rate\n</code></pre>"},{"location":"documentation/tpot/search_spaces/nodes/genetic_feature_selection/#tpot.search_spaces.nodes.genetic_feature_selection.MaskSelector","title":"<code>MaskSelector</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>SelectorMixin</code></p> <p>Select predefined feature subsets.</p> Source code in <code>tpot/search_spaces/nodes/genetic_feature_selection.py</code> <pre><code>class MaskSelector(BaseEstimator, SelectorMixin):\n    \"\"\"Select predefined feature subsets.\"\"\"\n\n    def __init__(self, mask, set_output_transform=None):\n        self.mask = mask\n        self.set_output_transform = set_output_transform\n        if set_output_transform is not None:\n            self.set_output(transform=set_output_transform)\n\n    def fit(self, X, y=None):\n        self.n_features_in_ = X.shape[1]\n        if isinstance(X, pd.DataFrame):\n            self.feature_names_in_ = X.columns\n        #     self.set_output(transform=\"pandas\")\n        self.is_fitted_ = True #so sklearn knows it's fitted\n        return self\n\n    def _get_tags(self):\n        tags = {\"allow_nan\": True, \"requires_y\": False}\n        return tags\n\n    def _get_support_mask(self):\n        return np.array(self.mask)\n\n    def get_feature_names_out(self, input_features=None):\n        return self.feature_names_in_[self.get_support()]\n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/choice/","title":"Choice","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/search_spaces/pipelines/dynamic_linear/","title":"Dynamic linear","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/search_spaces/pipelines/dynamicunion/","title":"Dynamicunion","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/search_spaces/pipelines/dynamicunion/#tpot.search_spaces.pipelines.dynamicunion.DynamicUnionPipeline","title":"<code>DynamicUnionPipeline</code>","text":"<p>               Bases: <code>SearchSpace</code></p> Source code in <code>tpot/search_spaces/pipelines/dynamicunion.py</code> <pre><code>class DynamicUnionPipeline(SearchSpace):\n    def __init__(self, search_space : SearchSpace, max_estimators=None, allow_repeats=False ) -&gt; None:\n        \"\"\"\n        Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.\n        \"\"\"\n\n        self.search_space = search_space\n        self.max_estimators = max_estimators\n        self.allow_repeats = allow_repeats\n\n    def generate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        return DynamicUnionPipelineIndividual(self.search_space, max_estimators=self.max_estimators, allow_repeats=self.allow_repeats, rng=rng)\n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/dynamicunion/#tpot.search_spaces.pipelines.dynamicunion.DynamicUnionPipeline.__init__","title":"<code>__init__(search_space, max_estimators=None, allow_repeats=False)</code>","text":"<p>Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.</p> Source code in <code>tpot/search_spaces/pipelines/dynamicunion.py</code> <pre><code>def __init__(self, search_space : SearchSpace, max_estimators=None, allow_repeats=False ) -&gt; None:\n    \"\"\"\n    Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.\n    \"\"\"\n\n    self.search_space = search_space\n    self.max_estimators = max_estimators\n    self.allow_repeats = allow_repeats\n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/dynamicunion/#tpot.search_spaces.pipelines.dynamicunion.DynamicUnionPipelineIndividual","title":"<code>DynamicUnionPipelineIndividual</code>","text":"<p>               Bases: <code>SklearnIndividual</code></p> <p>Takes in one search space. Will produce a FeatureUnion of up to max_estimators number of steps. The output of the FeatureUnion will the all of the steps concatenated together.</p> Source code in <code>tpot/search_spaces/pipelines/dynamicunion.py</code> <pre><code>class DynamicUnionPipelineIndividual(SklearnIndividual):\n    \"\"\"\n    Takes in one search space.\n    Will produce a FeatureUnion of up to max_estimators number of steps.\n    The output of the FeatureUnion will the all of the steps concatenated together.\n\n    \"\"\"\n\n    def __init__(self, search_space : SearchSpace, max_estimators=None, allow_repeats=False, rng=None) -&gt; None:\n        super().__init__()\n        self.search_space = search_space\n\n        if max_estimators is None:\n            self.max_estimators = np.inf\n        else:\n            self.max_estimators = max_estimators\n\n        self.allow_repeats = allow_repeats\n\n        self.union_dict = {}\n\n        if self.max_estimators == np.inf:\n            init_max = 3\n        else:\n            init_max = self.max_estimators\n\n        rng = np.random.default_rng(rng)\n\n        for _ in range(rng.integers(1, init_max)):\n            self._mutate_add_step(rng)\n\n\n    def mutate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        mutation_funcs = [self._mutate_add_step, self._mutate_remove_step, self._mutate_replace_step, self._mutate_note]\n        rng.shuffle(mutation_funcs)\n        for mutation_func in mutation_funcs:\n            if mutation_func(rng):\n                return True\n\n    def _mutate_add_step(self, rng):\n        rng = np.random.default_rng(rng)\n        max_attempts = 10\n        if len(self.union_dict) &lt; self.max_estimators:\n            for _ in range(max_attempts):\n                new_step = self.search_space.generate(rng)\n                if new_step.unique_id() not in self.union_dict:\n                    self.union_dict[new_step.unique_id()] = new_step\n                    return True\n        return False\n\n    def _mutate_remove_step(self, rng):\n        rng = np.random.default_rng(rng)\n        if len(self.union_dict) &gt; 1:\n            self.union_dict.pop( rng.choice(list(self.union_dict.keys())))  \n            return True\n        return False\n\n    def _mutate_replace_step(self, rng):\n        rng = np.random.default_rng(rng)        \n        changed = self._mutate_remove_step(rng) or self._mutate_add_step(rng)\n        return changed\n\n    #TODO mutate one step or multiple?\n    def _mutate_note(self, rng):\n        rng = np.random.default_rng(rng)\n        changed = False\n        values = list(self.union_dict.values())\n        for step in values:\n            if rng.random() &lt; 0.5:\n                changed = step.mutate(rng) or changed\n\n        self.union_dict = {step.unique_id(): step for step in values}\n\n        return changed\n\n\n    def crossover(self, other, rng=None):\n        rng = np.random.default_rng(rng)\n\n        cx_funcs = [self._crossover_swap_multiple_nodes, self._crossover_node]\n        rng.shuffle(cx_funcs)\n        for cx_func in cx_funcs:\n            if cx_func(other, rng):\n                return True\n\n        return False\n\n\n    def _crossover_swap_multiple_nodes(self, other, rng):\n        rng = np.random.default_rng(rng)\n        self_values = list(self.union_dict.values())\n        other_values = list(other.union_dict.values())\n\n        rng.shuffle(self_values)\n        rng.shuffle(other_values)\n\n        self_idx = rng.integers(0,len(self_values))\n        other_idx = rng.integers(0,len(other_values))\n\n        #Note that this is not one-point-crossover since the sequence doesn't matter. this is just a quick way to swap multiple random items\n        self_values[:self_idx], other_values[:other_idx] = other_values[:other_idx], self_values[:self_idx]\n\n        self.union_dict = {step.unique_id(): step for step in self_values}\n        other.union_dict = {step.unique_id(): step for step in other_values}\n\n        return True\n\n\n    def _crossover_node(self, other, rng):\n        rng = np.random.default_rng(rng)\n\n        changed = False\n        self_values = list(self.union_dict.values())\n        other_values = list(other.union_dict.values())\n\n        rng.shuffle(self_values)\n        rng.shuffle(other_values)\n\n        for self_step, other_step in zip(self_values, other_values):\n            if rng.random() &lt; 0.5:\n                changed = self_step.crossover(other_step, rng) or changed\n\n        self.union_dict = {step.unique_id(): step for step in self_values}\n        other.union_dict = {step.unique_id(): step for step in other_values}\n\n        return changed\n\n    def export_pipeline(self, **kwargs):\n        values = list(self.union_dict.values())\n        return sklearn.pipeline.make_union(*[step.export_pipeline(**kwargs) for step in values])\n\n    def unique_id(self):\n        values = list(self.union_dict.values())\n        l = [step.unique_id() for step in values]\n        # if all items are strings, then sort them\n        if all([isinstance(x, str) for x in l]):\n            l.sort()\n        l = [\"FeatureUnion\"] + l\n        return TupleIndex(frozenset(l))\n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/graph/","title":"Graph","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/search_spaces/pipelines/graph/#tpot.search_spaces.pipelines.graph.GraphKey","title":"<code>GraphKey</code>","text":"<p>A class that can be used as a key for a graph.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>Graph</code> <p>The graph to use as a key. Node Attributes are used for the hash.</p> required <code>matched_label</code> <code>str</code> <p>The node attribute to consider for the hash.</p> <code>'label'</code> Source code in <code>tpot/search_spaces/pipelines/graph.py</code> <pre><code>class GraphKey():\n    '''\n    A class that can be used as a key for a graph.\n\n    Parameters\n    ----------\n    graph : (nx.Graph)\n        The graph to use as a key. Node Attributes are used for the hash.\n    matched_label : (str)\n        The node attribute to consider for the hash.\n    '''\n\n    def __init__(self, graph, matched_label='label') -&gt; None:#['hyperparameters', 'method_class']) -&gt; None:\n\n\n        self.graph = graph\n        self.matched_label = matched_label\n        self.node_match = partial(node_match, matched_labels=[matched_label])\n        self.key = int(nx.weisfeiler_lehman_graph_hash(self.graph, node_attr=self.matched_label),16) #hash(tuple(sorted([val for (node, val) in self.graph.degree()])))\n\n\n    #If hash is different, node is definitely different\n    # https://arxiv.org/pdf/2002.06653.pdf\n    def __hash__(self) -&gt; int:\n\n        return self.key\n\n    #If hash is same, use __eq__ to know if they are actually different\n    def __eq__(self, other):\n        return nx.is_isomorphic(self.graph, other.graph, node_match=self.node_match)\n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/graph/#tpot.search_spaces.pipelines.graph.GraphPipelineIndividual","title":"<code>GraphPipelineIndividual</code>","text":"<p>               Bases: <code>SklearnIndividual</code></p> <p>Defines a search space of pipelines in the shape of a Directed Acyclic Graphs. The search spaces for root, leaf, and inner nodes can be defined separately if desired. Each graph will have a single root serving as the final estimator which is drawn from the <code>root_search_space</code>. If the <code>leaf_search_space</code> is defined, all leaves  in the pipeline will be drawn from that search space. If the <code>leaf_search_space</code> is not defined, all leaves will be drawn from the <code>inner_search_space</code>. Nodes that are not leaves or roots will be drawn from the <code>inner_search_space</code>. If the <code>inner_search_space</code> is not defined, there will be no inner nodes.</p> <p><code>cross_val_predict_cv</code>, <code>method</code>, <code>memory</code>, and <code>use_label_encoder</code> are passed to the GraphPipeline object when the pipeline is exported and not directly used in the search space.</p> <p>Exports to a GraphPipeline object.</p> <p>Parameters:</p> Name Type Description Default <code>root_search_space</code> <code>SearchSpace</code> <p>The search space for the root node of the graph. This node will be the final estimator in the pipeline.</p> required <code>inner_search_space</code> <code>SearchSpace</code> <p>The search space for the inner nodes of the graph. If not defined, there will be no inner nodes.</p> <code>None</code> <code>leaf_search_space</code> <code>SearchSpace</code> <p>The search space for the leaf nodes of the graph. If not defined, the leaf nodes will be drawn from the inner_search_space.</p> <code>None</code> <code>crossover_same_depth</code> <code>bool</code> <p>If True, crossover will only occur between nodes at the same depth in the graph. If False, crossover will occur between nodes at any depth.</p> <code>False</code> <code>cross_val_predict_cv</code> <code>Union[int, Callable]</code> <p>Determines the cross-validation splitting strategy used in inner classifiers or regressors</p> <code>0</code> <code>method</code> <code>str</code> <p>The prediction method to use for the inner classifiers or regressors. If 'auto', it will try to use predict_proba, decision_function, or predict in that order.</p> <code>'auto'</code> <code>memory</code> <p>Used to cache the input and outputs of nodes to prevent refitting or computationally heavy transformations. By default, no caching is performed. If a string is given, it is the path to the caching directory.</p> required <code>use_label_encoder</code> <code>bool</code> <p>If True, the label encoder is used to encode the labels to be 0 to N. If False, the label encoder is not used. Mainly useful for classifiers (XGBoost) that require labels to be ints from 0 to N. Can also be a sklearn.preprocessing.LabelEncoder object. If so, that label encoder is used.</p> <code>False</code> <code>rng</code> <p>Seed for sampling the first graph instance.</p> <code>None</code> Source code in <code>tpot/search_spaces/pipelines/graph.py</code> <pre><code>class GraphPipelineIndividual(SklearnIndividual):\n    \"\"\"\n        Defines a search space of pipelines in the shape of a Directed Acyclic Graphs. The search spaces for root, leaf, and inner nodes can be defined separately if desired.\n        Each graph will have a single root serving as the final estimator which is drawn from the `root_search_space`. If the `leaf_search_space` is defined, all leaves \n        in the pipeline will be drawn from that search space. If the `leaf_search_space` is not defined, all leaves will be drawn from the `inner_search_space`.\n        Nodes that are not leaves or roots will be drawn from the `inner_search_space`. If the `inner_search_space` is not defined, there will be no inner nodes.\n\n        `cross_val_predict_cv`, `method`, `memory`, and `use_label_encoder` are passed to the GraphPipeline object when the pipeline is exported and not directly used in the search space.\n\n        Exports to a GraphPipeline object.\n\n        Parameters\n        ----------\n\n        root_search_space: SearchSpace\n            The search space for the root node of the graph. This node will be the final estimator in the pipeline.\n\n        inner_search_space: SearchSpace, optional\n            The search space for the inner nodes of the graph. If not defined, there will be no inner nodes.\n\n        leaf_search_space: SearchSpace, optional\n            The search space for the leaf nodes of the graph. If not defined, the leaf nodes will be drawn from the inner_search_space.\n\n        crossover_same_depth: bool, optional\n            If True, crossover will only occur between nodes at the same depth in the graph. If False, crossover will occur between nodes at any depth.\n\n        cross_val_predict_cv: int, cross-validation generator or an iterable, optional\n            Determines the cross-validation splitting strategy used in inner classifiers or regressors\n\n        method: str, optional\n            The prediction method to use for the inner classifiers or regressors. If 'auto', it will try to use predict_proba, decision_function, or predict in that order.\n\n        memory: str or object with the joblib.Memory interface, optional\n            Used to cache the input and outputs of nodes to prevent refitting or computationally heavy transformations. By default, no caching is performed. If a string is given, it is the path to the caching directory.\n\n        use_label_encoder: bool, optional\n            If True, the label encoder is used to encode the labels to be 0 to N. If False, the label encoder is not used.\n            Mainly useful for classifiers (XGBoost) that require labels to be ints from 0 to N.\n            Can also be a sklearn.preprocessing.LabelEncoder object. If so, that label encoder is used.\n\n        rng: int, RandomState instance or None, optional\n            Seed for sampling the first graph instance. \n\n        \"\"\"\n\n    def __init__(\n            self,  \n            root_search_space: SearchSpace, \n            leaf_search_space: SearchSpace = None, \n            inner_search_space: SearchSpace = None, \n            max_size: int = np.inf,\n            crossover_same_depth: bool = False,\n            cross_val_predict_cv: Union[int, Callable] = 0, #signature function(estimator, X, y=none)\n            method: str = 'auto',\n            use_label_encoder: bool = False,\n            rng=None):\n\n        super().__init__()\n\n        self.__debug = False\n\n        rng = np.random.default_rng(rng)\n\n        self.root_search_space = root_search_space\n        self.leaf_search_space = leaf_search_space\n        self.inner_search_space = inner_search_space\n        self.max_size = max_size\n        self.crossover_same_depth = crossover_same_depth\n\n        self.cross_val_predict_cv = cross_val_predict_cv\n        self.method = method\n        self.use_label_encoder = use_label_encoder\n\n        self.root = self.root_search_space.generate(rng)\n        self.graph = nx.DiGraph()\n        self.graph.add_node(self.root)\n\n        if self.leaf_search_space is not None:\n            self.leaf = self.leaf_search_space.generate(rng)\n            self.graph.add_node(self.leaf)\n            self.graph.add_edge(self.root, self.leaf)\n\n        if self.inner_search_space is None and self.leaf_search_space is None:\n            self.mutate_methods_list = [self._mutate_node]\n            self.crossover_methods_list = [self._crossover_swap_branch,]#[self._crossover_swap_branch, self._crossover_swap_node, self._crossover_take_branch]  #TODO self._crossover_nodes, \n\n        else:\n            self.mutate_methods_list = [self._mutate_insert_leaf, self._mutate_insert_inner_node, self._mutate_remove_node, self._mutate_node, self._mutate_insert_bypass_node]\n            self.crossover_methods_list = [self._crossover_swap_branch, self._crossover_nodes, self._crossover_take_branch ]#[self._crossover_swap_branch, self._crossover_swap_node, self._crossover_take_branch]  #TODO self._crossover_nodes, \n\n        self.merge_duplicated_nodes_toggle = True\n\n        self.graphkey = None\n\n\n    def mutate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        rng.shuffle(self.mutate_methods_list)\n        for mutate_method in self.mutate_methods_list:\n            if mutate_method(rng=rng):\n\n                if self.merge_duplicated_nodes_toggle:\n                    self._merge_duplicated_nodes()\n\n                if self.__debug:\n                    print(mutate_method)\n\n                    if self.root not in self.graph.nodes:\n                        print('lost root something went wrong with ', mutate_method)\n\n                    if len(self.graph.predecessors(self.root)) &gt; 0:\n                        print('root has parents ', mutate_method)\n\n                    if any([n in nx.ancestors(self.graph,n) for n in self.graph.nodes]):\n                        print('a node is connecting to itself...')\n\n                    if self.__debug:\n                        try:\n                            nx.find_cycle(self.graph)\n                            print('something went wrong with ', mutate_method)\n                        except:\n                            pass\n\n                self.graphkey = None\n\n        return False\n\n\n\n\n    def _mutate_insert_leaf(self, rng=None):\n        rng = np.random.default_rng(rng)\n        if self.max_size &gt; self.graph.number_of_nodes():\n            sorted_nodes_list = list(self.graph.nodes)\n            rng.shuffle(sorted_nodes_list) #TODO: sort by number of children and/or parents? bias model one way or another\n            for node in sorted_nodes_list:\n                #if leafs are protected, check if node is a leaf\n                #if node is a leaf, skip because we don't want to add node on top of node\n                if (self.leaf_search_space is not None #if leafs are protected\n                    and   len(list(self.graph.successors(node))) == 0 #if node is leaf\n                    and  len(list(self.graph.predecessors(node))) &gt; 0 #except if node is root, in which case we want to add a leaf even if it happens to be a leaf too\n                    ):\n\n                    continue\n\n                #If node *is* the root or is not a leaf, add leaf node. (dont want to add leaf on top of leaf)\n                if self.leaf_search_space is not None:\n                    new_node = self.leaf_search_space.generate(rng)\n                else:\n                    new_node = self.inner_search_space.generate(rng)\n\n                self.graph.add_node(new_node)\n                self.graph.add_edge(node, new_node)\n                return True\n\n        return False\n\n    def _mutate_insert_inner_node(self, rng=None):\n        \"\"\"\n        Finds an edge in the graph and inserts a new node between the two nodes. Removes the edge between the two nodes.\n        \"\"\"\n        rng = np.random.default_rng(rng)\n        if self.max_size &gt; self.graph.number_of_nodes():\n            sorted_nodes_list = list(self.graph.nodes)\n            sorted_nodes_list2 = list(self.graph.nodes)\n            rng.shuffle(sorted_nodes_list) #TODO: sort by number of children and/or parents? bias model one way or another\n            rng.shuffle(sorted_nodes_list2)\n            for node in sorted_nodes_list:\n                #loop through children of node\n                for child_node in list(self.graph.successors(node)):\n\n                    if child_node is not node and child_node not in nx.ancestors(self.graph, node):\n                        if self.leaf_search_space is not None:\n                            #If if we are protecting leafs, dont add connection into a leaf\n                            if len(list(nx.descendants(self.graph,node))) ==0 :\n                                continue\n\n                        new_node = self.inner_search_space.generate(rng)\n\n                        self.graph.add_node(new_node)\n                        self.graph.add_edges_from([(node, new_node), (new_node, child_node)])\n                        self.graph.remove_edge(node, child_node)\n                        return True\n\n        return False\n\n\n    def _mutate_remove_node(self, rng=None):\n        '''\n        Removes a randomly chosen node and connects its parents to its children.\n        If the node is the only leaf for an inner node and 'leaf_search_space' is not none, we do not remove it.\n        '''\n        rng = np.random.default_rng(rng)\n        nodes_list = list(self.graph.nodes)\n        nodes_list.remove(self.root)\n        leaves = get_leaves(self.graph)\n\n        while len(nodes_list) &gt; 0:\n            node = rng.choice(nodes_list)\n            nodes_list.remove(node)\n\n            if self.leaf_search_space is not None and len(list(nx.descendants(self.graph,node))) == 0 : #if the node is a leaf\n                if len(leaves) &lt;= 1:\n                    continue #dont remove the last leaf\n                leaf_parents = self.graph.predecessors(node)\n\n                # if any of the parents of the node has one one child, continue\n                if any([len(list(self.graph.successors(lp))) &lt; 2 for lp in leaf_parents]): #dont remove a leaf if it is the only input into another node.\n                    continue\n\n                remove_and_stitch(self.graph, node)\n                remove_nodes_disconnected_from_node(self.graph, self.root)\n                return True\n\n            else:\n                remove_and_stitch(self.graph, node)\n                remove_nodes_disconnected_from_node(self.graph, self.root)\n                return True\n\n        return False\n\n\n\n    def _mutate_node(self, rng=None):\n        '''\n        Mutates the hyperparameters for a randomly chosen node in the graph.\n        '''\n        rng = np.random.default_rng(rng)\n        sorted_nodes_list = list(self.graph.nodes)\n        rng.shuffle(sorted_nodes_list)\n        completed_one = False\n        for node in sorted_nodes_list:\n            if node.mutate(rng):\n                return True\n        return False\n\n    def _mutate_remove_edge(self, rng=None):\n        '''\n        Deletes an edge as long as deleting that edge does not make the graph disconnected.\n        '''\n        rng = np.random.default_rng(rng)\n        sorted_nodes_list = list(self.graph.nodes)\n        rng.shuffle(sorted_nodes_list)\n        for child_node in sorted_nodes_list:\n            parents = list(self.graph.predecessors(child_node))\n            if len(parents) &gt; 1: # if it has more than one parent, you can remove an edge (if this is the only child of a node, it will become a leaf)\n\n                for parent_node in parents:\n                    # if removing the egde will make the parent_node a leaf node, skip\n                    if self.leaf_search_space is not None and len(list(self.graph.successors(parent_node))) &lt; 2:\n                        continue\n\n                    self.graph.remove_edge(parent_node, child_node)\n                    return True\n        return False   \n\n    def _mutate_add_edge(self, rng=None):\n        '''\n        Randomly add an edge from a node to another node that is not an ancestor of the first node.\n        '''\n        rng = np.random.default_rng(rng)\n        sorted_nodes_list = list(self.graph.nodes)\n        rng.shuffle(sorted_nodes_list)\n        for child_node in sorted_nodes_list:\n            for parent_node in sorted_nodes_list:\n                if self.leaf_search_space is not None:\n                    if len(list(self.graph.successors(parent_node))) == 0:\n                        continue\n\n                # skip if\n                # - parent and child are the same node\n                # - edge already exists\n                # - child is an ancestor of parent\n                if  (child_node is not parent_node) and not self.graph.has_edge(parent_node,child_node) and (child_node not in nx.ancestors(self.graph, parent_node)):\n                    self.graph.add_edge(parent_node,child_node)\n                    return True\n\n        return False\n\n    def _mutate_insert_bypass_node(self, rng=None):\n        \"\"\"\n        Pick two nodes (doesn't necessarily need to be connected). Create a new node. connect one node to the new node and the new node to the other node.\n        Does not remove any edges.\n        \"\"\"\n        rng = np.random.default_rng(rng)\n        if self.max_size &gt; self.graph.number_of_nodes():\n            sorted_nodes_list = list(self.graph.nodes)\n            sorted_nodes_list2 = list(self.graph.nodes)\n            rng.shuffle(sorted_nodes_list) #TODO: sort by number of children and/or parents? bias model one way or another\n            rng.shuffle(sorted_nodes_list2)\n            for node in sorted_nodes_list:\n                for child_node in sorted_nodes_list2:\n                    if child_node is not node and child_node not in nx.ancestors(self.graph, node):\n                        if self.leaf_search_space is not None:\n                            #If if we are protecting leafs, dont add connection into a leaf\n                            if len(list(nx.descendants(self.graph,node))) ==0 :\n                                continue\n\n                        new_node = self.inner_search_space.generate(rng)\n\n                        self.graph.add_node(new_node)\n                        self.graph.add_edges_from([(node, new_node), (new_node, child_node)])\n                        return True\n\n        return False\n\n\n    def crossover(self, ind2, rng=None):\n        '''\n        self is the first individual, ind2 is the second individual\n        If crossover_same_depth, it will select graphindividuals at the same recursive depth.\n        Otherwise, it will select graphindividuals randomly from the entire graph and its subgraphs.\n\n        This does not impact graphs without subgraphs. And it does not impacts nodes that are not graphindividuals. Cros\n        '''\n\n        rng = np.random.default_rng(rng)\n\n        rng.shuffle(self.crossover_methods_list)\n\n        finished = False\n\n        for crossover_method in self.crossover_methods_list:\n            if crossover_method(ind2, rng=rng):\n                self._merge_duplicated_nodes()\n                finished = True\n                break\n\n        if self.__debug:\n            try:\n                nx.find_cycle(self.graph)\n                print('something went wrong with ', crossover_method)\n            except:\n                pass\n\n        if finished:\n            self.graphkey = None\n\n        return finished\n\n\n    def _crossover_swap_branch(self, G2, rng=None):\n        '''\n        swaps a branch from parent1 with a branch from parent2. does not modify parent2\n        '''\n        rng = np.random.default_rng(rng)\n\n        if self.crossover_same_depth:\n            pair_gen = select_nodes_same_depth(self.graph, self.root, G2.graph, G2.root, rng=rng)\n        else:\n            pair_gen = select_nodes_randomly(self.graph, G2.graph, rng=rng)\n\n        for node1, node2 in pair_gen:\n            #TODO: if root is in inner_search_space, then do use it?\n            if node1 is self.root or node2 is G2.root: #dont want to add root as inner node\n                continue\n\n            #check if node1 is a leaf and leafs are protected, don't add an input to the leave\n            if self.leaf_search_space is not None: #if we are protecting leaves,\n                node1_is_leaf = len(list(self.graph.successors(node1))) == 0\n                node2_is_leaf = len(list(G2.graph.successors(node2))) == 0\n                #if not ((node1_is_leaf and node1_is_leaf) or (not node1_is_leaf and not node2_is_leaf)): #if node1 is a leaf\n                #if (node1_is_leaf and (not node2_is_leaf)) or ( (not node1_is_leaf) and node2_is_leaf):\n                if not node1_is_leaf:\n                    #only continue if node1 and node2 are both leaves or both not leaves\n                    continue\n\n            temp_graph_1 = self.graph.copy()\n            temp_graph_1.remove_node(node1)\n            remove_nodes_disconnected_from_node(temp_graph_1, self.root)\n\n            #isolating the branch\n            branch2 = G2.graph.copy()\n            n2_descendants = nx.descendants(branch2,node2)\n            for n in list(branch2.nodes):\n                if n not in n2_descendants and n is not node2: #removes all nodes not in the branch\n                    branch2.remove_node(n)\n\n            branch2 = copy.deepcopy(branch2)\n            branch2_root = get_roots(branch2)[0]\n            temp_graph_1.add_edges_from(branch2.edges)\n            for p in list(self.graph.predecessors(node1)):\n                temp_graph_1.add_edge(p,branch2_root)\n\n            if temp_graph_1.number_of_nodes() &gt; self.max_size:\n                continue\n\n            self.graph = temp_graph_1\n\n            return True\n        return False\n\n\n    def _crossover_take_branch(self, G2, rng=None):\n        '''\n        Takes a subgraph from Parent2 and add it to a randomly chosen node in Parent1.\n        '''\n        rng = np.random.default_rng(rng)\n\n        if self.crossover_same_depth:\n            pair_gen = select_nodes_same_depth(self.graph, self.root, G2.graph, G2.root, rng=rng)\n        else:\n            pair_gen = select_nodes_randomly(self.graph, G2.graph, rng=rng)\n\n        for node1, node2 in pair_gen:\n            #TODO: if root is in inner_search_space, then do use it?\n            if node2 is G2.root: #dont want to add root as inner node\n                continue\n\n\n            #check if node1 is a leaf and leafs are protected, don't add an input to the leave\n            if self.leaf_search_space is not None and len(list(self.graph.successors(node1))) == 0:\n                continue\n\n            #icheck if node2 is graph individual\n            # if isinstance(node2,GraphIndividual):\n            #     if not ((isinstance(node2,GraphIndividual) and (\"Recursive\" in self.inner_search_space or \"Recursive\" in self.leaf_search_space))):\n            #         continue\n\n            #isolating the branch\n            branch2 = G2.graph.copy()\n            n2_descendants = nx.descendants(branch2,node2)\n            for n in list(branch2.nodes):\n                if n not in n2_descendants and n is not node2: #removes all nodes not in the branch\n                    branch2.remove_node(n)\n\n            #if node1 plus node2 branch has more than max_children, skip\n            if branch2.number_of_nodes() + self.graph.number_of_nodes() &gt; self.max_size:\n                continue\n\n            branch2 = copy.deepcopy(branch2)\n            branch2_root = get_roots(branch2)[0]\n            self.graph.add_edges_from(branch2.edges)\n            self.graph.add_edge(node1,branch2_root)\n\n            return True\n        return False\n\n\n\n    def _crossover_nodes(self, G2, rng=None):\n        '''\n        Swaps the hyperparamters of one randomly chosen node in Parent1 with the hyperparameters of randomly chosen node in Parent2.\n        '''\n        rng = np.random.default_rng(rng)\n\n        if self.crossover_same_depth:\n            pair_gen = select_nodes_same_depth(self.graph, self.root, G2.graph, G2.root, rng=rng)\n        else:\n            pair_gen = select_nodes_randomly(self.graph, G2.graph, rng=rng)\n\n        for node1, node2 in pair_gen:\n\n            #if both nodes are leaves\n            if len(list(self.graph.successors(node1)))==0 and len(list(G2.graph.successors(node2)))==0:\n                if node1.crossover(node2):\n                    return True\n\n\n            #if both nodes are inner nodes\n            if len(list(self.graph.successors(node1)))&gt;0 and len(list(G2.graph.successors(node2)))&gt;0:\n                if len(list(self.graph.predecessors(node1)))&gt;0 and len(list(G2.graph.predecessors(node2)))&gt;0:\n                    if node1.crossover(node2):\n                        return True\n\n            #if both nodes are root nodes\n            if node1 is self.root and node2 is G2.root:\n                if node1.crossover(node2):\n                    return True\n\n\n        return False\n\n    #not including the nodes, just their children\n    #Finds leaves attached to nodes and swaps them\n    def _crossover_swap_leaf_at_node(self, G2, rng=None):\n        rng = np.random.default_rng(rng)\n\n        if self.crossover_same_depth:\n            pair_gen = select_nodes_same_depth(self.graph, self.root, G2.graph, G2.root, rng=rng)\n        else:\n            pair_gen = select_nodes_randomly(self.graph, G2.graph, rng=rng)\n\n        success = False\n        for node1, node2 in pair_gen:\n            # if leaves are protected node1 and node2 must both be leaves or both be inner nodes\n            if self.leaf_search_space is not None and not (len(list(self.graph.successors(node1)))==0 ^ len(list(G2.graph.successors(node2)))==0):\n                continue\n            #self_leafs = [c for c in nx.descendants(self.graph,node1) if len(list(self.graph.successors(c)))==0 and c is not node1]\n            node_leafs = [c for c in nx.descendants(G2.graph,node2) if len(list(G2.graph.successors(c)))==0 and c is not node2]\n\n            # if len(self_leafs) &gt;0:\n            #     for c in self_leafs:\n            #         if random.choice([True,False]):\n            #             self.graph.remove_node(c)\n            #             G2.graph.add_edge(node2, c)\n            #             success = True\n\n            if len(node_leafs) &gt;0:\n                for c in node_leafs:\n                    if rng.choice([True,False]):\n                        G2.graph.remove_node(c)\n                        self.graph.add_edge(node1, c)\n                        success = True\n\n        return success\n\n\n\n    #TODO edit so that G2 is not modified\n    def _crossover_swap_node(self, G2, rng=None):\n        '''\n        Swaps randomly chosen node from Parent1 with a randomly chosen node from Parent2.\n        '''\n        rng = np.random.default_rng(rng)\n\n        if self.crossover_same_depth:\n            pair_gen = select_nodes_same_depth(self.graph, self.root, G2.graph, G2.root, rng=rng)\n        else:\n            pair_gen = select_nodes_randomly(self.graph, G2.graph, rng=rng)\n\n        for node1, node2 in pair_gen:\n            if node1 is self.root or node2 is G2.root: #TODO: allow root\n                continue\n\n            #if leaves are protected\n            if self.leaf_search_space is not None:\n                #if one node is a leaf, the other must be a leaf\n                if not((len(list(self.graph.successors(node1)))==0) ^ (len(list(G2.graph.successors(node2)))==0)):\n                    continue #only continue if both are leaves, or both are not leaves\n\n\n            n1_s = self.graph.successors(node1)\n            n1_p = self.graph.predecessors(node1)\n\n            n2_s = G2.graph.successors(node2)\n            n2_p = G2.graph.predecessors(node2)\n\n            self.graph.remove_node(node1)\n            G2.graph.remove_node(node2)\n\n            self.graph.add_node(node2)\n\n            self.graph.add_edges_from([ (node2, n) for n in n1_s])\n            G2.graph.add_edges_from([ (node1, n) for n in n2_s])\n\n            self.graph.add_edges_from([ (n, node2) for n in n1_p])\n            G2.graph.add_edges_from([ (n, node1) for n in n2_p])\n\n            return True\n\n        return False\n\n\n    def _merge_duplicated_nodes(self):\n\n        graph_changed = False\n        merged = False\n        while(not merged):\n            node_list = list(self.graph.nodes)\n            merged = True\n            for node, other_node in itertools.product(node_list, node_list):\n                if node is other_node:\n                    continue\n\n                #If nodes are same class/hyperparameters\n                if node.unique_id() == other_node.unique_id():\n                    node_children = set(self.graph.successors(node))\n                    other_node_children = set(self.graph.successors(other_node))\n                    #if nodes have identical children, they can be merged\n                    if node_children == other_node_children:\n                        for other_node_parent in list(self.graph.predecessors(other_node)):\n                            if other_node_parent not in self.graph.predecessors(node):\n                                self.graph.add_edge(other_node_parent,node)\n\n                        self.graph.remove_node(other_node)\n                        merged=False\n                        graph_changed = True\n                        break\n\n        return graph_changed\n\n\n    def export_pipeline(self, memory=None, **kwargs):\n        estimator_graph = self.graph.copy()\n\n        #mapping = {node:node.method_class(**node.hyperparameters) for node in estimator_graph}\n        label_remapping = {}\n        label_to_instance = {}\n\n        for node in estimator_graph:\n            this_pipeline_node = node.export_pipeline(memory=memory, **kwargs)\n            found_unique_label = False\n            i=1\n            while not found_unique_label:\n                label = \"{0}_{1}\".format(this_pipeline_node.__class__.__name__, i)\n                if label not in label_to_instance:\n                    found_unique_label = True\n                else:\n                    i+=1\n\n            label_remapping[node] = label\n            label_to_instance[label] = this_pipeline_node\n\n        estimator_graph = nx.relabel_nodes(estimator_graph, label_remapping)\n\n        for label, instance in label_to_instance.items():\n            estimator_graph.nodes[label][\"instance\"] = instance\n\n        return tpot.GraphPipeline(graph=estimator_graph, memory=memory, use_label_encoder=self.use_label_encoder, method=self.method, cross_val_predict_cv=self.cross_val_predict_cv)\n\n\n    def plot(self):\n        G = self.graph.reverse()\n        #TODO clean this up\n        try:\n            pos = nx.planar_layout(G)  # positions for all nodes\n        except:\n            pos = nx.shell_layout(G)\n        # nodes\n        options = {'edgecolors': 'tab:gray', 'node_size': 800, 'alpha': 0.9}\n        nodelist = list(G.nodes)\n        node_color = [plt.cm.Set1(G.nodes[n]['recursive depth']) for n in G]\n\n        fig, ax = plt.subplots()\n\n        nx.draw(G, pos, nodelist=nodelist, node_color=node_color, ax=ax,  **options)\n\n\n        '''edgelist = []\n        for n in n1.node_set:\n            for child in n.children:\n                edgelist.append((n,child))'''\n\n        # edges\n        #nx.draw_networkx_edges(G, pos, width=3.0, arrows=True)\n        '''nx.draw_networkx_edges(\n            G,\n            pos,\n            edgelist=[edgelist],\n            width=8,\n            alpha=0.5,\n            edge_color='tab:red',\n        )'''\n\n\n\n        # some math labels\n        labels = {}\n        for i, n in enumerate(G.nodes):\n            labels[n] = n.method_class.__name__ + \"\\n\" + str(n.hyperparameters)\n\n\n        nx.draw_networkx_labels(G, pos, labels,ax=ax, font_size=7, font_color='black')\n\n        plt.tight_layout()\n        plt.axis('off')\n        plt.show()\n\n\n    def unique_id(self):\n        if self.graphkey is None:\n            #copy self.graph\n            new_graph = self.graph.copy()\n            for n in new_graph.nodes:\n                new_graph.nodes[n]['label'] = n.unique_id()\n\n            new_graph = nx.convert_node_labels_to_integers(new_graph)\n            self.graphkey = GraphKey(new_graph)\n\n        return self.graphkey\n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/graph/#tpot.search_spaces.pipelines.graph.GraphPipelineIndividual.crossover","title":"<code>crossover(ind2, rng=None)</code>","text":"<p>self is the first individual, ind2 is the second individual If crossover_same_depth, it will select graphindividuals at the same recursive depth. Otherwise, it will select graphindividuals randomly from the entire graph and its subgraphs.</p> <p>This does not impact graphs without subgraphs. And it does not impacts nodes that are not graphindividuals. Cros</p> Source code in <code>tpot/search_spaces/pipelines/graph.py</code> <pre><code>def crossover(self, ind2, rng=None):\n    '''\n    self is the first individual, ind2 is the second individual\n    If crossover_same_depth, it will select graphindividuals at the same recursive depth.\n    Otherwise, it will select graphindividuals randomly from the entire graph and its subgraphs.\n\n    This does not impact graphs without subgraphs. And it does not impacts nodes that are not graphindividuals. Cros\n    '''\n\n    rng = np.random.default_rng(rng)\n\n    rng.shuffle(self.crossover_methods_list)\n\n    finished = False\n\n    for crossover_method in self.crossover_methods_list:\n        if crossover_method(ind2, rng=rng):\n            self._merge_duplicated_nodes()\n            finished = True\n            break\n\n    if self.__debug:\n        try:\n            nx.find_cycle(self.graph)\n            print('something went wrong with ', crossover_method)\n        except:\n            pass\n\n    if finished:\n        self.graphkey = None\n\n    return finished\n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/graph/#tpot.search_spaces.pipelines.graph.GraphSearchPipeline","title":"<code>GraphSearchPipeline</code>","text":"<p>               Bases: <code>SearchSpace</code></p> Source code in <code>tpot/search_spaces/pipelines/graph.py</code> <pre><code>class GraphSearchPipeline(SearchSpace):\n    def __init__(self, \n        root_search_space: SearchSpace, \n        leaf_search_space: SearchSpace = None, \n        inner_search_space: SearchSpace = None, \n        max_size: int = np.inf,\n        crossover_same_depth: bool = False,\n        cross_val_predict_cv: Union[int, Callable] = 0, #signature function(estimator, X, y=none)\n        method: str = 'auto',\n        use_label_encoder: bool = False):\n\n        \"\"\"\n        Defines a search space of pipelines in the shape of a Directed Acyclic Graphs. The search spaces for root, leaf, and inner nodes can be defined separately if desired.\n        Each graph will have a single root serving as the final estimator which is drawn from the `root_search_space`. If the `leaf_search_space` is defined, all leaves \n        in the pipeline will be drawn from that search space. If the `leaf_search_space` is not defined, all leaves will be drawn from the `inner_search_space`.\n        Nodes that are not leaves or roots will be drawn from the `inner_search_space`. If the `inner_search_space` is not defined, there will be no inner nodes.\n\n        `cross_val_predict_cv`, `method`, `memory`, and `use_label_encoder` are passed to the GraphPipeline object when the pipeline is exported and not directly used in the search space.\n\n        Exports to a GraphPipeline object.\n\n        Parameters\n        ----------\n\n        root_search_space: SearchSpace\n            The search space for the root node of the graph. This node will be the final estimator in the pipeline.\n\n        inner_search_space: SearchSpace, optional\n            The search space for the inner nodes of the graph. If not defined, there will be no inner nodes.\n\n        leaf_search_space: SearchSpace, optional\n            The search space for the leaf nodes of the graph. If not defined, the leaf nodes will be drawn from the inner_search_space.\n\n        crossover_same_depth: bool, optional\n            If True, crossover will only occur between nodes at the same depth in the graph. If False, crossover will occur between nodes at any depth.\n\n        cross_val_predict_cv : int, default=0\n            Number of folds to use for the cross_val_predict function for inner classifiers and regressors. Estimators will still be fit on the full dataset, but the following node will get the outputs from cross_val_predict.\n\n            - 0-1 : When set to 0 or 1, the cross_val_predict function will not be used. The next layer will get the outputs from fitting and transforming the full dataset.\n            - &gt;=2 : When fitting pipelines with inner classifiers or regressors, they will still be fit on the full dataset.\n                    However, the output to the next node will come from cross_val_predict with the specified number of folds.\n\n        method: str, optional\n            The prediction method to use for the inner classifiers or regressors. If 'auto', it will try to use predict_proba, decision_function, or predict in that order.\n\n        memory: str or object with the joblib.Memory interface, optional\n            Used to cache the input and outputs of nodes to prevent refitting or computationally heavy transformations. By default, no caching is performed. If a string is given, it is the path to the caching directory.\n\n        use_label_encoder: bool, optional\n            If True, the label encoder is used to encode the labels to be 0 to N. If False, the label encoder is not used.\n            Mainly useful for classifiers (XGBoost) that require labels to be ints from 0 to N.\n            Can also be a sklearn.preprocessing.LabelEncoder object. If so, that label encoder is used.\n\n        \"\"\"\n\n\n        self.root_search_space = root_search_space\n        self.leaf_search_space = leaf_search_space\n        self.inner_search_space = inner_search_space\n        self.max_size = max_size\n        self.crossover_same_depth = crossover_same_depth\n\n        self.cross_val_predict_cv = cross_val_predict_cv\n        self.method = method\n        self.use_label_encoder = use_label_encoder\n\n    def generate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        ind =  GraphPipelineIndividual(self.root_search_space, self.leaf_search_space, self.inner_search_space, self.max_size, self.crossover_same_depth, \n                                       self.cross_val_predict_cv, self.method, self.use_label_encoder, rng=rng)  \n            # if user specified limit, grab a random number between that limit\n\n        if self.max_size is None or self.max_size == np.inf:\n            n_nodes = rng.integers(1, 5)\n        else:\n            n_nodes = min(rng.integers(1, self.max_size), 5)\n\n        starting_ops = []\n        if self.inner_search_space is not None:\n            starting_ops.append(ind._mutate_insert_inner_node)\n        if self.leaf_search_space is not None or self.inner_search_space is not None:\n            starting_ops.append(ind._mutate_insert_leaf)\n            n_nodes -= 1\n\n        if len(starting_ops) &gt; 0:\n            for _ in range(n_nodes-1):\n                func = rng.choice(starting_ops)\n                func(rng=rng)\n\n        ind._merge_duplicated_nodes()\n\n        return ind\n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/graph/#tpot.search_spaces.pipelines.graph.GraphSearchPipeline.__init__","title":"<code>__init__(root_search_space, leaf_search_space=None, inner_search_space=None, max_size=np.inf, crossover_same_depth=False, cross_val_predict_cv=0, method='auto', use_label_encoder=False)</code>","text":"<p>Defines a search space of pipelines in the shape of a Directed Acyclic Graphs. The search spaces for root, leaf, and inner nodes can be defined separately if desired. Each graph will have a single root serving as the final estimator which is drawn from the <code>root_search_space</code>. If the <code>leaf_search_space</code> is defined, all leaves  in the pipeline will be drawn from that search space. If the <code>leaf_search_space</code> is not defined, all leaves will be drawn from the <code>inner_search_space</code>. Nodes that are not leaves or roots will be drawn from the <code>inner_search_space</code>. If the <code>inner_search_space</code> is not defined, there will be no inner nodes.</p> <p><code>cross_val_predict_cv</code>, <code>method</code>, <code>memory</code>, and <code>use_label_encoder</code> are passed to the GraphPipeline object when the pipeline is exported and not directly used in the search space.</p> <p>Exports to a GraphPipeline object.</p> <p>Parameters:</p> Name Type Description Default <code>root_search_space</code> <code>SearchSpace</code> <p>The search space for the root node of the graph. This node will be the final estimator in the pipeline.</p> required <code>inner_search_space</code> <code>SearchSpace</code> <p>The search space for the inner nodes of the graph. If not defined, there will be no inner nodes.</p> <code>None</code> <code>leaf_search_space</code> <code>SearchSpace</code> <p>The search space for the leaf nodes of the graph. If not defined, the leaf nodes will be drawn from the inner_search_space.</p> <code>None</code> <code>crossover_same_depth</code> <code>bool</code> <p>If True, crossover will only occur between nodes at the same depth in the graph. If False, crossover will occur between nodes at any depth.</p> <code>False</code> <code>cross_val_predict_cv</code> <code>int</code> <p>Number of folds to use for the cross_val_predict function for inner classifiers and regressors. Estimators will still be fit on the full dataset, but the following node will get the outputs from cross_val_predict.</p> <ul> <li>0-1 : When set to 0 or 1, the cross_val_predict function will not be used. The next layer will get the outputs from fitting and transforming the full dataset.</li> <li> <p>=2 : When fitting pipelines with inner classifiers or regressors, they will still be fit on the full dataset.         However, the output to the next node will come from cross_val_predict with the specified number of folds.</p> </li> </ul> <code>0</code> <code>method</code> <code>str</code> <p>The prediction method to use for the inner classifiers or regressors. If 'auto', it will try to use predict_proba, decision_function, or predict in that order.</p> <code>'auto'</code> <code>memory</code> <p>Used to cache the input and outputs of nodes to prevent refitting or computationally heavy transformations. By default, no caching is performed. If a string is given, it is the path to the caching directory.</p> required <code>use_label_encoder</code> <code>bool</code> <p>If True, the label encoder is used to encode the labels to be 0 to N. If False, the label encoder is not used. Mainly useful for classifiers (XGBoost) that require labels to be ints from 0 to N. Can also be a sklearn.preprocessing.LabelEncoder object. If so, that label encoder is used.</p> <code>False</code> Source code in <code>tpot/search_spaces/pipelines/graph.py</code> <pre><code>def __init__(self, \n    root_search_space: SearchSpace, \n    leaf_search_space: SearchSpace = None, \n    inner_search_space: SearchSpace = None, \n    max_size: int = np.inf,\n    crossover_same_depth: bool = False,\n    cross_val_predict_cv: Union[int, Callable] = 0, #signature function(estimator, X, y=none)\n    method: str = 'auto',\n    use_label_encoder: bool = False):\n\n    \"\"\"\n    Defines a search space of pipelines in the shape of a Directed Acyclic Graphs. The search spaces for root, leaf, and inner nodes can be defined separately if desired.\n    Each graph will have a single root serving as the final estimator which is drawn from the `root_search_space`. If the `leaf_search_space` is defined, all leaves \n    in the pipeline will be drawn from that search space. If the `leaf_search_space` is not defined, all leaves will be drawn from the `inner_search_space`.\n    Nodes that are not leaves or roots will be drawn from the `inner_search_space`. If the `inner_search_space` is not defined, there will be no inner nodes.\n\n    `cross_val_predict_cv`, `method`, `memory`, and `use_label_encoder` are passed to the GraphPipeline object when the pipeline is exported and not directly used in the search space.\n\n    Exports to a GraphPipeline object.\n\n    Parameters\n    ----------\n\n    root_search_space: SearchSpace\n        The search space for the root node of the graph. This node will be the final estimator in the pipeline.\n\n    inner_search_space: SearchSpace, optional\n        The search space for the inner nodes of the graph. If not defined, there will be no inner nodes.\n\n    leaf_search_space: SearchSpace, optional\n        The search space for the leaf nodes of the graph. If not defined, the leaf nodes will be drawn from the inner_search_space.\n\n    crossover_same_depth: bool, optional\n        If True, crossover will only occur between nodes at the same depth in the graph. If False, crossover will occur between nodes at any depth.\n\n    cross_val_predict_cv : int, default=0\n        Number of folds to use for the cross_val_predict function for inner classifiers and regressors. Estimators will still be fit on the full dataset, but the following node will get the outputs from cross_val_predict.\n\n        - 0-1 : When set to 0 or 1, the cross_val_predict function will not be used. The next layer will get the outputs from fitting and transforming the full dataset.\n        - &gt;=2 : When fitting pipelines with inner classifiers or regressors, they will still be fit on the full dataset.\n                However, the output to the next node will come from cross_val_predict with the specified number of folds.\n\n    method: str, optional\n        The prediction method to use for the inner classifiers or regressors. If 'auto', it will try to use predict_proba, decision_function, or predict in that order.\n\n    memory: str or object with the joblib.Memory interface, optional\n        Used to cache the input and outputs of nodes to prevent refitting or computationally heavy transformations. By default, no caching is performed. If a string is given, it is the path to the caching directory.\n\n    use_label_encoder: bool, optional\n        If True, the label encoder is used to encode the labels to be 0 to N. If False, the label encoder is not used.\n        Mainly useful for classifiers (XGBoost) that require labels to be ints from 0 to N.\n        Can also be a sklearn.preprocessing.LabelEncoder object. If so, that label encoder is used.\n\n    \"\"\"\n\n\n    self.root_search_space = root_search_space\n    self.leaf_search_space = leaf_search_space\n    self.inner_search_space = inner_search_space\n    self.max_size = max_size\n    self.crossover_same_depth = crossover_same_depth\n\n    self.cross_val_predict_cv = cross_val_predict_cv\n    self.method = method\n    self.use_label_encoder = use_label_encoder\n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/sequential/","title":"Sequential","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/search_spaces/pipelines/sequential/#tpot.search_spaces.pipelines.sequential.SequentialPipeline","title":"<code>SequentialPipeline</code>","text":"<p>               Bases: <code>SearchSpace</code></p> Source code in <code>tpot/search_spaces/pipelines/sequential.py</code> <pre><code>class SequentialPipeline(SearchSpace):\n    def __init__(self, search_spaces : List[SearchSpace] ) -&gt; None:\n        \"\"\"\n        Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.\n        \"\"\"\n\n        self.search_spaces = search_spaces\n\n    def generate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        return SequentialPipelineIndividual(self.search_spaces, rng=rng)\n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/sequential/#tpot.search_spaces.pipelines.sequential.SequentialPipeline.__init__","title":"<code>__init__(search_spaces)</code>","text":"<p>Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.</p> Source code in <code>tpot/search_spaces/pipelines/sequential.py</code> <pre><code>def __init__(self, search_spaces : List[SearchSpace] ) -&gt; None:\n    \"\"\"\n    Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.\n    \"\"\"\n\n    self.search_spaces = search_spaces\n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/tree/","title":"Tree","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/search_spaces/pipelines/tree/#tpot.search_spaces.pipelines.tree.TreePipeline","title":"<code>TreePipeline</code>","text":"<p>               Bases: <code>SearchSpace</code></p> Source code in <code>tpot/search_spaces/pipelines/tree.py</code> <pre><code>class TreePipeline(SearchSpace):\n    def __init__(self, root_search_space : SearchSpace, \n                        leaf_search_space : SearchSpace = None, \n                        inner_search_space : SearchSpace =None, \n                        min_size: int = 2, \n                        max_size: int = 10,\n                        crossover_same_depth=False) -&gt; None:\n\n        \"\"\"\n        Generates a pipeline of variable length. Pipeline will have a tree structure similar to TPOT1.\n\n        \"\"\"\n\n        self.search_space = root_search_space\n        self.leaf_search_space = leaf_search_space\n        self.inner_search_space = inner_search_space\n        self.min_size = min_size\n        self.max_size = max_size\n        self.crossover_same_depth = crossover_same_depth\n\n    def generate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        return TreePipelineIndividual(self.search_space, self.leaf_search_space, self.inner_search_space, self.min_size, self.max_size, self.crossover_same_depth, rng=rng) \n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/tree/#tpot.search_spaces.pipelines.tree.TreePipeline.__init__","title":"<code>__init__(root_search_space, leaf_search_space=None, inner_search_space=None, min_size=2, max_size=10, crossover_same_depth=False)</code>","text":"<p>Generates a pipeline of variable length. Pipeline will have a tree structure similar to TPOT1.</p> Source code in <code>tpot/search_spaces/pipelines/tree.py</code> <pre><code>def __init__(self, root_search_space : SearchSpace, \n                    leaf_search_space : SearchSpace = None, \n                    inner_search_space : SearchSpace =None, \n                    min_size: int = 2, \n                    max_size: int = 10,\n                    crossover_same_depth=False) -&gt; None:\n\n    \"\"\"\n    Generates a pipeline of variable length. Pipeline will have a tree structure similar to TPOT1.\n\n    \"\"\"\n\n    self.search_space = root_search_space\n    self.leaf_search_space = leaf_search_space\n    self.inner_search_space = inner_search_space\n    self.min_size = min_size\n    self.max_size = max_size\n    self.crossover_same_depth = crossover_same_depth\n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/union/","title":"Union","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/search_spaces/pipelines/union/#tpot.search_spaces.pipelines.union.UnionPipeline","title":"<code>UnionPipeline</code>","text":"<p>               Bases: <code>SearchSpace</code></p> Source code in <code>tpot/search_spaces/pipelines/union.py</code> <pre><code>class UnionPipeline(SearchSpace):\n    def __init__(self, search_spaces : List[SearchSpace] ) -&gt; None:\n        \"\"\"\n        Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.\n        \"\"\"\n\n        self.search_spaces = search_spaces\n\n    def generate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        return UnionPipelineIndividual(self.search_spaces, rng=rng)\n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/union/#tpot.search_spaces.pipelines.union.UnionPipeline.__init__","title":"<code>__init__(search_spaces)</code>","text":"<p>Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.</p> Source code in <code>tpot/search_spaces/pipelines/union.py</code> <pre><code>def __init__(self, search_spaces : List[SearchSpace] ) -&gt; None:\n    \"\"\"\n    Takes in a list of search spaces. will produce a pipeline of Sequential length. Each step in the pipeline will correspond to the the search space provided in the same index.\n    \"\"\"\n\n    self.search_spaces = search_spaces\n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/union/#tpot.search_spaces.pipelines.union.UnionPipelineIndividual","title":"<code>UnionPipelineIndividual</code>","text":"<p>               Bases: <code>SklearnIndividual</code></p> <p>Takes in a list of search spaces. each space is a list of SearchSpaces. Will produce a FeatureUnion pipeline. Each step in the pipeline will correspond to the the search space provided in the same index. The resulting pipeline will be a FeatureUnion of the steps in the pipeline.</p> Source code in <code>tpot/search_spaces/pipelines/union.py</code> <pre><code>class UnionPipelineIndividual(SklearnIndividual):\n    \"\"\"\n    Takes in a list of search spaces. each space is a list of SearchSpaces.\n    Will produce a FeatureUnion pipeline. Each step in the pipeline will correspond to the the search space provided in the same index.\n    The resulting pipeline will be a FeatureUnion of the steps in the pipeline.\n\n    \"\"\"\n\n    def __init__(self, search_spaces : List[SearchSpace], rng=None) -&gt; None:\n        super().__init__()\n        self.search_spaces = search_spaces\n\n        self.pipeline = []\n        for space in self.search_spaces:\n            self.pipeline.append(space.generate(rng))\n\n    def mutate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        step = rng.choice(self.pipeline)\n        return step.mutate(rng)\n\n\n    def crossover(self, other, rng=None):\n        #swap a random step in the pipeline with the corresponding step in the other pipeline\n        rng = np.random.default_rng(rng)\n\n        cx_funcs = [self._crossover_node, self._crossover_swap_node]\n        rng.shuffle(cx_funcs)\n        for cx_func in cx_funcs:\n            if cx_func(other, rng):\n                return True\n\n        return False\n\n    def _crossover_swap_node(self, other, rng):\n        rng = np.random.default_rng(rng)\n        idx = rng.integers(1,len(self.pipeline))\n\n        self.pipeline[idx], other.pipeline[idx] = other.pipeline[idx], self.pipeline[idx]\n        return True\n\n    def _crossover_node(self, other, rng):\n        rng = np.random.default_rng(rng)\n\n        crossover_success = False\n        for idx in range(len(self.pipeline)):\n            if rng.random() &lt; 0.5:\n                if self.pipeline[idx].crossover(other.pipeline[idx], rng):\n                    crossover_success = True\n\n        return crossover_success\n\n    def export_pipeline(self, **kwargs):\n        return sklearn.pipeline.make_union(*[step.export_pipeline(**kwargs) for step in self.pipeline])\n\n    def unique_id(self):\n        l = [step.unique_id() for step in self.pipeline]\n        l = [\"FeatureUnion\"] + l\n        return TupleIndex(tuple(l))\n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/wrapper/","title":"Wrapper","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/search_spaces/pipelines/wrapper/#tpot.search_spaces.pipelines.wrapper.WrapperPipeline","title":"<code>WrapperPipeline</code>","text":"<p>               Bases: <code>SearchSpace</code></p> Source code in <code>tpot/search_spaces/pipelines/wrapper.py</code> <pre><code>class WrapperPipeline(SearchSpace):\n    def __init__(\n            self, \n            method: type, \n            space: ConfigurationSpace,\n            estimator_search_space: SearchSpace,\n            hyperparameter_parser: callable = None, \n            wrapped_param_name: str = None\n            ) -&gt; None:\n\n        \"\"\"\n        This search space is for wrapping a sklearn estimator with a method that takes another estimator and hyperparameters as arguments.\n        For example, this can be used with sklearn.ensemble.BaggingClassifier or sklearn.ensemble.AdaBoostClassifier.\n\n        \"\"\"\n\n\n        self.estimator_search_space = estimator_search_space\n        self.method = method\n        self.space = space\n        self.hyperparameter_parser=hyperparameter_parser\n        self.wrapped_param_name = wrapped_param_name\n\n    def generate(self, rng=None):\n        rng = np.random.default_rng(rng)\n        return WrapperPipelineIndividual(method=self.method, space=self.space, estimator_search_space=self.estimator_search_space, hyperparameter_parser=self.hyperparameter_parser, wrapped_param_name=self.wrapped_param_name,  rng=rng)\n</code></pre>"},{"location":"documentation/tpot/search_spaces/pipelines/wrapper/#tpot.search_spaces.pipelines.wrapper.WrapperPipeline.__init__","title":"<code>__init__(method, space, estimator_search_space, hyperparameter_parser=None, wrapped_param_name=None)</code>","text":"<p>This search space is for wrapping a sklearn estimator with a method that takes another estimator and hyperparameters as arguments. For example, this can be used with sklearn.ensemble.BaggingClassifier or sklearn.ensemble.AdaBoostClassifier.</p> Source code in <code>tpot/search_spaces/pipelines/wrapper.py</code> <pre><code>def __init__(\n        self, \n        method: type, \n        space: ConfigurationSpace,\n        estimator_search_space: SearchSpace,\n        hyperparameter_parser: callable = None, \n        wrapped_param_name: str = None\n        ) -&gt; None:\n\n    \"\"\"\n    This search space is for wrapping a sklearn estimator with a method that takes another estimator and hyperparameters as arguments.\n    For example, this can be used with sklearn.ensemble.BaggingClassifier or sklearn.ensemble.AdaBoostClassifier.\n\n    \"\"\"\n\n\n    self.estimator_search_space = estimator_search_space\n    self.method = method\n    self.space = space\n    self.hyperparameter_parser=hyperparameter_parser\n    self.wrapped_param_name = wrapped_param_name\n</code></pre>"},{"location":"documentation/tpot/selectors/lexicase_selection/","title":"Lexicase selection","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/selectors/lexicase_selection/#tpot.selectors.lexicase_selection.lexicase_selection","title":"<code>lexicase_selection(scores, k, n_parents=1, rng=None)</code>","text":"<p>Select the best individual according to Lexicase Selection, k times. The returned list contains the indices of the chosen individuals.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>ndarray</code> <p>The score matrix, where rows the individuals and the columns are the corresponds to scores on different objectives.</p> required <code>k</code> <code>int</code> <p>The number of individuals to select.</p> required <code>n_parents</code> <code>int</code> <p>The number of parents to select per individual. The default is 1.</p> <code>1</code> <code>rng</code> <code>(int, Generator)</code> <p>The random number generator. The default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>    A array of indices of selected individuals of shape (k, n_parents).</code> Source code in <code>tpot/selectors/lexicase_selection.py</code> <pre><code>def lexicase_selection(scores, k, n_parents=1, rng=None):\n    \"\"\"\n    Select the best individual according to Lexicase Selection, *k* times.\n    The returned list contains the indices of the chosen *individuals*.\n\n    Parameters\n    ----------\n    scores : np.ndarray\n        The score matrix, where rows the individuals and the columns are the corresponds to scores on different objectives.\n    k : int\n        The number of individuals to select.\n    n_parents : int, optional\n        The number of parents to select per individual. The default is 1.\n    rng : int, np.random.Generator, optional\n        The random number generator. The default is None.\n    Returns\n    -------\n        A array of indices of selected individuals of shape (k, n_parents).\n    \"\"\"\n    rng = np.random.default_rng(rng)\n    chosen =[]\n    for i in range(k*n_parents):\n        candidates = list(range(len(scores)))\n        cases = list(range(len(scores[0])))\n        rng.shuffle(cases)\n\n        while len(cases) &gt; 0 and len(candidates) &gt; 1:\n            best_val_for_case = max(scores[candidates,cases[0]])\n            candidates = [x for x in candidates if scores[x, cases[0]] == best_val_for_case]\n            cases.pop(0)\n        chosen.append(rng.choice(candidates))\n\n    return np.reshape(chosen, (k, n_parents))\n</code></pre>"},{"location":"documentation/tpot/selectors/map_elites_selection/","title":"Map elites selection","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/selectors/map_elites_selection/#tpot.selectors.map_elites_selection.create_nd_matrix","title":"<code>create_nd_matrix(matrix, grid_steps=None, bins=None)</code>","text":"<p>Create an n-dimensional matrix with the highest score for each cell</p> <p>Parameters:</p> Name Type Description Default <code>matrix</code> <code>ndarray</code> <p>The score matrix, where the first column is the score and the rest are the features for the map-elites algorithm.</p> required <code>grid_steps</code> <code>int</code> <p>The number of steps to use for each feature to automatically create the bin thresholds. The default is None.</p> <code>None</code> <code>bins</code> <code>list</code> <p>A list of lists containing the bin edges for each feature (other than the score). The default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>An n-dimensional matrix with the highest score for each cell and the index of the individual with that score. The value in the cell is a dictionary with the keys \"score\" and \"idx\" containing the score and index of the individual respectively.</p> Source code in <code>tpot/selectors/map_elites_selection.py</code> <pre><code>def create_nd_matrix(matrix, grid_steps=None, bins=None):\n    \"\"\"\n    Create an n-dimensional matrix with the highest score for each cell\n\n    Parameters\n    ----------\n    matrix : np.ndarray\n        The score matrix, where the first column is the score and the rest are the features for the map-elites algorithm.\n    grid_steps : int, optional\n        The number of steps to use for each feature to automatically create the bin thresholds. The default is None.\n    bins : list, optional\n        A list of lists containing the bin edges for each feature (other than the score). The default is None.\n\n    Returns\n    -------\n    np.ndarray\n        An n-dimensional matrix with the highest score for each cell and the index of the individual with that score.\n        The value in the cell is a dictionary with the keys \"score\" and \"idx\" containing the score and index of the individual respectively.\n    \"\"\"\n    if grid_steps is not None and bins is not None:\n        raise ValueError(\"Either grid_steps or bins must be provided but not both\")\n\n    # Extract scores and features\n    scores = matrix[:, 0]\n    features = matrix[:, 1:]\n\n    # Determine the min and max of each feature\n    min_vals = np.min(features, axis=0)\n    max_vals = np.max(features, axis=0)\n\n    # Create bins for each feature\n    if bins is None:\n        bins = [np.linspace(min_vals[i], max_vals[i], grid_steps) for i in range(len(min_vals))]\n\n    # Initialize n-dimensional matrix with negative infinity\n    nd_matrix = np.full([len(b)+1 for b in bins], {\"score\": -np.inf, \"idx\": None})\n    # Fill in each cell with the highest score for that cell\n    for idx, (score, feature) in enumerate(zip(scores, features)):\n        indices = [np.digitize(f, bin) for f, bin in zip(feature, bins)]\n        cur_score = nd_matrix[tuple(indices)][\"score\"]\n        if score &gt; cur_score:\n            nd_matrix[tuple(indices)] = {\"score\": score, \"idx\": idx}\n\n    return nd_matrix\n</code></pre>"},{"location":"documentation/tpot/selectors/map_elites_selection/#tpot.selectors.map_elites_selection.get_bins","title":"<code>get_bins(arr, k)</code>","text":"<p>Get equally spaced bin thresholds between the min and max values for the array of scores.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>The list of values to calculate the bins for.</p> required <code>k</code> <code>int</code> <p>The number of bins to create.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of bin thresholds calculated to be k equally spaced bins between the min and max of the array.</p> Source code in <code>tpot/selectors/map_elites_selection.py</code> <pre><code>def get_bins(arr, k):\n    \"\"\"\n    Get equally spaced bin thresholds between the min and max values for the array of scores.\n\n    Parameters\n    ----------\n    arr : np.ndarray\n        The list of values to calculate the bins for.\n    k : int\n        The number of bins to create.\n\n    Returns\n    -------\n    list\n        A list of bin thresholds calculated to be k equally spaced bins between the min and max of the array.\n\n    \"\"\"\n    min_vals = np.min(arr, axis=0)\n    max_vals = np.max(arr, axis=0)\n    [np.linspace(min_vals[i], max_vals[i], k) for i in range(len(min_vals))]\n</code></pre>"},{"location":"documentation/tpot/selectors/map_elites_selection/#tpot.selectors.map_elites_selection.get_bins_quantiles","title":"<code>get_bins_quantiles(arr, k=None, q=None)</code>","text":"<p>Takes a matrix and returns the bin thresholds based on quantiles.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>The matrix to calculate the bins for.</p> required <code>k</code> <code>int</code> <p>The number of bins to create. This parameter creates k equally spaced quantiles.  For example, k=3 will create quantiles at array([0.25, 0.5 , 0.75]).</p> <code>None</code> <code>q</code> <code>ndarray</code> <p>Custom quantiles to use for the bins. This parameter creates bins based on the quantiles of the data. The default is None.</p> <code>None</code> Source code in <code>tpot/selectors/map_elites_selection.py</code> <pre><code>def get_bins_quantiles(arr, k=None, q=None):\n    \"\"\"\n    Takes a matrix and returns the bin thresholds based on quantiles.\n\n    Parameters\n    ----------\n    arr : np.ndarray\n        The matrix to calculate the bins for.\n    k : int, optional\n        The number of bins to create. This parameter creates k equally spaced quantiles. \n        For example, k=3 will create quantiles at array([0.25, 0.5 , 0.75]).\n    q : np.ndarray, optional\n        Custom quantiles to use for the bins. This parameter creates bins based on the quantiles of the data. The default is None.\n    \"\"\"\n    bins = []\n\n    if q is not None and k is not None:\n        raise ValueError(\"Only one of k or q can be specified\")\n\n    if q is not None:\n        final_q = q\n    elif k is not None:\n        final_q = np.linspace(0, 1, k+2)[1:-1]\n\n    for i in range(arr.shape[1]):\n        bins.append(np.quantile(arr[:,i], final_q))\n    return bins\n</code></pre>"},{"location":"documentation/tpot/selectors/map_elites_selection/#tpot.selectors.map_elites_selection.manhattan","title":"<code>manhattan(a, b)</code>","text":"<p>Calculate the Manhattan distance between two points.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>ndarray</code> <p>The first point.</p> required <code>b</code> <code>ndarray</code> <p>The second point.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The Manhattan distance between the two points.</p> Source code in <code>tpot/selectors/map_elites_selection.py</code> <pre><code>def manhattan(a, b):\n    \"\"\"\n    Calculate the Manhattan distance between two points.\n\n    Parameters\n    ----------\n    a : np.ndarray\n        The first point.\n    b : np.ndarray\n        The second point.\n\n    Returns\n    -------\n    float\n        The Manhattan distance between the two points.\n    \"\"\"\n    return sum(abs(val1-val2) for val1, val2 in zip(a,b))\n</code></pre>"},{"location":"documentation/tpot/selectors/map_elites_selection/#tpot.selectors.map_elites_selection.map_elites_parent_selector","title":"<code>map_elites_parent_selector(scores, k, n_parents=1, rng=None, manhattan_distance=2, grid_steps=10, bins=None)</code>","text":"<p>A parent selection algorithm for the map-elites algorithm. First creates a grid of the best individuals per cell and then selects parents based on the Manhattan distance between the cells of the best individuals.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>ndarray</code> <p>The score matrix, where the first column is the score and the rest are the features for the map-elites algorithm.</p> required <code>k</code> <code>int</code> <p>The number of individuals to select.</p> required <code>n_parents</code> <code>int</code> <p>The number of parents to select per individual. The default is 1.</p> <code>1</code> <code>rng</code> <code>(int, Generator)</code> <p>The random number generator. The default is None.</p> <code>None</code> <code>manhattan_distance</code> <code>int</code> <p>The maximum Manhattan distance between parents. The default is 2. If no parents are found within this distance, the distance is increased by 1 until at least one other parent is found.</p> <code>2</code> <code>grid_steps</code> <code>int</code> <p>The number of steps to use for each feature to automatically create the bin thresholds. The default is None.</p> <code>10</code> <code>bins</code> <code>list</code> <p>A list of lists containing the bin edges for each feature (other than the score). The default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>An array of indexes of the parents selected for each individual</p> Source code in <code>tpot/selectors/map_elites_selection.py</code> <pre><code>def map_elites_parent_selector(scores,  k, n_parents=1, rng=None, manhattan_distance = 2,  grid_steps= 10, bins=None):\n    \"\"\"\n    A parent selection algorithm for the map-elites algorithm. First creates a grid of the best individuals per cell and then selects parents based on the Manhattan distance between the cells of the best individuals.\n\n    Parameters\n    ----------\n    scores : np.ndarray\n        The score matrix, where the first column is the score and the rest are the features for the map-elites algorithm.\n    k : int\n        The number of individuals to select.\n    n_parents : int, optional\n        The number of parents to select per individual. The default is 1.\n    rng : int, np.random.Generator, optional\n        The random number generator. The default is None.\n    manhattan_distance : int, optional\n        The maximum Manhattan distance between parents. The default is 2. If no parents are found within this distance, the distance is increased by 1 until at least one other parent is found.\n    grid_steps : int, optional\n        The number of steps to use for each feature to automatically create the bin thresholds. The default is None.\n    bins : list, optional\n        A list of lists containing the bin edges for each feature (other than the score). The default is None.\n\n    Returns\n    -------\n    np.ndarray\n        An array of indexes of the parents selected for each individual\n\n    \"\"\"\n\n\n    if grid_steps is not None and bins is not None:\n        raise ValueError(\"Either grid_steps or bins must be provided but not both\")\n\n    rng = np.random.default_rng(rng)\n    scores = np.array(scores)\n    #create grid\n\n    matrix = create_nd_matrix(scores, grid_steps=grid_steps, bins=bins)\n\n    #return true if cell is not empty\n    f = np.vectorize(lambda x: x[\"idx\"] is not None)\n    valid_coordinates  = np.array(np.where(f(matrix))).T\n\n    idx_to_coordinates = {matrix[tuple(coordinates)][\"idx\"]: coordinates for coordinates in valid_coordinates}\n\n    idxes = [idx for idx in idx_to_coordinates.keys()] #all the indexes of best score per cell\n\n    distance_matrix = np.zeros((len(idxes), len(idxes)))\n\n    for i, idx1 in enumerate(idxes):\n        for j, idx2 in enumerate(idxes):\n            distance_matrix[i][j] = manhattan(idx_to_coordinates[idx1], idx_to_coordinates[idx2])\n\n\n    parents = []\n\n    for i in range(k):\n        #randomly select a cell\n        idx = rng.choice(idxes) #select random parent\n\n        #get the distance from this parent to all other parents \n        dm_idx = idxes.index(idx) \n        row = distance_matrix[dm_idx] \n\n        #get all second parents that are within manhattan distance. if none are found increase the distance\n        candidates = []\n        while len(candidates) == 0:\n            candidates = np.where(row &lt;= manhattan_distance)[0]\n            #remove self from candidates\n            candidates = candidates[candidates != dm_idx]\n            manhattan_distance += 1\n\n            if manhattan_distance &gt; np.max(distance_matrix):\n                break\n\n        if len(candidates) == 0:\n            parents.append([idx, idx]) #if no other parents are found, select the same parent twice. weird to crossover with itself though\n        else:\n            this_parents = [idx]\n            for p in range(n_parents-1):\n                idx2_cords = rng.choice(candidates)\n                this_parents.append(idxes[idx2_cords])\n            parents.append(this_parents)\n\n    return np.array(parents)\n</code></pre>"},{"location":"documentation/tpot/selectors/map_elites_selection/#tpot.selectors.map_elites_selection.map_elites_survival_selector","title":"<code>map_elites_survival_selector(scores, k=None, rng=None, grid_steps=10, bins=None)</code>","text":"<p>Takes a matrix of scores and returns the indexes of the individuals that are in the best cells of the map-elites grid. Can either take a grid_steps parameter to automatically create the bins or a bins parameter to specify the bins manually.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>ndarray</code> <p>The score matrix, where the first column is the score and the rest are the features for the map-elites algorithm.</p> required <code>k</code> <code>int</code> <p>The number of individuals to select. The default is None.</p> <code>None</code> <code>rng</code> <code>(int, Generator)</code> <p>The random number generator. The default is None.</p> <code>None</code> <code>grid_steps</code> <code>int</code> <p>The number of steps to use for each feature to automatically create the bin thresholds. The default is None.</p> <code>10</code> <code>bins</code> <code>list</code> <p>A list of lists containing the bin edges for each feature (other than the score). The default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>An array of indexes of the individuals in the best cells of the map-elites grid (without repeats).</p> Source code in <code>tpot/selectors/map_elites_selection.py</code> <pre><code>def map_elites_survival_selector(scores,  k=None, rng=None, grid_steps= 10, bins=None):\n    \"\"\"\n    Takes a matrix of scores and returns the indexes of the individuals that are in the best cells of the map-elites grid.\n    Can either take a grid_steps parameter to automatically create the bins or a bins parameter to specify the bins manually.\n\n    Parameters\n    ----------\n    scores : np.ndarray\n        The score matrix, where the first column is the score and the rest are the features for the map-elites algorithm.\n    k : int, optional\n        The number of individuals to select. The default is None.\n    rng : int, np.random.Generator, optional\n        The random number generator. The default is None.\n    grid_steps : int, optional\n        The number of steps to use for each feature to automatically create the bin thresholds. The default is None.\n    bins : list, optional\n        A list of lists containing the bin edges for each feature (other than the score). The default is None.\n\n    Returns\n    -------\n    np.ndarray\n        An array of indexes of the individuals in the best cells of the map-elites grid (without repeats).\n\n    \"\"\"\n\n    if grid_steps is not None and bins is not None:\n        raise ValueError(\"Either grid_steps or bins must be provided but not both\")\n\n    rng = np.random.default_rng(rng)\n    scores = np.array(scores)\n    #create grid\n\n    matrix = create_nd_matrix(scores, grid_steps=grid_steps, bins=bins)\n    matrix = matrix.flatten()\n\n    indexes =  [cell[\"idx\"] for cell in matrix if cell[\"idx\"] is not None]\n\n    return np.unique(indexes)\n</code></pre>"},{"location":"documentation/tpot/selectors/max_weighted_average_selector/","title":"Max weighted average selector","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/selectors/max_weighted_average_selector/#tpot.selectors.max_weighted_average_selector.max_weighted_average_selector","title":"<code>max_weighted_average_selector(scores, k, n_parents=1, rng=None)</code>","text":"<p>Select the best individual according to Max Weighted Average Selection, k times.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>ndarray</code> <p>The score matrix, where rows the individuals and the columns are the corresponds to scores on different objectives.</p> required <code>k</code> <code>int</code> <p>The number of individuals to select.</p> required <code>n_parents</code> <code>int</code> <p>The number of parents to select per individual. The default is 1.</p> <code>1</code> <code>rng</code> <code>(int, Generator)</code> <p>The random number generator. The default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>    A array of indices of selected individuals of shape (k, n_parents).</code> Source code in <code>tpot/selectors/max_weighted_average_selector.py</code> <pre><code>def max_weighted_average_selector(scores,k, n_parents=1, rng=None):\n    \"\"\"\n    Select the best individual according to Max Weighted Average Selection, *k* times.\n\n    Parameters\n    ----------\n    scores : np.ndarray\n        The score matrix, where rows the individuals and the columns are the corresponds to scores on different objectives.\n    k : int\n        The number of individuals to select.\n    n_parents : int, optional\n        The number of parents to select per individual. The default is 1.\n    rng : int, np.random.Generator, optional\n        The random number generator. The default is None.\n\n    Returns\n    -------\n        A array of indices of selected individuals of shape (k, n_parents).\n\n    \"\"\"\n    ave_scores = [np.nanmean(s ) for s in scores ] #TODO make this more efficient\n    chosen = np.argsort(ave_scores)[::-1][0:k] #TODO check this behavior with nans\n    return np.reshape(chosen, (k, n_parents))\n</code></pre>"},{"location":"documentation/tpot/selectors/nsgaii/","title":"Nsgaii","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/selectors/nsgaii/#tpot.selectors.nsgaii.crowding_distance","title":"<code>crowding_distance(matrix)</code>","text":"<p>Takes a matrix of scores and returns the crowding distance for each point.</p> <p>Parameters:</p> Name Type Description Default <code>matrix</code> <code>ndarray</code> <p>The score matrix, where rows the individuals and the columns are the corresponds to scores on different objectives.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of the crowding distances for each point in the score matrix.</p> Source code in <code>tpot/selectors/nsgaii.py</code> <pre><code>def crowding_distance(matrix):\n    \"\"\"\n    Takes a matrix of scores and returns the crowding distance for each point.\n\n    Parameters\n    ----------\n    matrix : np.ndarray\n        The score matrix, where rows the individuals and the columns are the corresponds to scores on different objectives.\n\n    Returns\n    -------\n    list\n        A list of the crowding distances for each point in the score matrix.\n    \"\"\"\n    matrix = np.array(matrix)\n    # Initialize the crowding distance for each point to zero\n    crowding_distances = [0 for _ in range(len(matrix))]\n\n    # Iterate over each objective\n    for objective_i in range(matrix.shape[1]):\n        # Sort the points according to the current objective\n        sorted_i = matrix[:, objective_i].argsort()\n\n        # Set the crowding distance of the first and last points to infinity\n        crowding_distances[sorted_i[0]] = float(\"inf\")\n        crowding_distances[sorted_i[-1]] = float(\"inf\")\n\n        if matrix[sorted_i[0]][objective_i] == matrix[sorted_i[-1]][objective_i]: # https://github.com/DEAP/deap/blob/f2a570567fa3dce156d7cfb0c50bc72f133258a1/deap/tools/emo.py#L135\n            continue\n\n        norm = matrix.shape[1] * float(matrix[sorted_i[0]][objective_i] - matrix[sorted_i[-1]][objective_i])\n        for prev, cur, following in zip(sorted_i[:-2], sorted_i[1:-1], sorted_i[2:]):\n            crowding_distances[cur] += (matrix[following][objective_i] - matrix[prev][objective_i]) / norm\n\n\n    return crowding_distances\n</code></pre>"},{"location":"documentation/tpot/selectors/nsgaii/#tpot.selectors.nsgaii.dominates","title":"<code>dominates(list1, list2)</code>","text":"<p>returns true is all values in list1 are not strictly worse than list2 AND at least one item in list1 is better than list2</p> <p>Parameters:</p> Name Type Description Default <code>list1</code> <code>list</code> <p>The first list of values to compare.</p> required <code>list2</code> <code>list</code> <p>The second list of values to compare.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if all values in list1 are not strictly worse than list2 AND at least one item in list1 is better than list2, False otherwise.</p> Source code in <code>tpot/selectors/nsgaii.py</code> <pre><code>def dominates(list1, list2):\n    \"\"\"\n    returns true is all values in list1 are not strictly worse than list2 AND at least one item in list1 is better than list2\n\n    Parameters\n    ----------\n    list1 : list\n        The first list of values to compare.\n    list2 : list\n        The second list of values to compare.\n\n    Returns\n    -------\n    bool\n        True if all values in list1 are not strictly worse than list2 AND at least one item in list1 is better than list2, False otherwise.\n\n    \"\"\"\n    return all(list1[i] &gt;= list2[i] for i in range(len(list1))) and any(list1[i] &gt; list2[i] for i in range(len(list1)))\n</code></pre>"},{"location":"documentation/tpot/selectors/nsgaii/#tpot.selectors.nsgaii.nondominated_sorting","title":"<code>nondominated_sorting(matrix)</code>","text":"<p>Returns the indices of the non-dominated rows in the scores matrix. Rows are considered samples, and columns are considered objectives.</p> <p>Parameters:</p> Name Type Description Default <code>matrix</code> <code>ndarray</code> <p>The score matrix, where rows the individuals and the columns are the corresponds to scores on different objectives.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of lists of indices of the non-dominated rows in the scores matrix.</p> Source code in <code>tpot/selectors/nsgaii.py</code> <pre><code>def nondominated_sorting(matrix):\n    \"\"\"\n    Returns the indices of the non-dominated rows in the scores matrix.\n    Rows are considered samples, and columns are considered objectives.\n\n    Parameters\n    ----------\n    matrix : np.ndarray\n        The score matrix, where rows the individuals and the columns are the corresponds to scores on different objectives.\n\n    Returns\n    -------\n    list\n        A list of lists of indices of the non-dominated rows in the scores matrix.\n\n    \"\"\"\n    # Initialize the front list and the rank list\n\n    # Initialize the current front\n    fronts = {0:set()}\n\n    # Initialize the list of dominated points\n    dominated = [set() for _ in range(len(matrix))] #si the set of solutions which solution i dominates\n\n    # Initialize the list of points that dominate the current point\n    dominating = [0 for _ in range(len(matrix))] #ni the number of solutions that denominate solution i\n\n\n    # Iterate over all points\n    for p, p_scores in enumerate(matrix):\n        # Iterate over all other points\n        for q, q_scores in enumerate(matrix):\n            # If the current point dominates the other point, increment the count of points dominated by the current point\n            if dominates(p_scores, q_scores):\n                dominated[p].add(q)\n            # If the current point is dominated by the other point, add it to the list of dominated points\n            elif dominates(q_scores, p_scores):\n                dominating[p] += 1\n\n        if dominating[p] == 0:\n            fronts[0].add(p)\n\n    i=0\n\n    # Iterate until all points have been added to a front\n    while len(fronts[i]) &gt; 0:\n        H = set()\n        for p in fronts[i]:\n            for q in dominated[p]:\n                dominating[q] -= 1\n                if dominating[q] == 0:\n                    H.add(q)\n\n        i += 1\n        fronts[i] = H\n\n\n    return [fronts[j] for j in range(i)]\n</code></pre>"},{"location":"documentation/tpot/selectors/nsgaii/#tpot.selectors.nsgaii.survival_select_NSGA2","title":"<code>survival_select_NSGA2(scores, k, rng=None)</code>","text":"<p>Select the top k individuals from the scores matrix using the NSGA-II algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>ndarray</code> <p>The score matrix, where rows the individuals and the columns are the corresponds to scores on different objectives.</p> required <code>k</code> <code>int</code> <p>The number of individuals to select.</p> required <code>rng</code> <code>(int, Generator)</code> <p>The random number generator. The default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of indices of the selected individuals (without repeats).</p> Source code in <code>tpot/selectors/nsgaii.py</code> <pre><code>def survival_select_NSGA2(scores, k, rng=None):\n    \"\"\"\n    Select the top k individuals from the scores matrix using the NSGA-II algorithm.\n\n    Parameters\n    ----------\n    scores : np.ndarray\n        The score matrix, where rows the individuals and the columns are the corresponds to scores on different objectives.\n    k : int\n        The number of individuals to select.\n    rng : int, np.random.Generator, optional\n        The random number generator. The default is None.\n\n    Returns\n    -------\n    list\n        A list of indices of the selected individuals (without repeats).\n\n    \"\"\"\n\n    pareto_fronts = nondominated_sorting(scores)\n\n    # chosen = list(itertools.chain.from_iterable(fronts))\n    # if len(chosen) &gt;= k:\n    #     return chosen[0:k]\n\n    chosen = []\n    current_front_number = 0\n    while len(chosen) &lt; k and current_front_number &lt; len(pareto_fronts):\n\n        current_front = np.array(list(pareto_fronts[current_front_number]))\n        front_scores = [scores[i] for i in current_front]\n        crowding_distances = crowding_distance(front_scores)\n\n        sorted_indeces = current_front[np.argsort(crowding_distances)[::-1]]\n\n        chosen.extend(sorted_indeces[0:(k-len(chosen))])\n\n        current_front_number += 1\n\n    return chosen\n</code></pre>"},{"location":"documentation/tpot/selectors/random_selector/","title":"Random selector","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/selectors/random_selector/#tpot.selectors.random_selector.random_selector","title":"<code>random_selector(scores, k, n_parents=1, rng=None)</code>","text":"<p>Randomly selects indeces of individuals from the scores matrix.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>ndarray</code> <p>The score matrix, where rows the individuals and the columns are the corresponds to scores on different objectives.</p> required <code>k</code> <code>int</code> <p>The number of individuals to select.</p> required <code>n_parents</code> <code>int</code> <p>The number of parents to select per individual. The default is 1.</p> <code>1</code> <code>rng</code> <code>(int, Generator)</code> <p>The random number generator. The default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>    A array of indices of randomly selected individuals (with replacement) of shape (k, n_parents).</code> Source code in <code>tpot/selectors/random_selector.py</code> <pre><code>def random_selector(scores,  k, n_parents=1, rng=None, ):\n    \"\"\"\n    Randomly selects indeces of individuals from the scores matrix.\n\n    Parameters\n    ----------\n    scores : np.ndarray\n        The score matrix, where rows the individuals and the columns are the corresponds to scores on different objectives.\n    k : int\n        The number of individuals to select.\n    n_parents : int, optional\n        The number of parents to select per individual. The default is 1.\n    rng : int, np.random.Generator, optional\n        The random number generator. The default is None.\n\n    Returns\n    -------\n        A array of indices of randomly selected individuals (with replacement) of shape (k, n_parents).\n\n    \"\"\"\n    rng = np.random.default_rng(rng)\n    chosen = rng.choice(list(range(0,len(scores))), size=k*n_parents)\n    return np.reshape(chosen, (k, n_parents))\n</code></pre>"},{"location":"documentation/tpot/selectors/tournament_selection/","title":"Tournament selection","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/selectors/tournament_selection/#tpot.selectors.tournament_selection.tournament_selection","title":"<code>tournament_selection(scores, k, n_parents=1, rng=None, tournament_size=2, score_index=0)</code>","text":"<p>Select the best individual among tournsize randomly chosen individuals, k times. The returned list contains the indices of the chosen individuals.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>ndarray</code> <p>The score matrix, where rows the individuals and the columns are the corresponds to scores on different objectives.</p> required <code>k</code> <code>int</code> <p>The number of individuals to select.</p> required <code>n_parents</code> <code>int</code> <p>The number of parents to select per individual. The default is 1.</p> <code>1</code> <code>rng</code> <code>(int, Generator)</code> <p>The random number generator. The default is None.</p> <code>None</code> <code>tournament_size</code> <code>int</code> <p>The number of individuals participating in each tournament.</p> <code>2</code> <code>score_index</code> <code>(int, str)</code> <p>The index of the score to use for selection. If \"average\" is passed, the average score is used. The default is 0 (only the first score is used).</p> <code>0</code> <p>Returns:</p> Type Description <code>    A array of indices of selected individuals of shape (k, n_parents).</code> Source code in <code>tpot/selectors/tournament_selection.py</code> <pre><code>def tournament_selection(scores, k, n_parents=1, rng=None, tournament_size=2, score_index=0):\n    \"\"\"\n    Select the best individual among *tournsize* randomly chosen\n    individuals, *k* times. The returned list contains the indices of the chosen *individuals*.\n\n    Parameters\n    ----------\n    scores : np.ndarray\n        The score matrix, where rows the individuals and the columns are the corresponds to scores on different objectives.\n    k : int\n        The number of individuals to select.\n    n_parents : int, optional\n        The number of parents to select per individual. The default is 1.\n    rng : int, np.random.Generator, optional\n        The random number generator. The default is None.\n    tournament_size : int, optional\n        The number of individuals participating in each tournament.\n    score_index : int, str, optional\n        The index of the score to use for selection. If \"average\" is passed, the average score is used. The default is 0 (only the first score is used).\n\n    Returns\n    -------\n        A array of indices of selected individuals of shape (k, n_parents).\n    \"\"\"\n\n    rng = np.random.default_rng(rng)\n\n    if isinstance(score_index,int):\n        key=lambda x:x[1][score_index]\n    elif score_index == \"average\":\n        key=lambda x:np.mean(x[1])\n\n    chosen = []\n    for i in range(k*n_parents):\n        aspirants_idx =[rng.choice(len(scores)) for i in range(tournament_size)]\n        aspirants  = list(zip(aspirants_idx, scores[aspirants_idx])) # Zip indices and elements together\n        chosen.append(max(aspirants, key=key)[0]) # Retrun the index of the maximum element\n\n    return np.reshape(chosen, (k, n_parents))\n</code></pre>"},{"location":"documentation/tpot/selectors/tournament_selection_dominated/","title":"Tournament selection dominated","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/selectors/tournament_selection_dominated/#tpot.selectors.tournament_selection_dominated.tournament_selection_dominated","title":"<code>tournament_selection_dominated(scores, k, n_parents=2, rng=None)</code>","text":"<p>Select the best individual among 2 randomly chosen individuals, k times. Selection is first attempted by checking if one individual dominates the other. Otherwise one with the highest crowding distance is selected. The returned list contains the indices of the chosen individuals.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>ndarray</code> <p>The score matrix, where rows the individuals and the columns are the corresponds to scores on different objectives.</p> required <code>k</code> <code>int</code> <p>The number of individuals to select.</p> required <code>n_parents</code> <code>int</code> <p>The number of parents to select per individual. The default is 2.</p> <code>2</code> <code>rng</code> <code>(int, Generator)</code> <p>The random number generator. The default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>    A array of indices of selected individuals of shape (k, n_parents).</code> Source code in <code>tpot/selectors/tournament_selection_dominated.py</code> <pre><code>def tournament_selection_dominated(scores, k, n_parents=2, rng=None):\n    \"\"\"\n    Select the best individual among 2 randomly chosen\n    individuals, *k* times. Selection is first attempted by checking if one individual dominates the other. Otherwise one with the highest crowding distance is selected.\n    The returned list contains the indices of the chosen *individuals*.\n\n    Parameters\n    ----------\n    scores : np.ndarray\n        The score matrix, where rows the individuals and the columns are the corresponds to scores on different objectives.\n    k : int\n        The number of individuals to select.\n    n_parents : int, optional\n        The number of parents to select per individual. The default is 2.\n    rng : int, np.random.Generator, optional\n        The random number generator. The default is None.\n\n    Returns\n    -------\n        A array of indices of selected individuals of shape (k, n_parents).\n\n    \"\"\"\n\n    rng = np.random.default_rng(rng)\n    pareto_fronts = nondominated_sorting(scores)\n\n    # chosen = list(itertools.chain.from_iterable(fronts))\n    # if len(chosen) &gt;= k:\n    #     return chosen[0:k]\n\n    crowding_dict = {}\n    chosen = []\n    current_front_number = 0\n    while current_front_number &lt; len(pareto_fronts):\n\n        current_front = np.array(list(pareto_fronts[current_front_number]))\n        front_scores = [scores[i] for i in current_front]\n        crowding_distances = crowding_distance(front_scores)\n        for i, crowding in zip(current_front,crowding_distances):\n            crowding_dict[i] = crowding\n\n        current_front_number += 1\n\n\n    chosen = []\n    for i in range(k*n_parents):\n        asp1 = rng.choice(len(scores))\n        asp2 = rng.choice(len(scores))\n\n        if dominates(scores[asp1], scores[asp2]):\n            chosen.append(asp1)\n        elif dominates(scores[asp2], scores[asp1]):\n            chosen.append(asp2)\n\n        elif crowding_dict[asp1] &gt; crowding_dict[asp2]:\n            chosen.append(asp1)\n        elif crowding_dict[asp1] &lt; crowding_dict[asp2]:\n            chosen.append(asp2)\n\n        else:\n            chosen.append(rng.choice([asp1,asp2]))\n\n    return np.reshape(chosen, (k, n_parents))\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/cross_val_utils/","title":"Cross val utils","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/tpot_estimator/cross_val_utils/#tpot.tpot_estimator.cross_val_utils.cross_val_score_objective","title":"<code>cross_val_score_objective(estimator, X, y, scorers, cv, fold=None)</code>","text":"<p>Compute the cross validated scores for a estimator. Only fits the estimator once per fold, and loops over the scorers to evaluate the estimator.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <p>The estimator to fit and score.</p> required <code>X</code> <p>The feature matrix.</p> required <code>y</code> <p>The target vector.</p> required <code>scorers</code> <p>The scorers to use.  If a list, will loop over the scorers and return a list of scorers. If a single scorer, will return a single score.</p> required <code>cv</code> <p>The cross-validator to use. For example, sklearn.model_selection.KFold or sklearn.model_selection.StratifiedKFold.</p> required <code>fold</code> <p>The fold to return the scores for. If None, will return the mean of all the scores (per scorer). Default is None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>scores</code> <code>ndarray or float</code> <p>The scores for the estimator per scorer. If fold is None, will return the mean of all the scores (per scorer). Returns a list if multiple scorers are used, otherwise returns a float for the single scorer.</p> Source code in <code>tpot/tpot_estimator/cross_val_utils.py</code> <pre><code>def cross_val_score_objective(estimator, X, y, scorers, cv, fold=None):\n    \"\"\"\n    Compute the cross validated scores for a estimator. Only fits the estimator once per fold, and loops over the scorers to evaluate the estimator.\n\n    Parameters\n    ----------\n    estimator: sklearn.base.BaseEstimator\n        The estimator to fit and score.\n    X: np.ndarray or pd.DataFrame\n        The feature matrix.\n    y: np.ndarray or pd.Series\n        The target vector.\n    scorers: list or scorer\n        The scorers to use. \n        If a list, will loop over the scorers and return a list of scorers.\n        If a single scorer, will return a single score.\n    cv: sklearn cross-validator\n        The cross-validator to use. For example, sklearn.model_selection.KFold or sklearn.model_selection.StratifiedKFold.\n    fold: int, optional\n        The fold to return the scores for. If None, will return the mean of all the scores (per scorer). Default is None.\n\n    Returns\n    -------\n    scores: np.ndarray or float\n        The scores for the estimator per scorer. If fold is None, will return the mean of all the scores (per scorer).\n        Returns a list if multiple scorers are used, otherwise returns a float for the single scorer.\n\n    \"\"\"\n\n    #check if scores is not iterable\n    if not isinstance(scorers, Iterable): \n        scorers = [scorers]\n    scores = []\n    if fold is None:\n        for train_index, test_index in cv.split(X, y):\n            this_fold_estimator = sklearn.base.clone(estimator)\n            if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n            else:\n                X_train, X_test = X[train_index], X[test_index]\n\n            if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n            else:\n                y_train, y_test = y[train_index], y[test_index]\n\n\n            start = time.time()\n            this_fold_estimator.fit(X_train,y_train)\n            duration = time.time() - start\n\n            this_fold_scores = [sklearn.metrics.get_scorer(scorer)(this_fold_estimator, X_test, y_test) for scorer in scorers] \n            scores.append(this_fold_scores)\n            del this_fold_estimator\n            del X_train\n            del X_test\n            del y_train\n            del y_test\n\n\n        return np.mean(scores,0)\n    else:\n        this_fold_estimator = sklearn.base.clone(estimator)\n        train_index, test_index = list(cv.split(X, y))[fold]\n        if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        else:\n            X_train, X_test = X[train_index], X[test_index]\n\n        if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n        else:\n            y_train, y_test = y[train_index], y[test_index]\n\n        start = time.time()\n        this_fold_estimator.fit(X_train,y_train)\n        duration = time.time() - start\n        this_fold_scores = [sklearn.metrics.get_scorer(scorer)(this_fold_estimator, X_test, y_test) for scorer in scorers] \n        return this_fold_scores\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/estimator/","title":"Estimator","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/tpot_estimator/estimator/#tpot.tpot_estimator.estimator.TPOTEstimator","title":"<code>TPOTEstimator</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> Source code in <code>tpot/tpot_estimator/estimator.py</code> <pre><code>class TPOTEstimator(BaseEstimator):\n    def __init__(self,  \n                        search_space,\n                        scorers,\n                        scorers_weights,\n                        classification,\n                        cv = 10,\n                        other_objective_functions=[],\n                        other_objective_functions_weights = [],\n                        objective_function_names = None,\n                        bigger_is_better = True,\n\n                        export_graphpipeline = False,\n                        memory = None,\n\n                        categorical_features = None,\n                        preprocessing = False,\n                        population_size = 50,\n                        initial_population_size = None,\n                        population_scaling = .5,\n                        generations_until_end_population = 1,\n                        generations = None,\n                        max_time_mins=60,\n                        max_eval_time_mins=10,\n                        validation_strategy = \"none\",\n                        validation_fraction = .2,\n                        disable_label_encoder = False,\n\n                        #early stopping parameters\n                        early_stop = None,\n                        scorers_early_stop_tol = 0.001,\n                        other_objectives_early_stop_tol =None,\n                        threshold_evaluation_pruning = None,\n                        threshold_evaluation_scaling = .5,\n                        selection_evaluation_pruning = None,\n                        selection_evaluation_scaling = .5,\n                        min_history_threshold = 20,\n\n                        #evolver parameters\n                        survival_percentage = 1,\n                        crossover_probability=.2,\n                        mutate_probability=.7,\n                        mutate_then_crossover_probability=.05,\n                        crossover_then_mutate_probability=.05,\n                        survival_selector = survival_select_NSGA2,\n                        parent_selector = tournament_selection_dominated,\n\n                        #budget parameters\n                        budget_range = None,\n                        budget_scaling = .5,\n                        generations_until_end_budget = 1,\n                        stepwise_steps = 5,\n\n                        #dask parameters\n                        n_jobs=1,\n                        memory_limit = None,\n                        client = None,\n                        processes = True,\n\n                        #debugging and logging parameters\n                        warm_start = False,\n                        periodic_checkpoint_folder = None,\n                        callback = None,\n\n                        verbose = 0,\n                        scatter = True,\n\n                         # random seed for random number generator (rng)\n                        random_state = None,\n\n                        ):\n\n        '''\n        An sklearn baseestimator that uses genetic programming to optimize a pipeline.\n\n        Parameters\n        ----------\n        search_space : (String, tpot.search_spaces.SearchSpace)\n            - String : The default search space to use for the optimization.\n            | String     | Description      |\n            | :---        |    :----:   |\n            | linear  | A linear pipeline with the structure of \"Selector-&gt;(transformers+Passthrough)-&gt;(classifiers/regressors+Passthrough)-&gt;final classifier/regressor.\" For both the transformer and inner estimator layers, TPOT may choose one or more transformers/classifiers, or it may choose none. The inner classifier/regressor layer is optional. |\n            | linear-light | Same search space as linear, but without the inner classifier/regressor layer and with a reduced set of faster running estimators. |\n            | graph | TPOT will optimize a pipeline in the shape of a directed acyclic graph. The nodes of the graph can include selectors, scalers, transformers, or classifiers/regressors (inner classifiers/regressors can optionally be not included). This will return a custom GraphPipeline rather than an sklearn Pipeline. More details in Tutorial 6. |\n            | graph-light | Same as graph search space, but without the inner classifier/regressors and with a reduced set of faster running estimators. |\n            | mdr |TPOT will search over a series of feature selectors and Multifactor Dimensionality Reduction models to find a series of operators that maximize prediction accuracy. The TPOT MDR configuration is specialized for genome-wide association studies (GWAS), and is described in detail online here.\n\n            Note that TPOT MDR may be slow to run because the feature selection routines are computationally expensive, especially on large datasets. |\n\n\n            - SearchSpace : The search space to use for the optimization. This should be an instance of a SearchSpace.\n                The search space to use for the optimization. This should be an instance of a SearchSpace.\n                TPOT has groups of search spaces found in the following folders, tpot.search_spaces.nodes for the nodes in the pipeline and tpot.search_spaces.pipelines for the pipeline structure.\n\n        scorers : (list, scorer)\n            A scorer or list of scorers to be used in the cross-validation process.\n            see https://scikit-learn.org/stable/modules/model_evaluation.html\n\n        scorers_weights : list\n            A list of weights to be applied to the scorers during the optimization process.\n\n        classification : bool\n            If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem.\n            Used to determine the CV strategy.\n\n        cv : int, cross-validator\n            - (int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.\n            - (sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.\n                - max_depth (int): The maximum depth from any node to the root of the pipelines to be generated.\n\n        other_objective_functions : list, default=[]\n            A list of other objective functions to apply to the pipeline. The function takes a single parameter for the graphpipeline estimator and returns either a single score or a list of scores.\n\n        other_objective_functions_weights : list, default=[]\n            A list of weights to be applied to the other objective functions.\n\n        objective_function_names : list, default=None\n            A list of names to be applied to the objective functions. If None, will use the names of the objective functions.\n\n        bigger_is_better : bool, default=True\n            If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n\n        memory: Memory object or string, default=None\n            If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. This feature\n            is used to avoid computing the fit transformers within a pipeline if the parameters\n            and input data are identical with another fitted pipeline during optimization process.\n            - String 'auto':\n                TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.\n            - String path of a caching directory\n                TPOT uses memory caching with the provided directory and TPOT does NOT clean\n                the caching directory up upon shutdown. If the directory does not exist, TPOT will\n                create it.\n            - Memory object:\n                TPOT uses the instance of joblib.Memory for memory caching,\n                and TPOT does NOT clean the caching directory up upon shutdown.\n            - None:\n                TPOT does not use memory caching.              \n\n        categorical_features: list or None\n            Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n            - None : If None, TPOT will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing.\n            - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices\n\n        preprocessing : bool or BaseEstimator/Pipeline,\n            EXPERIMENTAL - will be changed in future versions\n            A pipeline that will be used to preprocess the data before CV. Note that the parameters for these steps are not optimized. Add them to the search space to be optimized.\n            - bool : If True, will use a default preprocessing pipeline which includes imputation followed by one hot encoding.\n            - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.\n\n        population_size : int, default=50\n            Size of the population\n\n        initial_population_size : int, default=None\n            Size of the initial population. If None, population_size will be used.\n\n        population_scaling : int, default=0.5\n            Scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n\n        generations_until_end_population : int, default=1\n            Number of generations until the population size reaches population_size\n\n        generations : int, default=50\n            Number of generations to run\n\n        max_time_mins : float, default=float(\"inf\")\n            Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n\n        max_eval_time_mins : float, default=5\n            Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n\n        validation_strategy : str, default='none'\n            EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT may overfit the cross validation score. A second validation set can be used to select the final pipeline.\n            - 'auto' : Automatically determine the validation strategy based on the dataset shape.\n            - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets.\n            - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets.\n            - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.\n\n        validation_fraction : float, default=0.2\n          EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.\n\n        disable_label_encoder : bool, default=False\n            If True, TPOT will check if the target needs to be relabeled to be sequential ints from 0 to N. This is necessary for XGBoost compatibility. If the labels need to be encoded, TPOT will use sklearn.preprocessing.LabelEncoder to encode the labels. The encoder can be accessed via the self.label_encoder_ attribute.\n            If False, no additional label encoders will be used.\n\n        early_stop : int, default=None\n            Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered. In general a value of around 5-20 is good.\n\n        scorers_early_stop_tol :\n            -list of floats\n                list of tolerances for each scorer. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n                If an index of the list is None, that item will not be used for early stopping\n            -int\n                If an int is given, it will be used as the tolerance for all objectives\n\n        other_objectives_early_stop_tol :\n            -list of floats\n                list of tolerances for each of the other objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n                If an index of the list is None, that item will not be used for early stopping\n            -int\n                If an int is given, it will be used as the tolerance for all objectives\n\n        threshold_evaluation_pruning : list [start, end], default=None\n            starting and ending percentile to use as a threshold for the evaluation early stopping.\n            Values between 0 and 100.\n\n        threshold_evaluation_scaling : float [0,inf), default=0.5\n            A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n            Must be greater than zero. Higher numbers will move the threshold to the end faster.\n\n        selection_evaluation_pruning : list, default=None\n            A lower and upper percent of the population size to select each round of CV.\n            Values between 0 and 1.\n\n        selection_evaluation_scaling : float, default=0.5\n            A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n            Must be greater than zero. Higher numbers will move the threshold to the end faster.\n\n        min_history_threshold : int, default=0\n            The minimum number of previous scores needed before using threshold early stopping.\n\n        survival_percentage : float, default=1\n            Percentage of the population size to utilize for mutation and crossover at the beginning of the generation. The rest are discarded. Individuals are selected with the selector passed into survival_selector. The value of this parameter must be between 0 and 1, inclusive.\n            For example, if the population size is 100 and the survival percentage is .5, 50 individuals will be selected with NSGA2 from the existing population. These will be used for mutation and crossover to generate the next 100 individuals for the next generation. The remainder are discarded from the live population. In the next generation, there will now be the 50 parents + the 100 individuals for a total of 150. Surivival percentage is based of the population size parameter and not the existing population size (current population size when using successive halving). Therefore, in the next generation we will still select 50 individuals from the currently existing 150.\n\n        crossover_probability : float, default=.2\n            Probability of generating a new individual by crossover between two individuals.\n\n        mutate_probability : float, default=.7\n            Probability of generating a new individual by crossover between one individuals.\n\n        mutate_then_crossover_probability : float, default=.05\n            Probability of generating a new individual by mutating two individuals followed by crossover.\n\n        crossover_then_mutate_probability : float, default=.05\n            Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.\n\n        survival_selector : function, default=survival_select_NSGA2\n            Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes.\n            Used to selected population_size * survival_percentage individuals at the start of each generation to use for mutation and crossover.\n\n        parent_selector : function, default=parent_select_NSGA2\n            Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.\n\n        budget_range : list [start, end], default=None\n            A starting and ending budget to use for the budget scaling.\n\n        budget_scaling float : [0,1], default=0.5\n            A scaling factor to use when determining how fast we move the budget from the start to end budget.\n\n        generations_until_end_budget : int, default=1\n            The number of generations to run before reaching the max budget.\n\n        stepwise_steps : int, default=1\n            The number of staircase steps to take when scaling the budget and population size.\n\n        n_jobs : int, default=1\n            Number of processes to run in parallel.\n\n        memory_limit : str, default=None\n            Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n\n        client : dask.distributed.Client, default=None\n            A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.\n\n        processes : bool, default=True\n            If True, will use multiprocessing to parallelize the optimization process. If False, will use threading.\n            True seems to perform better. However, False is required for interactive debugging.\n\n        warm_start : bool, default=False\n            If True, will use the continue the evolutionary algorithm from the last generation of the previous run.\n\n        periodic_checkpoint_folder : str, default=None\n            Folder to save the population to periodically. If None, no periodic saving will be done.\n            If provided, training will resume from this checkpoint.\n\n        callback : tpot.CallBackInterface, default=None\n            Callback object. Not implemented\n\n        verbose : int, default=1\n            How much information to print during the optimization process. Higher values include the information from lower values.\n            0. nothing\n            1. progress bar\n\n            3. best individual\n            4. warnings\n            &gt;=5. full warnings trace\n            6. evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_mins in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)\n\n        scatter : bool, default=True\n            If True, will scatter the data to the dask workers. If False, will not scatter the data. This can be useful for debugging.\n\n        random_state : int, None, default=None\n            A seed for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes\n\n            - int\n                Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'\n            - None\n                Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS\n\n        Attributes\n        ----------\n\n        fitted_pipeline_ : GraphPipeline\n            A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.\n\n        evaluated_individuals : A pandas data frame containing data for all evaluated individuals in the run.\n            Columns:\n            - *objective functions : The first few columns correspond to the passed in scorers and objective functions\n            - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population.\n            - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population.\n            - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator.\n            - Generation : The generation the pipeline first appeared.\n            - Pareto_Front\t: The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.\n                            To save on computational time, the best frontier is updated iteratively each generation.\n                            The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.\n                            All other pipelines are set to NaN.\n            - Instance\t: The unfitted GraphPipeline BaseEstimator.\n            - *validation objective functions : Objective function scores evaluated on the validation set.\n            - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.\n\n        pareto_front : The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.\n        '''\n\n        # sklearn BaseEstimator must have a corresponding attribute for each parameter.\n        # These should not be modified once set.\n\n        self.scorers = scorers\n        self.scorers_weights = scorers_weights\n        self.classification = classification\n        self.cv = cv\n        self.other_objective_functions = other_objective_functions\n        self.other_objective_functions_weights = other_objective_functions_weights\n        self.objective_function_names = objective_function_names\n        self.bigger_is_better = bigger_is_better\n\n        self.search_space = search_space\n\n        self.export_graphpipeline = export_graphpipeline\n        self.memory = memory\n\n        self.categorical_features = categorical_features\n\n        self.preprocessing = preprocessing\n        self.validation_strategy = validation_strategy\n        self.validation_fraction = validation_fraction\n        self.disable_label_encoder = disable_label_encoder\n        self.population_size = population_size\n        self.initial_population_size = initial_population_size\n        self.population_scaling = population_scaling\n        self.generations_until_end_population = generations_until_end_population\n        self.generations = generations\n        self.early_stop = early_stop\n        self.scorers_early_stop_tol = scorers_early_stop_tol\n        self.other_objectives_early_stop_tol = other_objectives_early_stop_tol\n        self.max_time_mins = max_time_mins\n        self.max_eval_time_mins = max_eval_time_mins\n        self.n_jobs= n_jobs\n        self.memory_limit = memory_limit\n        self.client = client\n        self.survival_percentage = survival_percentage\n        self.crossover_probability = crossover_probability\n        self.mutate_probability = mutate_probability\n        self.mutate_then_crossover_probability= mutate_then_crossover_probability\n        self.crossover_then_mutate_probability= crossover_then_mutate_probability\n        self.survival_selector=survival_selector\n        self.parent_selector=parent_selector\n        self.budget_range = budget_range\n        self.budget_scaling = budget_scaling\n        self.generations_until_end_budget = generations_until_end_budget\n        self.stepwise_steps = stepwise_steps\n        self.threshold_evaluation_pruning =threshold_evaluation_pruning\n        self.threshold_evaluation_scaling =  threshold_evaluation_scaling\n        self.min_history_threshold = min_history_threshold\n        self.selection_evaluation_pruning = selection_evaluation_pruning\n        self.selection_evaluation_scaling =  selection_evaluation_scaling\n        self.warm_start = warm_start\n        self.verbose = verbose\n        self.periodic_checkpoint_folder = periodic_checkpoint_folder\n        self.callback = callback\n        self.processes = processes\n\n\n        self.scatter = scatter\n\n\n        timer_set = self.max_time_mins != float(\"inf\") and self.max_time_mins is not None\n        if self.generations is not None and timer_set:\n            warnings.warn(\"Both generations and max_time_mins are set. TPOT will terminate when the first condition is met.\")\n\n        # create random number generator based on rngseed\n        self.rng = np.random.default_rng(random_state)\n        # save random state passed to us for other functions that use random_state\n        self.random_state = random_state\n\n        #Initialize other used params\n\n\n        if self.initial_population_size is None:\n            self._initial_population_size = self.population_size\n        else:\n            self._initial_population_size = self.initial_population_size\n\n        if isinstance(self.scorers, str):\n            self._scorers = [self.scorers]\n\n        elif callable(self.scorers):\n            self._scorers = [self.scorers]\n        else:\n            self._scorers = self.scorers\n\n        self._scorers = [sklearn.metrics.get_scorer(scoring) for scoring in self._scorers]\n        self._scorers_early_stop_tol = self.scorers_early_stop_tol\n\n        self._evolver = tpot.evolvers.BaseEvolver\n\n        self.objective_function_weights = [*scorers_weights, *other_objective_functions_weights]\n\n\n        if self.objective_function_names is None:\n            obj_names = [f.__name__ for f in other_objective_functions]\n        else:\n            obj_names = self.objective_function_names\n        self.objective_names = [f._score_func.__name__ if hasattr(f,\"_score_func\") else f.__name__ for f in self._scorers] + obj_names\n\n\n        if not isinstance(self.other_objectives_early_stop_tol, list):\n            self._other_objectives_early_stop_tol = [self.other_objectives_early_stop_tol for _ in range(len(self.other_objective_functions))]\n        else:\n            self._other_objectives_early_stop_tol = self.other_objectives_early_stop_tol\n\n        if not isinstance(self._scorers_early_stop_tol, list):\n            self._scorers_early_stop_tol = [self._scorers_early_stop_tol for _ in range(len(self._scorers))]\n        else:\n            self._scorers_early_stop_tol = self._scorers_early_stop_tol\n\n        self.early_stop_tol = [*self._scorers_early_stop_tol, *self._other_objectives_early_stop_tol]\n\n        self._evolver_instance = None\n        self.evaluated_individuals = None\n\n\n        self.label_encoder_ = None\n\n\n        set_dask_settings()\n\n\n    def fit(self, X, y):\n        if self.client is not None: #If user passed in a client manually\n           _client = self.client\n        else:\n\n            if self.verbose &gt;= 4:\n                silence_logs = 30\n            elif self.verbose &gt;=5:\n                silence_logs = 40\n            else:\n                silence_logs = 50\n            cluster = LocalCluster(n_workers=self.n_jobs, #if no client is passed in and no global client exists, create our own\n                    threads_per_worker=1,\n                    processes=self.processes,\n                    silence_logs=silence_logs,\n                    memory_limit=self.memory_limit)\n            _client = Client(cluster)\n\n        if self.classification and not self.disable_label_encoder and not check_if_y_is_encoded(y):\n            warnings.warn(\"Labels are not encoded as ints from 0 to N. For compatibility with some classifiers such as sklearn, TPOT has encoded y with the sklearn LabelEncoder. When using pipelines outside the main TPOT estimator class, you can encode the labels with est.label_encoder_\")\n            self.label_encoder_ = LabelEncoder()\n            y = self.label_encoder_.fit_transform(y)\n\n        self.evaluated_individuals = None\n        #determine validation strategy\n        if self.validation_strategy == 'auto':\n            nrows = X.shape[0]\n            ncols = X.shape[1]\n\n            if nrows/ncols &lt; 20:\n                validation_strategy = 'reshuffled'\n            elif nrows/ncols &lt; 100:\n                validation_strategy = 'split'\n            else:\n                validation_strategy = 'none'\n        else:\n            validation_strategy = self.validation_strategy\n\n        if validation_strategy == 'split':\n            if self.classification:\n                X, X_val, y, y_val = train_test_split(X, y, test_size=self.validation_fraction, stratify=y, random_state=self.random_state)\n            else:\n                X, X_val, y, y_val = train_test_split(X, y, test_size=self.validation_fraction, random_state=self.random_state)\n\n\n        X_original = X\n        y_original = y\n        if isinstance(self.cv, int) or isinstance(self.cv, float):\n            n_folds = self.cv\n        else:\n            n_folds = self.cv.get_n_splits(X, y)\n\n        if self.classification:\n            X, y = remove_underrepresented_classes(X, y, n_folds)\n\n        if self.preprocessing:\n            #X = pd.DataFrame(X)\n\n            if not isinstance(self.preprocessing, bool) and isinstance(self.preprocessing, sklearn.base.BaseEstimator):\n                self._preprocessing_pipeline = sklearn.base.clone(self.preprocessing)\n\n            #TODO: check if there are missing values in X before imputation. If not, don't include imputation in pipeline. Check if there are categorical columns. If not, don't include one hot encoding in pipeline\n            else: #if self.preprocessing is True or not a sklearn estimator\n\n                pipeline_steps = []\n\n                if self.categorical_features is not None: #if categorical features are specified, use those\n                    pipeline_steps.append((\"impute_categorical\", tpot.builtin_modules.ColumnSimpleImputer(self.categorical_features, strategy='most_frequent')))\n                    pipeline_steps.append((\"impute_numeric\", tpot.builtin_modules.ColumnSimpleImputer(\"numeric\", strategy='mean')))\n                    pipeline_steps.append((\"ColumnOneHotEncoder\", tpot.builtin_modules.ColumnOneHotEncoder(self.categorical_features, min_frequency=0.0001))) # retain wrong param fix\n\n                else:\n                    if isinstance(X, pd.DataFrame):\n                        categorical_columns = X.select_dtypes(include=['object']).columns\n                        if len(categorical_columns) &gt; 0:\n                            pipeline_steps.append((\"impute_categorical\", tpot.builtin_modules.ColumnSimpleImputer(\"categorical\", strategy='most_frequent')))\n                            pipeline_steps.append((\"impute_numeric\", tpot.builtin_modules.ColumnSimpleImputer(\"numeric\", strategy='mean')))\n                            pipeline_steps.append((\"ColumnOneHotEncoder\", tpot.builtin_modules.ColumnOneHotEncoder(\"categorical\", min_frequency=0.0001))) # retain wrong param fix\n                        else:\n                            pipeline_steps.append((\"impute_numeric\", tpot.builtin_modules.ColumnSimpleImputer(\"all\", strategy='mean')))\n                    else:\n                        pipeline_steps.append((\"impute_numeric\", tpot.builtin_modules.ColumnSimpleImputer(\"all\", strategy='mean')))\n\n                self._preprocessing_pipeline = sklearn.pipeline.Pipeline(pipeline_steps)\n\n            X = self._preprocessing_pipeline.fit_transform(X, y)\n\n        else:\n            self._preprocessing_pipeline = None\n\n        #_, y = sklearn.utils.check_X_y(X, y, y_numeric=True)\n\n        #Set up the configuation dictionaries and the search spaces\n\n        #check if self.cv is a number\n        if isinstance(self.cv, int) or isinstance(self.cv, float):\n            if self.classification:\n                self.cv_gen = sklearn.model_selection.StratifiedKFold(n_splits=self.cv, shuffle=True, random_state=self.random_state)\n            else:\n                self.cv_gen = sklearn.model_selection.KFold(n_splits=self.cv, shuffle=True, random_state=self.random_state)\n\n        else:\n            self.cv_gen = sklearn.model_selection.check_cv(self.cv, y, classifier=self.classification)\n\n\n\n        n_samples= int(math.floor(X.shape[0]/n_folds))\n        n_features=X.shape[1]\n\n        if isinstance(X, pd.DataFrame):\n            self.feature_names = X.columns\n        else:\n            self.feature_names = None\n\n\n\n        def objective_function(pipeline_individual,\n                                            X,\n                                            y,\n                                            is_classification=self.classification,\n                                            scorers= self._scorers,\n                                            cv=self.cv_gen,\n                                            other_objective_functions=self.other_objective_functions,\n                                            export_graphpipeline=self.export_graphpipeline,\n                                            memory=self.memory,\n                                            **kwargs):\n            return objective_function_generator(\n                pipeline_individual,\n                X,\n                y,\n                is_classification=is_classification,\n                scorers= scorers,\n                cv=cv,\n                other_objective_functions=other_objective_functions,\n                export_graphpipeline=export_graphpipeline,\n                memory=memory,\n                **kwargs,\n            )\n\n\n\n        if self.threshold_evaluation_pruning is not None or self.selection_evaluation_pruning is not None:\n            evaluation_early_stop_steps = self.cv\n        else:\n            evaluation_early_stop_steps = None\n\n        if self.scatter:\n            X_future = _client.scatter(X)\n            y_future = _client.scatter(y)\n        else:\n            X_future = X\n            y_future = y\n\n        if self.classification:\n            n_classes = len(np.unique(y))\n        else:\n            n_classes = None\n\n        get_search_space_params = {\"n_classes\": n_classes, \n                        \"n_samples\":len(y), \n                        \"n_features\":X.shape[1], \n                        \"random_state\":self.random_state}\n\n        self._search_space = get_template_search_spaces(self.search_space, classification=self.classification, inner_predictors=True, **get_search_space_params)\n\n\n        # TODO : Add check for empty values in X and if so, add imputation to the search space\n        # make this depend on self.preprocessing\n        # if check_empty_values(X):\n        #     from sklearn.experimental import enable_iterative_imputer\n\n        #     from ConfigSpace import ConfigurationSpace\n        #     from ConfigSpace import ConfigurationSpace, Integer, Float, Categorical, Normal\n        #     iterative_imputer_cs = ConfigurationSpace(\n        #         space = {\n        #             'n_nearest_features' : Categorical('n_nearest_features', [100]),\n        #             'initial_strategy' : Categorical('initial_strategy', ['mean','median', 'most_frequent', ]),\n        #             'add_indicator' : Categorical('add_indicator', [True, False]),\n        #         }\n        #     )\n\n        #     imputation_search = tpot.search_spaces.pipelines.ChoicePipeline([\n        #         tpot.config.get_search_space(\"SimpleImputer\"),\n        #         tpot.search_spaces.nodes.EstimatorNode(sklearn.impute.IterativeImputer, iterative_imputer_cs)\n        #     ])\n\n\n\n\n        #     self.search_space_final = tpot.search_spaces.pipelines.SequentialPipeline(search_spaces=[ imputation_search, self._search_space], memory=\"sklearn_pipeline_memory\")\n        # else:\n        #     self.search_space_final = self._search_space\n\n        self.search_space_final = self._search_space\n\n        def ind_generator(rng):\n            rng = np.random.default_rng(rng)\n            while True:\n                yield self.search_space_final.generate(rng)\n\n        #If warm start and we have an evolver instance, use the existing one\n        if not(self.warm_start and self._evolver_instance is not None):\n            self._evolver_instance = self._evolver(   individual_generator=ind_generator(self.rng),\n                                            objective_functions= [objective_function],\n                                            objective_function_weights = self.objective_function_weights,\n                                            objective_names=self.objective_names,\n                                            bigger_is_better = self.bigger_is_better,\n                                            population_size= self.population_size,\n                                            generations=self.generations,\n                                            initial_population_size = self._initial_population_size,\n                                            n_jobs=self.n_jobs,\n                                            verbose = self.verbose,\n                                            max_time_mins =      self.max_time_mins ,\n                                            max_eval_time_mins = self.max_eval_time_mins,\n\n                                            periodic_checkpoint_folder = self.periodic_checkpoint_folder,\n                                            threshold_evaluation_pruning = self.threshold_evaluation_pruning,\n                                            threshold_evaluation_scaling =  self.threshold_evaluation_scaling,\n                                            min_history_threshold = self.min_history_threshold,\n\n                                            selection_evaluation_pruning = self.selection_evaluation_pruning,\n                                            selection_evaluation_scaling =  self.selection_evaluation_scaling,\n                                            evaluation_early_stop_steps = evaluation_early_stop_steps,\n\n                                            early_stop_tol = self.early_stop_tol,\n                                            early_stop= self.early_stop,\n\n                                            budget_range = self.budget_range,\n                                            budget_scaling = self.budget_scaling,\n                                            generations_until_end_budget = self.generations_until_end_budget,\n\n                                            population_scaling = self.population_scaling,\n                                            generations_until_end_population = self.generations_until_end_population,\n                                            stepwise_steps = self.stepwise_steps,\n                                            client = _client,\n                                            objective_kwargs = {\"X\": X_future, \"y\": y_future},\n                                            survival_selector=self.survival_selector,\n                                            parent_selector=self.parent_selector,\n                                            survival_percentage = self.survival_percentage,\n                                            crossover_probability = self.crossover_probability,\n                                            mutate_probability = self.mutate_probability,\n                                            mutate_then_crossover_probability= self.mutate_then_crossover_probability,\n                                            crossover_then_mutate_probability= self.crossover_then_mutate_probability,\n\n                                            rng=self.rng,\n                                            )\n\n\n        self._evolver_instance.optimize()\n        #self._evolver_instance.population.update_pareto_fronts(self.objective_names, self.objective_function_weights)\n        self.make_evaluated_individuals()\n\n\n\n\n        tpot.utils.get_pareto_frontier(self.evaluated_individuals, column_names=self.objective_names, weights=self.objective_function_weights)\n\n        if validation_strategy == 'reshuffled':\n            best_pareto_front_idx = list(self.pareto_front.index)\n            best_pareto_front = list(self.pareto_front.loc[best_pareto_front_idx]['Individual'])\n\n            #reshuffle rows\n            X, y = sklearn.utils.shuffle(X, y, random_state=self.random_state)\n\n            if self.scatter:\n                X_future = _client.scatter(X)\n                y_future = _client.scatter(y)\n            else:\n                X_future = X\n                y_future = y\n\n            val_objective_function_list = [lambda   ind,\n                                                    X,\n                                                    y,\n                                                    is_classification=self.classification,\n                                                    scorers= self._scorers,\n                                                    cv=self.cv_gen,\n                                                    other_objective_functions=self.other_objective_functions,\n                                                    export_graphpipeline=self.export_graphpipeline,\n                                                    memory=self.memory,\n                                                    **kwargs: objective_function_generator(\n                                                                                                ind,\n                                                                                                X,\n                                                                                                y,\n                                                                                                is_classification=is_classification,\n                                                                                                scorers= scorers,\n                                                                                                cv=cv,\n                                                                                                other_objective_functions=other_objective_functions,\n                                                                                                export_graphpipeline=export_graphpipeline,\n                                                                                                memory=memory,\n                                                                                                **kwargs,\n                                                                                                )]\n\n            objective_kwargs = {\"X\": X_future, \"y\": y_future}\n            val_scores, start_times, end_times, eval_errors = tpot.utils.eval_utils.parallel_eval_objective_list(best_pareto_front, val_objective_function_list, verbose=self.verbose, max_eval_time_mins=self.max_eval_time_mins, n_expected_columns=len(self.objective_names), client=_client, **objective_kwargs)\n\n\n\n            val_objective_names = ['validation_'+name for name in self.objective_names]\n            self.objective_names_for_selection = val_objective_names\n            self.evaluated_individuals.loc[best_pareto_front_idx,val_objective_names] = val_scores\n            self.evaluated_individuals.loc[best_pareto_front_idx,'validation_start_times'] = start_times\n            self.evaluated_individuals.loc[best_pareto_front_idx,'validation_end_times'] = end_times\n            self.evaluated_individuals.loc[best_pareto_front_idx,'validation_eval_errors'] = eval_errors\n\n            self.evaluated_individuals[\"Validation_Pareto_Front\"] = tpot.utils.get_pareto_frontier(self.evaluated_individuals, column_names=val_objective_names, weights=self.objective_function_weights)\n\n\n        elif validation_strategy == 'split':\n\n\n            if self.scatter:\n                X_future = _client.scatter(X)\n                y_future = _client.scatter(y)\n                X_val_future = _client.scatter(X_val)\n                y_val_future = _client.scatter(y_val)\n            else:\n                X_future = X\n                y_future = y\n                X_val_future = X_val\n                y_val_future = y_val\n\n            objective_kwargs = {\"X\": X_future, \"y\": y_future, \"X_val\" : X_val_future, \"y_val\":y_val_future }\n\n            best_pareto_front_idx = list(self.pareto_front.index)\n            best_pareto_front = list(self.pareto_front.loc[best_pareto_front_idx]['Individual'])\n            val_objective_function_list = [lambda   ind,\n                                                    X,\n                                                    y,\n                                                    X_val,\n                                                    y_val,\n                                                    scorers= self._scorers,\n                                                    other_objective_functions=self.other_objective_functions,\n                                                    export_graphpipeline=self.export_graphpipeline,\n                                                    memory=self.memory,\n                                                    **kwargs: val_objective_function_generator(\n                                                        ind,\n                                                        X,\n                                                        y,\n                                                        X_val,\n                                                        y_val,\n                                                        scorers= scorers,\n                                                        other_objective_functions=other_objective_functions,\n                                                        export_graphpipeline=export_graphpipeline,\n                                                        memory=memory,\n                                                        **kwargs,\n                                                        )]\n\n            val_scores, start_times, end_times, eval_errors = tpot.utils.eval_utils.parallel_eval_objective_list(best_pareto_front, val_objective_function_list, verbose=self.verbose, max_eval_time_mins=self.max_eval_time_mins, n_expected_columns=len(self.objective_names), client=_client, **objective_kwargs)\n\n\n\n            val_objective_names = ['validation_'+name for name in self.objective_names]\n            self.objective_names_for_selection = val_objective_names\n            self.evaluated_individuals.loc[best_pareto_front_idx,val_objective_names] = val_scores\n            self.evaluated_individuals.loc[best_pareto_front_idx,'validation_start_times'] = start_times\n            self.evaluated_individuals.loc[best_pareto_front_idx,'validation_end_times'] = end_times\n            self.evaluated_individuals.loc[best_pareto_front_idx,'validation_eval_errors'] = eval_errors\n\n            self.evaluated_individuals[\"Validation_Pareto_Front\"] = tpot.utils.get_pareto_frontier(self.evaluated_individuals, column_names=val_objective_names, weights=self.objective_function_weights)\n\n        else:\n            self.objective_names_for_selection = self.objective_names\n\n        val_scores = self.evaluated_individuals[self.evaluated_individuals[self.objective_names_for_selection].isna().all(1).ne(True)][self.objective_names_for_selection]\n        weighted_scores = val_scores*self.objective_function_weights\n\n        if self.bigger_is_better:\n            best_indices = list(weighted_scores.sort_values(by=self.objective_names_for_selection, ascending=False).index)\n        else:\n            best_indices = list(weighted_scores.sort_values(by=self.objective_names_for_selection, ascending=True).index)\n\n        for best_idx in best_indices:\n\n            best_individual = self.evaluated_individuals.loc[best_idx]['Individual']\n            self.selected_best_score =  self.evaluated_individuals.loc[best_idx]\n\n\n            #TODO\n            #best_individual_pipeline = best_individual.export_pipeline(memory=self.memory, cross_val_predict_cv=self.cross_val_predict_cv)\n            if self.export_graphpipeline:\n                best_individual_pipeline = best_individual.export_flattened_graphpipeline(memory=self.memory)\n            else:\n                best_individual_pipeline = best_individual.export_pipeline(memory=self.memory)\n\n            if self.preprocessing:\n                self.fitted_pipeline_ = sklearn.pipeline.make_pipeline(sklearn.base.clone(self._preprocessing_pipeline), best_individual_pipeline )\n            else:\n                self.fitted_pipeline_ = best_individual_pipeline\n\n            try:\n                self.fitted_pipeline_.fit(X_original,y_original) #TODO use y_original as well?\n                break\n            except Exception as e:\n                if self.verbose &gt;= 4:\n                    warnings.warn(\"Final pipeline failed to fit. Rarely, the pipeline might work on the objective function but fail on the full dataset. Generally due to interactions with different features being selected or transformations having different properties. Trying next pipeline\")\n                    print(e)\n                continue\n\n\n        if self.client is None: #no client was passed in\n            #close cluster and client\n            # _client.close()\n            # cluster.close()\n            try:\n                _client.shutdown()\n                cluster.close()\n            #catch exception\n            except Exception as e:\n                print(\"Error shutting down client and cluster\")\n                Warning(e)\n\n        return self\n\n    def _estimator_has(attr):\n        '''Check if we can delegate a method to the underlying estimator.\n        First, we check the first fitted final estimator if available, otherwise we\n        check the unfitted final estimator.\n        '''\n        return  lambda self: (self.fitted_pipeline_ is not None and\n            hasattr(self.fitted_pipeline_, attr)\n        )\n\n\n\n\n\n\n    @available_if(_estimator_has('predict'))\n    def predict(self, X, **predict_params):\n        check_is_fitted(self)\n        #X = check_array(X)\n\n        preds = self.fitted_pipeline_.predict(X,**predict_params)\n        if self.classification and self.label_encoder_:\n            preds = self.label_encoder_.inverse_transform(preds)\n\n        return preds\n\n    @available_if(_estimator_has('predict_proba'))\n    def predict_proba(self, X, **predict_params):\n        check_is_fitted(self)\n        #X = check_array(X)\n        return self.fitted_pipeline_.predict_proba(X,**predict_params)\n\n    @available_if(_estimator_has('decision_function'))\n    def decision_function(self, X, **predict_params):\n        check_is_fitted(self)\n        #X = check_array(X)\n        return self.fitted_pipeline_.decision_function(X,**predict_params)\n\n    @available_if(_estimator_has('transform'))\n    def transform(self, X, **predict_params):\n        check_is_fitted(self)\n        #X = check_array(X)\n        return self.fitted_pipeline_.transform(X,**predict_params)\n\n    @property\n    def classes_(self):\n        \"\"\"The classes labels. Only exist if the last step is a classifier.\"\"\"\n        if self.label_encoder_:\n            return self.label_encoder_.classes_\n        else:\n            return self.fitted_pipeline_.classes_\n\n\n    @property\n    def _estimator_type(self):\n        return self.fitted_pipeline_._estimator_type\n\n\n    def make_evaluated_individuals(self):\n        #check if _evolver_instance exists\n        if self.evaluated_individuals is None:\n            self.evaluated_individuals  =  self._evolver_instance.population.evaluated_individuals.copy()\n            objects = list(self.evaluated_individuals.index)\n            object_to_int = dict(zip(objects, range(len(objects))))\n            self.evaluated_individuals = self.evaluated_individuals.set_index(self.evaluated_individuals.index.map(object_to_int))\n            self.evaluated_individuals['Parents'] = self.evaluated_individuals['Parents'].apply(lambda row: convert_parents_tuples_to_integers(row, object_to_int))\n\n            self.evaluated_individuals[\"Instance\"] = self.evaluated_individuals[\"Individual\"].apply(lambda ind: apply_make_pipeline(ind, preprocessing_pipeline=self._preprocessing_pipeline, export_graphpipeline=self.export_graphpipeline, memory=self.memory))\n\n        return self.evaluated_individuals\n\n    @property\n    def pareto_front(self):\n        #check if _evolver_instance exists\n        if self.evaluated_individuals is None:\n            return None\n        else:\n            if \"Pareto_Front\" not in self.evaluated_individuals:\n                return self.evaluated_individuals\n            else:\n                return self.evaluated_individuals[self.evaluated_individuals[\"Pareto_Front\"]==1]\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/estimator/#tpot.tpot_estimator.estimator.TPOTEstimator.classes_","title":"<code>classes_</code>  <code>property</code>","text":"<p>The classes labels. Only exist if the last step is a classifier.</p>"},{"location":"documentation/tpot/tpot_estimator/estimator/#tpot.tpot_estimator.estimator.TPOTEstimator.__init__","title":"<code>__init__(search_space, scorers, scorers_weights, classification, cv=10, other_objective_functions=[], other_objective_functions_weights=[], objective_function_names=None, bigger_is_better=True, export_graphpipeline=False, memory=None, categorical_features=None, preprocessing=False, population_size=50, initial_population_size=None, population_scaling=0.5, generations_until_end_population=1, generations=None, max_time_mins=60, max_eval_time_mins=10, validation_strategy='none', validation_fraction=0.2, disable_label_encoder=False, early_stop=None, scorers_early_stop_tol=0.001, other_objectives_early_stop_tol=None, threshold_evaluation_pruning=None, threshold_evaluation_scaling=0.5, selection_evaluation_pruning=None, selection_evaluation_scaling=0.5, min_history_threshold=20, survival_percentage=1, crossover_probability=0.2, mutate_probability=0.7, mutate_then_crossover_probability=0.05, crossover_then_mutate_probability=0.05, survival_selector=survival_select_NSGA2, parent_selector=tournament_selection_dominated, budget_range=None, budget_scaling=0.5, generations_until_end_budget=1, stepwise_steps=5, n_jobs=1, memory_limit=None, client=None, processes=True, warm_start=False, periodic_checkpoint_folder=None, callback=None, verbose=0, scatter=True, random_state=None)</code>","text":"<p>An sklearn baseestimator that uses genetic programming to optimize a pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>search_space</code> <code>(String, SearchSpace)</code> <ul> <li>String : The default search space to use for the optimization. | String     | Description      | | :---        |    :----:   | | linear  | A linear pipeline with the structure of \"Selector-&gt;(transformers+Passthrough)-&gt;(classifiers/regressors+Passthrough)-&gt;final classifier/regressor.\" For both the transformer and inner estimator layers, TPOT may choose one or more transformers/classifiers, or it may choose none. The inner classifier/regressor layer is optional. | | linear-light | Same search space as linear, but without the inner classifier/regressor layer and with a reduced set of faster running estimators. | | graph | TPOT will optimize a pipeline in the shape of a directed acyclic graph. The nodes of the graph can include selectors, scalers, transformers, or classifiers/regressors (inner classifiers/regressors can optionally be not included). This will return a custom GraphPipeline rather than an sklearn Pipeline. More details in Tutorial 6. | | graph-light | Same as graph search space, but without the inner classifier/regressors and with a reduced set of faster running estimators. | | mdr |TPOT will search over a series of feature selectors and Multifactor Dimensionality Reduction models to find a series of operators that maximize prediction accuracy. The TPOT MDR configuration is specialized for genome-wide association studies (GWAS), and is described in detail online here.</li> </ul> <p>Note that TPOT MDR may be slow to run because the feature selection routines are computationally expensive, especially on large datasets. |</p> <ul> <li>SearchSpace : The search space to use for the optimization. This should be an instance of a SearchSpace.     The search space to use for the optimization. This should be an instance of a SearchSpace.     TPOT has groups of search spaces found in the following folders, tpot.search_spaces.nodes for the nodes in the pipeline and tpot.search_spaces.pipelines for the pipeline structure.</li> </ul> required <code>scorers</code> <code>(list, scorer)</code> <p>A scorer or list of scorers to be used in the cross-validation process. see https://scikit-learn.org/stable/modules/model_evaluation.html</p> required <code>scorers_weights</code> <code>list</code> <p>A list of weights to be applied to the scorers during the optimization process.</p> required <code>classification</code> <code>bool</code> <p>If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem. Used to determine the CV strategy.</p> required <code>cv</code> <code>(int, cross - validator)</code> <ul> <li>(int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.</li> <li>(sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.<ul> <li>max_depth (int): The maximum depth from any node to the root of the pipelines to be generated.</li> </ul> </li> </ul> <code>10</code> <code>other_objective_functions</code> <code>list</code> <p>A list of other objective functions to apply to the pipeline. The function takes a single parameter for the graphpipeline estimator and returns either a single score or a list of scores.</p> <code>[]</code> <code>other_objective_functions_weights</code> <code>list</code> <p>A list of weights to be applied to the other objective functions.</p> <code>[]</code> <code>objective_function_names</code> <code>list</code> <p>A list of names to be applied to the objective functions. If None, will use the names of the objective functions.</p> <code>None</code> <code>bigger_is_better</code> <code>bool</code> <p>If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.</p> <code>True</code> <code>memory</code> <p>If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. This feature is used to avoid computing the fit transformers within a pipeline if the parameters and input data are identical with another fitted pipeline during optimization process. - String 'auto':     TPOT uses memory caching with a temporary directory and cleans it up upon shutdown. - String path of a caching directory     TPOT uses memory caching with the provided directory and TPOT does NOT clean     the caching directory up upon shutdown. If the directory does not exist, TPOT will     create it. - Memory object:     TPOT uses the instance of joblib.Memory for memory caching,     and TPOT does NOT clean the caching directory up upon shutdown. - None:     TPOT does not use memory caching.</p> <code>None</code> <code>categorical_features</code> <p>Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False. - None : If None, TPOT will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing. - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices</p> <code>None</code> <code>preprocessing</code> <code>(bool or BaseEstimator / Pipeline)</code> <p>EXPERIMENTAL - will be changed in future versions A pipeline that will be used to preprocess the data before CV. Note that the parameters for these steps are not optimized. Add them to the search space to be optimized. - bool : If True, will use a default preprocessing pipeline which includes imputation followed by one hot encoding. - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.</p> <code>False</code> <code>population_size</code> <code>int</code> <p>Size of the population</p> <code>50</code> <code>initial_population_size</code> <code>int</code> <p>Size of the initial population. If None, population_size will be used.</p> <code>None</code> <code>population_scaling</code> <code>int</code> <p>Scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.</p> <code>0.5</code> <code>generations_until_end_population</code> <code>int</code> <p>Number of generations until the population size reaches population_size</p> <code>1</code> <code>generations</code> <code>int</code> <p>Number of generations to run</p> <code>50</code> <code>max_time_mins</code> <code>float</code> <p>Maximum time to run the optimization. If none or inf, will run until the end of the generations.</p> <code>float(\"inf\")</code> <code>max_eval_time_mins</code> <code>float</code> <p>Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.</p> <code>5</code> <code>validation_strategy</code> <code>str</code> <p>EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT may overfit the cross validation score. A second validation set can be used to select the final pipeline. - 'auto' : Automatically determine the validation strategy based on the dataset shape. - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets. - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets. - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.</p> <code>'none'</code> <code>validation_fraction</code> <code>float</code> <p>EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.</p> <code>0.2</code> <code>disable_label_encoder</code> <code>bool</code> <p>If True, TPOT will check if the target needs to be relabeled to be sequential ints from 0 to N. This is necessary for XGBoost compatibility. If the labels need to be encoded, TPOT will use sklearn.preprocessing.LabelEncoder to encode the labels. The encoder can be accessed via the self.label_encoder_ attribute. If False, no additional label encoders will be used.</p> <code>False</code> <code>early_stop</code> <code>int</code> <p>Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered. In general a value of around 5-20 is good.</p> <code>None</code> <code>scorers_early_stop_tol</code> <p>-list of floats     list of tolerances for each scorer. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged     If an index of the list is None, that item will not be used for early stopping -int     If an int is given, it will be used as the tolerance for all objectives</p> <code>0.001</code> <code>other_objectives_early_stop_tol</code> <p>-list of floats     list of tolerances for each of the other objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged     If an index of the list is None, that item will not be used for early stopping -int     If an int is given, it will be used as the tolerance for all objectives</p> <code>None</code> <code>threshold_evaluation_pruning</code> <code>list[start, end]</code> <p>starting and ending percentile to use as a threshold for the evaluation early stopping. Values between 0 and 100.</p> <code>None</code> <code>threshold_evaluation_scaling</code> <code>float [0,inf)</code> <p>A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile. Must be greater than zero. Higher numbers will move the threshold to the end faster.</p> <code>0.5</code> <code>selection_evaluation_pruning</code> <code>list</code> <p>A lower and upper percent of the population size to select each round of CV. Values between 0 and 1.</p> <code>None</code> <code>selection_evaluation_scaling</code> <code>float</code> <p>A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile. Must be greater than zero. Higher numbers will move the threshold to the end faster.</p> <code>0.5</code> <code>min_history_threshold</code> <code>int</code> <p>The minimum number of previous scores needed before using threshold early stopping.</p> <code>0</code> <code>survival_percentage</code> <code>float</code> <p>Percentage of the population size to utilize for mutation and crossover at the beginning of the generation. The rest are discarded. Individuals are selected with the selector passed into survival_selector. The value of this parameter must be between 0 and 1, inclusive. For example, if the population size is 100 and the survival percentage is .5, 50 individuals will be selected with NSGA2 from the existing population. These will be used for mutation and crossover to generate the next 100 individuals for the next generation. The remainder are discarded from the live population. In the next generation, there will now be the 50 parents + the 100 individuals for a total of 150. Surivival percentage is based of the population size parameter and not the existing population size (current population size when using successive halving). Therefore, in the next generation we will still select 50 individuals from the currently existing 150.</p> <code>1</code> <code>crossover_probability</code> <code>float</code> <p>Probability of generating a new individual by crossover between two individuals.</p> <code>.2</code> <code>mutate_probability</code> <code>float</code> <p>Probability of generating a new individual by crossover between one individuals.</p> <code>.7</code> <code>mutate_then_crossover_probability</code> <code>float</code> <p>Probability of generating a new individual by mutating two individuals followed by crossover.</p> <code>.05</code> <code>crossover_then_mutate_probability</code> <code>float</code> <p>Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.</p> <code>.05</code> <code>survival_selector</code> <code>function</code> <p>Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes. Used to selected population_size * survival_percentage individuals at the start of each generation to use for mutation and crossover.</p> <code>survival_select_NSGA2</code> <code>parent_selector</code> <code>function</code> <p>Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.</p> <code>parent_select_NSGA2</code> <code>budget_range</code> <code>list[start, end]</code> <p>A starting and ending budget to use for the budget scaling.</p> <code>None</code> <code>budget_scaling</code> <p>A scaling factor to use when determining how fast we move the budget from the start to end budget.</p> <code>0.5</code> <code>generations_until_end_budget</code> <code>int</code> <p>The number of generations to run before reaching the max budget.</p> <code>1</code> <code>stepwise_steps</code> <code>int</code> <p>The number of staircase steps to take when scaling the budget and population size.</p> <code>1</code> <code>n_jobs</code> <code>int</code> <p>Number of processes to run in parallel.</p> <code>1</code> <code>memory_limit</code> <code>str</code> <p>Memory limit for each job. See Dask LocalCluster documentation for more information.</p> <code>None</code> <code>client</code> <code>Client</code> <p>A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.</p> <code>None</code> <code>processes</code> <code>bool</code> <p>If True, will use multiprocessing to parallelize the optimization process. If False, will use threading. True seems to perform better. However, False is required for interactive debugging.</p> <code>True</code> <code>warm_start</code> <code>bool</code> <p>If True, will use the continue the evolutionary algorithm from the last generation of the previous run.</p> <code>False</code> <code>periodic_checkpoint_folder</code> <code>str</code> <p>Folder to save the population to periodically. If None, no periodic saving will be done. If provided, training will resume from this checkpoint.</p> <code>None</code> <code>callback</code> <code>CallBackInterface</code> <p>Callback object. Not implemented</p> <code>None</code> <code>verbose</code> <code>int</code> <p>How much information to print during the optimization process. Higher values include the information from lower values. 0. nothing 1. progress bar</p> <ol> <li>best individual</li> <li>warnings <p>=5. full warnings trace</p> </li> <li>evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_mins in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)</li> </ol> <code>1</code> <code>scatter</code> <code>bool</code> <p>If True, will scatter the data to the dask workers. If False, will not scatter the data. This can be useful for debugging.</p> <code>True</code> <code>random_state</code> <code>(int, None)</code> <p>A seed for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes</p> <ul> <li>int     Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'</li> <li>None     Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS</li> </ul> <code>None</code> <p>Attributes:</p> Name Type Description <code>fitted_pipeline_</code> <code>GraphPipeline</code> <p>A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.</p> <code>evaluated_individuals</code> <code>A pandas data frame containing data for all evaluated individuals in the run.</code> <p>Columns: - objective functions : The first few columns correspond to the passed in scorers and objective functions - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population. - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population. - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator. - Generation : The generation the pipeline first appeared. - Pareto_Front      : The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.                 To save on computational time, the best frontier is updated iteratively each generation.                 The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.                 All other pipelines are set to NaN. - Instance  : The unfitted GraphPipeline BaseEstimator. - validation objective functions : Objective function scores evaluated on the validation set. - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.</p> <code>pareto_front</code> <code>The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.</code> Source code in <code>tpot/tpot_estimator/estimator.py</code> <pre><code>def __init__(self,  \n                    search_space,\n                    scorers,\n                    scorers_weights,\n                    classification,\n                    cv = 10,\n                    other_objective_functions=[],\n                    other_objective_functions_weights = [],\n                    objective_function_names = None,\n                    bigger_is_better = True,\n\n                    export_graphpipeline = False,\n                    memory = None,\n\n                    categorical_features = None,\n                    preprocessing = False,\n                    population_size = 50,\n                    initial_population_size = None,\n                    population_scaling = .5,\n                    generations_until_end_population = 1,\n                    generations = None,\n                    max_time_mins=60,\n                    max_eval_time_mins=10,\n                    validation_strategy = \"none\",\n                    validation_fraction = .2,\n                    disable_label_encoder = False,\n\n                    #early stopping parameters\n                    early_stop = None,\n                    scorers_early_stop_tol = 0.001,\n                    other_objectives_early_stop_tol =None,\n                    threshold_evaluation_pruning = None,\n                    threshold_evaluation_scaling = .5,\n                    selection_evaluation_pruning = None,\n                    selection_evaluation_scaling = .5,\n                    min_history_threshold = 20,\n\n                    #evolver parameters\n                    survival_percentage = 1,\n                    crossover_probability=.2,\n                    mutate_probability=.7,\n                    mutate_then_crossover_probability=.05,\n                    crossover_then_mutate_probability=.05,\n                    survival_selector = survival_select_NSGA2,\n                    parent_selector = tournament_selection_dominated,\n\n                    #budget parameters\n                    budget_range = None,\n                    budget_scaling = .5,\n                    generations_until_end_budget = 1,\n                    stepwise_steps = 5,\n\n                    #dask parameters\n                    n_jobs=1,\n                    memory_limit = None,\n                    client = None,\n                    processes = True,\n\n                    #debugging and logging parameters\n                    warm_start = False,\n                    periodic_checkpoint_folder = None,\n                    callback = None,\n\n                    verbose = 0,\n                    scatter = True,\n\n                     # random seed for random number generator (rng)\n                    random_state = None,\n\n                    ):\n\n    '''\n    An sklearn baseestimator that uses genetic programming to optimize a pipeline.\n\n    Parameters\n    ----------\n    search_space : (String, tpot.search_spaces.SearchSpace)\n        - String : The default search space to use for the optimization.\n        | String     | Description      |\n        | :---        |    :----:   |\n        | linear  | A linear pipeline with the structure of \"Selector-&gt;(transformers+Passthrough)-&gt;(classifiers/regressors+Passthrough)-&gt;final classifier/regressor.\" For both the transformer and inner estimator layers, TPOT may choose one or more transformers/classifiers, or it may choose none. The inner classifier/regressor layer is optional. |\n        | linear-light | Same search space as linear, but without the inner classifier/regressor layer and with a reduced set of faster running estimators. |\n        | graph | TPOT will optimize a pipeline in the shape of a directed acyclic graph. The nodes of the graph can include selectors, scalers, transformers, or classifiers/regressors (inner classifiers/regressors can optionally be not included). This will return a custom GraphPipeline rather than an sklearn Pipeline. More details in Tutorial 6. |\n        | graph-light | Same as graph search space, but without the inner classifier/regressors and with a reduced set of faster running estimators. |\n        | mdr |TPOT will search over a series of feature selectors and Multifactor Dimensionality Reduction models to find a series of operators that maximize prediction accuracy. The TPOT MDR configuration is specialized for genome-wide association studies (GWAS), and is described in detail online here.\n\n        Note that TPOT MDR may be slow to run because the feature selection routines are computationally expensive, especially on large datasets. |\n\n\n        - SearchSpace : The search space to use for the optimization. This should be an instance of a SearchSpace.\n            The search space to use for the optimization. This should be an instance of a SearchSpace.\n            TPOT has groups of search spaces found in the following folders, tpot.search_spaces.nodes for the nodes in the pipeline and tpot.search_spaces.pipelines for the pipeline structure.\n\n    scorers : (list, scorer)\n        A scorer or list of scorers to be used in the cross-validation process.\n        see https://scikit-learn.org/stable/modules/model_evaluation.html\n\n    scorers_weights : list\n        A list of weights to be applied to the scorers during the optimization process.\n\n    classification : bool\n        If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem.\n        Used to determine the CV strategy.\n\n    cv : int, cross-validator\n        - (int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.\n        - (sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.\n            - max_depth (int): The maximum depth from any node to the root of the pipelines to be generated.\n\n    other_objective_functions : list, default=[]\n        A list of other objective functions to apply to the pipeline. The function takes a single parameter for the graphpipeline estimator and returns either a single score or a list of scores.\n\n    other_objective_functions_weights : list, default=[]\n        A list of weights to be applied to the other objective functions.\n\n    objective_function_names : list, default=None\n        A list of names to be applied to the objective functions. If None, will use the names of the objective functions.\n\n    bigger_is_better : bool, default=True\n        If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n\n    memory: Memory object or string, default=None\n        If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. This feature\n        is used to avoid computing the fit transformers within a pipeline if the parameters\n        and input data are identical with another fitted pipeline during optimization process.\n        - String 'auto':\n            TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.\n        - String path of a caching directory\n            TPOT uses memory caching with the provided directory and TPOT does NOT clean\n            the caching directory up upon shutdown. If the directory does not exist, TPOT will\n            create it.\n        - Memory object:\n            TPOT uses the instance of joblib.Memory for memory caching,\n            and TPOT does NOT clean the caching directory up upon shutdown.\n        - None:\n            TPOT does not use memory caching.              \n\n    categorical_features: list or None\n        Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n        - None : If None, TPOT will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing.\n        - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices\n\n    preprocessing : bool or BaseEstimator/Pipeline,\n        EXPERIMENTAL - will be changed in future versions\n        A pipeline that will be used to preprocess the data before CV. Note that the parameters for these steps are not optimized. Add them to the search space to be optimized.\n        - bool : If True, will use a default preprocessing pipeline which includes imputation followed by one hot encoding.\n        - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.\n\n    population_size : int, default=50\n        Size of the population\n\n    initial_population_size : int, default=None\n        Size of the initial population. If None, population_size will be used.\n\n    population_scaling : int, default=0.5\n        Scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n\n    generations_until_end_population : int, default=1\n        Number of generations until the population size reaches population_size\n\n    generations : int, default=50\n        Number of generations to run\n\n    max_time_mins : float, default=float(\"inf\")\n        Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n\n    max_eval_time_mins : float, default=5\n        Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n\n    validation_strategy : str, default='none'\n        EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT may overfit the cross validation score. A second validation set can be used to select the final pipeline.\n        - 'auto' : Automatically determine the validation strategy based on the dataset shape.\n        - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets.\n        - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets.\n        - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.\n\n    validation_fraction : float, default=0.2\n      EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.\n\n    disable_label_encoder : bool, default=False\n        If True, TPOT will check if the target needs to be relabeled to be sequential ints from 0 to N. This is necessary for XGBoost compatibility. If the labels need to be encoded, TPOT will use sklearn.preprocessing.LabelEncoder to encode the labels. The encoder can be accessed via the self.label_encoder_ attribute.\n        If False, no additional label encoders will be used.\n\n    early_stop : int, default=None\n        Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered. In general a value of around 5-20 is good.\n\n    scorers_early_stop_tol :\n        -list of floats\n            list of tolerances for each scorer. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n            If an index of the list is None, that item will not be used for early stopping\n        -int\n            If an int is given, it will be used as the tolerance for all objectives\n\n    other_objectives_early_stop_tol :\n        -list of floats\n            list of tolerances for each of the other objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n            If an index of the list is None, that item will not be used for early stopping\n        -int\n            If an int is given, it will be used as the tolerance for all objectives\n\n    threshold_evaluation_pruning : list [start, end], default=None\n        starting and ending percentile to use as a threshold for the evaluation early stopping.\n        Values between 0 and 100.\n\n    threshold_evaluation_scaling : float [0,inf), default=0.5\n        A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n        Must be greater than zero. Higher numbers will move the threshold to the end faster.\n\n    selection_evaluation_pruning : list, default=None\n        A lower and upper percent of the population size to select each round of CV.\n        Values between 0 and 1.\n\n    selection_evaluation_scaling : float, default=0.5\n        A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n        Must be greater than zero. Higher numbers will move the threshold to the end faster.\n\n    min_history_threshold : int, default=0\n        The minimum number of previous scores needed before using threshold early stopping.\n\n    survival_percentage : float, default=1\n        Percentage of the population size to utilize for mutation and crossover at the beginning of the generation. The rest are discarded. Individuals are selected with the selector passed into survival_selector. The value of this parameter must be between 0 and 1, inclusive.\n        For example, if the population size is 100 and the survival percentage is .5, 50 individuals will be selected with NSGA2 from the existing population. These will be used for mutation and crossover to generate the next 100 individuals for the next generation. The remainder are discarded from the live population. In the next generation, there will now be the 50 parents + the 100 individuals for a total of 150. Surivival percentage is based of the population size parameter and not the existing population size (current population size when using successive halving). Therefore, in the next generation we will still select 50 individuals from the currently existing 150.\n\n    crossover_probability : float, default=.2\n        Probability of generating a new individual by crossover between two individuals.\n\n    mutate_probability : float, default=.7\n        Probability of generating a new individual by crossover between one individuals.\n\n    mutate_then_crossover_probability : float, default=.05\n        Probability of generating a new individual by mutating two individuals followed by crossover.\n\n    crossover_then_mutate_probability : float, default=.05\n        Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.\n\n    survival_selector : function, default=survival_select_NSGA2\n        Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes.\n        Used to selected population_size * survival_percentage individuals at the start of each generation to use for mutation and crossover.\n\n    parent_selector : function, default=parent_select_NSGA2\n        Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.\n\n    budget_range : list [start, end], default=None\n        A starting and ending budget to use for the budget scaling.\n\n    budget_scaling float : [0,1], default=0.5\n        A scaling factor to use when determining how fast we move the budget from the start to end budget.\n\n    generations_until_end_budget : int, default=1\n        The number of generations to run before reaching the max budget.\n\n    stepwise_steps : int, default=1\n        The number of staircase steps to take when scaling the budget and population size.\n\n    n_jobs : int, default=1\n        Number of processes to run in parallel.\n\n    memory_limit : str, default=None\n        Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n\n    client : dask.distributed.Client, default=None\n        A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.\n\n    processes : bool, default=True\n        If True, will use multiprocessing to parallelize the optimization process. If False, will use threading.\n        True seems to perform better. However, False is required for interactive debugging.\n\n    warm_start : bool, default=False\n        If True, will use the continue the evolutionary algorithm from the last generation of the previous run.\n\n    periodic_checkpoint_folder : str, default=None\n        Folder to save the population to periodically. If None, no periodic saving will be done.\n        If provided, training will resume from this checkpoint.\n\n    callback : tpot.CallBackInterface, default=None\n        Callback object. Not implemented\n\n    verbose : int, default=1\n        How much information to print during the optimization process. Higher values include the information from lower values.\n        0. nothing\n        1. progress bar\n\n        3. best individual\n        4. warnings\n        &gt;=5. full warnings trace\n        6. evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_mins in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)\n\n    scatter : bool, default=True\n        If True, will scatter the data to the dask workers. If False, will not scatter the data. This can be useful for debugging.\n\n    random_state : int, None, default=None\n        A seed for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes\n\n        - int\n            Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'\n        - None\n            Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS\n\n    Attributes\n    ----------\n\n    fitted_pipeline_ : GraphPipeline\n        A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.\n\n    evaluated_individuals : A pandas data frame containing data for all evaluated individuals in the run.\n        Columns:\n        - *objective functions : The first few columns correspond to the passed in scorers and objective functions\n        - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population.\n        - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population.\n        - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator.\n        - Generation : The generation the pipeline first appeared.\n        - Pareto_Front\t: The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.\n                        To save on computational time, the best frontier is updated iteratively each generation.\n                        The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.\n                        All other pipelines are set to NaN.\n        - Instance\t: The unfitted GraphPipeline BaseEstimator.\n        - *validation objective functions : Objective function scores evaluated on the validation set.\n        - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.\n\n    pareto_front : The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.\n    '''\n\n    # sklearn BaseEstimator must have a corresponding attribute for each parameter.\n    # These should not be modified once set.\n\n    self.scorers = scorers\n    self.scorers_weights = scorers_weights\n    self.classification = classification\n    self.cv = cv\n    self.other_objective_functions = other_objective_functions\n    self.other_objective_functions_weights = other_objective_functions_weights\n    self.objective_function_names = objective_function_names\n    self.bigger_is_better = bigger_is_better\n\n    self.search_space = search_space\n\n    self.export_graphpipeline = export_graphpipeline\n    self.memory = memory\n\n    self.categorical_features = categorical_features\n\n    self.preprocessing = preprocessing\n    self.validation_strategy = validation_strategy\n    self.validation_fraction = validation_fraction\n    self.disable_label_encoder = disable_label_encoder\n    self.population_size = population_size\n    self.initial_population_size = initial_population_size\n    self.population_scaling = population_scaling\n    self.generations_until_end_population = generations_until_end_population\n    self.generations = generations\n    self.early_stop = early_stop\n    self.scorers_early_stop_tol = scorers_early_stop_tol\n    self.other_objectives_early_stop_tol = other_objectives_early_stop_tol\n    self.max_time_mins = max_time_mins\n    self.max_eval_time_mins = max_eval_time_mins\n    self.n_jobs= n_jobs\n    self.memory_limit = memory_limit\n    self.client = client\n    self.survival_percentage = survival_percentage\n    self.crossover_probability = crossover_probability\n    self.mutate_probability = mutate_probability\n    self.mutate_then_crossover_probability= mutate_then_crossover_probability\n    self.crossover_then_mutate_probability= crossover_then_mutate_probability\n    self.survival_selector=survival_selector\n    self.parent_selector=parent_selector\n    self.budget_range = budget_range\n    self.budget_scaling = budget_scaling\n    self.generations_until_end_budget = generations_until_end_budget\n    self.stepwise_steps = stepwise_steps\n    self.threshold_evaluation_pruning =threshold_evaluation_pruning\n    self.threshold_evaluation_scaling =  threshold_evaluation_scaling\n    self.min_history_threshold = min_history_threshold\n    self.selection_evaluation_pruning = selection_evaluation_pruning\n    self.selection_evaluation_scaling =  selection_evaluation_scaling\n    self.warm_start = warm_start\n    self.verbose = verbose\n    self.periodic_checkpoint_folder = periodic_checkpoint_folder\n    self.callback = callback\n    self.processes = processes\n\n\n    self.scatter = scatter\n\n\n    timer_set = self.max_time_mins != float(\"inf\") and self.max_time_mins is not None\n    if self.generations is not None and timer_set:\n        warnings.warn(\"Both generations and max_time_mins are set. TPOT will terminate when the first condition is met.\")\n\n    # create random number generator based on rngseed\n    self.rng = np.random.default_rng(random_state)\n    # save random state passed to us for other functions that use random_state\n    self.random_state = random_state\n\n    #Initialize other used params\n\n\n    if self.initial_population_size is None:\n        self._initial_population_size = self.population_size\n    else:\n        self._initial_population_size = self.initial_population_size\n\n    if isinstance(self.scorers, str):\n        self._scorers = [self.scorers]\n\n    elif callable(self.scorers):\n        self._scorers = [self.scorers]\n    else:\n        self._scorers = self.scorers\n\n    self._scorers = [sklearn.metrics.get_scorer(scoring) for scoring in self._scorers]\n    self._scorers_early_stop_tol = self.scorers_early_stop_tol\n\n    self._evolver = tpot.evolvers.BaseEvolver\n\n    self.objective_function_weights = [*scorers_weights, *other_objective_functions_weights]\n\n\n    if self.objective_function_names is None:\n        obj_names = [f.__name__ for f in other_objective_functions]\n    else:\n        obj_names = self.objective_function_names\n    self.objective_names = [f._score_func.__name__ if hasattr(f,\"_score_func\") else f.__name__ for f in self._scorers] + obj_names\n\n\n    if not isinstance(self.other_objectives_early_stop_tol, list):\n        self._other_objectives_early_stop_tol = [self.other_objectives_early_stop_tol for _ in range(len(self.other_objective_functions))]\n    else:\n        self._other_objectives_early_stop_tol = self.other_objectives_early_stop_tol\n\n    if not isinstance(self._scorers_early_stop_tol, list):\n        self._scorers_early_stop_tol = [self._scorers_early_stop_tol for _ in range(len(self._scorers))]\n    else:\n        self._scorers_early_stop_tol = self._scorers_early_stop_tol\n\n    self.early_stop_tol = [*self._scorers_early_stop_tol, *self._other_objectives_early_stop_tol]\n\n    self._evolver_instance = None\n    self.evaluated_individuals = None\n\n\n    self.label_encoder_ = None\n\n\n    set_dask_settings()\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/estimator/#tpot.tpot_estimator.estimator.apply_make_pipeline","title":"<code>apply_make_pipeline(ind, preprocessing_pipeline=None, export_graphpipeline=False, **pipeline_kwargs)</code>","text":"<p>Helper function to create a column of sklearn pipelines from the tpot individual class.</p> <p>Parameters:</p> Name Type Description Default <code>ind</code> <p>The individual to convert to a pipeline.</p> required <code>preprocessing_pipeline</code> <p>The preprocessing pipeline to include before the individual's pipeline.</p> <code>None</code> <code>export_graphpipeline</code> <p>Force the pipeline to be exported as a graph pipeline. Flattens all nested pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.</p> <code>False</code> <code>pipeline_kwargs</code> <p>Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>sklearn estimator</code> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def apply_make_pipeline(ind, preprocessing_pipeline=None, export_graphpipeline=False, **pipeline_kwargs):\n    \"\"\"\n    Helper function to create a column of sklearn pipelines from the tpot individual class.\n\n    Parameters\n    ----------\n    ind: tpot.SklearnIndividual\n        The individual to convert to a pipeline.\n    preprocessing_pipeline: sklearn.pipeline.Pipeline, optional\n        The preprocessing pipeline to include before the individual's pipeline.\n    export_graphpipeline: bool, default=False\n        Force the pipeline to be exported as a graph pipeline. Flattens all nested pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.\n    pipeline_kwargs: dict\n        Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.\n\n    Returns\n    -------\n    sklearn estimator\n    \"\"\"\n\n    try:\n\n        if export_graphpipeline:\n            est = ind.export_flattened_graphpipeline(**pipeline_kwargs)\n        else:\n            est = ind.export_pipeline(**pipeline_kwargs)\n\n\n        if preprocessing_pipeline is None:\n            return est\n        else:\n            return sklearn.pipeline.make_pipeline(sklearn.base.clone(preprocessing_pipeline), est)\n    except:\n        return None\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/estimator/#tpot.tpot_estimator.estimator.check_empty_values","title":"<code>check_empty_values(data)</code>","text":"<p>Checks for empty values in a dataset.</p> <p>Args:     data (numpy.ndarray or pandas.DataFrame): The dataset to check.</p> <p>Returns:     bool: True if the dataset contains empty values, False otherwise.</p> Source code in <code>tpot/tpot_estimator/estimator.py</code> <pre><code>def check_empty_values(data):\n    \"\"\"\n    Checks for empty values in a dataset.\n\n    Args:\n        data (numpy.ndarray or pandas.DataFrame): The dataset to check.\n\n    Returns:\n        bool: True if the dataset contains empty values, False otherwise.\n    \"\"\"\n    if isinstance(data, pd.DataFrame):\n        return data.isnull().values.any()\n    elif isinstance(data, np.ndarray):\n        return np.isnan(data).any()\n    else:\n        raise ValueError(\"Unsupported data type\")\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/estimator/#tpot.tpot_estimator.estimator.check_if_y_is_encoded","title":"<code>check_if_y_is_encoded(y)</code>","text":"<p>Checks if the target y is composed of sequential ints from 0 to N. XGBoost requires the target to be encoded in this way.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <p>The target vector.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the target is encoded as sequential ints from 0 to N, False otherwise</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def check_if_y_is_encoded(y):\n    '''\n    Checks if the target y is composed of sequential ints from 0 to N.\n    XGBoost requires the target to be encoded in this way.\n\n    Parameters\n    ----------\n    y: np.ndarray\n        The target vector.\n\n    Returns\n    -------\n    bool\n        True if the target is encoded as sequential ints from 0 to N, False otherwise\n    '''\n    y = sorted(set(y))\n    return all(i == j for i, j in enumerate(y))\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/estimator/#tpot.tpot_estimator.estimator.convert_parents_tuples_to_integers","title":"<code>convert_parents_tuples_to_integers(row, object_to_int)</code>","text":"<p>Helper function to convert the parent rows into integers representing the index of the parent in the population.</p> <p>Original pandas dataframe using a custom index for the parents. This function converts the custom index to an integer index for easier manipulation by end users.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <p>The row to convert.</p> required <code>object_to_int</code> <p>A dictionary mapping the object to an integer index.</p> required Returns <p>tuple     The row with the custom index converted to an integer index.</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def convert_parents_tuples_to_integers(row, object_to_int):\n    \"\"\"\n    Helper function to convert the parent rows into integers representing the index of the parent in the population.\n\n    Original pandas dataframe using a custom index for the parents. This function converts the custom index to an integer index for easier manipulation by end users.\n\n    Parameters\n    ----------\n    row: list, np.ndarray, tuple\n        The row to convert.\n    object_to_int: dict\n        A dictionary mapping the object to an integer index.\n\n    Returns \n    -------\n    tuple\n        The row with the custom index converted to an integer index.\n    \"\"\"\n    if type(row) == list or type(row) == np.ndarray or type(row) == tuple:\n        return tuple(object_to_int[obj] for obj in row)\n    else:\n        return np.nan\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/estimator/#tpot.tpot_estimator.estimator.cross_val_score_objective","title":"<code>cross_val_score_objective(estimator, X, y, scorers, cv, fold=None)</code>","text":"<p>Compute the cross validated scores for a estimator. Only fits the estimator once per fold, and loops over the scorers to evaluate the estimator.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <p>The estimator to fit and score.</p> required <code>X</code> <p>The feature matrix.</p> required <code>y</code> <p>The target vector.</p> required <code>scorers</code> <p>The scorers to use.  If a list, will loop over the scorers and return a list of scorers. If a single scorer, will return a single score.</p> required <code>cv</code> <p>The cross-validator to use. For example, sklearn.model_selection.KFold or sklearn.model_selection.StratifiedKFold.</p> required <code>fold</code> <p>The fold to return the scores for. If None, will return the mean of all the scores (per scorer). Default is None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>scores</code> <code>ndarray or float</code> <p>The scores for the estimator per scorer. If fold is None, will return the mean of all the scores (per scorer). Returns a list if multiple scorers are used, otherwise returns a float for the single scorer.</p> Source code in <code>tpot/tpot_estimator/cross_val_utils.py</code> <pre><code>def cross_val_score_objective(estimator, X, y, scorers, cv, fold=None):\n    \"\"\"\n    Compute the cross validated scores for a estimator. Only fits the estimator once per fold, and loops over the scorers to evaluate the estimator.\n\n    Parameters\n    ----------\n    estimator: sklearn.base.BaseEstimator\n        The estimator to fit and score.\n    X: np.ndarray or pd.DataFrame\n        The feature matrix.\n    y: np.ndarray or pd.Series\n        The target vector.\n    scorers: list or scorer\n        The scorers to use. \n        If a list, will loop over the scorers and return a list of scorers.\n        If a single scorer, will return a single score.\n    cv: sklearn cross-validator\n        The cross-validator to use. For example, sklearn.model_selection.KFold or sklearn.model_selection.StratifiedKFold.\n    fold: int, optional\n        The fold to return the scores for. If None, will return the mean of all the scores (per scorer). Default is None.\n\n    Returns\n    -------\n    scores: np.ndarray or float\n        The scores for the estimator per scorer. If fold is None, will return the mean of all the scores (per scorer).\n        Returns a list if multiple scorers are used, otherwise returns a float for the single scorer.\n\n    \"\"\"\n\n    #check if scores is not iterable\n    if not isinstance(scorers, Iterable): \n        scorers = [scorers]\n    scores = []\n    if fold is None:\n        for train_index, test_index in cv.split(X, y):\n            this_fold_estimator = sklearn.base.clone(estimator)\n            if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n            else:\n                X_train, X_test = X[train_index], X[test_index]\n\n            if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n            else:\n                y_train, y_test = y[train_index], y[test_index]\n\n\n            start = time.time()\n            this_fold_estimator.fit(X_train,y_train)\n            duration = time.time() - start\n\n            this_fold_scores = [sklearn.metrics.get_scorer(scorer)(this_fold_estimator, X_test, y_test) for scorer in scorers] \n            scores.append(this_fold_scores)\n            del this_fold_estimator\n            del X_train\n            del X_test\n            del y_train\n            del y_test\n\n\n        return np.mean(scores,0)\n    else:\n        this_fold_estimator = sklearn.base.clone(estimator)\n        train_index, test_index = list(cv.split(X, y))[fold]\n        if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        else:\n            X_train, X_test = X[train_index], X[test_index]\n\n        if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n        else:\n            y_train, y_test = y[train_index], y[test_index]\n\n        start = time.time()\n        this_fold_estimator.fit(X_train,y_train)\n        duration = time.time() - start\n        this_fold_scores = [sklearn.metrics.get_scorer(scorer)(this_fold_estimator, X_test, y_test) for scorer in scorers] \n        return this_fold_scores\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/estimator/#tpot.tpot_estimator.estimator.objective_function_generator","title":"<code>objective_function_generator(pipeline, x, y, scorers, cv, other_objective_functions, step=None, budget=None, is_classification=True, export_graphpipeline=False, **pipeline_kwargs)</code>","text":"<p>Uses cross validation to evaluate the pipeline using the scorers, and concatenates results with scores from standalone other objective functions.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <p>The individual to evaluate.</p> required <code>x</code> <p>The feature matrix.</p> required <code>y</code> <p>The target vector.</p> required <code>scorers</code> <p>The scorers to use for cross validation.</p> required <code>cv</code> <p>The cross-validator to use. For example, sklearn.model_selection.KFold or sklearn.model_selection.StratifiedKFold. If an int, will use sklearn.model_selection.KFold with n_splits=cv.</p> required <code>other_objective_functions</code> <p>A list of standalone objective functions to evaluate the pipeline. With signature obj(pipeline) -&gt; float. or obj(pipeline) -&gt; np.ndarray These functions take in the unfitted estimator.</p> required <code>step</code> <p>The fold to return the scores for. If None, will return the mean of all the scores (per scorer). Default is None.</p> <code>None</code> <code>budget</code> <p>The budget to subsample the data. If None, will use the full dataset. Default is None. Will subsample budget*len(x) samples.</p> <code>None</code> <code>is_classification</code> <p>If True, will stratify the subsampling. Default is True.</p> <code>True</code> <code>export_graphpipeline</code> <p>Force the pipeline to be exported as a graph pipeline. Flattens all nested sklearn pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.</p> <code>False</code> <code>pipeline_kwargs</code> <p>Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The concatenated scores for the pipeline. The first len(scorers) elements are the cross validation scores, and the remaining elements are the standalone objective functions.</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def objective_function_generator(pipeline, x,y, scorers, cv, other_objective_functions, step=None, budget=None, is_classification=True, export_graphpipeline=False, **pipeline_kwargs):\n    \"\"\"\n    Uses cross validation to evaluate the pipeline using the scorers, and concatenates results with scores from standalone other objective functions.\n\n    Parameters\n    ----------\n    pipeline: tpot.SklearnIndividual\n        The individual to evaluate.\n    x: np.ndarray\n        The feature matrix.\n    y: np.ndarray\n        The target vector.\n    scorers: list\n        The scorers to use for cross validation. \n    cv: int, float, or sklearn cross-validator\n        The cross-validator to use. For example, sklearn.model_selection.KFold or sklearn.model_selection.StratifiedKFold.\n        If an int, will use sklearn.model_selection.KFold with n_splits=cv.\n    other_objective_functions: list\n        A list of standalone objective functions to evaluate the pipeline. With signature obj(pipeline) -&gt; float. or obj(pipeline) -&gt; np.ndarray\n        These functions take in the unfitted estimator.\n    step: int, optional\n        The fold to return the scores for. If None, will return the mean of all the scores (per scorer). Default is None.\n    budget: float, optional\n        The budget to subsample the data. If None, will use the full dataset. Default is None.\n        Will subsample budget*len(x) samples.\n    is_classification: bool, default=True\n        If True, will stratify the subsampling. Default is True.\n    export_graphpipeline: bool, default=False\n        Force the pipeline to be exported as a graph pipeline. Flattens all nested sklearn pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.\n    pipeline_kwargs: dict\n        Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.\n\n    Returns\n    -------\n    np.ndarray\n        The concatenated scores for the pipeline. The first len(scorers) elements are the cross validation scores, and the remaining elements are the standalone objective functions.\n\n    \"\"\"\n\n    if export_graphpipeline:\n        pipeline = pipeline.export_flattened_graphpipeline(**pipeline_kwargs)\n    else:\n        pipeline = pipeline.export_pipeline(**pipeline_kwargs)\n\n    if budget is not None and budget &lt; 1:\n        if is_classification:\n            x,y = sklearn.utils.resample(x,y, stratify=y, n_samples=int(budget*len(x)), replace=False, random_state=1)\n        else:\n            x,y = sklearn.utils.resample(x,y, n_samples=int(budget*len(x)), replace=False, random_state=1)\n\n        if isinstance(cv, int) or isinstance(cv, float):\n            n_splits = cv\n        else:\n            n_splits = cv.n_splits\n\n    if len(scorers) &gt; 0:\n        cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)\n    else:\n        cv_obj_scores = []\n\n    if other_objective_functions is not None and len(other_objective_functions) &gt;0:\n        other_scores = [obj(sklearn.base.clone(pipeline)) for obj in other_objective_functions]\n        #flatten\n        other_scores = np.array(other_scores).flatten().tolist()\n    else:\n        other_scores = []\n\n    return np.concatenate([cv_obj_scores,other_scores])\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/estimator/#tpot.tpot_estimator.estimator.remove_underrepresented_classes","title":"<code>remove_underrepresented_classes(x, y, min_count)</code>","text":"<p>Helper function to remove classes with less than min_count samples from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>The feature matrix.</p> required <code>y</code> <p>The target vector.</p> required <code>min_count</code> <p>The minimum number of samples to keep a class.</p> required <p>Returns:</p> Type Description <code>(ndarray, ndarray)</code> <p>The feature matrix and target vector with rows from classes with less than min_count samples removed.</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def remove_underrepresented_classes(x, y, min_count):\n    \"\"\"\n    Helper function to remove classes with less than min_count samples from the dataset.\n\n    Parameters\n    ----------\n    x: np.ndarray or pd.DataFrame\n        The feature matrix.\n    y: np.ndarray or pd.Series\n        The target vector.\n    min_count: int\n        The minimum number of samples to keep a class.\n\n    Returns\n    -------\n    np.ndarray, np.ndarray\n        The feature matrix and target vector with rows from classes with less than min_count samples removed.\n    \"\"\"\n    if isinstance(y, (np.ndarray, pd.Series)):\n        unique, counts = np.unique(y, return_counts=True)\n        if min(counts) &gt;= min_count:\n            return x, y\n        keep_classes = unique[counts &gt;= min_count]\n        mask = np.isin(y, keep_classes)\n        x = x[mask]\n        y = y[mask]\n    elif isinstance(y, pd.DataFrame):\n        counts = y.apply(pd.Series.value_counts)\n        if min(counts) &gt;= min_count:\n            return x, y\n        keep_classes = counts.index[counts &gt;= min_count].tolist()\n        mask = y.isin(keep_classes).all(axis=1)\n        x = x[mask]\n        y = y[mask]\n    else:\n        raise TypeError(\"y must be a numpy array or a pandas Series/DataFrame\")\n    return x, y\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/estimator/#tpot.tpot_estimator.estimator.val_objective_function_generator","title":"<code>val_objective_function_generator(pipeline, X_train, y_train, X_test, y_test, scorers, other_objective_functions, export_graphpipeline=False, **pipeline_kwargs)</code>","text":"<p>Trains a pipeline on a training set and evaluates it on a test set using the scorers and other objective functions.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <p>The individual to evaluate.</p> required <code>X_train</code> <p>The feature matrix of the training set.</p> required <code>y_train</code> <p>The target vector of the training set.</p> required <code>X_test</code> <p>The feature matrix of the test set.</p> required <code>y_test</code> <p>The target vector of the test set.</p> required <code>scorers</code> <p>The scorers to use for cross validation.</p> required <code>other_objective_functions</code> <p>A list of standalone objective functions to evaluate the pipeline. With signature obj(pipeline) -&gt; float. or obj(pipeline) -&gt; np.ndarray These functions take in the unfitted estimator.</p> required <code>export_graphpipeline</code> <p>Force the pipeline to be exported as a graph pipeline. Flattens all nested sklearn pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.</p> <code>False</code> <code>pipeline_kwargs</code> <p>Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The concatenated scores for the pipeline. The first len(scorers) elements are the cross validation scores, and the remaining elements are the standalone objective functions.</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def val_objective_function_generator(pipeline, X_train, y_train, X_test, y_test, scorers, other_objective_functions, export_graphpipeline=False, **pipeline_kwargs):\n    \"\"\"\n    Trains a pipeline on a training set and evaluates it on a test set using the scorers and other objective functions.\n\n    Parameters\n    ----------\n\n    pipeline: tpot.SklearnIndividual\n        The individual to evaluate.\n    X_train: np.ndarray\n        The feature matrix of the training set.\n    y_train: np.ndarray\n        The target vector of the training set.\n    X_test: np.ndarray\n        The feature matrix of the test set.\n    y_test: np.ndarray\n        The target vector of the test set.\n    scorers: list\n        The scorers to use for cross validation.\n    other_objective_functions: list\n        A list of standalone objective functions to evaluate the pipeline. With signature obj(pipeline) -&gt; float. or obj(pipeline) -&gt; np.ndarray\n        These functions take in the unfitted estimator.\n    export_graphpipeline: bool, default=False\n        Force the pipeline to be exported as a graph pipeline. Flattens all nested sklearn pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.\n    pipeline_kwargs: dict\n        Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.\n\n    Returns\n    -------\n    np.ndarray\n        The concatenated scores for the pipeline. The first len(scorers) elements are the cross validation scores, and the remaining elements are the standalone objective functions.\n\n\n    \"\"\"\n\n    #subsample the data\n    if export_graphpipeline:\n        pipeline = pipeline.export_flattened_graphpipeline(**pipeline_kwargs)\n    else:\n        pipeline = pipeline.export_pipeline(**pipeline_kwargs)\n\n    fitted_pipeline = sklearn.base.clone(pipeline)\n    fitted_pipeline.fit(X_train, y_train)\n\n    if len(scorers) &gt; 0:\n        scores =[sklearn.metrics.get_scorer(scorer)(fitted_pipeline, X_test, y_test) for scorer in scorers]\n\n    other_scores = []\n    if other_objective_functions is not None and len(other_objective_functions) &gt;0:\n        other_scores = [obj(sklearn.base.clone(pipeline)) for obj in other_objective_functions]\n\n    return np.concatenate([scores,other_scores])\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/estimator_utils/","title":"Estimator utils","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/tpot_estimator/estimator_utils/#tpot.tpot_estimator.estimator_utils.apply_make_pipeline","title":"<code>apply_make_pipeline(ind, preprocessing_pipeline=None, export_graphpipeline=False, **pipeline_kwargs)</code>","text":"<p>Helper function to create a column of sklearn pipelines from the tpot individual class.</p> <p>Parameters:</p> Name Type Description Default <code>ind</code> <p>The individual to convert to a pipeline.</p> required <code>preprocessing_pipeline</code> <p>The preprocessing pipeline to include before the individual's pipeline.</p> <code>None</code> <code>export_graphpipeline</code> <p>Force the pipeline to be exported as a graph pipeline. Flattens all nested pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.</p> <code>False</code> <code>pipeline_kwargs</code> <p>Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>sklearn estimator</code> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def apply_make_pipeline(ind, preprocessing_pipeline=None, export_graphpipeline=False, **pipeline_kwargs):\n    \"\"\"\n    Helper function to create a column of sklearn pipelines from the tpot individual class.\n\n    Parameters\n    ----------\n    ind: tpot.SklearnIndividual\n        The individual to convert to a pipeline.\n    preprocessing_pipeline: sklearn.pipeline.Pipeline, optional\n        The preprocessing pipeline to include before the individual's pipeline.\n    export_graphpipeline: bool, default=False\n        Force the pipeline to be exported as a graph pipeline. Flattens all nested pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.\n    pipeline_kwargs: dict\n        Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.\n\n    Returns\n    -------\n    sklearn estimator\n    \"\"\"\n\n    try:\n\n        if export_graphpipeline:\n            est = ind.export_flattened_graphpipeline(**pipeline_kwargs)\n        else:\n            est = ind.export_pipeline(**pipeline_kwargs)\n\n\n        if preprocessing_pipeline is None:\n            return est\n        else:\n            return sklearn.pipeline.make_pipeline(sklearn.base.clone(preprocessing_pipeline), est)\n    except:\n        return None\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/estimator_utils/#tpot.tpot_estimator.estimator_utils.check_if_y_is_encoded","title":"<code>check_if_y_is_encoded(y)</code>","text":"<p>Checks if the target y is composed of sequential ints from 0 to N. XGBoost requires the target to be encoded in this way.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <p>The target vector.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the target is encoded as sequential ints from 0 to N, False otherwise</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def check_if_y_is_encoded(y):\n    '''\n    Checks if the target y is composed of sequential ints from 0 to N.\n    XGBoost requires the target to be encoded in this way.\n\n    Parameters\n    ----------\n    y: np.ndarray\n        The target vector.\n\n    Returns\n    -------\n    bool\n        True if the target is encoded as sequential ints from 0 to N, False otherwise\n    '''\n    y = sorted(set(y))\n    return all(i == j for i, j in enumerate(y))\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/estimator_utils/#tpot.tpot_estimator.estimator_utils.convert_parents_tuples_to_integers","title":"<code>convert_parents_tuples_to_integers(row, object_to_int)</code>","text":"<p>Helper function to convert the parent rows into integers representing the index of the parent in the population.</p> <p>Original pandas dataframe using a custom index for the parents. This function converts the custom index to an integer index for easier manipulation by end users.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <p>The row to convert.</p> required <code>object_to_int</code> <p>A dictionary mapping the object to an integer index.</p> required Returns <p>tuple     The row with the custom index converted to an integer index.</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def convert_parents_tuples_to_integers(row, object_to_int):\n    \"\"\"\n    Helper function to convert the parent rows into integers representing the index of the parent in the population.\n\n    Original pandas dataframe using a custom index for the parents. This function converts the custom index to an integer index for easier manipulation by end users.\n\n    Parameters\n    ----------\n    row: list, np.ndarray, tuple\n        The row to convert.\n    object_to_int: dict\n        A dictionary mapping the object to an integer index.\n\n    Returns \n    -------\n    tuple\n        The row with the custom index converted to an integer index.\n    \"\"\"\n    if type(row) == list or type(row) == np.ndarray or type(row) == tuple:\n        return tuple(object_to_int[obj] for obj in row)\n    else:\n        return np.nan\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/estimator_utils/#tpot.tpot_estimator.estimator_utils.objective_function_generator","title":"<code>objective_function_generator(pipeline, x, y, scorers, cv, other_objective_functions, step=None, budget=None, is_classification=True, export_graphpipeline=False, **pipeline_kwargs)</code>","text":"<p>Uses cross validation to evaluate the pipeline using the scorers, and concatenates results with scores from standalone other objective functions.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <p>The individual to evaluate.</p> required <code>x</code> <p>The feature matrix.</p> required <code>y</code> <p>The target vector.</p> required <code>scorers</code> <p>The scorers to use for cross validation.</p> required <code>cv</code> <p>The cross-validator to use. For example, sklearn.model_selection.KFold or sklearn.model_selection.StratifiedKFold. If an int, will use sklearn.model_selection.KFold with n_splits=cv.</p> required <code>other_objective_functions</code> <p>A list of standalone objective functions to evaluate the pipeline. With signature obj(pipeline) -&gt; float. or obj(pipeline) -&gt; np.ndarray These functions take in the unfitted estimator.</p> required <code>step</code> <p>The fold to return the scores for. If None, will return the mean of all the scores (per scorer). Default is None.</p> <code>None</code> <code>budget</code> <p>The budget to subsample the data. If None, will use the full dataset. Default is None. Will subsample budget*len(x) samples.</p> <code>None</code> <code>is_classification</code> <p>If True, will stratify the subsampling. Default is True.</p> <code>True</code> <code>export_graphpipeline</code> <p>Force the pipeline to be exported as a graph pipeline. Flattens all nested sklearn pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.</p> <code>False</code> <code>pipeline_kwargs</code> <p>Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The concatenated scores for the pipeline. The first len(scorers) elements are the cross validation scores, and the remaining elements are the standalone objective functions.</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def objective_function_generator(pipeline, x,y, scorers, cv, other_objective_functions, step=None, budget=None, is_classification=True, export_graphpipeline=False, **pipeline_kwargs):\n    \"\"\"\n    Uses cross validation to evaluate the pipeline using the scorers, and concatenates results with scores from standalone other objective functions.\n\n    Parameters\n    ----------\n    pipeline: tpot.SklearnIndividual\n        The individual to evaluate.\n    x: np.ndarray\n        The feature matrix.\n    y: np.ndarray\n        The target vector.\n    scorers: list\n        The scorers to use for cross validation. \n    cv: int, float, or sklearn cross-validator\n        The cross-validator to use. For example, sklearn.model_selection.KFold or sklearn.model_selection.StratifiedKFold.\n        If an int, will use sklearn.model_selection.KFold with n_splits=cv.\n    other_objective_functions: list\n        A list of standalone objective functions to evaluate the pipeline. With signature obj(pipeline) -&gt; float. or obj(pipeline) -&gt; np.ndarray\n        These functions take in the unfitted estimator.\n    step: int, optional\n        The fold to return the scores for. If None, will return the mean of all the scores (per scorer). Default is None.\n    budget: float, optional\n        The budget to subsample the data. If None, will use the full dataset. Default is None.\n        Will subsample budget*len(x) samples.\n    is_classification: bool, default=True\n        If True, will stratify the subsampling. Default is True.\n    export_graphpipeline: bool, default=False\n        Force the pipeline to be exported as a graph pipeline. Flattens all nested sklearn pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.\n    pipeline_kwargs: dict\n        Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.\n\n    Returns\n    -------\n    np.ndarray\n        The concatenated scores for the pipeline. The first len(scorers) elements are the cross validation scores, and the remaining elements are the standalone objective functions.\n\n    \"\"\"\n\n    if export_graphpipeline:\n        pipeline = pipeline.export_flattened_graphpipeline(**pipeline_kwargs)\n    else:\n        pipeline = pipeline.export_pipeline(**pipeline_kwargs)\n\n    if budget is not None and budget &lt; 1:\n        if is_classification:\n            x,y = sklearn.utils.resample(x,y, stratify=y, n_samples=int(budget*len(x)), replace=False, random_state=1)\n        else:\n            x,y = sklearn.utils.resample(x,y, n_samples=int(budget*len(x)), replace=False, random_state=1)\n\n        if isinstance(cv, int) or isinstance(cv, float):\n            n_splits = cv\n        else:\n            n_splits = cv.n_splits\n\n    if len(scorers) &gt; 0:\n        cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)\n    else:\n        cv_obj_scores = []\n\n    if other_objective_functions is not None and len(other_objective_functions) &gt;0:\n        other_scores = [obj(sklearn.base.clone(pipeline)) for obj in other_objective_functions]\n        #flatten\n        other_scores = np.array(other_scores).flatten().tolist()\n    else:\n        other_scores = []\n\n    return np.concatenate([cv_obj_scores,other_scores])\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/estimator_utils/#tpot.tpot_estimator.estimator_utils.remove_underrepresented_classes","title":"<code>remove_underrepresented_classes(x, y, min_count)</code>","text":"<p>Helper function to remove classes with less than min_count samples from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>The feature matrix.</p> required <code>y</code> <p>The target vector.</p> required <code>min_count</code> <p>The minimum number of samples to keep a class.</p> required <p>Returns:</p> Type Description <code>(ndarray, ndarray)</code> <p>The feature matrix and target vector with rows from classes with less than min_count samples removed.</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def remove_underrepresented_classes(x, y, min_count):\n    \"\"\"\n    Helper function to remove classes with less than min_count samples from the dataset.\n\n    Parameters\n    ----------\n    x: np.ndarray or pd.DataFrame\n        The feature matrix.\n    y: np.ndarray or pd.Series\n        The target vector.\n    min_count: int\n        The minimum number of samples to keep a class.\n\n    Returns\n    -------\n    np.ndarray, np.ndarray\n        The feature matrix and target vector with rows from classes with less than min_count samples removed.\n    \"\"\"\n    if isinstance(y, (np.ndarray, pd.Series)):\n        unique, counts = np.unique(y, return_counts=True)\n        if min(counts) &gt;= min_count:\n            return x, y\n        keep_classes = unique[counts &gt;= min_count]\n        mask = np.isin(y, keep_classes)\n        x = x[mask]\n        y = y[mask]\n    elif isinstance(y, pd.DataFrame):\n        counts = y.apply(pd.Series.value_counts)\n        if min(counts) &gt;= min_count:\n            return x, y\n        keep_classes = counts.index[counts &gt;= min_count].tolist()\n        mask = y.isin(keep_classes).all(axis=1)\n        x = x[mask]\n        y = y[mask]\n    else:\n        raise TypeError(\"y must be a numpy array or a pandas Series/DataFrame\")\n    return x, y\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/estimator_utils/#tpot.tpot_estimator.estimator_utils.val_objective_function_generator","title":"<code>val_objective_function_generator(pipeline, X_train, y_train, X_test, y_test, scorers, other_objective_functions, export_graphpipeline=False, **pipeline_kwargs)</code>","text":"<p>Trains a pipeline on a training set and evaluates it on a test set using the scorers and other objective functions.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <p>The individual to evaluate.</p> required <code>X_train</code> <p>The feature matrix of the training set.</p> required <code>y_train</code> <p>The target vector of the training set.</p> required <code>X_test</code> <p>The feature matrix of the test set.</p> required <code>y_test</code> <p>The target vector of the test set.</p> required <code>scorers</code> <p>The scorers to use for cross validation.</p> required <code>other_objective_functions</code> <p>A list of standalone objective functions to evaluate the pipeline. With signature obj(pipeline) -&gt; float. or obj(pipeline) -&gt; np.ndarray These functions take in the unfitted estimator.</p> required <code>export_graphpipeline</code> <p>Force the pipeline to be exported as a graph pipeline. Flattens all nested sklearn pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.</p> <code>False</code> <code>pipeline_kwargs</code> <p>Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The concatenated scores for the pipeline. The first len(scorers) elements are the cross validation scores, and the remaining elements are the standalone objective functions.</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def val_objective_function_generator(pipeline, X_train, y_train, X_test, y_test, scorers, other_objective_functions, export_graphpipeline=False, **pipeline_kwargs):\n    \"\"\"\n    Trains a pipeline on a training set and evaluates it on a test set using the scorers and other objective functions.\n\n    Parameters\n    ----------\n\n    pipeline: tpot.SklearnIndividual\n        The individual to evaluate.\n    X_train: np.ndarray\n        The feature matrix of the training set.\n    y_train: np.ndarray\n        The target vector of the training set.\n    X_test: np.ndarray\n        The feature matrix of the test set.\n    y_test: np.ndarray\n        The target vector of the test set.\n    scorers: list\n        The scorers to use for cross validation.\n    other_objective_functions: list\n        A list of standalone objective functions to evaluate the pipeline. With signature obj(pipeline) -&gt; float. or obj(pipeline) -&gt; np.ndarray\n        These functions take in the unfitted estimator.\n    export_graphpipeline: bool, default=False\n        Force the pipeline to be exported as a graph pipeline. Flattens all nested sklearn pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.\n    pipeline_kwargs: dict\n        Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.\n\n    Returns\n    -------\n    np.ndarray\n        The concatenated scores for the pipeline. The first len(scorers) elements are the cross validation scores, and the remaining elements are the standalone objective functions.\n\n\n    \"\"\"\n\n    #subsample the data\n    if export_graphpipeline:\n        pipeline = pipeline.export_flattened_graphpipeline(**pipeline_kwargs)\n    else:\n        pipeline = pipeline.export_pipeline(**pipeline_kwargs)\n\n    fitted_pipeline = sklearn.base.clone(pipeline)\n    fitted_pipeline.fit(X_train, y_train)\n\n    if len(scorers) &gt; 0:\n        scores =[sklearn.metrics.get_scorer(scorer)(fitted_pipeline, X_test, y_test) for scorer in scorers]\n\n    other_scores = []\n    if other_objective_functions is not None and len(other_objective_functions) &gt;0:\n        other_scores = [obj(sklearn.base.clone(pipeline)) for obj in other_objective_functions]\n\n    return np.concatenate([scores,other_scores])\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/steady_state_estimator/","title":"Steady state estimator","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/tpot_estimator/steady_state_estimator/#tpot.tpot_estimator.steady_state_estimator.TPOTEstimatorSteadyState","title":"<code>TPOTEstimatorSteadyState</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> Source code in <code>tpot/tpot_estimator/steady_state_estimator.py</code> <pre><code>class TPOTEstimatorSteadyState(BaseEstimator):\n    def __init__(self,  \n                        search_space,\n                        scorers= [],\n                        scorers_weights = [],\n                        classification = False,\n                        cv = 10,\n                        other_objective_functions=[], #tpot.objectives.estimator_objective_functions.number_of_nodes_objective],\n                        other_objective_functions_weights = [],\n                        objective_function_names = None,\n                        bigger_is_better = True,\n\n\n                        export_graphpipeline = False,\n                        memory = None,\n\n                        categorical_features = None,\n                        subsets = None,\n                        preprocessing = False,\n                        validation_strategy = \"none\",\n                        validation_fraction = .2,\n                        disable_label_encoder = False,\n\n                        initial_population_size = 50,\n                        population_size = 50,\n                        max_evaluated_individuals = None,\n\n\n\n                        early_stop = None,\n                        early_stop_mins = None,\n                        scorers_early_stop_tol = 0.001,\n                        other_objectives_early_stop_tol = None,\n                        max_time_mins=None,\n                        max_eval_time_mins=10,\n                        n_jobs=1,\n                        memory_limit = None,\n                        client = None,\n\n                        crossover_probability=.2,\n                        mutate_probability=.7,\n                        mutate_then_crossover_probability=.05,\n                        crossover_then_mutate_probability=.05,\n                        survival_selector = survival_select_NSGA2,\n                        parent_selector = tournament_selection_dominated,\n                        budget_range = None,\n                        budget_scaling = .5,\n                        individuals_until_end_budget = 1,\n                        stepwise_steps = 5,\n\n                        warm_start = False,\n\n                        verbose = 0,\n                        periodic_checkpoint_folder = None,\n                        callback = None,\n                        processes = True,\n\n                        scatter = True,\n\n                        # random seed for random number generator (rng)\n                        random_state = None,\n\n                        optuna_optimize_pareto_front = False,\n                        optuna_optimize_pareto_front_trials = 100,\n                        optuna_optimize_pareto_front_timeout = 60*10,\n                        optuna_storage = \"sqlite:///optuna.db\",\n                        ):\n\n        '''\n        An sklearn baseestimator that uses genetic programming to optimize a pipeline.\n\n        Parameters\n        ----------\n\n        scorers : (list, scorer)\n            A scorer or list of scorers to be used in the cross-validation process.\n            see https://scikit-learn.org/stable/modules/model_evaluation.html\n\n        scorers_weights : list\n            A list of weights to be applied to the scorers during the optimization process.\n\n        classification : bool\n            If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem.\n            Used to determine the CV strategy.\n\n        cv : int, cross-validator\n            - (int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.\n            - (sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.\n\n        other_objective_functions : list, default=[]\n            A list of other objective functions to apply to the pipeline. The function takes a single parameter for the graphpipeline estimator and returns either a single score or a list of scores.\n\n        other_objective_functions_weights : list, default=[]\n            A list of weights to be applied to the other objective functions.\n\n        objective_function_names : list, default=None\n            A list of names to be applied to the objective functions. If None, will use the names of the objective functions.\n\n        bigger_is_better : bool, default=True\n            If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n\n\n        max_size : int, default=np.inf\n            The maximum number of nodes of the pipelines to be generated.\n\n        linear_pipeline : bool, default=False\n            If True, the pipelines generated will be linear. If False, the pipelines generated will be directed acyclic graphs.\n\n        root_config_dict : dict, default='auto'\n            The configuration dictionary to use for the root node of the model.\n            If 'auto', will use \"classifiers\" if classification=True, else \"regressors\".\n            - 'selectors' : A selection of sklearn Selector methods.\n            - 'classifiers' : A selection of sklearn Classifier methods.\n            - 'regressors' : A selection of sklearn Regressor methods.\n            - 'transformers' : A selection of sklearn Transformer methods.\n            - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.\n            - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.\n            - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.\n                                        Subsets are set with the subsets parameter.\n            - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.\n            - 'MDR' : Includes MDR.\n            - 'ContinuousMDR' : Includes ContinuousMDR.\n            - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.\n            - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.\n            - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.\n\n        inner_config_dict : dict, default=[\"selectors\", \"transformers\"]\n            The configuration dictionary to use for the inner nodes of the model generation.\n            Default [\"selectors\", \"transformers\"]\n            - 'selectors' : A selection of sklearn Selector methods.\n            - 'classifiers' : A selection of sklearn Classifier methods.\n            - 'regressors' : A selection of sklearn Regressor methods.\n            - 'transformers' : A selection of sklearn Transformer methods.\n            - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.\n            - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.\n            - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.\n                                        Subsets are set with the subsets parameter.\n            - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.\n            - 'MDR' : Includes MDR.\n            - 'ContinuousMDR' : Includes ContinuousMDR.\n            - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.\n            - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.\n            - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.\n            - None : If None and max_depth&gt;1, the root_config_dict will be used for the inner nodes as well.\n\n        leaf_config_dict : dict, default=None\n            The configuration dictionary to use for the leaf node of the model. If set, leaf nodes must be from this dictionary.\n            Otherwise leaf nodes will be generated from the root_config_dict.\n            Default None\n            - 'selectors' : A selection of sklearn Selector methods.\n            - 'classifiers' : A selection of sklearn Classifier methods.\n            - 'regressors' : A selection of sklearn Regressor methods.\n            - 'transformers' : A selection of sklearn Transformer methods.\n            - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.\n            - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.\n            - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.\n                                        Subsets are set with the subsets parameter.\n            - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.\n            - 'MDR' : Includes MDR.\n            - 'ContinuousMDR' : Includes ContinuousMDR.\n            - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.\n            - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.\n            - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.\n            - None : If None, a leaf will not be required (i.e. the pipeline can be a single root node). Leaf nodes will be generated from the inner_config_dict.\n\n        categorical_features: list or None\n            Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n            - None : If None, TPOT will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing.\n            - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices\n\n\n        memory: Memory object or string, default=None\n            If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. This feature\n            is used to avoid computing the fit transformers within a pipeline if the parameters\n            and input data are identical with another fitted pipeline during optimization process.\n            - String 'auto':\n                TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.\n            - String path of a caching directory\n                TPOT uses memory caching with the provided directory and TPOT does NOT clean\n                the caching directory up upon shutdown. If the directory does not exist, TPOT will\n                create it.\n            - Memory object:\n                TPOT uses the instance of joblib.Memory for memory caching,\n                and TPOT does NOT clean the caching directory up upon shutdown.\n            - None:\n                TPOT does not use memory caching.\n\n        preprocessing : bool or BaseEstimator/Pipeline,\n            EXPERIMENTAL\n            A pipeline that will be used to preprocess the data before CV.\n            - bool : If True, will use a default preprocessing pipeline.\n            - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.\n\n        validation_strategy : str, default='none'\n            EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT may overfit the cross validation score. A second validation set can be used to select the final pipeline.\n            - 'auto' : Automatically determine the validation strategy based on the dataset shape.\n            - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets.\n            - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets.\n            - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.\n\n        validation_fraction : float, default=0.2\n          EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.\n\n        disable_label_encoder : bool, default=False\n            If True, TPOT will check if the target needs to be relabeled to be sequential ints from 0 to N. This is necessary for XGBoost compatibility. If the labels need to be encoded, TPOT will use sklearn.preprocessing.LabelEncoder to encode the labels. The encoder can be accessed via the self.label_encoder_ attribute.\n            If False, no additional label encoders will be used.\n\n        population_size : int, default=50\n            Size of the population\n\n        initial_population_size : int, default=None\n            Size of the initial population. If None, population_size will be used.\n\n        population_scaling : int, default=0.5\n            Scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n\n        generations_until_end_population : int, default=1\n            Number of generations until the population size reaches population_size\n\n        generations : int, default=50\n            Number of generations to run\n\n        early_stop : int, default=None\n            Number of evaluated individuals without improvement before early stopping. Counted across all objectives independently. Triggered when all objectives have not improved by the given number of individuals.\n\n        early_stop_mins : float, default=None\n            Number of seconds without improvement before early stopping. All objectives must not have improved for the given number of seconds for this to be triggered.\n\n        scorers_early_stop_tol :\n            -list of floats\n                list of tolerances for each scorer. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n                If an index of the list is None, that item will not be used for early stopping\n            -int\n                If an int is given, it will be used as the tolerance for all objectives\n\n        other_objectives_early_stop_tol :\n            -list of floats\n                list of tolerances for each of the other objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n                If an index of the list is None, that item will not be used for early stopping\n            -int\n                If an int is given, it will be used as the tolerance for all objectives\n\n        max_time_mins : float, default=float(\"inf\")\n            Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n\n        max_eval_time_mins : float, default=60*5\n            Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n\n        n_jobs : int, default=1\n            Number of processes to run in parallel.\n\n        memory_limit : str, default=None\n            Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n\n        client : dask.distributed.Client, default=None\n            A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.\n\n        crossover_probability : float, default=.2\n            Probability of generating a new individual by crossover between two individuals.\n\n        mutate_probability : float, default=.7\n            Probability of generating a new individual by crossover between one individuals.\n\n        mutate_then_crossover_probability : float, default=.05\n            Probability of generating a new individual by mutating two individuals followed by crossover.\n\n        crossover_then_mutate_probability : float, default=.05\n            Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.\n\n        survival_selector : function, default=survival_select_NSGA2\n            Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes.\n            Used to selected population_size individuals at the start of each generation to use for mutation and crossover.\n\n        parent_selector : function, default=parent_select_NSGA2\n            Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.\n\n        budget_range : list [start, end], default=None\n            A starting and ending budget to use for the budget scaling.\n\n        budget_scaling float : [0,1], default=0.5\n            A scaling factor to use when determining how fast we move the budget from the start to end budget.\n\n        individuals_until_end_budget : int, default=1\n            The number of generations to run before reaching the max budget.\n\n        stepwise_steps : int, default=1\n            The number of staircase steps to take when scaling the budget and population size.\n\n        threshold_evaluation_pruning : list [start, end], default=None\n            starting and ending percentile to use as a threshold for the evaluation early stopping.\n            Values between 0 and 100.\n\n        threshold_evaluation_scaling : float [0,inf), default=0.5\n            A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n            Must be greater than zero. Higher numbers will move the threshold to the end faster.\n\n        min_history_threshold : int, default=0\n            The minimum number of previous scores needed before using threshold early stopping.\n\n        selection_evaluation_pruning : list, default=None\n            A lower and upper percent of the population size to select each round of CV.\n            Values between 0 and 1.\n\n        selection_evaluation_scaling : float, default=0.5\n            A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n            Must be greater than zero. Higher numbers will move the threshold to the end faster.\n\n        n_initial_optimizations : int, default=0\n            Number of individuals to optimize before starting the evolution.\n\n        optimization_cv : int\n           Number of folds to use for the optuna optimization's internal cross-validation.\n\n        max_optimize_time_seconds : float, default=60*5\n            Maximum time to run an optimization\n\n        optimization_steps : int, default=10\n            Number of steps per optimization\n\n        warm_start : bool, default=False\n            If True, will use the continue the evolutionary algorithm from the last generation of the previous run.\n\n\n        verbose : int, default=1\n            How much information to print during the optimization process. Higher values include the information from lower values.\n            0. nothing\n            1. progress bar\n\n            3. best individual\n            4. warnings\n            &gt;=5. full warnings trace\n\n        random_state : int, None, default=None\n            A seed for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes\n            - int\n                Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'\n            - None\n                Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS\n\n\n        periodic_checkpoint_folder : str, default=None\n            Folder to save the population to periodically. If None, no periodic saving will be done.\n            If provided, training will resume from this checkpoint.\n\n        callback : tpot.CallBackInterface, default=None\n            Callback object. Not implemented\n\n        processes : bool, default=True\n            If True, will use multiprocessing to parallelize the optimization process. If False, will use threading.\n            True seems to perform better. However, False is required for interactive debugging.\n\n        Attributes\n        ----------\n\n        fitted_pipeline_ : GraphPipeline\n            A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.\n\n        evaluated_individuals : A pandas data frame containing data for all evaluated individuals in the run.\n            Columns:\n            - *objective functions : The first few columns correspond to the passed in scorers and objective functions\n            - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population.\n            - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population.\n            - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator.\n            - Generation : The generation the pipeline first appeared.\n            - Pareto_Front\t: The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.\n                            To save on computational time, the best frontier is updated iteratively each generation.\n                            The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.\n                            All other pipelines are set to NaN.\n            - Instance\t: The unfitted GraphPipeline BaseEstimator.\n            - *validation objective functions : Objective function scores evaluated on the validation set.\n            - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.\n\n        pareto_front : The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.\n        '''\n\n        # sklearn BaseEstimator must have a corresponding attribute for each parameter.\n        # These should not be modified once set.\n\n        self.search_space = search_space\n        self.scorers = scorers\n        self.scorers_weights = scorers_weights\n        self.classification = classification\n        self.cv = cv\n        self.other_objective_functions = other_objective_functions\n        self.other_objective_functions_weights = other_objective_functions_weights\n        self.objective_function_names = objective_function_names\n        self.bigger_is_better = bigger_is_better\n\n        self.export_graphpipeline = export_graphpipeline\n        self.memory = memory\n\n        self.categorical_features = categorical_features\n        self.preprocessing = preprocessing\n        self.validation_strategy = validation_strategy\n        self.validation_fraction = validation_fraction\n        self.disable_label_encoder = disable_label_encoder\n        self.population_size = population_size\n        self.initial_population_size = initial_population_size\n\n        self.early_stop = early_stop\n        self.early_stop_mins = early_stop_mins\n        self.scorers_early_stop_tol = scorers_early_stop_tol\n        self.other_objectives_early_stop_tol = other_objectives_early_stop_tol\n        self.max_time_mins = max_time_mins\n        self.max_eval_time_mins = max_eval_time_mins\n        self.n_jobs= n_jobs\n        self.memory_limit = memory_limit\n        self.client = client\n\n        self.crossover_probability = crossover_probability\n        self.mutate_probability = mutate_probability\n        self.mutate_then_crossover_probability= mutate_then_crossover_probability\n        self.crossover_then_mutate_probability= crossover_then_mutate_probability\n        self.survival_selector=survival_selector\n        self.parent_selector=parent_selector\n        self.budget_range = budget_range\n        self.budget_scaling = budget_scaling\n        self.individuals_until_end_budget = individuals_until_end_budget\n        self.stepwise_steps = stepwise_steps\n\n        self.warm_start = warm_start\n\n        self.verbose = verbose\n        self.periodic_checkpoint_folder = periodic_checkpoint_folder\n        self.callback = callback\n        self.processes = processes\n\n\n        self.scatter = scatter\n\n        self.optuna_optimize_pareto_front = optuna_optimize_pareto_front\n        self.optuna_optimize_pareto_front_trials = optuna_optimize_pareto_front_trials\n        self.optuna_optimize_pareto_front_timeout = optuna_optimize_pareto_front_timeout\n        self.optuna_storage = optuna_storage\n\n        # create random number generator based on rngseed\n        self.rng = np.random.default_rng(random_state)\n        # save random state passed to us for other functions that use random_state\n        self.random_state = random_state\n\n        self.max_evaluated_individuals = max_evaluated_individuals\n\n        #Initialize other used params\n\n        if self.initial_population_size is None:\n            self._initial_population_size = self.population_size\n        else:\n            self._initial_population_size = self.initial_population_size\n\n        if isinstance(self.scorers, str):\n            self._scorers = [self.scorers]\n\n        elif callable(self.scorers):\n            self._scorers = [self.scorers]\n        else:\n            self._scorers = self.scorers\n\n        self._scorers = [sklearn.metrics.get_scorer(scoring) for scoring in self._scorers]\n        self._scorers_early_stop_tol = self.scorers_early_stop_tol\n\n        self._evolver = tpot.evolvers.SteadyStateEvolver\n\n\n\n        self.objective_function_weights = [*scorers_weights, *other_objective_functions_weights]\n\n\n        if self.objective_function_names is None:\n            obj_names = [f.__name__ for f in other_objective_functions]\n        else:\n            obj_names = self.objective_function_names\n        self.objective_names = [f._score_func.__name__ if hasattr(f,\"_score_func\") else f.__name__ for f in self._scorers] + obj_names\n\n\n        if not isinstance(self.other_objectives_early_stop_tol, list):\n            self._other_objectives_early_stop_tol = [self.other_objectives_early_stop_tol for _ in range(len(self.other_objective_functions))]\n        else:\n            self._other_objectives_early_stop_tol = self.other_objectives_early_stop_tol\n\n        if not isinstance(self._scorers_early_stop_tol, list):\n            self._scorers_early_stop_tol = [self._scorers_early_stop_tol for _ in range(len(self._scorers))]\n        else:\n            self._scorers_early_stop_tol = self._scorers_early_stop_tol\n\n        self.early_stop_tol = [*self._scorers_early_stop_tol, *self._other_objectives_early_stop_tol]\n\n        self._evolver_instance = None\n        self.evaluated_individuals = None\n\n        self.label_encoder_ = None\n\n        set_dask_settings()\n\n\n    def fit(self, X, y):\n        if self.client is not None: #If user passed in a client manually\n           _client = self.client\n        else:\n\n            if self.verbose &gt;= 4:\n                silence_logs = 30\n            elif self.verbose &gt;=5:\n                silence_logs = 40\n            else:\n                silence_logs = 50\n            cluster = LocalCluster(n_workers=self.n_jobs, #if no client is passed in and no global client exists, create our own\n                    threads_per_worker=1,\n                    processes=self.processes,\n                    silence_logs=silence_logs,\n                    memory_limit=self.memory_limit)\n            _client = Client(cluster)\n\n\n        if self.classification and not self.disable_label_encoder and not check_if_y_is_encoded(y):\n            warnings.warn(\"Labels are not encoded as ints from 0 to N. For compatibility with some classifiers such as sklearn, TPOT has encoded y with the sklearn LabelEncoder. When using pipelines outside the main TPOT estimator class, you can encode the labels with est.label_encoder_\")\n            self.label_encoder_ = LabelEncoder()\n            y = self.label_encoder_.fit_transform(y)\n\n        self.evaluated_individuals = None\n        #determine validation strategy\n        if self.validation_strategy == 'auto':\n            nrows = X.shape[0]\n            ncols = X.shape[1]\n\n            if nrows/ncols &lt; 20:\n                validation_strategy = 'reshuffled'\n            elif nrows/ncols &lt; 100:\n                validation_strategy = 'split'\n            else:\n                validation_strategy = 'none'\n        else:\n            validation_strategy = self.validation_strategy\n\n        if validation_strategy == 'split':\n            if self.classification:\n                X, X_val, y, y_val = train_test_split(X, y, test_size=self.validation_fraction, stratify=y, random_state=self.random_state)\n            else:\n                X, X_val, y, y_val = train_test_split(X, y, test_size=self.validation_fraction, random_state=self.random_state)\n\n\n        X_original = X\n        y_original = y\n        if isinstance(self.cv, int) or isinstance(self.cv, float):\n            n_folds = self.cv\n        else:\n            n_folds = self.cv.get_n_splits(X, y)\n\n        if self.classification:\n            X, y = remove_underrepresented_classes(X, y, n_folds)\n\n        if self.preprocessing:\n            #X = pd.DataFrame(X)\n\n            if not isinstance(self.preprocessing, bool) and isinstance(self.preprocessing, sklearn.base.BaseEstimator):\n                self._preprocessing_pipeline = self.preprocessing\n\n            #TODO: check if there are missing values in X before imputation. If not, don't include imputation in pipeline. Check if there are categorical columns. If not, don't include one hot encoding in pipeline\n            else: #if self.preprocessing is True or not a sklearn estimator\n\n                pipeline_steps = []\n\n                if self.categorical_features is not None: #if categorical features are specified, use those\n                    pipeline_steps.append((\"impute_categorical\", tpot.builtin_modules.ColumnSimpleImputer(self.categorical_features, strategy='most_frequent')))\n                    pipeline_steps.append((\"impute_numeric\", tpot.builtin_modules.ColumnSimpleImputer(\"numeric\", strategy='mean')))\n                    pipeline_steps.append((\"ColumnOneHotEncoder\", tpot.builtin_modules.ColumnOneHotEncoder(self.categorical_features, strategy='most_frequent')))\n\n                else:\n                    if isinstance(X, pd.DataFrame):\n                        categorical_columns = X.select_dtypes(include=['object']).columns\n                        if len(categorical_columns) &gt; 0:\n                            pipeline_steps.append((\"impute_categorical\", tpot.builtin_modules.ColumnSimpleImputer(\"categorical\", strategy='most_frequent')))\n                            pipeline_steps.append((\"impute_numeric\", tpot.builtin_modules.ColumnSimpleImputer(\"numeric\", strategy='mean')))\n                            pipeline_steps.append((\"ColumnOneHotEncoder\", tpot.builtin_modules.ColumnOneHotEncoder(\"categorical\", strategy='most_frequent')))\n                        else:\n                            pipeline_steps.append((\"impute_numeric\", tpot.builtin_modules.ColumnSimpleImputer(\"all\", strategy='mean')))\n                    else:\n                        pipeline_steps.append((\"impute_numeric\", tpot.builtin_modules.ColumnSimpleImputer(\"all\", strategy='mean')))\n\n                self._preprocessing_pipeline = sklearn.pipeline.Pipeline(pipeline_steps)\n\n            X = self._preprocessing_pipeline.fit_transform(X, y)\n\n        else:\n            self._preprocessing_pipeline = None\n\n        #_, y = sklearn.utils.check_X_y(X, y, y_numeric=True)\n\n        #Set up the configuation dictionaries and the search spaces\n\n        #check if self.cv is a number\n        if isinstance(self.cv, int) or isinstance(self.cv, float):\n            if self.classification:\n                self.cv_gen = sklearn.model_selection.StratifiedKFold(n_splits=self.cv, shuffle=True, random_state=self.random_state)\n            else:\n                self.cv_gen = sklearn.model_selection.KFold(n_splits=self.cv, shuffle=True, random_state=self.random_state)\n\n        else:\n            self.cv_gen = sklearn.model_selection.check_cv(self.cv, y, classifier=self.classification)\n\n\n        n_samples= int(math.floor(X.shape[0]/n_folds))\n        n_features=X.shape[1]\n\n        if isinstance(X, pd.DataFrame):\n            self.feature_names = X.columns\n        else:\n            self.feature_names = None\n\n\n\n\n        def objective_function(pipeline_individual,\n                                            X,\n                                            y,\n                                            is_classification=self.classification,\n                                            scorers= self._scorers,\n                                            cv=self.cv_gen,\n                                            other_objective_functions=self.other_objective_functions,\n                                            export_graphpipeline=self.export_graphpipeline,\n                                            memory=self.memory,\n                                            **kwargs):\n            return objective_function_generator(\n                pipeline_individual,\n                X,\n                y,\n                is_classification=is_classification,\n                scorers= scorers,\n                cv=cv,\n                other_objective_functions=other_objective_functions,\n                export_graphpipeline=export_graphpipeline,\n                memory=memory,\n                **kwargs,\n            )\n\n        def ind_generator(rng):\n            rng = np.random.default_rng(rng)\n            while True:\n                yield self.search_space.generate(rng)\n\n\n\n        if self.scatter:\n            X_future = _client.scatter(X)\n            y_future = _client.scatter(y)\n        else:\n            X_future = X\n            y_future = y\n\n        #If warm start and we have an evolver instance, use the existing one\n        if not(self.warm_start and self._evolver_instance is not None):\n            self._evolver_instance = self._evolver(   individual_generator=ind_generator(self.rng),\n                                            objective_functions= [objective_function],\n                                            objective_function_weights = self.objective_function_weights,\n                                            objective_names=self.objective_names,\n                                            bigger_is_better = self.bigger_is_better,\n                                            population_size= self.population_size,\n\n                                            initial_population_size = self._initial_population_size,\n                                            n_jobs=self.n_jobs,\n                                            verbose = self.verbose,\n                                            max_time_mins =      self.max_time_mins ,\n                                            max_eval_time_mins = self.max_eval_time_mins,\n\n\n\n                                            periodic_checkpoint_folder = self.periodic_checkpoint_folder,\n\n\n                                            early_stop_tol = self.early_stop_tol,\n                                            early_stop= self.early_stop,\n                                            early_stop_mins =  self.early_stop_mins,\n\n                                            budget_range = self.budget_range,\n                                            budget_scaling = self.budget_scaling,\n                                            individuals_until_end_budget = self.individuals_until_end_budget,\n\n\n                                            stepwise_steps = self.stepwise_steps,\n                                            client = _client,\n                                            objective_kwargs = {\"X\": X_future, \"y\": y_future},\n                                            survival_selector=self.survival_selector,\n                                            parent_selector=self.parent_selector,\n\n                                            crossover_probability = self.crossover_probability,\n                                            mutate_probability = self.mutate_probability,\n                                            mutate_then_crossover_probability= self.mutate_then_crossover_probability,\n                                            crossover_then_mutate_probability= self.crossover_then_mutate_probability,\n\n\n                                            max_evaluated_individuals = self.max_evaluated_individuals,\n\n                                            rng=self.rng,\n                                            )\n\n\n        self._evolver_instance.optimize()\n        #self._evolver_instance.population.update_pareto_fronts(self.objective_names, self.objective_function_weights)\n        self.make_evaluated_individuals()\n\n\n        if self.optuna_optimize_pareto_front:\n            pareto_front_inds = self.pareto_front['Individual'].values\n            all_graphs, all_scores = tpot.individual_representations.graph_pipeline_individual.simple_parallel_optuna(pareto_front_inds,  objective_function, self.objective_function_weights, _client, storage=self.optuna_storage, steps=self.optuna_optimize_pareto_front_trials, verbose=self.verbose, max_eval_time_mins=self.max_eval_time_mins, max_time_mins=self.optuna_optimize_pareto_front_timeout, **{\"X\": X, \"y\": y})\n            all_scores = tpot.utils.eval_utils.process_scores(all_scores, len(self.objective_function_weights))\n\n            if len(all_graphs) &gt; 0:\n                df = pd.DataFrame(np.column_stack((all_graphs, all_scores,np.repeat(\"Optuna\",len(all_graphs)))), columns=[\"Individual\"] + self.objective_names +[\"Parents\"])\n                for obj in self.objective_names:\n                    df[obj] = df[obj].apply(convert_to_float)\n\n                self.evaluated_individuals = pd.concat([self.evaluated_individuals, df], ignore_index=True)\n            else:\n                print(\"WARNING NO OPTUNA TRIALS COMPLETED\")\n\n        tpot.utils.get_pareto_frontier(self.evaluated_individuals, column_names=self.objective_names, weights=self.objective_function_weights)\n\n        if validation_strategy == 'reshuffled':\n            best_pareto_front_idx = list(self.pareto_front.index)\n            best_pareto_front = list(self.pareto_front.loc[best_pareto_front_idx]['Individual'])\n\n            #reshuffle rows\n            X, y = sklearn.utils.shuffle(X, y, random_state=self.random_state)\n\n            if self.scatter:\n                X_future = _client.scatter(X)\n                y_future = _client.scatter(y)\n            else:\n                X_future = X\n                y_future = y\n\n            val_objective_function_list = [lambda   ind,\n                                                    X,\n                                                    y,\n                                                    is_classification=self.classification,\n                                                    scorers= self._scorers,\n                                                    cv=self.cv_gen,\n                                                    other_objective_functions=self.other_objective_functions,\n                                                    export_graphpipeline=self.export_graphpipeline,\n                                                    memory=self.memory,\n\n                                                    **kwargs: objective_function_generator(\n                                                                                                ind,\n                                                                                                X,\n                                                                                                y,\n                                                                                                is_classification=is_classification,\n                                                                                                scorers= scorers,\n                                                                                                cv=cv,\n                                                                                                other_objective_functions=other_objective_functions,\n                                                                                                export_graphpipeline=export_graphpipeline,\n                                                                                                memory=memory,\n                                                                                                **kwargs,\n                                                                                                )]\n\n            objective_kwargs = {\"X\": X_future, \"y\": y_future}\n            val_scores, start_times, end_times, eval_errors = tpot.utils.eval_utils.parallel_eval_objective_list(best_pareto_front, val_objective_function_list, verbose=self.verbose, max_eval_time_mins=self.max_eval_time_mins, n_expected_columns=len(self.objective_names), client=_client, **objective_kwargs)\n\n            val_objective_names = ['validation_'+name for name in self.objective_names]\n            self.objective_names_for_selection = val_objective_names\n            self.evaluated_individuals.loc[best_pareto_front_idx,val_objective_names] = val_scores\n            self.evaluated_individuals.loc[best_pareto_front_idx,'validation_start_times'] = start_times\n            self.evaluated_individuals.loc[best_pareto_front_idx,'validation_end_times'] = end_times\n            self.evaluated_individuals.loc[best_pareto_front_idx,'validation_eval_errors'] = eval_errors\n\n            self.evaluated_individuals[\"Validation_Pareto_Front\"] = tpot.utils.get_pareto_frontier(self.evaluated_individuals, column_names=val_objective_names, weights=self.objective_function_weights)\n        elif validation_strategy == 'split':\n\n\n            if self.scatter:\n                X_future = _client.scatter(X)\n                y_future = _client.scatter(y)\n                X_val_future = _client.scatter(X_val)\n                y_val_future = _client.scatter(y_val)\n            else:\n                X_future = X\n                y_future = y\n                X_val_future = X_val\n                y_val_future = y_val\n\n            objective_kwargs = {\"X\": X_future, \"y\": y_future, \"X_val\" : X_val_future, \"y_val\":y_val_future }\n\n            best_pareto_front_idx = list(self.pareto_front.index)\n            best_pareto_front = list(self.pareto_front.loc[best_pareto_front_idx]['Individual'])\n            val_objective_function_list = [lambda   ind,\n                                                    X,\n                                                    y,\n                                                    X_val,\n                                                    y_val,\n                                                    scorers= self._scorers,\n                                                    other_objective_functions=self.other_objective_functions,\n                                                    export_graphpipeline=self.export_graphpipeline,\n                                                    memory=self.memory,\n                                                    **kwargs: val_objective_function_generator(\n                                                        ind,\n                                                        X,\n                                                        y,\n                                                        X_val,\n                                                        y_val,\n                                                        scorers= scorers,\n                                                        other_objective_functions=other_objective_functions,\n                                                        export_graphpipeline=export_graphpipeline,\n                                                        memory=memory,\n                                                        **kwargs,\n                                                        )]\n\n            val_scores, start_times, end_times, eval_errors = tpot.utils.eval_utils.parallel_eval_objective_list(best_pareto_front, val_objective_function_list, verbose=self.verbose, max_eval_time_mins=self.max_eval_time_mins, n_expected_columns=len(self.objective_names), client=_client, **objective_kwargs)\n\n\n\n            val_objective_names = ['validation_'+name for name in self.objective_names]\n            self.objective_names_for_selection = val_objective_names\n            self.evaluated_individuals.loc[best_pareto_front_idx,val_objective_names] = val_scores\n            self.evaluated_individuals.loc[best_pareto_front_idx,'validation_start_times'] = start_times\n            self.evaluated_individuals.loc[best_pareto_front_idx,'validation_end_times'] = end_times\n            self.evaluated_individuals.loc[best_pareto_front_idx,'validation_eval_errors'] = eval_errors\n\n            self.evaluated_individuals[\"Validation_Pareto_Front\"] = tpot.utils.get_pareto_frontier(self.evaluated_individuals, column_names=val_objective_names, weights=self.objective_function_weights)\n        else:\n            self.objective_names_for_selection = self.objective_names\n\n        val_scores = self.evaluated_individuals[self.evaluated_individuals[self.objective_names_for_selection].isin([\"TIMEOUT\",\"INVALID\"]).any(axis=1).ne(True)][self.objective_names_for_selection].astype(float)\n        weighted_scores = val_scores*self.objective_function_weights\n\n        if self.bigger_is_better:\n            best_indices = list(weighted_scores.sort_values(by=self.objective_names_for_selection, ascending=False).index)\n        else:\n            best_indices = list(weighted_scores.sort_values(by=self.objective_names_for_selection, ascending=True).index)\n\n        for best_idx in best_indices:\n\n            best_individual = self.evaluated_individuals.loc[best_idx]['Individual']\n            self.selected_best_score =  self.evaluated_individuals.loc[best_idx]\n\n\n            #TODO\n            #best_individual_pipeline = best_individual.export_pipeline(memory=self.memory, cross_val_predict_cv=self.cross_val_predict_cv)\n            if self.export_graphpipeline:\n                best_individual_pipeline = best_individual.export_flattened_graphpipeline(memory=self.memory)\n            else:\n                best_individual_pipeline = best_individual.export_pipeline(memory=self.memory)\n\n            if self.preprocessing:\n                self.fitted_pipeline_ = sklearn.pipeline.make_pipeline(sklearn.base.clone(self._preprocessing_pipeline), best_individual_pipeline )\n            else:\n                self.fitted_pipeline_ = best_individual_pipeline\n\n            try:\n                self.fitted_pipeline_.fit(X_original,y_original) #TODO use y_original as well?\n                break\n            except Exception as e:\n                if self.verbose &gt;= 4:\n                    warnings.warn(\"Final pipeline failed to fit. Rarely, the pipeline might work on the objective function but fail on the full dataset. Generally due to interactions with different features being selected or transformations having different properties. Trying next pipeline\")\n                    print(e)\n                continue\n\n\n        if self.client is None: #no client was passed in\n            #close cluster and client\n            # _client.close()\n            # cluster.close()\n            try:\n                _client.shutdown()\n                cluster.close()\n            #catch exception\n            except Exception as e:\n                print(\"Error shutting down client and cluster\")\n                Warning(e)\n\n        return self\n\n    def _estimator_has(attr):\n        '''Check if we can delegate a method to the underlying estimator.\n        First, we check the first fitted final estimator if available, otherwise we\n        check the unfitted final estimator.\n        '''\n        return  lambda self: (self.fitted_pipeline_ is not None and\n            hasattr(self.fitted_pipeline_, attr)\n        )\n\n\n\n\n\n\n    @available_if(_estimator_has('predict'))\n    def predict(self, X, **predict_params):\n        check_is_fitted(self)\n        #X = check_array(X)\n        preds = self.fitted_pipeline_.predict(X,**predict_params)\n        if self.classification and self.label_encoder_:\n            preds = self.label_encoder_.inverse_transform(preds)\n\n        return preds\n\n    @available_if(_estimator_has('predict_proba'))\n    def predict_proba(self, X, **predict_params):\n        check_is_fitted(self)\n        #X = check_array(X)\n        return self.fitted_pipeline_.predict_proba(X,**predict_params)\n\n    @available_if(_estimator_has('decision_function'))\n    def decision_function(self, X, **predict_params):\n        check_is_fitted(self)\n        #X = check_array(X)\n        return self.fitted_pipeline_.decision_function(X,**predict_params)\n\n    @available_if(_estimator_has('transform'))\n    def transform(self, X, **predict_params):\n        check_is_fitted(self)\n        #X = check_array(X)\n        return self.fitted_pipeline_.transform(X,**predict_params)\n\n    @property\n    def classes_(self):\n        \"\"\"The classes labels. Only exist if the last step is a classifier.\"\"\"\n\n        if self.label_encoder_:\n            return self.label_encoder_.classes_\n        else:\n            return self.fitted_pipeline_.classes_\n\n    @property\n    def _estimator_type(self):\n        return self.fitted_pipeline_._estimator_type\n\n    def make_evaluated_individuals(self):\n        #check if _evolver_instance exists\n        if self.evaluated_individuals is None:\n            self.evaluated_individuals  =  self._evolver_instance.population.evaluated_individuals.copy()\n            objects = list(self.evaluated_individuals.index)\n            object_to_int = dict(zip(objects, range(len(objects))))\n            self.evaluated_individuals = self.evaluated_individuals.set_index(self.evaluated_individuals.index.map(object_to_int))\n            self.evaluated_individuals['Parents'] = self.evaluated_individuals['Parents'].apply(lambda row: convert_parents_tuples_to_integers(row, object_to_int))\n\n            self.evaluated_individuals[\"Instance\"] = self.evaluated_individuals[\"Individual\"].apply(lambda ind: apply_make_pipeline(ind, preprocessing_pipeline=self._preprocessing_pipeline, export_graphpipeline=self.export_graphpipeline, memory=self.memory))\n\n        return self.evaluated_individuals\n\n    @property\n    def pareto_front(self):\n        #check if _evolver_instance exists\n        if self.evaluated_individuals is None:\n            return None\n        else:\n            if \"Pareto_Front\" not in self.evaluated_individuals:\n                return self.evaluated_individuals\n            else:\n                return self.evaluated_individuals[self.evaluated_individuals[\"Pareto_Front\"]==1]\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/steady_state_estimator/#tpot.tpot_estimator.steady_state_estimator.TPOTEstimatorSteadyState.classes_","title":"<code>classes_</code>  <code>property</code>","text":"<p>The classes labels. Only exist if the last step is a classifier.</p>"},{"location":"documentation/tpot/tpot_estimator/steady_state_estimator/#tpot.tpot_estimator.steady_state_estimator.TPOTEstimatorSteadyState.__init__","title":"<code>__init__(search_space, scorers=[], scorers_weights=[], classification=False, cv=10, other_objective_functions=[], other_objective_functions_weights=[], objective_function_names=None, bigger_is_better=True, export_graphpipeline=False, memory=None, categorical_features=None, subsets=None, preprocessing=False, validation_strategy='none', validation_fraction=0.2, disable_label_encoder=False, initial_population_size=50, population_size=50, max_evaluated_individuals=None, early_stop=None, early_stop_mins=None, scorers_early_stop_tol=0.001, other_objectives_early_stop_tol=None, max_time_mins=None, max_eval_time_mins=10, n_jobs=1, memory_limit=None, client=None, crossover_probability=0.2, mutate_probability=0.7, mutate_then_crossover_probability=0.05, crossover_then_mutate_probability=0.05, survival_selector=survival_select_NSGA2, parent_selector=tournament_selection_dominated, budget_range=None, budget_scaling=0.5, individuals_until_end_budget=1, stepwise_steps=5, warm_start=False, verbose=0, periodic_checkpoint_folder=None, callback=None, processes=True, scatter=True, random_state=None, optuna_optimize_pareto_front=False, optuna_optimize_pareto_front_trials=100, optuna_optimize_pareto_front_timeout=60 * 10, optuna_storage='sqlite:///optuna.db')</code>","text":"<p>An sklearn baseestimator that uses genetic programming to optimize a pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>scorers</code> <code>(list, scorer)</code> <p>A scorer or list of scorers to be used in the cross-validation process. see https://scikit-learn.org/stable/modules/model_evaluation.html</p> <code>[]</code> <code>scorers_weights</code> <code>list</code> <p>A list of weights to be applied to the scorers during the optimization process.</p> <code>[]</code> <code>classification</code> <code>bool</code> <p>If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem. Used to determine the CV strategy.</p> <code>False</code> <code>cv</code> <code>(int, cross - validator)</code> <ul> <li>(int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.</li> <li>(sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.</li> </ul> <code>10</code> <code>other_objective_functions</code> <code>list</code> <p>A list of other objective functions to apply to the pipeline. The function takes a single parameter for the graphpipeline estimator and returns either a single score or a list of scores.</p> <code>[]</code> <code>other_objective_functions_weights</code> <code>list</code> <p>A list of weights to be applied to the other objective functions.</p> <code>[]</code> <code>objective_function_names</code> <code>list</code> <p>A list of names to be applied to the objective functions. If None, will use the names of the objective functions.</p> <code>None</code> <code>bigger_is_better</code> <code>bool</code> <p>If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.</p> <code>True</code> <code>max_size</code> <code>int</code> <p>The maximum number of nodes of the pipelines to be generated.</p> <code>np.inf</code> <code>linear_pipeline</code> <code>bool</code> <p>If True, the pipelines generated will be linear. If False, the pipelines generated will be directed acyclic graphs.</p> <code>False</code> <code>root_config_dict</code> <code>dict</code> <p>The configuration dictionary to use for the root node of the model. If 'auto', will use \"classifiers\" if classification=True, else \"regressors\". - 'selectors' : A selection of sklearn Selector methods. - 'classifiers' : A selection of sklearn Classifier methods. - 'regressors' : A selection of sklearn Regressor methods. - 'transformers' : A selection of sklearn Transformer methods. - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators. - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes. - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.                             Subsets are set with the subsets parameter. - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF. - 'MDR' : Includes MDR. - 'ContinuousMDR' : Includes ContinuousMDR. - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL. - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL. - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.</p> <code>'auto'</code> <code>inner_config_dict</code> <code>dict</code> <p>The configuration dictionary to use for the inner nodes of the model generation. Default [\"selectors\", \"transformers\"] - 'selectors' : A selection of sklearn Selector methods. - 'classifiers' : A selection of sklearn Classifier methods. - 'regressors' : A selection of sklearn Regressor methods. - 'transformers' : A selection of sklearn Transformer methods. - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators. - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes. - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.                             Subsets are set with the subsets parameter. - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF. - 'MDR' : Includes MDR. - 'ContinuousMDR' : Includes ContinuousMDR. - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL. - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL. - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary. - None : If None and max_depth&gt;1, the root_config_dict will be used for the inner nodes as well.</p> <code>[\"selectors\", \"transformers\"]</code> <code>leaf_config_dict</code> <code>dict</code> <p>The configuration dictionary to use for the leaf node of the model. If set, leaf nodes must be from this dictionary. Otherwise leaf nodes will be generated from the root_config_dict. Default None - 'selectors' : A selection of sklearn Selector methods. - 'classifiers' : A selection of sklearn Classifier methods. - 'regressors' : A selection of sklearn Regressor methods. - 'transformers' : A selection of sklearn Transformer methods. - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators. - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes. - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.                             Subsets are set with the subsets parameter. - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF. - 'MDR' : Includes MDR. - 'ContinuousMDR' : Includes ContinuousMDR. - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL. - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL. - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary. - None : If None, a leaf will not be required (i.e. the pipeline can be a single root node). Leaf nodes will be generated from the inner_config_dict.</p> <code>None</code> <code>categorical_features</code> <p>Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False. - None : If None, TPOT will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing. - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices</p> <code>None</code> <code>memory</code> <p>If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. This feature is used to avoid computing the fit transformers within a pipeline if the parameters and input data are identical with another fitted pipeline during optimization process. - String 'auto':     TPOT uses memory caching with a temporary directory and cleans it up upon shutdown. - String path of a caching directory     TPOT uses memory caching with the provided directory and TPOT does NOT clean     the caching directory up upon shutdown. If the directory does not exist, TPOT will     create it. - Memory object:     TPOT uses the instance of joblib.Memory for memory caching,     and TPOT does NOT clean the caching directory up upon shutdown. - None:     TPOT does not use memory caching.</p> <code>None</code> <code>preprocessing</code> <code>(bool or BaseEstimator / Pipeline)</code> <p>EXPERIMENTAL A pipeline that will be used to preprocess the data before CV. - bool : If True, will use a default preprocessing pipeline. - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.</p> <code>False</code> <code>validation_strategy</code> <code>str</code> <p>EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT may overfit the cross validation score. A second validation set can be used to select the final pipeline. - 'auto' : Automatically determine the validation strategy based on the dataset shape. - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets. - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets. - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.</p> <code>'none'</code> <code>validation_fraction</code> <code>float</code> <p>EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.</p> <code>0.2</code> <code>disable_label_encoder</code> <code>bool</code> <p>If True, TPOT will check if the target needs to be relabeled to be sequential ints from 0 to N. This is necessary for XGBoost compatibility. If the labels need to be encoded, TPOT will use sklearn.preprocessing.LabelEncoder to encode the labels. The encoder can be accessed via the self.label_encoder_ attribute. If False, no additional label encoders will be used.</p> <code>False</code> <code>population_size</code> <code>int</code> <p>Size of the population</p> <code>50</code> <code>initial_population_size</code> <code>int</code> <p>Size of the initial population. If None, population_size will be used.</p> <code>None</code> <code>population_scaling</code> <code>int</code> <p>Scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.</p> <code>0.5</code> <code>generations_until_end_population</code> <code>int</code> <p>Number of generations until the population size reaches population_size</p> <code>1</code> <code>generations</code> <code>int</code> <p>Number of generations to run</p> <code>50</code> <code>early_stop</code> <code>int</code> <p>Number of evaluated individuals without improvement before early stopping. Counted across all objectives independently. Triggered when all objectives have not improved by the given number of individuals.</p> <code>None</code> <code>early_stop_mins</code> <code>float</code> <p>Number of seconds without improvement before early stopping. All objectives must not have improved for the given number of seconds for this to be triggered.</p> <code>None</code> <code>scorers_early_stop_tol</code> <p>-list of floats     list of tolerances for each scorer. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged     If an index of the list is None, that item will not be used for early stopping -int     If an int is given, it will be used as the tolerance for all objectives</p> <code>0.001</code> <code>other_objectives_early_stop_tol</code> <p>-list of floats     list of tolerances for each of the other objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged     If an index of the list is None, that item will not be used for early stopping -int     If an int is given, it will be used as the tolerance for all objectives</p> <code>None</code> <code>max_time_mins</code> <code>float</code> <p>Maximum time to run the optimization. If none or inf, will run until the end of the generations.</p> <code>float(\"inf\")</code> <code>max_eval_time_mins</code> <code>float</code> <p>Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.</p> <code>60*5</code> <code>n_jobs</code> <code>int</code> <p>Number of processes to run in parallel.</p> <code>1</code> <code>memory_limit</code> <code>str</code> <p>Memory limit for each job. See Dask LocalCluster documentation for more information.</p> <code>None</code> <code>client</code> <code>Client</code> <p>A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.</p> <code>None</code> <code>crossover_probability</code> <code>float</code> <p>Probability of generating a new individual by crossover between two individuals.</p> <code>.2</code> <code>mutate_probability</code> <code>float</code> <p>Probability of generating a new individual by crossover between one individuals.</p> <code>.7</code> <code>mutate_then_crossover_probability</code> <code>float</code> <p>Probability of generating a new individual by mutating two individuals followed by crossover.</p> <code>.05</code> <code>crossover_then_mutate_probability</code> <code>float</code> <p>Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.</p> <code>.05</code> <code>survival_selector</code> <code>function</code> <p>Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes. Used to selected population_size individuals at the start of each generation to use for mutation and crossover.</p> <code>survival_select_NSGA2</code> <code>parent_selector</code> <code>function</code> <p>Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.</p> <code>parent_select_NSGA2</code> <code>budget_range</code> <code>list[start, end]</code> <p>A starting and ending budget to use for the budget scaling.</p> <code>None</code> <code>budget_scaling</code> <p>A scaling factor to use when determining how fast we move the budget from the start to end budget.</p> <code>0.5</code> <code>individuals_until_end_budget</code> <code>int</code> <p>The number of generations to run before reaching the max budget.</p> <code>1</code> <code>stepwise_steps</code> <code>int</code> <p>The number of staircase steps to take when scaling the budget and population size.</p> <code>1</code> <code>threshold_evaluation_pruning</code> <code>list[start, end]</code> <p>starting and ending percentile to use as a threshold for the evaluation early stopping. Values between 0 and 100.</p> <code>None</code> <code>threshold_evaluation_scaling</code> <code>float [0,inf)</code> <p>A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile. Must be greater than zero. Higher numbers will move the threshold to the end faster.</p> <code>0.5</code> <code>min_history_threshold</code> <code>int</code> <p>The minimum number of previous scores needed before using threshold early stopping.</p> <code>0</code> <code>selection_evaluation_pruning</code> <code>list</code> <p>A lower and upper percent of the population size to select each round of CV. Values between 0 and 1.</p> <code>None</code> <code>selection_evaluation_scaling</code> <code>float</code> <p>A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile. Must be greater than zero. Higher numbers will move the threshold to the end faster.</p> <code>0.5</code> <code>n_initial_optimizations</code> <code>int</code> <p>Number of individuals to optimize before starting the evolution.</p> <code>0</code> <code>optimization_cv</code> <code>int</code> <p>Number of folds to use for the optuna optimization's internal cross-validation.</p> required <code>max_optimize_time_seconds</code> <code>float</code> <p>Maximum time to run an optimization</p> <code>60*5</code> <code>optimization_steps</code> <code>int</code> <p>Number of steps per optimization</p> <code>10</code> <code>warm_start</code> <code>bool</code> <p>If True, will use the continue the evolutionary algorithm from the last generation of the previous run.</p> <code>False</code> <code>verbose</code> <code>int</code> <p>How much information to print during the optimization process. Higher values include the information from lower values. 0. nothing 1. progress bar</p> <ol> <li>best individual</li> <li>warnings <p>=5. full warnings trace</p> </li> </ol> <code>1</code> <code>random_state</code> <code>(int, None)</code> <p>A seed for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes - int     Will be used to create and lock in Generator instance with 'numpy.random.default_rng()' - None     Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS</p> <code>None</code> <code>periodic_checkpoint_folder</code> <code>str</code> <p>Folder to save the population to periodically. If None, no periodic saving will be done. If provided, training will resume from this checkpoint.</p> <code>None</code> <code>callback</code> <code>CallBackInterface</code> <p>Callback object. Not implemented</p> <code>None</code> <code>processes</code> <code>bool</code> <p>If True, will use multiprocessing to parallelize the optimization process. If False, will use threading. True seems to perform better. However, False is required for interactive debugging.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>fitted_pipeline_</code> <code>GraphPipeline</code> <p>A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.</p> <code>evaluated_individuals</code> <code>A pandas data frame containing data for all evaluated individuals in the run.</code> <p>Columns: - objective functions : The first few columns correspond to the passed in scorers and objective functions - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population. - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population. - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator. - Generation : The generation the pipeline first appeared. - Pareto_Front      : The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.                 To save on computational time, the best frontier is updated iteratively each generation.                 The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.                 All other pipelines are set to NaN. - Instance  : The unfitted GraphPipeline BaseEstimator. - validation objective functions : Objective function scores evaluated on the validation set. - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.</p> <code>pareto_front</code> <code>The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.</code> Source code in <code>tpot/tpot_estimator/steady_state_estimator.py</code> <pre><code>def __init__(self,  \n                    search_space,\n                    scorers= [],\n                    scorers_weights = [],\n                    classification = False,\n                    cv = 10,\n                    other_objective_functions=[], #tpot.objectives.estimator_objective_functions.number_of_nodes_objective],\n                    other_objective_functions_weights = [],\n                    objective_function_names = None,\n                    bigger_is_better = True,\n\n\n                    export_graphpipeline = False,\n                    memory = None,\n\n                    categorical_features = None,\n                    subsets = None,\n                    preprocessing = False,\n                    validation_strategy = \"none\",\n                    validation_fraction = .2,\n                    disable_label_encoder = False,\n\n                    initial_population_size = 50,\n                    population_size = 50,\n                    max_evaluated_individuals = None,\n\n\n\n                    early_stop = None,\n                    early_stop_mins = None,\n                    scorers_early_stop_tol = 0.001,\n                    other_objectives_early_stop_tol = None,\n                    max_time_mins=None,\n                    max_eval_time_mins=10,\n                    n_jobs=1,\n                    memory_limit = None,\n                    client = None,\n\n                    crossover_probability=.2,\n                    mutate_probability=.7,\n                    mutate_then_crossover_probability=.05,\n                    crossover_then_mutate_probability=.05,\n                    survival_selector = survival_select_NSGA2,\n                    parent_selector = tournament_selection_dominated,\n                    budget_range = None,\n                    budget_scaling = .5,\n                    individuals_until_end_budget = 1,\n                    stepwise_steps = 5,\n\n                    warm_start = False,\n\n                    verbose = 0,\n                    periodic_checkpoint_folder = None,\n                    callback = None,\n                    processes = True,\n\n                    scatter = True,\n\n                    # random seed for random number generator (rng)\n                    random_state = None,\n\n                    optuna_optimize_pareto_front = False,\n                    optuna_optimize_pareto_front_trials = 100,\n                    optuna_optimize_pareto_front_timeout = 60*10,\n                    optuna_storage = \"sqlite:///optuna.db\",\n                    ):\n\n    '''\n    An sklearn baseestimator that uses genetic programming to optimize a pipeline.\n\n    Parameters\n    ----------\n\n    scorers : (list, scorer)\n        A scorer or list of scorers to be used in the cross-validation process.\n        see https://scikit-learn.org/stable/modules/model_evaluation.html\n\n    scorers_weights : list\n        A list of weights to be applied to the scorers during the optimization process.\n\n    classification : bool\n        If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem.\n        Used to determine the CV strategy.\n\n    cv : int, cross-validator\n        - (int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.\n        - (sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.\n\n    other_objective_functions : list, default=[]\n        A list of other objective functions to apply to the pipeline. The function takes a single parameter for the graphpipeline estimator and returns either a single score or a list of scores.\n\n    other_objective_functions_weights : list, default=[]\n        A list of weights to be applied to the other objective functions.\n\n    objective_function_names : list, default=None\n        A list of names to be applied to the objective functions. If None, will use the names of the objective functions.\n\n    bigger_is_better : bool, default=True\n        If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n\n\n    max_size : int, default=np.inf\n        The maximum number of nodes of the pipelines to be generated.\n\n    linear_pipeline : bool, default=False\n        If True, the pipelines generated will be linear. If False, the pipelines generated will be directed acyclic graphs.\n\n    root_config_dict : dict, default='auto'\n        The configuration dictionary to use for the root node of the model.\n        If 'auto', will use \"classifiers\" if classification=True, else \"regressors\".\n        - 'selectors' : A selection of sklearn Selector methods.\n        - 'classifiers' : A selection of sklearn Classifier methods.\n        - 'regressors' : A selection of sklearn Regressor methods.\n        - 'transformers' : A selection of sklearn Transformer methods.\n        - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.\n        - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.\n        - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.\n                                    Subsets are set with the subsets parameter.\n        - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.\n        - 'MDR' : Includes MDR.\n        - 'ContinuousMDR' : Includes ContinuousMDR.\n        - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.\n        - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.\n        - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.\n\n    inner_config_dict : dict, default=[\"selectors\", \"transformers\"]\n        The configuration dictionary to use for the inner nodes of the model generation.\n        Default [\"selectors\", \"transformers\"]\n        - 'selectors' : A selection of sklearn Selector methods.\n        - 'classifiers' : A selection of sklearn Classifier methods.\n        - 'regressors' : A selection of sklearn Regressor methods.\n        - 'transformers' : A selection of sklearn Transformer methods.\n        - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.\n        - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.\n        - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.\n                                    Subsets are set with the subsets parameter.\n        - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.\n        - 'MDR' : Includes MDR.\n        - 'ContinuousMDR' : Includes ContinuousMDR.\n        - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.\n        - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.\n        - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.\n        - None : If None and max_depth&gt;1, the root_config_dict will be used for the inner nodes as well.\n\n    leaf_config_dict : dict, default=None\n        The configuration dictionary to use for the leaf node of the model. If set, leaf nodes must be from this dictionary.\n        Otherwise leaf nodes will be generated from the root_config_dict.\n        Default None\n        - 'selectors' : A selection of sklearn Selector methods.\n        - 'classifiers' : A selection of sklearn Classifier methods.\n        - 'regressors' : A selection of sklearn Regressor methods.\n        - 'transformers' : A selection of sklearn Transformer methods.\n        - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.\n        - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.\n        - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.\n                                    Subsets are set with the subsets parameter.\n        - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.\n        - 'MDR' : Includes MDR.\n        - 'ContinuousMDR' : Includes ContinuousMDR.\n        - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.\n        - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.\n        - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.\n        - None : If None, a leaf will not be required (i.e. the pipeline can be a single root node). Leaf nodes will be generated from the inner_config_dict.\n\n    categorical_features: list or None\n        Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n        - None : If None, TPOT will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing.\n        - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices\n\n\n    memory: Memory object or string, default=None\n        If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. This feature\n        is used to avoid computing the fit transformers within a pipeline if the parameters\n        and input data are identical with another fitted pipeline during optimization process.\n        - String 'auto':\n            TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.\n        - String path of a caching directory\n            TPOT uses memory caching with the provided directory and TPOT does NOT clean\n            the caching directory up upon shutdown. If the directory does not exist, TPOT will\n            create it.\n        - Memory object:\n            TPOT uses the instance of joblib.Memory for memory caching,\n            and TPOT does NOT clean the caching directory up upon shutdown.\n        - None:\n            TPOT does not use memory caching.\n\n    preprocessing : bool or BaseEstimator/Pipeline,\n        EXPERIMENTAL\n        A pipeline that will be used to preprocess the data before CV.\n        - bool : If True, will use a default preprocessing pipeline.\n        - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.\n\n    validation_strategy : str, default='none'\n        EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT may overfit the cross validation score. A second validation set can be used to select the final pipeline.\n        - 'auto' : Automatically determine the validation strategy based on the dataset shape.\n        - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets.\n        - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets.\n        - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.\n\n    validation_fraction : float, default=0.2\n      EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.\n\n    disable_label_encoder : bool, default=False\n        If True, TPOT will check if the target needs to be relabeled to be sequential ints from 0 to N. This is necessary for XGBoost compatibility. If the labels need to be encoded, TPOT will use sklearn.preprocessing.LabelEncoder to encode the labels. The encoder can be accessed via the self.label_encoder_ attribute.\n        If False, no additional label encoders will be used.\n\n    population_size : int, default=50\n        Size of the population\n\n    initial_population_size : int, default=None\n        Size of the initial population. If None, population_size will be used.\n\n    population_scaling : int, default=0.5\n        Scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n\n    generations_until_end_population : int, default=1\n        Number of generations until the population size reaches population_size\n\n    generations : int, default=50\n        Number of generations to run\n\n    early_stop : int, default=None\n        Number of evaluated individuals without improvement before early stopping. Counted across all objectives independently. Triggered when all objectives have not improved by the given number of individuals.\n\n    early_stop_mins : float, default=None\n        Number of seconds without improvement before early stopping. All objectives must not have improved for the given number of seconds for this to be triggered.\n\n    scorers_early_stop_tol :\n        -list of floats\n            list of tolerances for each scorer. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n            If an index of the list is None, that item will not be used for early stopping\n        -int\n            If an int is given, it will be used as the tolerance for all objectives\n\n    other_objectives_early_stop_tol :\n        -list of floats\n            list of tolerances for each of the other objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n            If an index of the list is None, that item will not be used for early stopping\n        -int\n            If an int is given, it will be used as the tolerance for all objectives\n\n    max_time_mins : float, default=float(\"inf\")\n        Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n\n    max_eval_time_mins : float, default=60*5\n        Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n\n    n_jobs : int, default=1\n        Number of processes to run in parallel.\n\n    memory_limit : str, default=None\n        Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n\n    client : dask.distributed.Client, default=None\n        A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.\n\n    crossover_probability : float, default=.2\n        Probability of generating a new individual by crossover between two individuals.\n\n    mutate_probability : float, default=.7\n        Probability of generating a new individual by crossover between one individuals.\n\n    mutate_then_crossover_probability : float, default=.05\n        Probability of generating a new individual by mutating two individuals followed by crossover.\n\n    crossover_then_mutate_probability : float, default=.05\n        Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.\n\n    survival_selector : function, default=survival_select_NSGA2\n        Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes.\n        Used to selected population_size individuals at the start of each generation to use for mutation and crossover.\n\n    parent_selector : function, default=parent_select_NSGA2\n        Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.\n\n    budget_range : list [start, end], default=None\n        A starting and ending budget to use for the budget scaling.\n\n    budget_scaling float : [0,1], default=0.5\n        A scaling factor to use when determining how fast we move the budget from the start to end budget.\n\n    individuals_until_end_budget : int, default=1\n        The number of generations to run before reaching the max budget.\n\n    stepwise_steps : int, default=1\n        The number of staircase steps to take when scaling the budget and population size.\n\n    threshold_evaluation_pruning : list [start, end], default=None\n        starting and ending percentile to use as a threshold for the evaluation early stopping.\n        Values between 0 and 100.\n\n    threshold_evaluation_scaling : float [0,inf), default=0.5\n        A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n        Must be greater than zero. Higher numbers will move the threshold to the end faster.\n\n    min_history_threshold : int, default=0\n        The minimum number of previous scores needed before using threshold early stopping.\n\n    selection_evaluation_pruning : list, default=None\n        A lower and upper percent of the population size to select each round of CV.\n        Values between 0 and 1.\n\n    selection_evaluation_scaling : float, default=0.5\n        A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n        Must be greater than zero. Higher numbers will move the threshold to the end faster.\n\n    n_initial_optimizations : int, default=0\n        Number of individuals to optimize before starting the evolution.\n\n    optimization_cv : int\n       Number of folds to use for the optuna optimization's internal cross-validation.\n\n    max_optimize_time_seconds : float, default=60*5\n        Maximum time to run an optimization\n\n    optimization_steps : int, default=10\n        Number of steps per optimization\n\n    warm_start : bool, default=False\n        If True, will use the continue the evolutionary algorithm from the last generation of the previous run.\n\n\n    verbose : int, default=1\n        How much information to print during the optimization process. Higher values include the information from lower values.\n        0. nothing\n        1. progress bar\n\n        3. best individual\n        4. warnings\n        &gt;=5. full warnings trace\n\n    random_state : int, None, default=None\n        A seed for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes\n        - int\n            Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'\n        - None\n            Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS\n\n\n    periodic_checkpoint_folder : str, default=None\n        Folder to save the population to periodically. If None, no periodic saving will be done.\n        If provided, training will resume from this checkpoint.\n\n    callback : tpot.CallBackInterface, default=None\n        Callback object. Not implemented\n\n    processes : bool, default=True\n        If True, will use multiprocessing to parallelize the optimization process. If False, will use threading.\n        True seems to perform better. However, False is required for interactive debugging.\n\n    Attributes\n    ----------\n\n    fitted_pipeline_ : GraphPipeline\n        A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.\n\n    evaluated_individuals : A pandas data frame containing data for all evaluated individuals in the run.\n        Columns:\n        - *objective functions : The first few columns correspond to the passed in scorers and objective functions\n        - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population.\n        - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population.\n        - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator.\n        - Generation : The generation the pipeline first appeared.\n        - Pareto_Front\t: The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.\n                        To save on computational time, the best frontier is updated iteratively each generation.\n                        The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.\n                        All other pipelines are set to NaN.\n        - Instance\t: The unfitted GraphPipeline BaseEstimator.\n        - *validation objective functions : Objective function scores evaluated on the validation set.\n        - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.\n\n    pareto_front : The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.\n    '''\n\n    # sklearn BaseEstimator must have a corresponding attribute for each parameter.\n    # These should not be modified once set.\n\n    self.search_space = search_space\n    self.scorers = scorers\n    self.scorers_weights = scorers_weights\n    self.classification = classification\n    self.cv = cv\n    self.other_objective_functions = other_objective_functions\n    self.other_objective_functions_weights = other_objective_functions_weights\n    self.objective_function_names = objective_function_names\n    self.bigger_is_better = bigger_is_better\n\n    self.export_graphpipeline = export_graphpipeline\n    self.memory = memory\n\n    self.categorical_features = categorical_features\n    self.preprocessing = preprocessing\n    self.validation_strategy = validation_strategy\n    self.validation_fraction = validation_fraction\n    self.disable_label_encoder = disable_label_encoder\n    self.population_size = population_size\n    self.initial_population_size = initial_population_size\n\n    self.early_stop = early_stop\n    self.early_stop_mins = early_stop_mins\n    self.scorers_early_stop_tol = scorers_early_stop_tol\n    self.other_objectives_early_stop_tol = other_objectives_early_stop_tol\n    self.max_time_mins = max_time_mins\n    self.max_eval_time_mins = max_eval_time_mins\n    self.n_jobs= n_jobs\n    self.memory_limit = memory_limit\n    self.client = client\n\n    self.crossover_probability = crossover_probability\n    self.mutate_probability = mutate_probability\n    self.mutate_then_crossover_probability= mutate_then_crossover_probability\n    self.crossover_then_mutate_probability= crossover_then_mutate_probability\n    self.survival_selector=survival_selector\n    self.parent_selector=parent_selector\n    self.budget_range = budget_range\n    self.budget_scaling = budget_scaling\n    self.individuals_until_end_budget = individuals_until_end_budget\n    self.stepwise_steps = stepwise_steps\n\n    self.warm_start = warm_start\n\n    self.verbose = verbose\n    self.periodic_checkpoint_folder = periodic_checkpoint_folder\n    self.callback = callback\n    self.processes = processes\n\n\n    self.scatter = scatter\n\n    self.optuna_optimize_pareto_front = optuna_optimize_pareto_front\n    self.optuna_optimize_pareto_front_trials = optuna_optimize_pareto_front_trials\n    self.optuna_optimize_pareto_front_timeout = optuna_optimize_pareto_front_timeout\n    self.optuna_storage = optuna_storage\n\n    # create random number generator based on rngseed\n    self.rng = np.random.default_rng(random_state)\n    # save random state passed to us for other functions that use random_state\n    self.random_state = random_state\n\n    self.max_evaluated_individuals = max_evaluated_individuals\n\n    #Initialize other used params\n\n    if self.initial_population_size is None:\n        self._initial_population_size = self.population_size\n    else:\n        self._initial_population_size = self.initial_population_size\n\n    if isinstance(self.scorers, str):\n        self._scorers = [self.scorers]\n\n    elif callable(self.scorers):\n        self._scorers = [self.scorers]\n    else:\n        self._scorers = self.scorers\n\n    self._scorers = [sklearn.metrics.get_scorer(scoring) for scoring in self._scorers]\n    self._scorers_early_stop_tol = self.scorers_early_stop_tol\n\n    self._evolver = tpot.evolvers.SteadyStateEvolver\n\n\n\n    self.objective_function_weights = [*scorers_weights, *other_objective_functions_weights]\n\n\n    if self.objective_function_names is None:\n        obj_names = [f.__name__ for f in other_objective_functions]\n    else:\n        obj_names = self.objective_function_names\n    self.objective_names = [f._score_func.__name__ if hasattr(f,\"_score_func\") else f.__name__ for f in self._scorers] + obj_names\n\n\n    if not isinstance(self.other_objectives_early_stop_tol, list):\n        self._other_objectives_early_stop_tol = [self.other_objectives_early_stop_tol for _ in range(len(self.other_objective_functions))]\n    else:\n        self._other_objectives_early_stop_tol = self.other_objectives_early_stop_tol\n\n    if not isinstance(self._scorers_early_stop_tol, list):\n        self._scorers_early_stop_tol = [self._scorers_early_stop_tol for _ in range(len(self._scorers))]\n    else:\n        self._scorers_early_stop_tol = self._scorers_early_stop_tol\n\n    self.early_stop_tol = [*self._scorers_early_stop_tol, *self._other_objectives_early_stop_tol]\n\n    self._evolver_instance = None\n    self.evaluated_individuals = None\n\n    self.label_encoder_ = None\n\n    set_dask_settings()\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/steady_state_estimator/#tpot.tpot_estimator.steady_state_estimator.apply_make_pipeline","title":"<code>apply_make_pipeline(ind, preprocessing_pipeline=None, export_graphpipeline=False, **pipeline_kwargs)</code>","text":"<p>Helper function to create a column of sklearn pipelines from the tpot individual class.</p> <p>Parameters:</p> Name Type Description Default <code>ind</code> <p>The individual to convert to a pipeline.</p> required <code>preprocessing_pipeline</code> <p>The preprocessing pipeline to include before the individual's pipeline.</p> <code>None</code> <code>export_graphpipeline</code> <p>Force the pipeline to be exported as a graph pipeline. Flattens all nested pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.</p> <code>False</code> <code>pipeline_kwargs</code> <p>Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>sklearn estimator</code> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def apply_make_pipeline(ind, preprocessing_pipeline=None, export_graphpipeline=False, **pipeline_kwargs):\n    \"\"\"\n    Helper function to create a column of sklearn pipelines from the tpot individual class.\n\n    Parameters\n    ----------\n    ind: tpot.SklearnIndividual\n        The individual to convert to a pipeline.\n    preprocessing_pipeline: sklearn.pipeline.Pipeline, optional\n        The preprocessing pipeline to include before the individual's pipeline.\n    export_graphpipeline: bool, default=False\n        Force the pipeline to be exported as a graph pipeline. Flattens all nested pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.\n    pipeline_kwargs: dict\n        Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.\n\n    Returns\n    -------\n    sklearn estimator\n    \"\"\"\n\n    try:\n\n        if export_graphpipeline:\n            est = ind.export_flattened_graphpipeline(**pipeline_kwargs)\n        else:\n            est = ind.export_pipeline(**pipeline_kwargs)\n\n\n        if preprocessing_pipeline is None:\n            return est\n        else:\n            return sklearn.pipeline.make_pipeline(sklearn.base.clone(preprocessing_pipeline), est)\n    except:\n        return None\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/steady_state_estimator/#tpot.tpot_estimator.steady_state_estimator.check_if_y_is_encoded","title":"<code>check_if_y_is_encoded(y)</code>","text":"<p>Checks if the target y is composed of sequential ints from 0 to N. XGBoost requires the target to be encoded in this way.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <p>The target vector.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the target is encoded as sequential ints from 0 to N, False otherwise</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def check_if_y_is_encoded(y):\n    '''\n    Checks if the target y is composed of sequential ints from 0 to N.\n    XGBoost requires the target to be encoded in this way.\n\n    Parameters\n    ----------\n    y: np.ndarray\n        The target vector.\n\n    Returns\n    -------\n    bool\n        True if the target is encoded as sequential ints from 0 to N, False otherwise\n    '''\n    y = sorted(set(y))\n    return all(i == j for i, j in enumerate(y))\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/steady_state_estimator/#tpot.tpot_estimator.steady_state_estimator.convert_parents_tuples_to_integers","title":"<code>convert_parents_tuples_to_integers(row, object_to_int)</code>","text":"<p>Helper function to convert the parent rows into integers representing the index of the parent in the population.</p> <p>Original pandas dataframe using a custom index for the parents. This function converts the custom index to an integer index for easier manipulation by end users.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <p>The row to convert.</p> required <code>object_to_int</code> <p>A dictionary mapping the object to an integer index.</p> required Returns <p>tuple     The row with the custom index converted to an integer index.</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def convert_parents_tuples_to_integers(row, object_to_int):\n    \"\"\"\n    Helper function to convert the parent rows into integers representing the index of the parent in the population.\n\n    Original pandas dataframe using a custom index for the parents. This function converts the custom index to an integer index for easier manipulation by end users.\n\n    Parameters\n    ----------\n    row: list, np.ndarray, tuple\n        The row to convert.\n    object_to_int: dict\n        A dictionary mapping the object to an integer index.\n\n    Returns \n    -------\n    tuple\n        The row with the custom index converted to an integer index.\n    \"\"\"\n    if type(row) == list or type(row) == np.ndarray or type(row) == tuple:\n        return tuple(object_to_int[obj] for obj in row)\n    else:\n        return np.nan\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/steady_state_estimator/#tpot.tpot_estimator.steady_state_estimator.cross_val_score_objective","title":"<code>cross_val_score_objective(estimator, X, y, scorers, cv, fold=None)</code>","text":"<p>Compute the cross validated scores for a estimator. Only fits the estimator once per fold, and loops over the scorers to evaluate the estimator.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <p>The estimator to fit and score.</p> required <code>X</code> <p>The feature matrix.</p> required <code>y</code> <p>The target vector.</p> required <code>scorers</code> <p>The scorers to use.  If a list, will loop over the scorers and return a list of scorers. If a single scorer, will return a single score.</p> required <code>cv</code> <p>The cross-validator to use. For example, sklearn.model_selection.KFold or sklearn.model_selection.StratifiedKFold.</p> required <code>fold</code> <p>The fold to return the scores for. If None, will return the mean of all the scores (per scorer). Default is None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>scores</code> <code>ndarray or float</code> <p>The scores for the estimator per scorer. If fold is None, will return the mean of all the scores (per scorer). Returns a list if multiple scorers are used, otherwise returns a float for the single scorer.</p> Source code in <code>tpot/tpot_estimator/cross_val_utils.py</code> <pre><code>def cross_val_score_objective(estimator, X, y, scorers, cv, fold=None):\n    \"\"\"\n    Compute the cross validated scores for a estimator. Only fits the estimator once per fold, and loops over the scorers to evaluate the estimator.\n\n    Parameters\n    ----------\n    estimator: sklearn.base.BaseEstimator\n        The estimator to fit and score.\n    X: np.ndarray or pd.DataFrame\n        The feature matrix.\n    y: np.ndarray or pd.Series\n        The target vector.\n    scorers: list or scorer\n        The scorers to use. \n        If a list, will loop over the scorers and return a list of scorers.\n        If a single scorer, will return a single score.\n    cv: sklearn cross-validator\n        The cross-validator to use. For example, sklearn.model_selection.KFold or sklearn.model_selection.StratifiedKFold.\n    fold: int, optional\n        The fold to return the scores for. If None, will return the mean of all the scores (per scorer). Default is None.\n\n    Returns\n    -------\n    scores: np.ndarray or float\n        The scores for the estimator per scorer. If fold is None, will return the mean of all the scores (per scorer).\n        Returns a list if multiple scorers are used, otherwise returns a float for the single scorer.\n\n    \"\"\"\n\n    #check if scores is not iterable\n    if not isinstance(scorers, Iterable): \n        scorers = [scorers]\n    scores = []\n    if fold is None:\n        for train_index, test_index in cv.split(X, y):\n            this_fold_estimator = sklearn.base.clone(estimator)\n            if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n            else:\n                X_train, X_test = X[train_index], X[test_index]\n\n            if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n            else:\n                y_train, y_test = y[train_index], y[test_index]\n\n\n            start = time.time()\n            this_fold_estimator.fit(X_train,y_train)\n            duration = time.time() - start\n\n            this_fold_scores = [sklearn.metrics.get_scorer(scorer)(this_fold_estimator, X_test, y_test) for scorer in scorers] \n            scores.append(this_fold_scores)\n            del this_fold_estimator\n            del X_train\n            del X_test\n            del y_train\n            del y_test\n\n\n        return np.mean(scores,0)\n    else:\n        this_fold_estimator = sklearn.base.clone(estimator)\n        train_index, test_index = list(cv.split(X, y))[fold]\n        if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        else:\n            X_train, X_test = X[train_index], X[test_index]\n\n        if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n        else:\n            y_train, y_test = y[train_index], y[test_index]\n\n        start = time.time()\n        this_fold_estimator.fit(X_train,y_train)\n        duration = time.time() - start\n        this_fold_scores = [sklearn.metrics.get_scorer(scorer)(this_fold_estimator, X_test, y_test) for scorer in scorers] \n        return this_fold_scores\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/steady_state_estimator/#tpot.tpot_estimator.steady_state_estimator.objective_function_generator","title":"<code>objective_function_generator(pipeline, x, y, scorers, cv, other_objective_functions, step=None, budget=None, is_classification=True, export_graphpipeline=False, **pipeline_kwargs)</code>","text":"<p>Uses cross validation to evaluate the pipeline using the scorers, and concatenates results with scores from standalone other objective functions.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <p>The individual to evaluate.</p> required <code>x</code> <p>The feature matrix.</p> required <code>y</code> <p>The target vector.</p> required <code>scorers</code> <p>The scorers to use for cross validation.</p> required <code>cv</code> <p>The cross-validator to use. For example, sklearn.model_selection.KFold or sklearn.model_selection.StratifiedKFold. If an int, will use sklearn.model_selection.KFold with n_splits=cv.</p> required <code>other_objective_functions</code> <p>A list of standalone objective functions to evaluate the pipeline. With signature obj(pipeline) -&gt; float. or obj(pipeline) -&gt; np.ndarray These functions take in the unfitted estimator.</p> required <code>step</code> <p>The fold to return the scores for. If None, will return the mean of all the scores (per scorer). Default is None.</p> <code>None</code> <code>budget</code> <p>The budget to subsample the data. If None, will use the full dataset. Default is None. Will subsample budget*len(x) samples.</p> <code>None</code> <code>is_classification</code> <p>If True, will stratify the subsampling. Default is True.</p> <code>True</code> <code>export_graphpipeline</code> <p>Force the pipeline to be exported as a graph pipeline. Flattens all nested sklearn pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.</p> <code>False</code> <code>pipeline_kwargs</code> <p>Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The concatenated scores for the pipeline. The first len(scorers) elements are the cross validation scores, and the remaining elements are the standalone objective functions.</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def objective_function_generator(pipeline, x,y, scorers, cv, other_objective_functions, step=None, budget=None, is_classification=True, export_graphpipeline=False, **pipeline_kwargs):\n    \"\"\"\n    Uses cross validation to evaluate the pipeline using the scorers, and concatenates results with scores from standalone other objective functions.\n\n    Parameters\n    ----------\n    pipeline: tpot.SklearnIndividual\n        The individual to evaluate.\n    x: np.ndarray\n        The feature matrix.\n    y: np.ndarray\n        The target vector.\n    scorers: list\n        The scorers to use for cross validation. \n    cv: int, float, or sklearn cross-validator\n        The cross-validator to use. For example, sklearn.model_selection.KFold or sklearn.model_selection.StratifiedKFold.\n        If an int, will use sklearn.model_selection.KFold with n_splits=cv.\n    other_objective_functions: list\n        A list of standalone objective functions to evaluate the pipeline. With signature obj(pipeline) -&gt; float. or obj(pipeline) -&gt; np.ndarray\n        These functions take in the unfitted estimator.\n    step: int, optional\n        The fold to return the scores for. If None, will return the mean of all the scores (per scorer). Default is None.\n    budget: float, optional\n        The budget to subsample the data. If None, will use the full dataset. Default is None.\n        Will subsample budget*len(x) samples.\n    is_classification: bool, default=True\n        If True, will stratify the subsampling. Default is True.\n    export_graphpipeline: bool, default=False\n        Force the pipeline to be exported as a graph pipeline. Flattens all nested sklearn pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.\n    pipeline_kwargs: dict\n        Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.\n\n    Returns\n    -------\n    np.ndarray\n        The concatenated scores for the pipeline. The first len(scorers) elements are the cross validation scores, and the remaining elements are the standalone objective functions.\n\n    \"\"\"\n\n    if export_graphpipeline:\n        pipeline = pipeline.export_flattened_graphpipeline(**pipeline_kwargs)\n    else:\n        pipeline = pipeline.export_pipeline(**pipeline_kwargs)\n\n    if budget is not None and budget &lt; 1:\n        if is_classification:\n            x,y = sklearn.utils.resample(x,y, stratify=y, n_samples=int(budget*len(x)), replace=False, random_state=1)\n        else:\n            x,y = sklearn.utils.resample(x,y, n_samples=int(budget*len(x)), replace=False, random_state=1)\n\n        if isinstance(cv, int) or isinstance(cv, float):\n            n_splits = cv\n        else:\n            n_splits = cv.n_splits\n\n    if len(scorers) &gt; 0:\n        cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)\n    else:\n        cv_obj_scores = []\n\n    if other_objective_functions is not None and len(other_objective_functions) &gt;0:\n        other_scores = [obj(sklearn.base.clone(pipeline)) for obj in other_objective_functions]\n        #flatten\n        other_scores = np.array(other_scores).flatten().tolist()\n    else:\n        other_scores = []\n\n    return np.concatenate([cv_obj_scores,other_scores])\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/steady_state_estimator/#tpot.tpot_estimator.steady_state_estimator.remove_underrepresented_classes","title":"<code>remove_underrepresented_classes(x, y, min_count)</code>","text":"<p>Helper function to remove classes with less than min_count samples from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>The feature matrix.</p> required <code>y</code> <p>The target vector.</p> required <code>min_count</code> <p>The minimum number of samples to keep a class.</p> required <p>Returns:</p> Type Description <code>(ndarray, ndarray)</code> <p>The feature matrix and target vector with rows from classes with less than min_count samples removed.</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def remove_underrepresented_classes(x, y, min_count):\n    \"\"\"\n    Helper function to remove classes with less than min_count samples from the dataset.\n\n    Parameters\n    ----------\n    x: np.ndarray or pd.DataFrame\n        The feature matrix.\n    y: np.ndarray or pd.Series\n        The target vector.\n    min_count: int\n        The minimum number of samples to keep a class.\n\n    Returns\n    -------\n    np.ndarray, np.ndarray\n        The feature matrix and target vector with rows from classes with less than min_count samples removed.\n    \"\"\"\n    if isinstance(y, (np.ndarray, pd.Series)):\n        unique, counts = np.unique(y, return_counts=True)\n        if min(counts) &gt;= min_count:\n            return x, y\n        keep_classes = unique[counts &gt;= min_count]\n        mask = np.isin(y, keep_classes)\n        x = x[mask]\n        y = y[mask]\n    elif isinstance(y, pd.DataFrame):\n        counts = y.apply(pd.Series.value_counts)\n        if min(counts) &gt;= min_count:\n            return x, y\n        keep_classes = counts.index[counts &gt;= min_count].tolist()\n        mask = y.isin(keep_classes).all(axis=1)\n        x = x[mask]\n        y = y[mask]\n    else:\n        raise TypeError(\"y must be a numpy array or a pandas Series/DataFrame\")\n    return x, y\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/steady_state_estimator/#tpot.tpot_estimator.steady_state_estimator.val_objective_function_generator","title":"<code>val_objective_function_generator(pipeline, X_train, y_train, X_test, y_test, scorers, other_objective_functions, export_graphpipeline=False, **pipeline_kwargs)</code>","text":"<p>Trains a pipeline on a training set and evaluates it on a test set using the scorers and other objective functions.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <p>The individual to evaluate.</p> required <code>X_train</code> <p>The feature matrix of the training set.</p> required <code>y_train</code> <p>The target vector of the training set.</p> required <code>X_test</code> <p>The feature matrix of the test set.</p> required <code>y_test</code> <p>The target vector of the test set.</p> required <code>scorers</code> <p>The scorers to use for cross validation.</p> required <code>other_objective_functions</code> <p>A list of standalone objective functions to evaluate the pipeline. With signature obj(pipeline) -&gt; float. or obj(pipeline) -&gt; np.ndarray These functions take in the unfitted estimator.</p> required <code>export_graphpipeline</code> <p>Force the pipeline to be exported as a graph pipeline. Flattens all nested sklearn pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.</p> <code>False</code> <code>pipeline_kwargs</code> <p>Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The concatenated scores for the pipeline. The first len(scorers) elements are the cross validation scores, and the remaining elements are the standalone objective functions.</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def val_objective_function_generator(pipeline, X_train, y_train, X_test, y_test, scorers, other_objective_functions, export_graphpipeline=False, **pipeline_kwargs):\n    \"\"\"\n    Trains a pipeline on a training set and evaluates it on a test set using the scorers and other objective functions.\n\n    Parameters\n    ----------\n\n    pipeline: tpot.SklearnIndividual\n        The individual to evaluate.\n    X_train: np.ndarray\n        The feature matrix of the training set.\n    y_train: np.ndarray\n        The target vector of the training set.\n    X_test: np.ndarray\n        The feature matrix of the test set.\n    y_test: np.ndarray\n        The target vector of the test set.\n    scorers: list\n        The scorers to use for cross validation.\n    other_objective_functions: list\n        A list of standalone objective functions to evaluate the pipeline. With signature obj(pipeline) -&gt; float. or obj(pipeline) -&gt; np.ndarray\n        These functions take in the unfitted estimator.\n    export_graphpipeline: bool, default=False\n        Force the pipeline to be exported as a graph pipeline. Flattens all nested sklearn pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.\n    pipeline_kwargs: dict\n        Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.\n\n    Returns\n    -------\n    np.ndarray\n        The concatenated scores for the pipeline. The first len(scorers) elements are the cross validation scores, and the remaining elements are the standalone objective functions.\n\n\n    \"\"\"\n\n    #subsample the data\n    if export_graphpipeline:\n        pipeline = pipeline.export_flattened_graphpipeline(**pipeline_kwargs)\n    else:\n        pipeline = pipeline.export_pipeline(**pipeline_kwargs)\n\n    fitted_pipeline = sklearn.base.clone(pipeline)\n    fitted_pipeline.fit(X_train, y_train)\n\n    if len(scorers) &gt; 0:\n        scores =[sklearn.metrics.get_scorer(scorer)(fitted_pipeline, X_test, y_test) for scorer in scorers]\n\n    other_scores = []\n    if other_objective_functions is not None and len(other_objective_functions) &gt;0:\n        other_scores = [obj(sklearn.base.clone(pipeline)) for obj in other_objective_functions]\n\n    return np.concatenate([scores,other_scores])\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/templates/tpot_autoimputer/","title":"Tpot autoimputer","text":""},{"location":"documentation/tpot/tpot_estimator/templates/tpottemplates/","title":"Tpottemplates","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/tpot_estimator/templates/tpottemplates/#tpot.tpot_estimator.templates.tpottemplates.TPOTClassifier","title":"<code>TPOTClassifier</code>","text":"<p>               Bases: <code>TPOTEstimator</code></p> Source code in <code>tpot/tpot_estimator/templates/tpottemplates.py</code> <pre><code>class TPOTClassifier(TPOTEstimator):\n    def __init__(       self,\n                        search_space = \"linear\",\n                        scorers=['roc_auc_ovr'], \n                        scorers_weights=[1],\n                        cv = 10,\n                        other_objective_functions=[], #tpot.objectives.estimator_objective_functions.number_of_nodes_objective],\n                        other_objective_functions_weights = [],\n                        objective_function_names = None,\n                        bigger_is_better = True,\n                        categorical_features = None,\n                        memory = None,\n                        preprocessing = False,\n                        max_time_mins=60, \n                        max_eval_time_mins=10, \n                        n_jobs = 1,\n                        validation_strategy = \"none\",\n                        validation_fraction = .2, \n                        early_stop = None,\n                        warm_start = False,\n                        periodic_checkpoint_folder = None, \n                        verbose = 2,\n                        memory_limit = None,\n                        client = None,\n                        random_state=None,\n                        allow_inner_classifiers=None,\n                        **tpotestimator_kwargs,\n\n        ):\n        \"\"\"\n        An sklearn baseestimator that uses genetic programming to optimize a classification pipeline.\n        For more parameters, see the TPOTEstimator class.\n\n        Parameters\n        ----------\n\n        search_space : (String, tpot.search_spaces.SearchSpace)\n            - String : The default search space to use for the optimization.\n            | String     | Description      |\n            | :---        |    :----:   |\n            | linear  | A linear pipeline with the structure of \"Selector-&gt;(transformers+Passthrough)-&gt;(classifiers/regressors+Passthrough)-&gt;final classifier/regressor.\" For both the transformer and inner estimator layers, TPOT may choose one or more transformers/classifiers, or it may choose none. The inner classifier/regressor layer is optional. |\n            | linear-light | Same search space as linear, but without the inner classifier/regressor layer and with a reduced set of faster running estimators. |\n            | graph | TPOT will optimize a pipeline in the shape of a directed acyclic graph. The nodes of the graph can include selectors, scalers, transformers, or classifiers/regressors (inner classifiers/regressors can optionally be not included). This will return a custom GraphPipeline rather than an sklearn Pipeline. More details in Tutorial 6. |\n            | graph-light | Same as graph search space, but without the inner classifier/regressors and with a reduced set of faster running estimators. |\n            | mdr |TPOT will search over a series of feature selectors and Multifactor Dimensionality Reduction models to find a series of operators that maximize prediction accuracy. The TPOT MDR configuration is specialized for genome-wide association studies (GWAS), and is described in detail online here.\n\n            Note that TPOT MDR may be slow to run because the feature selection routines are computationally expensive, especially on large datasets. |\n            - SearchSpace : The search space to use for the optimization. This should be an instance of a SearchSpace.\n                The search space to use for the optimization. This should be an instance of a SearchSpace.\n                TPOT has groups of search spaces found in the following folders, tpot.search_spaces.nodes for the nodes in the pipeline and tpot.search_spaces.pipelines for the pipeline structure.\n\n        scorers : (list, scorer)\n            A scorer or list of scorers to be used in the cross-validation process.\n            see https://scikit-learn.org/stable/modules/model_evaluation.html\n\n        scorers_weights : list\n            A list of weights to be applied to the scorers during the optimization process.\n\n        classification : bool\n            If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem.\n            Used to determine the CV strategy.\n\n        cv : int, cross-validator\n            - (int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.\n            - (sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.\n                - max_depth (int): The maximum depth from any node to the root of the pipelines to be generated.\n\n        other_objective_functions : list, default=[]\n            A list of other objective functions to apply to the pipeline. The function takes a single parameter for the graphpipeline estimator and returns either a single score or a list of scores.\n\n        other_objective_functions_weights : list, default=[]\n            A list of weights to be applied to the other objective functions.\n\n        objective_function_names : list, default=None\n            A list of names to be applied to the objective functions. If None, will use the names of the objective functions.\n\n        bigger_is_better : bool, default=True\n            If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n\n        categorical_features : list or None\n            Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n\n        categorical_features: list or None\n            Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n            - None : If None, TPOT will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing.\n            - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices\n\n\n        memory: Memory object or string, default=None\n            If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. This feature\n            is used to avoid computing the fit transformers within a pipeline if the parameters\n            and input data are identical with another fitted pipeline during optimization process.\n            - String 'auto':\n                TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.\n            - String path of a caching directory\n                TPOT uses memory caching with the provided directory and TPOT does NOT clean\n                the caching directory up upon shutdown. If the directory does not exist, TPOT will\n                create it.\n            - Memory object:\n                TPOT uses the instance of joblib.Memory for memory caching,\n                and TPOT does NOT clean the caching directory up upon shutdown.\n            - None:\n                TPOT does not use memory caching.\n\n        preprocessing : bool or BaseEstimator/Pipeline,\n            EXPERIMENTAL\n            A pipeline that will be used to preprocess the data before CV. Note that the parameters for these steps are not optimized. Add them to the search space to be optimized.\n            - bool : If True, will use a default preprocessing pipeline which includes imputation followed by one hot encoding.\n            - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.\n\n        max_time_mins : float, default=float(\"inf\")\n            Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n\n        max_eval_time_mins : float, default=60*5\n            Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n\n\n        n_jobs : int, default=1\n            Number of processes to run in parallel.\n\n        validation_strategy : str, default='none'\n            EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT may overfit the cross validation score. A second validation set can be used to select the final pipeline.\n            - 'auto' : Automatically determine the validation strategy based on the dataset shape.\n            - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets.\n            - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets.\n            - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.\n\n        validation_fraction : float, default=0.2\n          EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.\n\n        early_stop : int, default=None\n            Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered. In general a value of around 5-20 is good.\n\n        warm_start : bool, default=False\n            If True, will use the continue the evolutionary algorithm from the last generation of the previous run.\n\n        periodic_checkpoint_folder : str, default=None\n            Folder to save the population to periodically. If None, no periodic saving will be done.\n            If provided, training will resume from this checkpoint.\n\n\n        verbose : int, default=1\n            How much information to print during the optimization process. Higher values include the information from lower values.\n            0. nothing\n            1. progress bar\n\n            3. best individual\n            4. warnings\n            &gt;=5. full warnings trace\n            6. evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_mins in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)\n\n\n        memory_limit : str, default=None\n            Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n\n        client : dask.distributed.Client, default=None\n            A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.\n\n        random_state : int, None, default=None\n            A seed for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes\n\n            - int\n                Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'\n            - None\n                Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS\n\n        allow_inner_classifiers : bool, default=True\n            If True, the search space will include ensembled classifiers. \n\n        Attributes\n        ----------\n\n        fitted_pipeline_ : GraphPipeline\n            A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.\n\n        evaluated_individuals : A pandas data frame containing data for all evaluated individuals in the run.\n            Columns:\n            - *objective functions : The first few columns correspond to the passed in scorers and objective functions\n            - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population.\n            - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population.\n            - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator.\n            - Generation : The generation the pipeline first appeared.\n            - Pareto_Front\t: The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.\n                            To save on computational time, the best frontier is updated iteratively each generation.\n                            The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.\n                            All other pipelines are set to NaN.\n            - Instance\t: The unfitted GraphPipeline BaseEstimator.\n            - *validation objective functions : Objective function scores evaluated on the validation set.\n            - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.\n\n        pareto_front : The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.\n        \"\"\"\n        self.search_space = search_space\n        self.scorers = scorers\n        self.scorers_weights = scorers_weights\n        self.cv = cv\n        self.other_objective_functions = other_objective_functions\n        self.other_objective_functions_weights = other_objective_functions_weights\n        self.objective_function_names = objective_function_names\n        self.bigger_is_better = bigger_is_better\n        self.categorical_features = categorical_features\n        self.memory = memory\n        self.preprocessing = preprocessing\n        self.max_time_mins = max_time_mins\n        self.max_eval_time_mins = max_eval_time_mins\n        self.n_jobs = n_jobs\n        self.validation_strategy = validation_strategy\n        self.validation_fraction = validation_fraction\n        self.early_stop = early_stop\n        self.warm_start = warm_start\n        self.periodic_checkpoint_folder = periodic_checkpoint_folder\n        self.verbose = verbose\n        self.memory_limit = memory_limit\n        self.client = client\n        self.random_state = random_state\n        self.tpotestimator_kwargs = tpotestimator_kwargs\n        self.allow_inner_classifiers = allow_inner_classifiers\n\n        self.initialized = False\n\n    def fit(self, X, y):\n\n        if not self.initialized:\n\n            get_search_space_params = {\"n_classes\": len(np.unique(y)), \n                                       \"n_samples\":len(y), \n                                       \"n_features\":X.shape[1], \n                                       \"random_state\":self.random_state}\n\n            search_space = get_template_search_spaces(self.search_space, classification=True, inner_predictors=self.allow_inner_classifiers, **get_search_space_params)\n\n\n            super(TPOTClassifier,self).__init__(\n                search_space=search_space,\n                scorers=self.scorers, \n                scorers_weights=self.scorers_weights,\n                cv = self.cv,\n                other_objective_functions=self.other_objective_functions, #tpot.objectives.estimator_objective_functions.number_of_nodes_objective],\n                other_objective_functions_weights = self.other_objective_functions_weights,\n                objective_function_names = self.objective_function_names,\n                bigger_is_better = self.bigger_is_better,\n                categorical_features = self.categorical_features,\n                memory = self.memory,\n                preprocessing = self.preprocessing,\n                max_time_mins=self.max_time_mins, \n                max_eval_time_mins=self.max_eval_time_mins, \n                n_jobs=self.n_jobs,\n                validation_strategy = self.validation_strategy,\n                validation_fraction = self.validation_fraction, \n                early_stop = self.early_stop,\n                warm_start = self.warm_start,\n                periodic_checkpoint_folder = self.periodic_checkpoint_folder, \n                verbose = self.verbose,\n                classification=True,\n                memory_limit = self.memory_limit,\n                client = self.client,\n                random_state=self.random_state,\n                **self.tpotestimator_kwargs)\n            self.initialized = True\n\n        return super().fit(X,y)\n\n\n    def predict(self, X, **predict_params):\n        check_is_fitted(self)\n        #X=check_array(X)\n        return self.fitted_pipeline_.predict(X,**predict_params)\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/templates/tpottemplates/#tpot.tpot_estimator.templates.tpottemplates.TPOTClassifier.__init__","title":"<code>__init__(search_space='linear', scorers=['roc_auc_ovr'], scorers_weights=[1], cv=10, other_objective_functions=[], other_objective_functions_weights=[], objective_function_names=None, bigger_is_better=True, categorical_features=None, memory=None, preprocessing=False, max_time_mins=60, max_eval_time_mins=10, n_jobs=1, validation_strategy='none', validation_fraction=0.2, early_stop=None, warm_start=False, periodic_checkpoint_folder=None, verbose=2, memory_limit=None, client=None, random_state=None, allow_inner_classifiers=None, **tpotestimator_kwargs)</code>","text":"<p>An sklearn baseestimator that uses genetic programming to optimize a classification pipeline. For more parameters, see the TPOTEstimator class.</p> <p>Parameters:</p> Name Type Description Default <code>search_space</code> <code>(String, SearchSpace)</code> <ul> <li>String : The default search space to use for the optimization. | String     | Description      | | :---        |    :----:   | | linear  | A linear pipeline with the structure of \"Selector-&gt;(transformers+Passthrough)-&gt;(classifiers/regressors+Passthrough)-&gt;final classifier/regressor.\" For both the transformer and inner estimator layers, TPOT may choose one or more transformers/classifiers, or it may choose none. The inner classifier/regressor layer is optional. | | linear-light | Same search space as linear, but without the inner classifier/regressor layer and with a reduced set of faster running estimators. | | graph | TPOT will optimize a pipeline in the shape of a directed acyclic graph. The nodes of the graph can include selectors, scalers, transformers, or classifiers/regressors (inner classifiers/regressors can optionally be not included). This will return a custom GraphPipeline rather than an sklearn Pipeline. More details in Tutorial 6. | | graph-light | Same as graph search space, but without the inner classifier/regressors and with a reduced set of faster running estimators. | | mdr |TPOT will search over a series of feature selectors and Multifactor Dimensionality Reduction models to find a series of operators that maximize prediction accuracy. The TPOT MDR configuration is specialized for genome-wide association studies (GWAS), and is described in detail online here.</li> </ul> <p>Note that TPOT MDR may be slow to run because the feature selection routines are computationally expensive, especially on large datasets. | - SearchSpace : The search space to use for the optimization. This should be an instance of a SearchSpace.     The search space to use for the optimization. This should be an instance of a SearchSpace.     TPOT has groups of search spaces found in the following folders, tpot.search_spaces.nodes for the nodes in the pipeline and tpot.search_spaces.pipelines for the pipeline structure.</p> <code>'linear'</code> <code>scorers</code> <code>(list, scorer)</code> <p>A scorer or list of scorers to be used in the cross-validation process. see https://scikit-learn.org/stable/modules/model_evaluation.html</p> <code>['roc_auc_ovr']</code> <code>scorers_weights</code> <code>list</code> <p>A list of weights to be applied to the scorers during the optimization process.</p> <code>[1]</code> <code>classification</code> <code>bool</code> <p>If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem. Used to determine the CV strategy.</p> required <code>cv</code> <code>(int, cross - validator)</code> <ul> <li>(int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.</li> <li>(sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.<ul> <li>max_depth (int): The maximum depth from any node to the root of the pipelines to be generated.</li> </ul> </li> </ul> <code>10</code> <code>other_objective_functions</code> <code>list</code> <p>A list of other objective functions to apply to the pipeline. The function takes a single parameter for the graphpipeline estimator and returns either a single score or a list of scores.</p> <code>[]</code> <code>other_objective_functions_weights</code> <code>list</code> <p>A list of weights to be applied to the other objective functions.</p> <code>[]</code> <code>objective_function_names</code> <code>list</code> <p>A list of names to be applied to the objective functions. If None, will use the names of the objective functions.</p> <code>None</code> <code>bigger_is_better</code> <code>bool</code> <p>If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.</p> <code>True</code> <code>categorical_features</code> <code>list or None</code> <p>Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.</p> <code>None</code> <code>categorical_features</code> <p>Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False. - None : If None, TPOT will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing. - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices</p> <code>None</code> <code>memory</code> <p>If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. This feature is used to avoid computing the fit transformers within a pipeline if the parameters and input data are identical with another fitted pipeline during optimization process. - String 'auto':     TPOT uses memory caching with a temporary directory and cleans it up upon shutdown. - String path of a caching directory     TPOT uses memory caching with the provided directory and TPOT does NOT clean     the caching directory up upon shutdown. If the directory does not exist, TPOT will     create it. - Memory object:     TPOT uses the instance of joblib.Memory for memory caching,     and TPOT does NOT clean the caching directory up upon shutdown. - None:     TPOT does not use memory caching.</p> <code>None</code> <code>preprocessing</code> <code>(bool or BaseEstimator / Pipeline)</code> <p>EXPERIMENTAL A pipeline that will be used to preprocess the data before CV. Note that the parameters for these steps are not optimized. Add them to the search space to be optimized. - bool : If True, will use a default preprocessing pipeline which includes imputation followed by one hot encoding. - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.</p> <code>False</code> <code>max_time_mins</code> <code>float</code> <p>Maximum time to run the optimization. If none or inf, will run until the end of the generations.</p> <code>float(\"inf\")</code> <code>max_eval_time_mins</code> <code>float</code> <p>Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.</p> <code>60*5</code> <code>n_jobs</code> <code>int</code> <p>Number of processes to run in parallel.</p> <code>1</code> <code>validation_strategy</code> <code>str</code> <p>EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT may overfit the cross validation score. A second validation set can be used to select the final pipeline. - 'auto' : Automatically determine the validation strategy based on the dataset shape. - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets. - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets. - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.</p> <code>'none'</code> <code>validation_fraction</code> <code>float</code> <p>EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.</p> <code>0.2</code> <code>early_stop</code> <code>int</code> <p>Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered. In general a value of around 5-20 is good.</p> <code>None</code> <code>warm_start</code> <code>bool</code> <p>If True, will use the continue the evolutionary algorithm from the last generation of the previous run.</p> <code>False</code> <code>periodic_checkpoint_folder</code> <code>str</code> <p>Folder to save the population to periodically. If None, no periodic saving will be done. If provided, training will resume from this checkpoint.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>How much information to print during the optimization process. Higher values include the information from lower values. 0. nothing 1. progress bar</p> <ol> <li>best individual</li> <li>warnings <p>=5. full warnings trace</p> </li> <li>evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_mins in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)</li> </ol> <code>1</code> <code>memory_limit</code> <code>str</code> <p>Memory limit for each job. See Dask LocalCluster documentation for more information.</p> <code>None</code> <code>client</code> <code>Client</code> <p>A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.</p> <code>None</code> <code>random_state</code> <code>(int, None)</code> <p>A seed for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes</p> <ul> <li>int     Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'</li> <li>None     Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS</li> </ul> <code>None</code> <code>allow_inner_classifiers</code> <code>bool</code> <p>If True, the search space will include ensembled classifiers.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>fitted_pipeline_</code> <code>GraphPipeline</code> <p>A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.</p> <code>evaluated_individuals</code> <code>A pandas data frame containing data for all evaluated individuals in the run.</code> <p>Columns: - objective functions : The first few columns correspond to the passed in scorers and objective functions - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population. - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population. - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator. - Generation : The generation the pipeline first appeared. - Pareto_Front      : The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.                 To save on computational time, the best frontier is updated iteratively each generation.                 The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.                 All other pipelines are set to NaN. - Instance  : The unfitted GraphPipeline BaseEstimator. - validation objective functions : Objective function scores evaluated on the validation set. - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.</p> <code>pareto_front</code> <code>The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.</code> Source code in <code>tpot/tpot_estimator/templates/tpottemplates.py</code> <pre><code>def __init__(       self,\n                    search_space = \"linear\",\n                    scorers=['roc_auc_ovr'], \n                    scorers_weights=[1],\n                    cv = 10,\n                    other_objective_functions=[], #tpot.objectives.estimator_objective_functions.number_of_nodes_objective],\n                    other_objective_functions_weights = [],\n                    objective_function_names = None,\n                    bigger_is_better = True,\n                    categorical_features = None,\n                    memory = None,\n                    preprocessing = False,\n                    max_time_mins=60, \n                    max_eval_time_mins=10, \n                    n_jobs = 1,\n                    validation_strategy = \"none\",\n                    validation_fraction = .2, \n                    early_stop = None,\n                    warm_start = False,\n                    periodic_checkpoint_folder = None, \n                    verbose = 2,\n                    memory_limit = None,\n                    client = None,\n                    random_state=None,\n                    allow_inner_classifiers=None,\n                    **tpotestimator_kwargs,\n\n    ):\n    \"\"\"\n    An sklearn baseestimator that uses genetic programming to optimize a classification pipeline.\n    For more parameters, see the TPOTEstimator class.\n\n    Parameters\n    ----------\n\n    search_space : (String, tpot.search_spaces.SearchSpace)\n        - String : The default search space to use for the optimization.\n        | String     | Description      |\n        | :---        |    :----:   |\n        | linear  | A linear pipeline with the structure of \"Selector-&gt;(transformers+Passthrough)-&gt;(classifiers/regressors+Passthrough)-&gt;final classifier/regressor.\" For both the transformer and inner estimator layers, TPOT may choose one or more transformers/classifiers, or it may choose none. The inner classifier/regressor layer is optional. |\n        | linear-light | Same search space as linear, but without the inner classifier/regressor layer and with a reduced set of faster running estimators. |\n        | graph | TPOT will optimize a pipeline in the shape of a directed acyclic graph. The nodes of the graph can include selectors, scalers, transformers, or classifiers/regressors (inner classifiers/regressors can optionally be not included). This will return a custom GraphPipeline rather than an sklearn Pipeline. More details in Tutorial 6. |\n        | graph-light | Same as graph search space, but without the inner classifier/regressors and with a reduced set of faster running estimators. |\n        | mdr |TPOT will search over a series of feature selectors and Multifactor Dimensionality Reduction models to find a series of operators that maximize prediction accuracy. The TPOT MDR configuration is specialized for genome-wide association studies (GWAS), and is described in detail online here.\n\n        Note that TPOT MDR may be slow to run because the feature selection routines are computationally expensive, especially on large datasets. |\n        - SearchSpace : The search space to use for the optimization. This should be an instance of a SearchSpace.\n            The search space to use for the optimization. This should be an instance of a SearchSpace.\n            TPOT has groups of search spaces found in the following folders, tpot.search_spaces.nodes for the nodes in the pipeline and tpot.search_spaces.pipelines for the pipeline structure.\n\n    scorers : (list, scorer)\n        A scorer or list of scorers to be used in the cross-validation process.\n        see https://scikit-learn.org/stable/modules/model_evaluation.html\n\n    scorers_weights : list\n        A list of weights to be applied to the scorers during the optimization process.\n\n    classification : bool\n        If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem.\n        Used to determine the CV strategy.\n\n    cv : int, cross-validator\n        - (int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.\n        - (sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.\n            - max_depth (int): The maximum depth from any node to the root of the pipelines to be generated.\n\n    other_objective_functions : list, default=[]\n        A list of other objective functions to apply to the pipeline. The function takes a single parameter for the graphpipeline estimator and returns either a single score or a list of scores.\n\n    other_objective_functions_weights : list, default=[]\n        A list of weights to be applied to the other objective functions.\n\n    objective_function_names : list, default=None\n        A list of names to be applied to the objective functions. If None, will use the names of the objective functions.\n\n    bigger_is_better : bool, default=True\n        If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n\n    categorical_features : list or None\n        Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n\n    categorical_features: list or None\n        Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n        - None : If None, TPOT will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing.\n        - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices\n\n\n    memory: Memory object or string, default=None\n        If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. This feature\n        is used to avoid computing the fit transformers within a pipeline if the parameters\n        and input data are identical with another fitted pipeline during optimization process.\n        - String 'auto':\n            TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.\n        - String path of a caching directory\n            TPOT uses memory caching with the provided directory and TPOT does NOT clean\n            the caching directory up upon shutdown. If the directory does not exist, TPOT will\n            create it.\n        - Memory object:\n            TPOT uses the instance of joblib.Memory for memory caching,\n            and TPOT does NOT clean the caching directory up upon shutdown.\n        - None:\n            TPOT does not use memory caching.\n\n    preprocessing : bool or BaseEstimator/Pipeline,\n        EXPERIMENTAL\n        A pipeline that will be used to preprocess the data before CV. Note that the parameters for these steps are not optimized. Add them to the search space to be optimized.\n        - bool : If True, will use a default preprocessing pipeline which includes imputation followed by one hot encoding.\n        - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.\n\n    max_time_mins : float, default=float(\"inf\")\n        Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n\n    max_eval_time_mins : float, default=60*5\n        Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n\n\n    n_jobs : int, default=1\n        Number of processes to run in parallel.\n\n    validation_strategy : str, default='none'\n        EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT may overfit the cross validation score. A second validation set can be used to select the final pipeline.\n        - 'auto' : Automatically determine the validation strategy based on the dataset shape.\n        - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets.\n        - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets.\n        - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.\n\n    validation_fraction : float, default=0.2\n      EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.\n\n    early_stop : int, default=None\n        Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered. In general a value of around 5-20 is good.\n\n    warm_start : bool, default=False\n        If True, will use the continue the evolutionary algorithm from the last generation of the previous run.\n\n    periodic_checkpoint_folder : str, default=None\n        Folder to save the population to periodically. If None, no periodic saving will be done.\n        If provided, training will resume from this checkpoint.\n\n\n    verbose : int, default=1\n        How much information to print during the optimization process. Higher values include the information from lower values.\n        0. nothing\n        1. progress bar\n\n        3. best individual\n        4. warnings\n        &gt;=5. full warnings trace\n        6. evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_mins in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)\n\n\n    memory_limit : str, default=None\n        Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n\n    client : dask.distributed.Client, default=None\n        A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.\n\n    random_state : int, None, default=None\n        A seed for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes\n\n        - int\n            Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'\n        - None\n            Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS\n\n    allow_inner_classifiers : bool, default=True\n        If True, the search space will include ensembled classifiers. \n\n    Attributes\n    ----------\n\n    fitted_pipeline_ : GraphPipeline\n        A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.\n\n    evaluated_individuals : A pandas data frame containing data for all evaluated individuals in the run.\n        Columns:\n        - *objective functions : The first few columns correspond to the passed in scorers and objective functions\n        - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population.\n        - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population.\n        - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator.\n        - Generation : The generation the pipeline first appeared.\n        - Pareto_Front\t: The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.\n                        To save on computational time, the best frontier is updated iteratively each generation.\n                        The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.\n                        All other pipelines are set to NaN.\n        - Instance\t: The unfitted GraphPipeline BaseEstimator.\n        - *validation objective functions : Objective function scores evaluated on the validation set.\n        - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.\n\n    pareto_front : The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.\n    \"\"\"\n    self.search_space = search_space\n    self.scorers = scorers\n    self.scorers_weights = scorers_weights\n    self.cv = cv\n    self.other_objective_functions = other_objective_functions\n    self.other_objective_functions_weights = other_objective_functions_weights\n    self.objective_function_names = objective_function_names\n    self.bigger_is_better = bigger_is_better\n    self.categorical_features = categorical_features\n    self.memory = memory\n    self.preprocessing = preprocessing\n    self.max_time_mins = max_time_mins\n    self.max_eval_time_mins = max_eval_time_mins\n    self.n_jobs = n_jobs\n    self.validation_strategy = validation_strategy\n    self.validation_fraction = validation_fraction\n    self.early_stop = early_stop\n    self.warm_start = warm_start\n    self.periodic_checkpoint_folder = periodic_checkpoint_folder\n    self.verbose = verbose\n    self.memory_limit = memory_limit\n    self.client = client\n    self.random_state = random_state\n    self.tpotestimator_kwargs = tpotestimator_kwargs\n    self.allow_inner_classifiers = allow_inner_classifiers\n\n    self.initialized = False\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/templates/tpottemplates/#tpot.tpot_estimator.templates.tpottemplates.TPOTRegressor","title":"<code>TPOTRegressor</code>","text":"<p>               Bases: <code>TPOTEstimator</code></p> Source code in <code>tpot/tpot_estimator/templates/tpottemplates.py</code> <pre><code>class TPOTRegressor(TPOTEstimator):\n    def __init__(       self,\n                        search_space = \"linear\",\n                        scorers=['neg_mean_squared_error'], \n                        scorers_weights=[1],\n                        cv = 10, #remove this and use a value based on dataset size?\n                        other_objective_functions=[], #tpot.objectives.estimator_objective_functions.number_of_nodes_objective],\n                        other_objective_functions_weights = [],\n                        objective_function_names = None,\n                        bigger_is_better = True,\n                        categorical_features = None,\n                        memory = None,\n                        preprocessing = False,\n                        max_time_mins=60, \n                        max_eval_time_mins=10, \n                        n_jobs = 1,\n                        validation_strategy = \"none\",\n                        validation_fraction = .2, \n                        early_stop = None,\n                        warm_start = False,\n                        periodic_checkpoint_folder = None, \n                        verbose = 2,\n                        memory_limit = None,\n                        client = None,\n                        random_state=None,\n                        allow_inner_regressors=None,\n                        **tpotestimator_kwargs,\n        ):\n        '''\n        An sklearn baseestimator that uses genetic programming to optimize a regression pipeline.\n        For more parameters, see the TPOTEstimator class.\n\n        Parameters\n        ----------\n\n        search_space : (String, tpot.search_spaces.SearchSpace)\n                        - String : The default search space to use for the optimization.\n            | String     | Description      |\n            | :---        |    :----:   |\n            | linear  | A linear pipeline with the structure of \"Selector-&gt;(transformers+Passthrough)-&gt;(classifiers/regressors+Passthrough)-&gt;final classifier/regressor.\" For both the transformer and inner estimator layers, TPOT may choose one or more transformers/classifiers, or it may choose none. The inner classifier/regressor layer is optional. |\n            | linear-light | Same search space as linear, but without the inner classifier/regressor layer and with a reduced set of faster running estimators. |\n            | graph | TPOT will optimize a pipeline in the shape of a directed acyclic graph. The nodes of the graph can include selectors, scalers, transformers, or classifiers/regressors (inner classifiers/regressors can optionally be not included). This will return a custom GraphPipeline rather than an sklearn Pipeline. More details in Tutorial 6. |\n            | graph-light | Same as graph search space, but without the inner classifier/regressors and with a reduced set of faster running estimators. |\n            | mdr |TPOT will search over a series of feature selectors and Multifactor Dimensionality Reduction models to find a series of operators that maximize prediction accuracy. The TPOT MDR configuration is specialized for genome-wide association studies (GWAS), and is described in detail online here.\n\n            Note that TPOT MDR may be slow to run because the feature selection routines are computationally expensive, especially on large datasets. |\n            - SearchSpace : The search space to use for the optimization. This should be an instance of a SearchSpace.\n                The search space to use for the optimization. This should be an instance of a SearchSpace.\n                TPOT has groups of search spaces found in the following folders, tpot.search_spaces.nodes for the nodes in the pipeline and tpot.search_spaces.pipelines for the pipeline structure.\n\n        scorers : (list, scorer)\n            A scorer or list of scorers to be used in the cross-validation process.\n            see https://scikit-learn.org/stable/modules/model_evaluation.html\n\n        scorers_weights : list\n            A list of weights to be applied to the scorers during the optimization process.\n\n        classification : bool\n            If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem.\n            Used to determine the CV strategy.\n\n        cv : int, cross-validator\n            - (int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.\n            - (sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.\n                - max_depth (int): The maximum depth from any node to the root of the pipelines to be generated.\n\n        other_objective_functions : list, default=[]\n            A list of other objective functions to apply to the pipeline. The function takes a single parameter for the graphpipeline estimator and returns either a single score or a list of scores.\n\n        other_objective_functions_weights : list, default=[]\n            A list of weights to be applied to the other objective functions.\n\n        objective_function_names : list, default=None\n            A list of names to be applied to the objective functions. If None, will use the names of the objective functions.\n\n        bigger_is_better : bool, default=True\n            If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n\n        categorical_features : list or None\n            Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n\n        categorical_features: list or None\n            Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n            - None : If None, TPOT will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing.\n            - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices\n\n\n        memory: Memory object or string, default=None\n            If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. This feature\n            is used to avoid computing the fit transformers within a pipeline if the parameters\n            and input data are identical with another fitted pipeline during optimization process.\n            - String 'auto':\n                TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.\n            - String path of a caching directory\n                TPOT uses memory caching with the provided directory and TPOT does NOT clean\n                the caching directory up upon shutdown. If the directory does not exist, TPOT will\n                create it.\n            - Memory object:\n                TPOT uses the instance of joblib.Memory for memory caching,\n                and TPOT does NOT clean the caching directory up upon shutdown.\n            - None:\n                TPOT does not use memory caching.\n\n        preprocessing : bool or BaseEstimator/Pipeline,\n            EXPERIMENTAL\n            A pipeline that will be used to preprocess the data before CV. Note that the parameters for these steps are not optimized. Add them to the search space to be optimized.\n            - bool : If True, will use a default preprocessing pipeline which includes imputation followed by one hot encoding.\n            - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.\n\n        max_time_mins : float, default=float(\"inf\")\n            Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n\n        max_eval_time_mins : float, default=60*5\n            Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n\n\n        n_jobs : int, default=1\n            Number of processes to run in parallel.\n\n        validation_strategy : str, default='none'\n            EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT may overfit the cross validation score. A second validation set can be used to select the final pipeline.\n            - 'auto' : Automatically determine the validation strategy based on the dataset shape.\n            - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets.\n            - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets.\n            - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.\n\n        validation_fraction : float, default=0.2\n          EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.\n\n        early_stop : int, default=None\n            Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered. In general a value of around 5-20 is good.\n\n        warm_start : bool, default=False\n            If True, will use the continue the evolutionary algorithm from the last generation of the previous run.\n\n        periodic_checkpoint_folder : str, default=None\n            Folder to save the population to periodically. If None, no periodic saving will be done.\n            If provided, training will resume from this checkpoint.\n\n\n        verbose : int, default=1\n            How much information to print during the optimization process. Higher values include the information from lower values.\n            0. nothing\n            1. progress bar\n\n            3. best individual\n            4. warnings\n            &gt;=5. full warnings trace\n            6. evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_mins in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)\n\n\n        memory_limit : str, default=None\n            Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n\n        client : dask.distributed.Client, default=None\n            A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.\n\n        random_state : int, None, default=None\n            A seed for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes\n\n            - int\n                Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'\n            - None\n                Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS\n\n        allow_inner_regressors : bool, default=True\n            If True, the search space will include ensembled regressors.\n\n        Attributes\n        ----------\n\n        fitted_pipeline_ : GraphPipeline\n            A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.\n\n        evaluated_individuals : A pandas data frame containing data for all evaluated individuals in the run.\n            Columns:\n            - *objective functions : The first few columns correspond to the passed in scorers and objective functions\n            - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population.\n            - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population.\n            - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator.\n            - Generation : The generation the pipeline first appeared.\n            - Pareto_Front\t: The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.\n                            To save on computational time, the best frontier is updated iteratively each generation.\n                            The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.\n                            All other pipelines are set to NaN.\n            - Instance\t: The unfitted GraphPipeline BaseEstimator.\n            - *validation objective functions : Objective function scores evaluated on the validation set.\n            - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.\n\n        pareto_front : The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.\n        '''\n\n        self.search_space = search_space\n        self.scorers = scorers\n        self.scorers_weights = scorers_weights\n        self.cv = cv\n        self.other_objective_functions = other_objective_functions\n        self.other_objective_functions_weights = other_objective_functions_weights\n        self.objective_function_names = objective_function_names\n        self.bigger_is_better = bigger_is_better\n        self.categorical_features = categorical_features\n        self.memory = memory\n        self.preprocessing = preprocessing\n        self.max_time_mins = max_time_mins\n        self.max_eval_time_mins = max_eval_time_mins\n        self.n_jobs = n_jobs\n        self.validation_strategy = validation_strategy\n        self.validation_fraction = validation_fraction\n        self.early_stop = early_stop\n        self.warm_start = warm_start\n        self.periodic_checkpoint_folder = periodic_checkpoint_folder\n        self.verbose = verbose\n        self.memory_limit = memory_limit\n        self.client = client\n        self.random_state = random_state\n        self.allow_inner_regressors = allow_inner_regressors\n        self.tpotestimator_kwargs = tpotestimator_kwargs\n\n        self.initialized = False\n\n\n    def fit(self, X, y):\n\n        if not self.initialized:\n            get_search_space_params = {\"n_classes\": None, \n                                        \"n_samples\":len(y), \n                                        \"n_features\":X.shape[1], \n                                        \"random_state\":self.random_state}\n\n            search_space = get_template_search_spaces(self.search_space, classification=False, inner_predictors=self.allow_inner_regressors, **get_search_space_params)\n\n            super(TPOTRegressor,self).__init__(\n                search_space=search_space,\n                scorers=self.scorers, \n                scorers_weights=self.scorers_weights,\n                cv=self.cv,\n                other_objective_functions=self.other_objective_functions, #tpot.objectives.estimator_objective_functions.number_of_nodes_objective],\n                other_objective_functions_weights = self.other_objective_functions_weights,\n                objective_function_names = self.objective_function_names,\n                bigger_is_better = self.bigger_is_better,\n                categorical_features = self.categorical_features,\n                memory = self.memory,\n                preprocessing = self.preprocessing,\n                max_time_mins=self.max_time_mins, \n                max_eval_time_mins=self.max_eval_time_mins, \n                n_jobs=self.n_jobs,\n                validation_strategy = self.validation_strategy,\n                validation_fraction = self.validation_fraction, \n                early_stop = self.early_stop,\n                warm_start = self.warm_start,\n                periodic_checkpoint_folder = self.periodic_checkpoint_folder, \n                verbose = self.verbose,\n                classification=False,\n                memory_limit = self.memory_limit,\n                client = self.client,\n                random_state=self.random_state,\n                **self.tpotestimator_kwargs)\n            self.initialized = True\n\n        return super().fit(X,y)\n</code></pre>"},{"location":"documentation/tpot/tpot_estimator/templates/tpottemplates/#tpot.tpot_estimator.templates.tpottemplates.TPOTRegressor.__init__","title":"<code>__init__(search_space='linear', scorers=['neg_mean_squared_error'], scorers_weights=[1], cv=10, other_objective_functions=[], other_objective_functions_weights=[], objective_function_names=None, bigger_is_better=True, categorical_features=None, memory=None, preprocessing=False, max_time_mins=60, max_eval_time_mins=10, n_jobs=1, validation_strategy='none', validation_fraction=0.2, early_stop=None, warm_start=False, periodic_checkpoint_folder=None, verbose=2, memory_limit=None, client=None, random_state=None, allow_inner_regressors=None, **tpotestimator_kwargs)</code>","text":"<p>An sklearn baseestimator that uses genetic programming to optimize a regression pipeline. For more parameters, see the TPOTEstimator class.</p> <p>Parameters:</p> Name Type Description Default <code>search_space</code> <code>(String, SearchSpace)</code> <pre><code>        - String : The default search space to use for the optimization.\n</code></pre> String Description linear A linear pipeline with the structure of \"Selector-&gt;(transformers+Passthrough)-&gt;(classifiers/regressors+Passthrough)-&gt;final classifier/regressor.\" For both the transformer and inner estimator layers, TPOT may choose one or more transformers/classifiers, or it may choose none. The inner classifier/regressor layer is optional. linear-light Same search space as linear, but without the inner classifier/regressor layer and with a reduced set of faster running estimators. graph TPOT will optimize a pipeline in the shape of a directed acyclic graph. The nodes of the graph can include selectors, scalers, transformers, or classifiers/regressors (inner classifiers/regressors can optionally be not included). This will return a custom GraphPipeline rather than an sklearn Pipeline. More details in Tutorial 6. graph-light Same as graph search space, but without the inner classifier/regressors and with a reduced set of faster running estimators. mdr TPOT will search over a series of feature selectors and Multifactor Dimensionality Reduction models to find a series of operators that maximize prediction accuracy. The TPOT MDR configuration is specialized for genome-wide association studies (GWAS), and is described in detail online here. <p>Note that TPOT MDR may be slow to run because the feature selection routines are computationally expensive, especially on large datasets. | - SearchSpace : The search space to use for the optimization. This should be an instance of a SearchSpace.     The search space to use for the optimization. This should be an instance of a SearchSpace.     TPOT has groups of search spaces found in the following folders, tpot.search_spaces.nodes for the nodes in the pipeline and tpot.search_spaces.pipelines for the pipeline structure.</p> <code>'linear'</code> <code>scorers</code> <code>(list, scorer)</code> <p>A scorer or list of scorers to be used in the cross-validation process. see https://scikit-learn.org/stable/modules/model_evaluation.html</p> <code>['neg_mean_squared_error']</code> <code>scorers_weights</code> <code>list</code> <p>A list of weights to be applied to the scorers during the optimization process.</p> <code>[1]</code> <code>classification</code> <code>bool</code> <p>If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem. Used to determine the CV strategy.</p> required <code>cv</code> <code>(int, cross - validator)</code> <ul> <li>(int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.</li> <li>(sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.<ul> <li>max_depth (int): The maximum depth from any node to the root of the pipelines to be generated.</li> </ul> </li> </ul> <code>10</code> <code>other_objective_functions</code> <code>list</code> <p>A list of other objective functions to apply to the pipeline. The function takes a single parameter for the graphpipeline estimator and returns either a single score or a list of scores.</p> <code>[]</code> <code>other_objective_functions_weights</code> <code>list</code> <p>A list of weights to be applied to the other objective functions.</p> <code>[]</code> <code>objective_function_names</code> <code>list</code> <p>A list of names to be applied to the objective functions. If None, will use the names of the objective functions.</p> <code>None</code> <code>bigger_is_better</code> <code>bool</code> <p>If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.</p> <code>True</code> <code>categorical_features</code> <code>list or None</code> <p>Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.</p> <code>None</code> <code>categorical_features</code> <p>Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False. - None : If None, TPOT will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing. - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices</p> <code>None</code> <code>memory</code> <p>If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. This feature is used to avoid computing the fit transformers within a pipeline if the parameters and input data are identical with another fitted pipeline during optimization process. - String 'auto':     TPOT uses memory caching with a temporary directory and cleans it up upon shutdown. - String path of a caching directory     TPOT uses memory caching with the provided directory and TPOT does NOT clean     the caching directory up upon shutdown. If the directory does not exist, TPOT will     create it. - Memory object:     TPOT uses the instance of joblib.Memory for memory caching,     and TPOT does NOT clean the caching directory up upon shutdown. - None:     TPOT does not use memory caching.</p> <code>None</code> <code>preprocessing</code> <code>(bool or BaseEstimator / Pipeline)</code> <p>EXPERIMENTAL A pipeline that will be used to preprocess the data before CV. Note that the parameters for these steps are not optimized. Add them to the search space to be optimized. - bool : If True, will use a default preprocessing pipeline which includes imputation followed by one hot encoding. - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.</p> <code>False</code> <code>max_time_mins</code> <code>float</code> <p>Maximum time to run the optimization. If none or inf, will run until the end of the generations.</p> <code>float(\"inf\")</code> <code>max_eval_time_mins</code> <code>float</code> <p>Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.</p> <code>60*5</code> <code>n_jobs</code> <code>int</code> <p>Number of processes to run in parallel.</p> <code>1</code> <code>validation_strategy</code> <code>str</code> <p>EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT may overfit the cross validation score. A second validation set can be used to select the final pipeline. - 'auto' : Automatically determine the validation strategy based on the dataset shape. - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets. - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets. - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.</p> <code>'none'</code> <code>validation_fraction</code> <code>float</code> <p>EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.</p> <code>0.2</code> <code>early_stop</code> <code>int</code> <p>Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered. In general a value of around 5-20 is good.</p> <code>None</code> <code>warm_start</code> <code>bool</code> <p>If True, will use the continue the evolutionary algorithm from the last generation of the previous run.</p> <code>False</code> <code>periodic_checkpoint_folder</code> <code>str</code> <p>Folder to save the population to periodically. If None, no periodic saving will be done. If provided, training will resume from this checkpoint.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>How much information to print during the optimization process. Higher values include the information from lower values. 0. nothing 1. progress bar</p> <ol> <li>best individual</li> <li>warnings <p>=5. full warnings trace</p> </li> <li>evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_mins in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)</li> </ol> <code>1</code> <code>memory_limit</code> <code>str</code> <p>Memory limit for each job. See Dask LocalCluster documentation for more information.</p> <code>None</code> <code>client</code> <code>Client</code> <p>A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.</p> <code>None</code> <code>random_state</code> <code>(int, None)</code> <p>A seed for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes</p> <ul> <li>int     Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'</li> <li>None     Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS</li> </ul> <code>None</code> <code>allow_inner_regressors</code> <code>bool</code> <p>If True, the search space will include ensembled regressors.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>fitted_pipeline_</code> <code>GraphPipeline</code> <p>A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.</p> <code>evaluated_individuals</code> <code>A pandas data frame containing data for all evaluated individuals in the run.</code> <p>Columns: - objective functions : The first few columns correspond to the passed in scorers and objective functions - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population. - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population. - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator. - Generation : The generation the pipeline first appeared. - Pareto_Front      : The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.                 To save on computational time, the best frontier is updated iteratively each generation.                 The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.                 All other pipelines are set to NaN. - Instance  : The unfitted GraphPipeline BaseEstimator. - validation objective functions : Objective function scores evaluated on the validation set. - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.</p> <code>pareto_front</code> <code>The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.</code> Source code in <code>tpot/tpot_estimator/templates/tpottemplates.py</code> <pre><code>def __init__(       self,\n                    search_space = \"linear\",\n                    scorers=['neg_mean_squared_error'], \n                    scorers_weights=[1],\n                    cv = 10, #remove this and use a value based on dataset size?\n                    other_objective_functions=[], #tpot.objectives.estimator_objective_functions.number_of_nodes_objective],\n                    other_objective_functions_weights = [],\n                    objective_function_names = None,\n                    bigger_is_better = True,\n                    categorical_features = None,\n                    memory = None,\n                    preprocessing = False,\n                    max_time_mins=60, \n                    max_eval_time_mins=10, \n                    n_jobs = 1,\n                    validation_strategy = \"none\",\n                    validation_fraction = .2, \n                    early_stop = None,\n                    warm_start = False,\n                    periodic_checkpoint_folder = None, \n                    verbose = 2,\n                    memory_limit = None,\n                    client = None,\n                    random_state=None,\n                    allow_inner_regressors=None,\n                    **tpotestimator_kwargs,\n    ):\n    '''\n    An sklearn baseestimator that uses genetic programming to optimize a regression pipeline.\n    For more parameters, see the TPOTEstimator class.\n\n    Parameters\n    ----------\n\n    search_space : (String, tpot.search_spaces.SearchSpace)\n                    - String : The default search space to use for the optimization.\n        | String     | Description      |\n        | :---        |    :----:   |\n        | linear  | A linear pipeline with the structure of \"Selector-&gt;(transformers+Passthrough)-&gt;(classifiers/regressors+Passthrough)-&gt;final classifier/regressor.\" For both the transformer and inner estimator layers, TPOT may choose one or more transformers/classifiers, or it may choose none. The inner classifier/regressor layer is optional. |\n        | linear-light | Same search space as linear, but without the inner classifier/regressor layer and with a reduced set of faster running estimators. |\n        | graph | TPOT will optimize a pipeline in the shape of a directed acyclic graph. The nodes of the graph can include selectors, scalers, transformers, or classifiers/regressors (inner classifiers/regressors can optionally be not included). This will return a custom GraphPipeline rather than an sklearn Pipeline. More details in Tutorial 6. |\n        | graph-light | Same as graph search space, but without the inner classifier/regressors and with a reduced set of faster running estimators. |\n        | mdr |TPOT will search over a series of feature selectors and Multifactor Dimensionality Reduction models to find a series of operators that maximize prediction accuracy. The TPOT MDR configuration is specialized for genome-wide association studies (GWAS), and is described in detail online here.\n\n        Note that TPOT MDR may be slow to run because the feature selection routines are computationally expensive, especially on large datasets. |\n        - SearchSpace : The search space to use for the optimization. This should be an instance of a SearchSpace.\n            The search space to use for the optimization. This should be an instance of a SearchSpace.\n            TPOT has groups of search spaces found in the following folders, tpot.search_spaces.nodes for the nodes in the pipeline and tpot.search_spaces.pipelines for the pipeline structure.\n\n    scorers : (list, scorer)\n        A scorer or list of scorers to be used in the cross-validation process.\n        see https://scikit-learn.org/stable/modules/model_evaluation.html\n\n    scorers_weights : list\n        A list of weights to be applied to the scorers during the optimization process.\n\n    classification : bool\n        If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem.\n        Used to determine the CV strategy.\n\n    cv : int, cross-validator\n        - (int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.\n        - (sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.\n            - max_depth (int): The maximum depth from any node to the root of the pipelines to be generated.\n\n    other_objective_functions : list, default=[]\n        A list of other objective functions to apply to the pipeline. The function takes a single parameter for the graphpipeline estimator and returns either a single score or a list of scores.\n\n    other_objective_functions_weights : list, default=[]\n        A list of weights to be applied to the other objective functions.\n\n    objective_function_names : list, default=None\n        A list of names to be applied to the objective functions. If None, will use the names of the objective functions.\n\n    bigger_is_better : bool, default=True\n        If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n\n    categorical_features : list or None\n        Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n\n    categorical_features: list or None\n        Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n        - None : If None, TPOT will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing.\n        - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices\n\n\n    memory: Memory object or string, default=None\n        If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. This feature\n        is used to avoid computing the fit transformers within a pipeline if the parameters\n        and input data are identical with another fitted pipeline during optimization process.\n        - String 'auto':\n            TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.\n        - String path of a caching directory\n            TPOT uses memory caching with the provided directory and TPOT does NOT clean\n            the caching directory up upon shutdown. If the directory does not exist, TPOT will\n            create it.\n        - Memory object:\n            TPOT uses the instance of joblib.Memory for memory caching,\n            and TPOT does NOT clean the caching directory up upon shutdown.\n        - None:\n            TPOT does not use memory caching.\n\n    preprocessing : bool or BaseEstimator/Pipeline,\n        EXPERIMENTAL\n        A pipeline that will be used to preprocess the data before CV. Note that the parameters for these steps are not optimized. Add them to the search space to be optimized.\n        - bool : If True, will use a default preprocessing pipeline which includes imputation followed by one hot encoding.\n        - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.\n\n    max_time_mins : float, default=float(\"inf\")\n        Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n\n    max_eval_time_mins : float, default=60*5\n        Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n\n\n    n_jobs : int, default=1\n        Number of processes to run in parallel.\n\n    validation_strategy : str, default='none'\n        EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT may overfit the cross validation score. A second validation set can be used to select the final pipeline.\n        - 'auto' : Automatically determine the validation strategy based on the dataset shape.\n        - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets.\n        - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets.\n        - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.\n\n    validation_fraction : float, default=0.2\n      EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.\n\n    early_stop : int, default=None\n        Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered. In general a value of around 5-20 is good.\n\n    warm_start : bool, default=False\n        If True, will use the continue the evolutionary algorithm from the last generation of the previous run.\n\n    periodic_checkpoint_folder : str, default=None\n        Folder to save the population to periodically. If None, no periodic saving will be done.\n        If provided, training will resume from this checkpoint.\n\n\n    verbose : int, default=1\n        How much information to print during the optimization process. Higher values include the information from lower values.\n        0. nothing\n        1. progress bar\n\n        3. best individual\n        4. warnings\n        &gt;=5. full warnings trace\n        6. evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_mins in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)\n\n\n    memory_limit : str, default=None\n        Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n\n    client : dask.distributed.Client, default=None\n        A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.\n\n    random_state : int, None, default=None\n        A seed for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes\n\n        - int\n            Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'\n        - None\n            Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS\n\n    allow_inner_regressors : bool, default=True\n        If True, the search space will include ensembled regressors.\n\n    Attributes\n    ----------\n\n    fitted_pipeline_ : GraphPipeline\n        A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.\n\n    evaluated_individuals : A pandas data frame containing data for all evaluated individuals in the run.\n        Columns:\n        - *objective functions : The first few columns correspond to the passed in scorers and objective functions\n        - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population.\n        - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population.\n        - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator.\n        - Generation : The generation the pipeline first appeared.\n        - Pareto_Front\t: The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.\n                        To save on computational time, the best frontier is updated iteratively each generation.\n                        The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.\n                        All other pipelines are set to NaN.\n        - Instance\t: The unfitted GraphPipeline BaseEstimator.\n        - *validation objective functions : Objective function scores evaluated on the validation set.\n        - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.\n\n    pareto_front : The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.\n    '''\n\n    self.search_space = search_space\n    self.scorers = scorers\n    self.scorers_weights = scorers_weights\n    self.cv = cv\n    self.other_objective_functions = other_objective_functions\n    self.other_objective_functions_weights = other_objective_functions_weights\n    self.objective_function_names = objective_function_names\n    self.bigger_is_better = bigger_is_better\n    self.categorical_features = categorical_features\n    self.memory = memory\n    self.preprocessing = preprocessing\n    self.max_time_mins = max_time_mins\n    self.max_eval_time_mins = max_eval_time_mins\n    self.n_jobs = n_jobs\n    self.validation_strategy = validation_strategy\n    self.validation_fraction = validation_fraction\n    self.early_stop = early_stop\n    self.warm_start = warm_start\n    self.periodic_checkpoint_folder = periodic_checkpoint_folder\n    self.verbose = verbose\n    self.memory_limit = memory_limit\n    self.client = client\n    self.random_state = random_state\n    self.allow_inner_regressors = allow_inner_regressors\n    self.tpotestimator_kwargs = tpotestimator_kwargs\n\n    self.initialized = False\n</code></pre>"},{"location":"documentation/tpot/utils/amltk_parser/","title":"Amltk parser","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/utils/amltk_parser/#tpot.utils.amltk_parser.tpot_parser","title":"<code>tpot_parser(node)</code>","text":"<p>Convert amltk pipeline search space into a tpot pipeline search space.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The node to convert.</p> required <p>Returns:</p> Type Description <code>SearchSpace</code> <p>The equivalent TPOT search space which can be optimized by TPOT.</p> Source code in <code>tpot/utils/amltk_parser.py</code> <pre><code>def tpot_parser(\n    node: Node,\n    ):\n    \"\"\"\n    Convert amltk pipeline search space into a tpot pipeline search space.\n\n    Parameters\n    ----------\n    node: amltk.pipeline.Node\n        The node to convert.\n\n    Returns\n    -------\n    tpot.search_spaces.base.SearchSpace\n        The equivalent TPOT search space which can be optimized by TPOT.\n    \"\"\"\n\n    if isinstance(node, Component):\n        return component_to_estimatornode(node)\n    elif isinstance(node, Sequential):\n        return sequential_to_sequentialpipeline(node)\n    elif isinstance(node, Choice):\n        return choice_to_choicepipeline(node)\n    elif isinstance(node, Fixed):\n        return fixed_to_estimatornode(node)\n    elif isinstance(node, Split):\n        return split_to_unionpipeline(node)\n    else:\n        raise ValueError(f\"Node type {type(node)} not supported\")\n</code></pre>"},{"location":"documentation/tpot/utils/eval_utils/","title":"Eval utils","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/utils/eval_utils/#tpot.utils.eval_utils.process_scores","title":"<code>process_scores(scores, n)</code>","text":"<p>Purpose: This function processes a list of scores to ensure that each score list has the same length, n. If a score list is shorter than n, the function fills the list with either \"TIMEOUT\" or \"INVALID\" values.</p> <p>Parameters:</p> <pre><code>scores: A list of score lists. Each score list represents a set of scores for a particular player or team. The score lists may have different lengths.\nn: An integer representing the desired length for each score list.\n</code></pre> <p>Returns:</p> <pre><code>The scores list, after processing.\n</code></pre> Source code in <code>tpot/utils/eval_utils.py</code> <pre><code>def process_scores(scores, n):\n    '''\n    Purpose: This function processes a list of scores to ensure that each score list has the same length, n. If a score list is shorter than n, the function fills the list with either \"TIMEOUT\" or \"INVALID\" values.\n\n    Parameters:\n\n        scores: A list of score lists. Each score list represents a set of scores for a particular player or team. The score lists may have different lengths.\n        n: An integer representing the desired length for each score list.\n\n    Returns:\n\n        The scores list, after processing.\n\n    '''\n    for i in range(len(scores)):\n        if len(scores[i]) &lt; n:\n            if \"TIMEOUT\" in scores[i]:\n                scores[i] = [\"TIMEOUT\" for j in range(n)]\n            else:\n                scores[i] = [\"INVALID\" for j in range(n)]\n    return scores\n</code></pre>"},{"location":"documentation/tpot/utils/utils/","title":"Utils","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot/utils/utils/#tpot.utils.utils.is_pareto_efficient","title":"<code>is_pareto_efficient(scores, return_mask=True)</code>","text":"<p>Find the pareto-efficient points :param scores: An (n_points, n_scores) array :param return_mask: True to return a mask :return: An array of indices of pareto-efficient points.     If return_mask is True, this will be an (n_points, ) boolean array     Otherwise it will be a (n_efficient_points, ) integer array of indices.</p> Source code in <code>tpot/utils/utils.py</code> <pre><code>def is_pareto_efficient(scores, return_mask = True):\n    \"\"\"\n    Find the pareto-efficient points\n    :param scores: An (n_points, n_scores) array\n    :param return_mask: True to return a mask\n    :return: An array of indices of pareto-efficient points.\n        If return_mask is True, this will be an (n_points, ) boolean array\n        Otherwise it will be a (n_efficient_points, ) integer array of indices.\n    \"\"\"\n    is_efficient = np.arange(scores.shape[0])\n    n_points = scores.shape[0]\n    next_point_index = 0  # Next index in the is_efficient array to search for\n    while next_point_index&lt;len(scores):\n        nondominated_point_mask = np.any(scores&gt;scores[next_point_index], axis=1)\n        nondominated_point_mask[next_point_index] = True\n        is_efficient = is_efficient[nondominated_point_mask]  # Remove dominated points\n        scores = scores[nondominated_point_mask]\n        next_point_index = np.sum(nondominated_point_mask[:next_point_index])+1\n    if return_mask:\n        is_efficient_mask = np.zeros(n_points, dtype = bool)\n        is_efficient_mask[is_efficient] = True\n        return is_efficient_mask\n    else:\n        return is_efficient\n</code></pre>"},{"location":"tpot_api/classifier/","title":"Classifier","text":"<p>               Bases: <code>TPOTEstimator</code></p> Source code in <code>tpot/tpot_estimator/templates/tpottemplates.py</code> <pre><code>class TPOTClassifier(TPOTEstimator):\n    def __init__(       self,\n                        search_space = \"linear\",\n                        scorers=['roc_auc_ovr'], \n                        scorers_weights=[1],\n                        cv = 10,\n                        other_objective_functions=[], #tpot.objectives.estimator_objective_functions.number_of_nodes_objective],\n                        other_objective_functions_weights = [],\n                        objective_function_names = None,\n                        bigger_is_better = True,\n                        categorical_features = None,\n                        memory = None,\n                        preprocessing = False,\n                        max_time_mins=60, \n                        max_eval_time_mins=10, \n                        n_jobs = 1,\n                        validation_strategy = \"none\",\n                        validation_fraction = .2, \n                        early_stop = None,\n                        warm_start = False,\n                        periodic_checkpoint_folder = None, \n                        verbose = 2,\n                        memory_limit = None,\n                        client = None,\n                        random_state=None,\n                        allow_inner_classifiers=None,\n                        **tpotestimator_kwargs,\n\n        ):\n        \"\"\"\n        An sklearn baseestimator that uses genetic programming to optimize a classification pipeline.\n        For more parameters, see the TPOTEstimator class.\n\n        Parameters\n        ----------\n\n        search_space : (String, tpot.search_spaces.SearchSpace)\n            - String : The default search space to use for the optimization.\n            | String     | Description      |\n            | :---        |    :----:   |\n            | linear  | A linear pipeline with the structure of \"Selector-&gt;(transformers+Passthrough)-&gt;(classifiers/regressors+Passthrough)-&gt;final classifier/regressor.\" For both the transformer and inner estimator layers, TPOT may choose one or more transformers/classifiers, or it may choose none. The inner classifier/regressor layer is optional. |\n            | linear-light | Same search space as linear, but without the inner classifier/regressor layer and with a reduced set of faster running estimators. |\n            | graph | TPOT will optimize a pipeline in the shape of a directed acyclic graph. The nodes of the graph can include selectors, scalers, transformers, or classifiers/regressors (inner classifiers/regressors can optionally be not included). This will return a custom GraphPipeline rather than an sklearn Pipeline. More details in Tutorial 6. |\n            | graph-light | Same as graph search space, but without the inner classifier/regressors and with a reduced set of faster running estimators. |\n            | mdr |TPOT will search over a series of feature selectors and Multifactor Dimensionality Reduction models to find a series of operators that maximize prediction accuracy. The TPOT MDR configuration is specialized for genome-wide association studies (GWAS), and is described in detail online here.\n\n            Note that TPOT MDR may be slow to run because the feature selection routines are computationally expensive, especially on large datasets. |\n            - SearchSpace : The search space to use for the optimization. This should be an instance of a SearchSpace.\n                The search space to use for the optimization. This should be an instance of a SearchSpace.\n                TPOT has groups of search spaces found in the following folders, tpot.search_spaces.nodes for the nodes in the pipeline and tpot.search_spaces.pipelines for the pipeline structure.\n\n        scorers : (list, scorer)\n            A scorer or list of scorers to be used in the cross-validation process.\n            see https://scikit-learn.org/stable/modules/model_evaluation.html\n\n        scorers_weights : list\n            A list of weights to be applied to the scorers during the optimization process.\n\n        classification : bool\n            If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem.\n            Used to determine the CV strategy.\n\n        cv : int, cross-validator\n            - (int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.\n            - (sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.\n                - max_depth (int): The maximum depth from any node to the root of the pipelines to be generated.\n\n        other_objective_functions : list, default=[]\n            A list of other objective functions to apply to the pipeline. The function takes a single parameter for the graphpipeline estimator and returns either a single score or a list of scores.\n\n        other_objective_functions_weights : list, default=[]\n            A list of weights to be applied to the other objective functions.\n\n        objective_function_names : list, default=None\n            A list of names to be applied to the objective functions. If None, will use the names of the objective functions.\n\n        bigger_is_better : bool, default=True\n            If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n\n        categorical_features : list or None\n            Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n\n        categorical_features: list or None\n            Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n            - None : If None, TPOT will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing.\n            - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices\n\n\n        memory: Memory object or string, default=None\n            If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. This feature\n            is used to avoid computing the fit transformers within a pipeline if the parameters\n            and input data are identical with another fitted pipeline during optimization process.\n            - String 'auto':\n                TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.\n            - String path of a caching directory\n                TPOT uses memory caching with the provided directory and TPOT does NOT clean\n                the caching directory up upon shutdown. If the directory does not exist, TPOT will\n                create it.\n            - Memory object:\n                TPOT uses the instance of joblib.Memory for memory caching,\n                and TPOT does NOT clean the caching directory up upon shutdown.\n            - None:\n                TPOT does not use memory caching.\n\n        preprocessing : bool or BaseEstimator/Pipeline,\n            EXPERIMENTAL\n            A pipeline that will be used to preprocess the data before CV. Note that the parameters for these steps are not optimized. Add them to the search space to be optimized.\n            - bool : If True, will use a default preprocessing pipeline which includes imputation followed by one hot encoding.\n            - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.\n\n        max_time_mins : float, default=float(\"inf\")\n            Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n\n        max_eval_time_mins : float, default=60*5\n            Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n\n\n        n_jobs : int, default=1\n            Number of processes to run in parallel.\n\n        validation_strategy : str, default='none'\n            EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT may overfit the cross validation score. A second validation set can be used to select the final pipeline.\n            - 'auto' : Automatically determine the validation strategy based on the dataset shape.\n            - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets.\n            - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets.\n            - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.\n\n        validation_fraction : float, default=0.2\n          EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.\n\n        early_stop : int, default=None\n            Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered. In general a value of around 5-20 is good.\n\n        warm_start : bool, default=False\n            If True, will use the continue the evolutionary algorithm from the last generation of the previous run.\n\n        periodic_checkpoint_folder : str, default=None\n            Folder to save the population to periodically. If None, no periodic saving will be done.\n            If provided, training will resume from this checkpoint.\n\n\n        verbose : int, default=1\n            How much information to print during the optimization process. Higher values include the information from lower values.\n            0. nothing\n            1. progress bar\n\n            3. best individual\n            4. warnings\n            &gt;=5. full warnings trace\n            6. evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_mins in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)\n\n\n        memory_limit : str, default=None\n            Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n\n        client : dask.distributed.Client, default=None\n            A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.\n\n        random_state : int, None, default=None\n            A seed for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes\n\n            - int\n                Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'\n            - None\n                Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS\n\n        allow_inner_classifiers : bool, default=True\n            If True, the search space will include ensembled classifiers. \n\n        Attributes\n        ----------\n\n        fitted_pipeline_ : GraphPipeline\n            A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.\n\n        evaluated_individuals : A pandas data frame containing data for all evaluated individuals in the run.\n            Columns:\n            - *objective functions : The first few columns correspond to the passed in scorers and objective functions\n            - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population.\n            - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population.\n            - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator.\n            - Generation : The generation the pipeline first appeared.\n            - Pareto_Front\t: The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.\n                            To save on computational time, the best frontier is updated iteratively each generation.\n                            The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.\n                            All other pipelines are set to NaN.\n            - Instance\t: The unfitted GraphPipeline BaseEstimator.\n            - *validation objective functions : Objective function scores evaluated on the validation set.\n            - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.\n\n        pareto_front : The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.\n        \"\"\"\n        self.search_space = search_space\n        self.scorers = scorers\n        self.scorers_weights = scorers_weights\n        self.cv = cv\n        self.other_objective_functions = other_objective_functions\n        self.other_objective_functions_weights = other_objective_functions_weights\n        self.objective_function_names = objective_function_names\n        self.bigger_is_better = bigger_is_better\n        self.categorical_features = categorical_features\n        self.memory = memory\n        self.preprocessing = preprocessing\n        self.max_time_mins = max_time_mins\n        self.max_eval_time_mins = max_eval_time_mins\n        self.n_jobs = n_jobs\n        self.validation_strategy = validation_strategy\n        self.validation_fraction = validation_fraction\n        self.early_stop = early_stop\n        self.warm_start = warm_start\n        self.periodic_checkpoint_folder = periodic_checkpoint_folder\n        self.verbose = verbose\n        self.memory_limit = memory_limit\n        self.client = client\n        self.random_state = random_state\n        self.tpotestimator_kwargs = tpotestimator_kwargs\n        self.allow_inner_classifiers = allow_inner_classifiers\n\n        self.initialized = False\n\n    def fit(self, X, y):\n\n        if not self.initialized:\n\n            get_search_space_params = {\"n_classes\": len(np.unique(y)), \n                                       \"n_samples\":len(y), \n                                       \"n_features\":X.shape[1], \n                                       \"random_state\":self.random_state}\n\n            search_space = get_template_search_spaces(self.search_space, classification=True, inner_predictors=self.allow_inner_classifiers, **get_search_space_params)\n\n\n            super(TPOTClassifier,self).__init__(\n                search_space=search_space,\n                scorers=self.scorers, \n                scorers_weights=self.scorers_weights,\n                cv = self.cv,\n                other_objective_functions=self.other_objective_functions, #tpot.objectives.estimator_objective_functions.number_of_nodes_objective],\n                other_objective_functions_weights = self.other_objective_functions_weights,\n                objective_function_names = self.objective_function_names,\n                bigger_is_better = self.bigger_is_better,\n                categorical_features = self.categorical_features,\n                memory = self.memory,\n                preprocessing = self.preprocessing,\n                max_time_mins=self.max_time_mins, \n                max_eval_time_mins=self.max_eval_time_mins, \n                n_jobs=self.n_jobs,\n                validation_strategy = self.validation_strategy,\n                validation_fraction = self.validation_fraction, \n                early_stop = self.early_stop,\n                warm_start = self.warm_start,\n                periodic_checkpoint_folder = self.periodic_checkpoint_folder, \n                verbose = self.verbose,\n                classification=True,\n                memory_limit = self.memory_limit,\n                client = self.client,\n                random_state=self.random_state,\n                **self.tpotestimator_kwargs)\n            self.initialized = True\n\n        return super().fit(X,y)\n\n\n    def predict(self, X, **predict_params):\n        check_is_fitted(self)\n        #X=check_array(X)\n        return self.fitted_pipeline_.predict(X,**predict_params)\n</code></pre>"},{"location":"tpot_api/classifier/#tpot.tpot_estimator.templates.tpottemplates.TPOTClassifier.__init__","title":"<code>__init__(search_space='linear', scorers=['roc_auc_ovr'], scorers_weights=[1], cv=10, other_objective_functions=[], other_objective_functions_weights=[], objective_function_names=None, bigger_is_better=True, categorical_features=None, memory=None, preprocessing=False, max_time_mins=60, max_eval_time_mins=10, n_jobs=1, validation_strategy='none', validation_fraction=0.2, early_stop=None, warm_start=False, periodic_checkpoint_folder=None, verbose=2, memory_limit=None, client=None, random_state=None, allow_inner_classifiers=None, **tpotestimator_kwargs)</code>","text":"<p>An sklearn baseestimator that uses genetic programming to optimize a classification pipeline. For more parameters, see the TPOTEstimator class.</p> <p>Parameters:</p> Name Type Description Default <code>search_space</code> <code>(String, SearchSpace)</code> <ul> <li>String : The default search space to use for the optimization. | String     | Description      | | :---        |    :----:   | | linear  | A linear pipeline with the structure of \"Selector-&gt;(transformers+Passthrough)-&gt;(classifiers/regressors+Passthrough)-&gt;final classifier/regressor.\" For both the transformer and inner estimator layers, TPOT may choose one or more transformers/classifiers, or it may choose none. The inner classifier/regressor layer is optional. | | linear-light | Same search space as linear, but without the inner classifier/regressor layer and with a reduced set of faster running estimators. | | graph | TPOT will optimize a pipeline in the shape of a directed acyclic graph. The nodes of the graph can include selectors, scalers, transformers, or classifiers/regressors (inner classifiers/regressors can optionally be not included). This will return a custom GraphPipeline rather than an sklearn Pipeline. More details in Tutorial 6. | | graph-light | Same as graph search space, but without the inner classifier/regressors and with a reduced set of faster running estimators. | | mdr |TPOT will search over a series of feature selectors and Multifactor Dimensionality Reduction models to find a series of operators that maximize prediction accuracy. The TPOT MDR configuration is specialized for genome-wide association studies (GWAS), and is described in detail online here.</li> </ul> <p>Note that TPOT MDR may be slow to run because the feature selection routines are computationally expensive, especially on large datasets. | - SearchSpace : The search space to use for the optimization. This should be an instance of a SearchSpace.     The search space to use for the optimization. This should be an instance of a SearchSpace.     TPOT has groups of search spaces found in the following folders, tpot.search_spaces.nodes for the nodes in the pipeline and tpot.search_spaces.pipelines for the pipeline structure.</p> <code>'linear'</code> <code>scorers</code> <code>(list, scorer)</code> <p>A scorer or list of scorers to be used in the cross-validation process. see https://scikit-learn.org/stable/modules/model_evaluation.html</p> <code>['roc_auc_ovr']</code> <code>scorers_weights</code> <code>list</code> <p>A list of weights to be applied to the scorers during the optimization process.</p> <code>[1]</code> <code>classification</code> <code>bool</code> <p>If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem. Used to determine the CV strategy.</p> required <code>cv</code> <code>(int, cross - validator)</code> <ul> <li>(int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.</li> <li>(sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.<ul> <li>max_depth (int): The maximum depth from any node to the root of the pipelines to be generated.</li> </ul> </li> </ul> <code>10</code> <code>other_objective_functions</code> <code>list</code> <p>A list of other objective functions to apply to the pipeline. The function takes a single parameter for the graphpipeline estimator and returns either a single score or a list of scores.</p> <code>[]</code> <code>other_objective_functions_weights</code> <code>list</code> <p>A list of weights to be applied to the other objective functions.</p> <code>[]</code> <code>objective_function_names</code> <code>list</code> <p>A list of names to be applied to the objective functions. If None, will use the names of the objective functions.</p> <code>None</code> <code>bigger_is_better</code> <code>bool</code> <p>If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.</p> <code>True</code> <code>categorical_features</code> <code>list or None</code> <p>Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.</p> <code>None</code> <code>categorical_features</code> <p>Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False. - None : If None, TPOT will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing. - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices</p> <code>None</code> <code>memory</code> <p>If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. This feature is used to avoid computing the fit transformers within a pipeline if the parameters and input data are identical with another fitted pipeline during optimization process. - String 'auto':     TPOT uses memory caching with a temporary directory and cleans it up upon shutdown. - String path of a caching directory     TPOT uses memory caching with the provided directory and TPOT does NOT clean     the caching directory up upon shutdown. If the directory does not exist, TPOT will     create it. - Memory object:     TPOT uses the instance of joblib.Memory for memory caching,     and TPOT does NOT clean the caching directory up upon shutdown. - None:     TPOT does not use memory caching.</p> <code>None</code> <code>preprocessing</code> <code>(bool or BaseEstimator / Pipeline)</code> <p>EXPERIMENTAL A pipeline that will be used to preprocess the data before CV. Note that the parameters for these steps are not optimized. Add them to the search space to be optimized. - bool : If True, will use a default preprocessing pipeline which includes imputation followed by one hot encoding. - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.</p> <code>False</code> <code>max_time_mins</code> <code>float</code> <p>Maximum time to run the optimization. If none or inf, will run until the end of the generations.</p> <code>float(\"inf\")</code> <code>max_eval_time_mins</code> <code>float</code> <p>Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.</p> <code>60*5</code> <code>n_jobs</code> <code>int</code> <p>Number of processes to run in parallel.</p> <code>1</code> <code>validation_strategy</code> <code>str</code> <p>EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT may overfit the cross validation score. A second validation set can be used to select the final pipeline. - 'auto' : Automatically determine the validation strategy based on the dataset shape. - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets. - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets. - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.</p> <code>'none'</code> <code>validation_fraction</code> <code>float</code> <p>EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.</p> <code>0.2</code> <code>early_stop</code> <code>int</code> <p>Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered. In general a value of around 5-20 is good.</p> <code>None</code> <code>warm_start</code> <code>bool</code> <p>If True, will use the continue the evolutionary algorithm from the last generation of the previous run.</p> <code>False</code> <code>periodic_checkpoint_folder</code> <code>str</code> <p>Folder to save the population to periodically. If None, no periodic saving will be done. If provided, training will resume from this checkpoint.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>How much information to print during the optimization process. Higher values include the information from lower values. 0. nothing 1. progress bar</p> <ol> <li>best individual</li> <li>warnings <p>=5. full warnings trace</p> </li> <li>evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_mins in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)</li> </ol> <code>1</code> <code>memory_limit</code> <code>str</code> <p>Memory limit for each job. See Dask LocalCluster documentation for more information.</p> <code>None</code> <code>client</code> <code>Client</code> <p>A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.</p> <code>None</code> <code>random_state</code> <code>(int, None)</code> <p>A seed for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes</p> <ul> <li>int     Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'</li> <li>None     Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS</li> </ul> <code>None</code> <code>allow_inner_classifiers</code> <code>bool</code> <p>If True, the search space will include ensembled classifiers.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>fitted_pipeline_</code> <code>GraphPipeline</code> <p>A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.</p> <code>evaluated_individuals</code> <code>A pandas data frame containing data for all evaluated individuals in the run.</code> <p>Columns: - objective functions : The first few columns correspond to the passed in scorers and objective functions - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population. - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population. - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator. - Generation : The generation the pipeline first appeared. - Pareto_Front      : The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.                 To save on computational time, the best frontier is updated iteratively each generation.                 The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.                 All other pipelines are set to NaN. - Instance  : The unfitted GraphPipeline BaseEstimator. - validation objective functions : Objective function scores evaluated on the validation set. - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.</p> <code>pareto_front</code> <code>The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.</code> Source code in <code>tpot/tpot_estimator/templates/tpottemplates.py</code> <pre><code>def __init__(       self,\n                    search_space = \"linear\",\n                    scorers=['roc_auc_ovr'], \n                    scorers_weights=[1],\n                    cv = 10,\n                    other_objective_functions=[], #tpot.objectives.estimator_objective_functions.number_of_nodes_objective],\n                    other_objective_functions_weights = [],\n                    objective_function_names = None,\n                    bigger_is_better = True,\n                    categorical_features = None,\n                    memory = None,\n                    preprocessing = False,\n                    max_time_mins=60, \n                    max_eval_time_mins=10, \n                    n_jobs = 1,\n                    validation_strategy = \"none\",\n                    validation_fraction = .2, \n                    early_stop = None,\n                    warm_start = False,\n                    periodic_checkpoint_folder = None, \n                    verbose = 2,\n                    memory_limit = None,\n                    client = None,\n                    random_state=None,\n                    allow_inner_classifiers=None,\n                    **tpotestimator_kwargs,\n\n    ):\n    \"\"\"\n    An sklearn baseestimator that uses genetic programming to optimize a classification pipeline.\n    For more parameters, see the TPOTEstimator class.\n\n    Parameters\n    ----------\n\n    search_space : (String, tpot.search_spaces.SearchSpace)\n        - String : The default search space to use for the optimization.\n        | String     | Description      |\n        | :---        |    :----:   |\n        | linear  | A linear pipeline with the structure of \"Selector-&gt;(transformers+Passthrough)-&gt;(classifiers/regressors+Passthrough)-&gt;final classifier/regressor.\" For both the transformer and inner estimator layers, TPOT may choose one or more transformers/classifiers, or it may choose none. The inner classifier/regressor layer is optional. |\n        | linear-light | Same search space as linear, but without the inner classifier/regressor layer and with a reduced set of faster running estimators. |\n        | graph | TPOT will optimize a pipeline in the shape of a directed acyclic graph. The nodes of the graph can include selectors, scalers, transformers, or classifiers/regressors (inner classifiers/regressors can optionally be not included). This will return a custom GraphPipeline rather than an sklearn Pipeline. More details in Tutorial 6. |\n        | graph-light | Same as graph search space, but without the inner classifier/regressors and with a reduced set of faster running estimators. |\n        | mdr |TPOT will search over a series of feature selectors and Multifactor Dimensionality Reduction models to find a series of operators that maximize prediction accuracy. The TPOT MDR configuration is specialized for genome-wide association studies (GWAS), and is described in detail online here.\n\n        Note that TPOT MDR may be slow to run because the feature selection routines are computationally expensive, especially on large datasets. |\n        - SearchSpace : The search space to use for the optimization. This should be an instance of a SearchSpace.\n            The search space to use for the optimization. This should be an instance of a SearchSpace.\n            TPOT has groups of search spaces found in the following folders, tpot.search_spaces.nodes for the nodes in the pipeline and tpot.search_spaces.pipelines for the pipeline structure.\n\n    scorers : (list, scorer)\n        A scorer or list of scorers to be used in the cross-validation process.\n        see https://scikit-learn.org/stable/modules/model_evaluation.html\n\n    scorers_weights : list\n        A list of weights to be applied to the scorers during the optimization process.\n\n    classification : bool\n        If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem.\n        Used to determine the CV strategy.\n\n    cv : int, cross-validator\n        - (int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.\n        - (sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.\n            - max_depth (int): The maximum depth from any node to the root of the pipelines to be generated.\n\n    other_objective_functions : list, default=[]\n        A list of other objective functions to apply to the pipeline. The function takes a single parameter for the graphpipeline estimator and returns either a single score or a list of scores.\n\n    other_objective_functions_weights : list, default=[]\n        A list of weights to be applied to the other objective functions.\n\n    objective_function_names : list, default=None\n        A list of names to be applied to the objective functions. If None, will use the names of the objective functions.\n\n    bigger_is_better : bool, default=True\n        If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n\n    categorical_features : list or None\n        Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n\n    categorical_features: list or None\n        Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n        - None : If None, TPOT will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing.\n        - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices\n\n\n    memory: Memory object or string, default=None\n        If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. This feature\n        is used to avoid computing the fit transformers within a pipeline if the parameters\n        and input data are identical with another fitted pipeline during optimization process.\n        - String 'auto':\n            TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.\n        - String path of a caching directory\n            TPOT uses memory caching with the provided directory and TPOT does NOT clean\n            the caching directory up upon shutdown. If the directory does not exist, TPOT will\n            create it.\n        - Memory object:\n            TPOT uses the instance of joblib.Memory for memory caching,\n            and TPOT does NOT clean the caching directory up upon shutdown.\n        - None:\n            TPOT does not use memory caching.\n\n    preprocessing : bool or BaseEstimator/Pipeline,\n        EXPERIMENTAL\n        A pipeline that will be used to preprocess the data before CV. Note that the parameters for these steps are not optimized. Add them to the search space to be optimized.\n        - bool : If True, will use a default preprocessing pipeline which includes imputation followed by one hot encoding.\n        - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.\n\n    max_time_mins : float, default=float(\"inf\")\n        Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n\n    max_eval_time_mins : float, default=60*5\n        Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n\n\n    n_jobs : int, default=1\n        Number of processes to run in parallel.\n\n    validation_strategy : str, default='none'\n        EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT may overfit the cross validation score. A second validation set can be used to select the final pipeline.\n        - 'auto' : Automatically determine the validation strategy based on the dataset shape.\n        - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets.\n        - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets.\n        - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.\n\n    validation_fraction : float, default=0.2\n      EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.\n\n    early_stop : int, default=None\n        Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered. In general a value of around 5-20 is good.\n\n    warm_start : bool, default=False\n        If True, will use the continue the evolutionary algorithm from the last generation of the previous run.\n\n    periodic_checkpoint_folder : str, default=None\n        Folder to save the population to periodically. If None, no periodic saving will be done.\n        If provided, training will resume from this checkpoint.\n\n\n    verbose : int, default=1\n        How much information to print during the optimization process. Higher values include the information from lower values.\n        0. nothing\n        1. progress bar\n\n        3. best individual\n        4. warnings\n        &gt;=5. full warnings trace\n        6. evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_mins in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)\n\n\n    memory_limit : str, default=None\n        Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n\n    client : dask.distributed.Client, default=None\n        A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.\n\n    random_state : int, None, default=None\n        A seed for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes\n\n        - int\n            Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'\n        - None\n            Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS\n\n    allow_inner_classifiers : bool, default=True\n        If True, the search space will include ensembled classifiers. \n\n    Attributes\n    ----------\n\n    fitted_pipeline_ : GraphPipeline\n        A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.\n\n    evaluated_individuals : A pandas data frame containing data for all evaluated individuals in the run.\n        Columns:\n        - *objective functions : The first few columns correspond to the passed in scorers and objective functions\n        - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population.\n        - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population.\n        - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator.\n        - Generation : The generation the pipeline first appeared.\n        - Pareto_Front\t: The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.\n                        To save on computational time, the best frontier is updated iteratively each generation.\n                        The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.\n                        All other pipelines are set to NaN.\n        - Instance\t: The unfitted GraphPipeline BaseEstimator.\n        - *validation objective functions : Objective function scores evaluated on the validation set.\n        - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.\n\n    pareto_front : The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.\n    \"\"\"\n    self.search_space = search_space\n    self.scorers = scorers\n    self.scorers_weights = scorers_weights\n    self.cv = cv\n    self.other_objective_functions = other_objective_functions\n    self.other_objective_functions_weights = other_objective_functions_weights\n    self.objective_function_names = objective_function_names\n    self.bigger_is_better = bigger_is_better\n    self.categorical_features = categorical_features\n    self.memory = memory\n    self.preprocessing = preprocessing\n    self.max_time_mins = max_time_mins\n    self.max_eval_time_mins = max_eval_time_mins\n    self.n_jobs = n_jobs\n    self.validation_strategy = validation_strategy\n    self.validation_fraction = validation_fraction\n    self.early_stop = early_stop\n    self.warm_start = warm_start\n    self.periodic_checkpoint_folder = periodic_checkpoint_folder\n    self.verbose = verbose\n    self.memory_limit = memory_limit\n    self.client = client\n    self.random_state = random_state\n    self.tpotestimator_kwargs = tpotestimator_kwargs\n    self.allow_inner_classifiers = allow_inner_classifiers\n\n    self.initialized = False\n</code></pre>"},{"location":"tpot_api/estimator/","title":"Estimator","text":"<p>This file is part of the TPOT library.</p> <p>The current version of TPOT was developed at Cedars-Sinai by:     - Pedro Henrique Ribeiro (https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)     - Anil Saini (anil.saini@cshs.org)     - Jose Hernandez (jgh9094@gmail.com)     - Jay Moran (jay.moran@cshs.org)     - Nicholas Matsumoto (nicholas.matsumoto@cshs.org)     - Hyunjun Choi (hyunjun.choi@cshs.org)     - Gabriel Ketron (gabriel.ketron@cshs.org)     - Miguel E. Hernandez (miguel.e.hernandez@cshs.org)     - Jason Moore (moorejh28@gmail.com)</p> <p>The original version of TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - Jason Moore (moorejh28@gmail.com)     - and many more generous open-source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"tpot_api/estimator/#tpot.tpot_estimator.estimator.TPOTEstimator","title":"<code>TPOTEstimator</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> Source code in <code>tpot/tpot_estimator/estimator.py</code> <pre><code>class TPOTEstimator(BaseEstimator):\n    def __init__(self,  \n                        search_space,\n                        scorers,\n                        scorers_weights,\n                        classification,\n                        cv = 10,\n                        other_objective_functions=[],\n                        other_objective_functions_weights = [],\n                        objective_function_names = None,\n                        bigger_is_better = True,\n\n                        export_graphpipeline = False,\n                        memory = None,\n\n                        categorical_features = None,\n                        preprocessing = False,\n                        population_size = 50,\n                        initial_population_size = None,\n                        population_scaling = .5,\n                        generations_until_end_population = 1,\n                        generations = None,\n                        max_time_mins=60,\n                        max_eval_time_mins=10,\n                        validation_strategy = \"none\",\n                        validation_fraction = .2,\n                        disable_label_encoder = False,\n\n                        #early stopping parameters\n                        early_stop = None,\n                        scorers_early_stop_tol = 0.001,\n                        other_objectives_early_stop_tol =None,\n                        threshold_evaluation_pruning = None,\n                        threshold_evaluation_scaling = .5,\n                        selection_evaluation_pruning = None,\n                        selection_evaluation_scaling = .5,\n                        min_history_threshold = 20,\n\n                        #evolver parameters\n                        survival_percentage = 1,\n                        crossover_probability=.2,\n                        mutate_probability=.7,\n                        mutate_then_crossover_probability=.05,\n                        crossover_then_mutate_probability=.05,\n                        survival_selector = survival_select_NSGA2,\n                        parent_selector = tournament_selection_dominated,\n\n                        #budget parameters\n                        budget_range = None,\n                        budget_scaling = .5,\n                        generations_until_end_budget = 1,\n                        stepwise_steps = 5,\n\n                        #dask parameters\n                        n_jobs=1,\n                        memory_limit = None,\n                        client = None,\n                        processes = True,\n\n                        #debugging and logging parameters\n                        warm_start = False,\n                        periodic_checkpoint_folder = None,\n                        callback = None,\n\n                        verbose = 0,\n                        scatter = True,\n\n                         # random seed for random number generator (rng)\n                        random_state = None,\n\n                        ):\n\n        '''\n        An sklearn baseestimator that uses genetic programming to optimize a pipeline.\n\n        Parameters\n        ----------\n        search_space : (String, tpot.search_spaces.SearchSpace)\n            - String : The default search space to use for the optimization.\n            | String     | Description      |\n            | :---        |    :----:   |\n            | linear  | A linear pipeline with the structure of \"Selector-&gt;(transformers+Passthrough)-&gt;(classifiers/regressors+Passthrough)-&gt;final classifier/regressor.\" For both the transformer and inner estimator layers, TPOT may choose one or more transformers/classifiers, or it may choose none. The inner classifier/regressor layer is optional. |\n            | linear-light | Same search space as linear, but without the inner classifier/regressor layer and with a reduced set of faster running estimators. |\n            | graph | TPOT will optimize a pipeline in the shape of a directed acyclic graph. The nodes of the graph can include selectors, scalers, transformers, or classifiers/regressors (inner classifiers/regressors can optionally be not included). This will return a custom GraphPipeline rather than an sklearn Pipeline. More details in Tutorial 6. |\n            | graph-light | Same as graph search space, but without the inner classifier/regressors and with a reduced set of faster running estimators. |\n            | mdr |TPOT will search over a series of feature selectors and Multifactor Dimensionality Reduction models to find a series of operators that maximize prediction accuracy. The TPOT MDR configuration is specialized for genome-wide association studies (GWAS), and is described in detail online here.\n\n            Note that TPOT MDR may be slow to run because the feature selection routines are computationally expensive, especially on large datasets. |\n\n\n            - SearchSpace : The search space to use for the optimization. This should be an instance of a SearchSpace.\n                The search space to use for the optimization. This should be an instance of a SearchSpace.\n                TPOT has groups of search spaces found in the following folders, tpot.search_spaces.nodes for the nodes in the pipeline and tpot.search_spaces.pipelines for the pipeline structure.\n\n        scorers : (list, scorer)\n            A scorer or list of scorers to be used in the cross-validation process.\n            see https://scikit-learn.org/stable/modules/model_evaluation.html\n\n        scorers_weights : list\n            A list of weights to be applied to the scorers during the optimization process.\n\n        classification : bool\n            If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem.\n            Used to determine the CV strategy.\n\n        cv : int, cross-validator\n            - (int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.\n            - (sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.\n                - max_depth (int): The maximum depth from any node to the root of the pipelines to be generated.\n\n        other_objective_functions : list, default=[]\n            A list of other objective functions to apply to the pipeline. The function takes a single parameter for the graphpipeline estimator and returns either a single score or a list of scores.\n\n        other_objective_functions_weights : list, default=[]\n            A list of weights to be applied to the other objective functions.\n\n        objective_function_names : list, default=None\n            A list of names to be applied to the objective functions. If None, will use the names of the objective functions.\n\n        bigger_is_better : bool, default=True\n            If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n\n        memory: Memory object or string, default=None\n            If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. This feature\n            is used to avoid computing the fit transformers within a pipeline if the parameters\n            and input data are identical with another fitted pipeline during optimization process.\n            - String 'auto':\n                TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.\n            - String path of a caching directory\n                TPOT uses memory caching with the provided directory and TPOT does NOT clean\n                the caching directory up upon shutdown. If the directory does not exist, TPOT will\n                create it.\n            - Memory object:\n                TPOT uses the instance of joblib.Memory for memory caching,\n                and TPOT does NOT clean the caching directory up upon shutdown.\n            - None:\n                TPOT does not use memory caching.              \n\n        categorical_features: list or None\n            Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n            - None : If None, TPOT will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing.\n            - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices\n\n        preprocessing : bool or BaseEstimator/Pipeline,\n            EXPERIMENTAL - will be changed in future versions\n            A pipeline that will be used to preprocess the data before CV. Note that the parameters for these steps are not optimized. Add them to the search space to be optimized.\n            - bool : If True, will use a default preprocessing pipeline which includes imputation followed by one hot encoding.\n            - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.\n\n        population_size : int, default=50\n            Size of the population\n\n        initial_population_size : int, default=None\n            Size of the initial population. If None, population_size will be used.\n\n        population_scaling : int, default=0.5\n            Scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n\n        generations_until_end_population : int, default=1\n            Number of generations until the population size reaches population_size\n\n        generations : int, default=50\n            Number of generations to run\n\n        max_time_mins : float, default=float(\"inf\")\n            Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n\n        max_eval_time_mins : float, default=5\n            Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n\n        validation_strategy : str, default='none'\n            EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT may overfit the cross validation score. A second validation set can be used to select the final pipeline.\n            - 'auto' : Automatically determine the validation strategy based on the dataset shape.\n            - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets.\n            - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets.\n            - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.\n\n        validation_fraction : float, default=0.2\n          EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.\n\n        disable_label_encoder : bool, default=False\n            If True, TPOT will check if the target needs to be relabeled to be sequential ints from 0 to N. This is necessary for XGBoost compatibility. If the labels need to be encoded, TPOT will use sklearn.preprocessing.LabelEncoder to encode the labels. The encoder can be accessed via the self.label_encoder_ attribute.\n            If False, no additional label encoders will be used.\n\n        early_stop : int, default=None\n            Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered. In general a value of around 5-20 is good.\n\n        scorers_early_stop_tol :\n            -list of floats\n                list of tolerances for each scorer. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n                If an index of the list is None, that item will not be used for early stopping\n            -int\n                If an int is given, it will be used as the tolerance for all objectives\n\n        other_objectives_early_stop_tol :\n            -list of floats\n                list of tolerances for each of the other objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n                If an index of the list is None, that item will not be used for early stopping\n            -int\n                If an int is given, it will be used as the tolerance for all objectives\n\n        threshold_evaluation_pruning : list [start, end], default=None\n            starting and ending percentile to use as a threshold for the evaluation early stopping.\n            Values between 0 and 100.\n\n        threshold_evaluation_scaling : float [0,inf), default=0.5\n            A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n            Must be greater than zero. Higher numbers will move the threshold to the end faster.\n\n        selection_evaluation_pruning : list, default=None\n            A lower and upper percent of the population size to select each round of CV.\n            Values between 0 and 1.\n\n        selection_evaluation_scaling : float, default=0.5\n            A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n            Must be greater than zero. Higher numbers will move the threshold to the end faster.\n\n        min_history_threshold : int, default=0\n            The minimum number of previous scores needed before using threshold early stopping.\n\n        survival_percentage : float, default=1\n            Percentage of the population size to utilize for mutation and crossover at the beginning of the generation. The rest are discarded. Individuals are selected with the selector passed into survival_selector. The value of this parameter must be between 0 and 1, inclusive.\n            For example, if the population size is 100 and the survival percentage is .5, 50 individuals will be selected with NSGA2 from the existing population. These will be used for mutation and crossover to generate the next 100 individuals for the next generation. The remainder are discarded from the live population. In the next generation, there will now be the 50 parents + the 100 individuals for a total of 150. Surivival percentage is based of the population size parameter and not the existing population size (current population size when using successive halving). Therefore, in the next generation we will still select 50 individuals from the currently existing 150.\n\n        crossover_probability : float, default=.2\n            Probability of generating a new individual by crossover between two individuals.\n\n        mutate_probability : float, default=.7\n            Probability of generating a new individual by crossover between one individuals.\n\n        mutate_then_crossover_probability : float, default=.05\n            Probability of generating a new individual by mutating two individuals followed by crossover.\n\n        crossover_then_mutate_probability : float, default=.05\n            Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.\n\n        survival_selector : function, default=survival_select_NSGA2\n            Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes.\n            Used to selected population_size * survival_percentage individuals at the start of each generation to use for mutation and crossover.\n\n        parent_selector : function, default=parent_select_NSGA2\n            Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.\n\n        budget_range : list [start, end], default=None\n            A starting and ending budget to use for the budget scaling.\n\n        budget_scaling float : [0,1], default=0.5\n            A scaling factor to use when determining how fast we move the budget from the start to end budget.\n\n        generations_until_end_budget : int, default=1\n            The number of generations to run before reaching the max budget.\n\n        stepwise_steps : int, default=1\n            The number of staircase steps to take when scaling the budget and population size.\n\n        n_jobs : int, default=1\n            Number of processes to run in parallel.\n\n        memory_limit : str, default=None\n            Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n\n        client : dask.distributed.Client, default=None\n            A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.\n\n        processes : bool, default=True\n            If True, will use multiprocessing to parallelize the optimization process. If False, will use threading.\n            True seems to perform better. However, False is required for interactive debugging.\n\n        warm_start : bool, default=False\n            If True, will use the continue the evolutionary algorithm from the last generation of the previous run.\n\n        periodic_checkpoint_folder : str, default=None\n            Folder to save the population to periodically. If None, no periodic saving will be done.\n            If provided, training will resume from this checkpoint.\n\n        callback : tpot.CallBackInterface, default=None\n            Callback object. Not implemented\n\n        verbose : int, default=1\n            How much information to print during the optimization process. Higher values include the information from lower values.\n            0. nothing\n            1. progress bar\n\n            3. best individual\n            4. warnings\n            &gt;=5. full warnings trace\n            6. evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_mins in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)\n\n        scatter : bool, default=True\n            If True, will scatter the data to the dask workers. If False, will not scatter the data. This can be useful for debugging.\n\n        random_state : int, None, default=None\n            A seed for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes\n\n            - int\n                Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'\n            - None\n                Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS\n\n        Attributes\n        ----------\n\n        fitted_pipeline_ : GraphPipeline\n            A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.\n\n        evaluated_individuals : A pandas data frame containing data for all evaluated individuals in the run.\n            Columns:\n            - *objective functions : The first few columns correspond to the passed in scorers and objective functions\n            - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population.\n            - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population.\n            - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator.\n            - Generation : The generation the pipeline first appeared.\n            - Pareto_Front\t: The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.\n                            To save on computational time, the best frontier is updated iteratively each generation.\n                            The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.\n                            All other pipelines are set to NaN.\n            - Instance\t: The unfitted GraphPipeline BaseEstimator.\n            - *validation objective functions : Objective function scores evaluated on the validation set.\n            - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.\n\n        pareto_front : The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.\n        '''\n\n        # sklearn BaseEstimator must have a corresponding attribute for each parameter.\n        # These should not be modified once set.\n\n        self.scorers = scorers\n        self.scorers_weights = scorers_weights\n        self.classification = classification\n        self.cv = cv\n        self.other_objective_functions = other_objective_functions\n        self.other_objective_functions_weights = other_objective_functions_weights\n        self.objective_function_names = objective_function_names\n        self.bigger_is_better = bigger_is_better\n\n        self.search_space = search_space\n\n        self.export_graphpipeline = export_graphpipeline\n        self.memory = memory\n\n        self.categorical_features = categorical_features\n\n        self.preprocessing = preprocessing\n        self.validation_strategy = validation_strategy\n        self.validation_fraction = validation_fraction\n        self.disable_label_encoder = disable_label_encoder\n        self.population_size = population_size\n        self.initial_population_size = initial_population_size\n        self.population_scaling = population_scaling\n        self.generations_until_end_population = generations_until_end_population\n        self.generations = generations\n        self.early_stop = early_stop\n        self.scorers_early_stop_tol = scorers_early_stop_tol\n        self.other_objectives_early_stop_tol = other_objectives_early_stop_tol\n        self.max_time_mins = max_time_mins\n        self.max_eval_time_mins = max_eval_time_mins\n        self.n_jobs= n_jobs\n        self.memory_limit = memory_limit\n        self.client = client\n        self.survival_percentage = survival_percentage\n        self.crossover_probability = crossover_probability\n        self.mutate_probability = mutate_probability\n        self.mutate_then_crossover_probability= mutate_then_crossover_probability\n        self.crossover_then_mutate_probability= crossover_then_mutate_probability\n        self.survival_selector=survival_selector\n        self.parent_selector=parent_selector\n        self.budget_range = budget_range\n        self.budget_scaling = budget_scaling\n        self.generations_until_end_budget = generations_until_end_budget\n        self.stepwise_steps = stepwise_steps\n        self.threshold_evaluation_pruning =threshold_evaluation_pruning\n        self.threshold_evaluation_scaling =  threshold_evaluation_scaling\n        self.min_history_threshold = min_history_threshold\n        self.selection_evaluation_pruning = selection_evaluation_pruning\n        self.selection_evaluation_scaling =  selection_evaluation_scaling\n        self.warm_start = warm_start\n        self.verbose = verbose\n        self.periodic_checkpoint_folder = periodic_checkpoint_folder\n        self.callback = callback\n        self.processes = processes\n\n\n        self.scatter = scatter\n\n\n        timer_set = self.max_time_mins != float(\"inf\") and self.max_time_mins is not None\n        if self.generations is not None and timer_set:\n            warnings.warn(\"Both generations and max_time_mins are set. TPOT will terminate when the first condition is met.\")\n\n        # create random number generator based on rngseed\n        self.rng = np.random.default_rng(random_state)\n        # save random state passed to us for other functions that use random_state\n        self.random_state = random_state\n\n        #Initialize other used params\n\n\n        if self.initial_population_size is None:\n            self._initial_population_size = self.population_size\n        else:\n            self._initial_population_size = self.initial_population_size\n\n        if isinstance(self.scorers, str):\n            self._scorers = [self.scorers]\n\n        elif callable(self.scorers):\n            self._scorers = [self.scorers]\n        else:\n            self._scorers = self.scorers\n\n        self._scorers = [sklearn.metrics.get_scorer(scoring) for scoring in self._scorers]\n        self._scorers_early_stop_tol = self.scorers_early_stop_tol\n\n        self._evolver = tpot.evolvers.BaseEvolver\n\n        self.objective_function_weights = [*scorers_weights, *other_objective_functions_weights]\n\n\n        if self.objective_function_names is None:\n            obj_names = [f.__name__ for f in other_objective_functions]\n        else:\n            obj_names = self.objective_function_names\n        self.objective_names = [f._score_func.__name__ if hasattr(f,\"_score_func\") else f.__name__ for f in self._scorers] + obj_names\n\n\n        if not isinstance(self.other_objectives_early_stop_tol, list):\n            self._other_objectives_early_stop_tol = [self.other_objectives_early_stop_tol for _ in range(len(self.other_objective_functions))]\n        else:\n            self._other_objectives_early_stop_tol = self.other_objectives_early_stop_tol\n\n        if not isinstance(self._scorers_early_stop_tol, list):\n            self._scorers_early_stop_tol = [self._scorers_early_stop_tol for _ in range(len(self._scorers))]\n        else:\n            self._scorers_early_stop_tol = self._scorers_early_stop_tol\n\n        self.early_stop_tol = [*self._scorers_early_stop_tol, *self._other_objectives_early_stop_tol]\n\n        self._evolver_instance = None\n        self.evaluated_individuals = None\n\n\n        self.label_encoder_ = None\n\n\n        set_dask_settings()\n\n\n    def fit(self, X, y):\n        if self.client is not None: #If user passed in a client manually\n           _client = self.client\n        else:\n\n            if self.verbose &gt;= 4:\n                silence_logs = 30\n            elif self.verbose &gt;=5:\n                silence_logs = 40\n            else:\n                silence_logs = 50\n            cluster = LocalCluster(n_workers=self.n_jobs, #if no client is passed in and no global client exists, create our own\n                    threads_per_worker=1,\n                    processes=self.processes,\n                    silence_logs=silence_logs,\n                    memory_limit=self.memory_limit)\n            _client = Client(cluster)\n\n        if self.classification and not self.disable_label_encoder and not check_if_y_is_encoded(y):\n            warnings.warn(\"Labels are not encoded as ints from 0 to N. For compatibility with some classifiers such as sklearn, TPOT has encoded y with the sklearn LabelEncoder. When using pipelines outside the main TPOT estimator class, you can encode the labels with est.label_encoder_\")\n            self.label_encoder_ = LabelEncoder()\n            y = self.label_encoder_.fit_transform(y)\n\n        self.evaluated_individuals = None\n        #determine validation strategy\n        if self.validation_strategy == 'auto':\n            nrows = X.shape[0]\n            ncols = X.shape[1]\n\n            if nrows/ncols &lt; 20:\n                validation_strategy = 'reshuffled'\n            elif nrows/ncols &lt; 100:\n                validation_strategy = 'split'\n            else:\n                validation_strategy = 'none'\n        else:\n            validation_strategy = self.validation_strategy\n\n        if validation_strategy == 'split':\n            if self.classification:\n                X, X_val, y, y_val = train_test_split(X, y, test_size=self.validation_fraction, stratify=y, random_state=self.random_state)\n            else:\n                X, X_val, y, y_val = train_test_split(X, y, test_size=self.validation_fraction, random_state=self.random_state)\n\n\n        X_original = X\n        y_original = y\n        if isinstance(self.cv, int) or isinstance(self.cv, float):\n            n_folds = self.cv\n        else:\n            n_folds = self.cv.get_n_splits(X, y)\n\n        if self.classification:\n            X, y = remove_underrepresented_classes(X, y, n_folds)\n\n        if self.preprocessing:\n            #X = pd.DataFrame(X)\n\n            if not isinstance(self.preprocessing, bool) and isinstance(self.preprocessing, sklearn.base.BaseEstimator):\n                self._preprocessing_pipeline = sklearn.base.clone(self.preprocessing)\n\n            #TODO: check if there are missing values in X before imputation. If not, don't include imputation in pipeline. Check if there are categorical columns. If not, don't include one hot encoding in pipeline\n            else: #if self.preprocessing is True or not a sklearn estimator\n\n                pipeline_steps = []\n\n                if self.categorical_features is not None: #if categorical features are specified, use those\n                    pipeline_steps.append((\"impute_categorical\", tpot.builtin_modules.ColumnSimpleImputer(self.categorical_features, strategy='most_frequent')))\n                    pipeline_steps.append((\"impute_numeric\", tpot.builtin_modules.ColumnSimpleImputer(\"numeric\", strategy='mean')))\n                    pipeline_steps.append((\"ColumnOneHotEncoder\", tpot.builtin_modules.ColumnOneHotEncoder(self.categorical_features, min_frequency=0.0001))) # retain wrong param fix\n\n                else:\n                    if isinstance(X, pd.DataFrame):\n                        categorical_columns = X.select_dtypes(include=['object']).columns\n                        if len(categorical_columns) &gt; 0:\n                            pipeline_steps.append((\"impute_categorical\", tpot.builtin_modules.ColumnSimpleImputer(\"categorical\", strategy='most_frequent')))\n                            pipeline_steps.append((\"impute_numeric\", tpot.builtin_modules.ColumnSimpleImputer(\"numeric\", strategy='mean')))\n                            pipeline_steps.append((\"ColumnOneHotEncoder\", tpot.builtin_modules.ColumnOneHotEncoder(\"categorical\", min_frequency=0.0001))) # retain wrong param fix\n                        else:\n                            pipeline_steps.append((\"impute_numeric\", tpot.builtin_modules.ColumnSimpleImputer(\"all\", strategy='mean')))\n                    else:\n                        pipeline_steps.append((\"impute_numeric\", tpot.builtin_modules.ColumnSimpleImputer(\"all\", strategy='mean')))\n\n                self._preprocessing_pipeline = sklearn.pipeline.Pipeline(pipeline_steps)\n\n            X = self._preprocessing_pipeline.fit_transform(X, y)\n\n        else:\n            self._preprocessing_pipeline = None\n\n        #_, y = sklearn.utils.check_X_y(X, y, y_numeric=True)\n\n        #Set up the configuation dictionaries and the search spaces\n\n        #check if self.cv is a number\n        if isinstance(self.cv, int) or isinstance(self.cv, float):\n            if self.classification:\n                self.cv_gen = sklearn.model_selection.StratifiedKFold(n_splits=self.cv, shuffle=True, random_state=self.random_state)\n            else:\n                self.cv_gen = sklearn.model_selection.KFold(n_splits=self.cv, shuffle=True, random_state=self.random_state)\n\n        else:\n            self.cv_gen = sklearn.model_selection.check_cv(self.cv, y, classifier=self.classification)\n\n\n\n        n_samples= int(math.floor(X.shape[0]/n_folds))\n        n_features=X.shape[1]\n\n        if isinstance(X, pd.DataFrame):\n            self.feature_names = X.columns\n        else:\n            self.feature_names = None\n\n\n\n        def objective_function(pipeline_individual,\n                                            X,\n                                            y,\n                                            is_classification=self.classification,\n                                            scorers= self._scorers,\n                                            cv=self.cv_gen,\n                                            other_objective_functions=self.other_objective_functions,\n                                            export_graphpipeline=self.export_graphpipeline,\n                                            memory=self.memory,\n                                            **kwargs):\n            return objective_function_generator(\n                pipeline_individual,\n                X,\n                y,\n                is_classification=is_classification,\n                scorers= scorers,\n                cv=cv,\n                other_objective_functions=other_objective_functions,\n                export_graphpipeline=export_graphpipeline,\n                memory=memory,\n                **kwargs,\n            )\n\n\n\n        if self.threshold_evaluation_pruning is not None or self.selection_evaluation_pruning is not None:\n            evaluation_early_stop_steps = self.cv\n        else:\n            evaluation_early_stop_steps = None\n\n        if self.scatter:\n            X_future = _client.scatter(X)\n            y_future = _client.scatter(y)\n        else:\n            X_future = X\n            y_future = y\n\n        if self.classification:\n            n_classes = len(np.unique(y))\n        else:\n            n_classes = None\n\n        get_search_space_params = {\"n_classes\": n_classes, \n                        \"n_samples\":len(y), \n                        \"n_features\":X.shape[1], \n                        \"random_state\":self.random_state}\n\n        self._search_space = get_template_search_spaces(self.search_space, classification=self.classification, inner_predictors=True, **get_search_space_params)\n\n\n        # TODO : Add check for empty values in X and if so, add imputation to the search space\n        # make this depend on self.preprocessing\n        # if check_empty_values(X):\n        #     from sklearn.experimental import enable_iterative_imputer\n\n        #     from ConfigSpace import ConfigurationSpace\n        #     from ConfigSpace import ConfigurationSpace, Integer, Float, Categorical, Normal\n        #     iterative_imputer_cs = ConfigurationSpace(\n        #         space = {\n        #             'n_nearest_features' : Categorical('n_nearest_features', [100]),\n        #             'initial_strategy' : Categorical('initial_strategy', ['mean','median', 'most_frequent', ]),\n        #             'add_indicator' : Categorical('add_indicator', [True, False]),\n        #         }\n        #     )\n\n        #     imputation_search = tpot.search_spaces.pipelines.ChoicePipeline([\n        #         tpot.config.get_search_space(\"SimpleImputer\"),\n        #         tpot.search_spaces.nodes.EstimatorNode(sklearn.impute.IterativeImputer, iterative_imputer_cs)\n        #     ])\n\n\n\n\n        #     self.search_space_final = tpot.search_spaces.pipelines.SequentialPipeline(search_spaces=[ imputation_search, self._search_space], memory=\"sklearn_pipeline_memory\")\n        # else:\n        #     self.search_space_final = self._search_space\n\n        self.search_space_final = self._search_space\n\n        def ind_generator(rng):\n            rng = np.random.default_rng(rng)\n            while True:\n                yield self.search_space_final.generate(rng)\n\n        #If warm start and we have an evolver instance, use the existing one\n        if not(self.warm_start and self._evolver_instance is not None):\n            self._evolver_instance = self._evolver(   individual_generator=ind_generator(self.rng),\n                                            objective_functions= [objective_function],\n                                            objective_function_weights = self.objective_function_weights,\n                                            objective_names=self.objective_names,\n                                            bigger_is_better = self.bigger_is_better,\n                                            population_size= self.population_size,\n                                            generations=self.generations,\n                                            initial_population_size = self._initial_population_size,\n                                            n_jobs=self.n_jobs,\n                                            verbose = self.verbose,\n                                            max_time_mins =      self.max_time_mins ,\n                                            max_eval_time_mins = self.max_eval_time_mins,\n\n                                            periodic_checkpoint_folder = self.periodic_checkpoint_folder,\n                                            threshold_evaluation_pruning = self.threshold_evaluation_pruning,\n                                            threshold_evaluation_scaling =  self.threshold_evaluation_scaling,\n                                            min_history_threshold = self.min_history_threshold,\n\n                                            selection_evaluation_pruning = self.selection_evaluation_pruning,\n                                            selection_evaluation_scaling =  self.selection_evaluation_scaling,\n                                            evaluation_early_stop_steps = evaluation_early_stop_steps,\n\n                                            early_stop_tol = self.early_stop_tol,\n                                            early_stop= self.early_stop,\n\n                                            budget_range = self.budget_range,\n                                            budget_scaling = self.budget_scaling,\n                                            generations_until_end_budget = self.generations_until_end_budget,\n\n                                            population_scaling = self.population_scaling,\n                                            generations_until_end_population = self.generations_until_end_population,\n                                            stepwise_steps = self.stepwise_steps,\n                                            client = _client,\n                                            objective_kwargs = {\"X\": X_future, \"y\": y_future},\n                                            survival_selector=self.survival_selector,\n                                            parent_selector=self.parent_selector,\n                                            survival_percentage = self.survival_percentage,\n                                            crossover_probability = self.crossover_probability,\n                                            mutate_probability = self.mutate_probability,\n                                            mutate_then_crossover_probability= self.mutate_then_crossover_probability,\n                                            crossover_then_mutate_probability= self.crossover_then_mutate_probability,\n\n                                            rng=self.rng,\n                                            )\n\n\n        self._evolver_instance.optimize()\n        #self._evolver_instance.population.update_pareto_fronts(self.objective_names, self.objective_function_weights)\n        self.make_evaluated_individuals()\n\n\n\n\n        tpot.utils.get_pareto_frontier(self.evaluated_individuals, column_names=self.objective_names, weights=self.objective_function_weights)\n\n        if validation_strategy == 'reshuffled':\n            best_pareto_front_idx = list(self.pareto_front.index)\n            best_pareto_front = list(self.pareto_front.loc[best_pareto_front_idx]['Individual'])\n\n            #reshuffle rows\n            X, y = sklearn.utils.shuffle(X, y, random_state=self.random_state)\n\n            if self.scatter:\n                X_future = _client.scatter(X)\n                y_future = _client.scatter(y)\n            else:\n                X_future = X\n                y_future = y\n\n            val_objective_function_list = [lambda   ind,\n                                                    X,\n                                                    y,\n                                                    is_classification=self.classification,\n                                                    scorers= self._scorers,\n                                                    cv=self.cv_gen,\n                                                    other_objective_functions=self.other_objective_functions,\n                                                    export_graphpipeline=self.export_graphpipeline,\n                                                    memory=self.memory,\n                                                    **kwargs: objective_function_generator(\n                                                                                                ind,\n                                                                                                X,\n                                                                                                y,\n                                                                                                is_classification=is_classification,\n                                                                                                scorers= scorers,\n                                                                                                cv=cv,\n                                                                                                other_objective_functions=other_objective_functions,\n                                                                                                export_graphpipeline=export_graphpipeline,\n                                                                                                memory=memory,\n                                                                                                **kwargs,\n                                                                                                )]\n\n            objective_kwargs = {\"X\": X_future, \"y\": y_future}\n            val_scores, start_times, end_times, eval_errors = tpot.utils.eval_utils.parallel_eval_objective_list(best_pareto_front, val_objective_function_list, verbose=self.verbose, max_eval_time_mins=self.max_eval_time_mins, n_expected_columns=len(self.objective_names), client=_client, **objective_kwargs)\n\n\n\n            val_objective_names = ['validation_'+name for name in self.objective_names]\n            self.objective_names_for_selection = val_objective_names\n            self.evaluated_individuals.loc[best_pareto_front_idx,val_objective_names] = val_scores\n            self.evaluated_individuals.loc[best_pareto_front_idx,'validation_start_times'] = start_times\n            self.evaluated_individuals.loc[best_pareto_front_idx,'validation_end_times'] = end_times\n            self.evaluated_individuals.loc[best_pareto_front_idx,'validation_eval_errors'] = eval_errors\n\n            self.evaluated_individuals[\"Validation_Pareto_Front\"] = tpot.utils.get_pareto_frontier(self.evaluated_individuals, column_names=val_objective_names, weights=self.objective_function_weights)\n\n\n        elif validation_strategy == 'split':\n\n\n            if self.scatter:\n                X_future = _client.scatter(X)\n                y_future = _client.scatter(y)\n                X_val_future = _client.scatter(X_val)\n                y_val_future = _client.scatter(y_val)\n            else:\n                X_future = X\n                y_future = y\n                X_val_future = X_val\n                y_val_future = y_val\n\n            objective_kwargs = {\"X\": X_future, \"y\": y_future, \"X_val\" : X_val_future, \"y_val\":y_val_future }\n\n            best_pareto_front_idx = list(self.pareto_front.index)\n            best_pareto_front = list(self.pareto_front.loc[best_pareto_front_idx]['Individual'])\n            val_objective_function_list = [lambda   ind,\n                                                    X,\n                                                    y,\n                                                    X_val,\n                                                    y_val,\n                                                    scorers= self._scorers,\n                                                    other_objective_functions=self.other_objective_functions,\n                                                    export_graphpipeline=self.export_graphpipeline,\n                                                    memory=self.memory,\n                                                    **kwargs: val_objective_function_generator(\n                                                        ind,\n                                                        X,\n                                                        y,\n                                                        X_val,\n                                                        y_val,\n                                                        scorers= scorers,\n                                                        other_objective_functions=other_objective_functions,\n                                                        export_graphpipeline=export_graphpipeline,\n                                                        memory=memory,\n                                                        **kwargs,\n                                                        )]\n\n            val_scores, start_times, end_times, eval_errors = tpot.utils.eval_utils.parallel_eval_objective_list(best_pareto_front, val_objective_function_list, verbose=self.verbose, max_eval_time_mins=self.max_eval_time_mins, n_expected_columns=len(self.objective_names), client=_client, **objective_kwargs)\n\n\n\n            val_objective_names = ['validation_'+name for name in self.objective_names]\n            self.objective_names_for_selection = val_objective_names\n            self.evaluated_individuals.loc[best_pareto_front_idx,val_objective_names] = val_scores\n            self.evaluated_individuals.loc[best_pareto_front_idx,'validation_start_times'] = start_times\n            self.evaluated_individuals.loc[best_pareto_front_idx,'validation_end_times'] = end_times\n            self.evaluated_individuals.loc[best_pareto_front_idx,'validation_eval_errors'] = eval_errors\n\n            self.evaluated_individuals[\"Validation_Pareto_Front\"] = tpot.utils.get_pareto_frontier(self.evaluated_individuals, column_names=val_objective_names, weights=self.objective_function_weights)\n\n        else:\n            self.objective_names_for_selection = self.objective_names\n\n        val_scores = self.evaluated_individuals[self.evaluated_individuals[self.objective_names_for_selection].isna().all(1).ne(True)][self.objective_names_for_selection]\n        weighted_scores = val_scores*self.objective_function_weights\n\n        if self.bigger_is_better:\n            best_indices = list(weighted_scores.sort_values(by=self.objective_names_for_selection, ascending=False).index)\n        else:\n            best_indices = list(weighted_scores.sort_values(by=self.objective_names_for_selection, ascending=True).index)\n\n        for best_idx in best_indices:\n\n            best_individual = self.evaluated_individuals.loc[best_idx]['Individual']\n            self.selected_best_score =  self.evaluated_individuals.loc[best_idx]\n\n\n            #TODO\n            #best_individual_pipeline = best_individual.export_pipeline(memory=self.memory, cross_val_predict_cv=self.cross_val_predict_cv)\n            if self.export_graphpipeline:\n                best_individual_pipeline = best_individual.export_flattened_graphpipeline(memory=self.memory)\n            else:\n                best_individual_pipeline = best_individual.export_pipeline(memory=self.memory)\n\n            if self.preprocessing:\n                self.fitted_pipeline_ = sklearn.pipeline.make_pipeline(sklearn.base.clone(self._preprocessing_pipeline), best_individual_pipeline )\n            else:\n                self.fitted_pipeline_ = best_individual_pipeline\n\n            try:\n                self.fitted_pipeline_.fit(X_original,y_original) #TODO use y_original as well?\n                break\n            except Exception as e:\n                if self.verbose &gt;= 4:\n                    warnings.warn(\"Final pipeline failed to fit. Rarely, the pipeline might work on the objective function but fail on the full dataset. Generally due to interactions with different features being selected or transformations having different properties. Trying next pipeline\")\n                    print(e)\n                continue\n\n\n        if self.client is None: #no client was passed in\n            #close cluster and client\n            # _client.close()\n            # cluster.close()\n            try:\n                _client.shutdown()\n                cluster.close()\n            #catch exception\n            except Exception as e:\n                print(\"Error shutting down client and cluster\")\n                Warning(e)\n\n        return self\n\n    def _estimator_has(attr):\n        '''Check if we can delegate a method to the underlying estimator.\n        First, we check the first fitted final estimator if available, otherwise we\n        check the unfitted final estimator.\n        '''\n        return  lambda self: (self.fitted_pipeline_ is not None and\n            hasattr(self.fitted_pipeline_, attr)\n        )\n\n\n\n\n\n\n    @available_if(_estimator_has('predict'))\n    def predict(self, X, **predict_params):\n        check_is_fitted(self)\n        #X = check_array(X)\n\n        preds = self.fitted_pipeline_.predict(X,**predict_params)\n        if self.classification and self.label_encoder_:\n            preds = self.label_encoder_.inverse_transform(preds)\n\n        return preds\n\n    @available_if(_estimator_has('predict_proba'))\n    def predict_proba(self, X, **predict_params):\n        check_is_fitted(self)\n        #X = check_array(X)\n        return self.fitted_pipeline_.predict_proba(X,**predict_params)\n\n    @available_if(_estimator_has('decision_function'))\n    def decision_function(self, X, **predict_params):\n        check_is_fitted(self)\n        #X = check_array(X)\n        return self.fitted_pipeline_.decision_function(X,**predict_params)\n\n    @available_if(_estimator_has('transform'))\n    def transform(self, X, **predict_params):\n        check_is_fitted(self)\n        #X = check_array(X)\n        return self.fitted_pipeline_.transform(X,**predict_params)\n\n    @property\n    def classes_(self):\n        \"\"\"The classes labels. Only exist if the last step is a classifier.\"\"\"\n        if self.label_encoder_:\n            return self.label_encoder_.classes_\n        else:\n            return self.fitted_pipeline_.classes_\n\n\n    @property\n    def _estimator_type(self):\n        return self.fitted_pipeline_._estimator_type\n\n\n    def make_evaluated_individuals(self):\n        #check if _evolver_instance exists\n        if self.evaluated_individuals is None:\n            self.evaluated_individuals  =  self._evolver_instance.population.evaluated_individuals.copy()\n            objects = list(self.evaluated_individuals.index)\n            object_to_int = dict(zip(objects, range(len(objects))))\n            self.evaluated_individuals = self.evaluated_individuals.set_index(self.evaluated_individuals.index.map(object_to_int))\n            self.evaluated_individuals['Parents'] = self.evaluated_individuals['Parents'].apply(lambda row: convert_parents_tuples_to_integers(row, object_to_int))\n\n            self.evaluated_individuals[\"Instance\"] = self.evaluated_individuals[\"Individual\"].apply(lambda ind: apply_make_pipeline(ind, preprocessing_pipeline=self._preprocessing_pipeline, export_graphpipeline=self.export_graphpipeline, memory=self.memory))\n\n        return self.evaluated_individuals\n\n    @property\n    def pareto_front(self):\n        #check if _evolver_instance exists\n        if self.evaluated_individuals is None:\n            return None\n        else:\n            if \"Pareto_Front\" not in self.evaluated_individuals:\n                return self.evaluated_individuals\n            else:\n                return self.evaluated_individuals[self.evaluated_individuals[\"Pareto_Front\"]==1]\n</code></pre>"},{"location":"tpot_api/estimator/#tpot.tpot_estimator.estimator.TPOTEstimator.classes_","title":"<code>classes_</code>  <code>property</code>","text":"<p>The classes labels. Only exist if the last step is a classifier.</p>"},{"location":"tpot_api/estimator/#tpot.tpot_estimator.estimator.TPOTEstimator.__init__","title":"<code>__init__(search_space, scorers, scorers_weights, classification, cv=10, other_objective_functions=[], other_objective_functions_weights=[], objective_function_names=None, bigger_is_better=True, export_graphpipeline=False, memory=None, categorical_features=None, preprocessing=False, population_size=50, initial_population_size=None, population_scaling=0.5, generations_until_end_population=1, generations=None, max_time_mins=60, max_eval_time_mins=10, validation_strategy='none', validation_fraction=0.2, disable_label_encoder=False, early_stop=None, scorers_early_stop_tol=0.001, other_objectives_early_stop_tol=None, threshold_evaluation_pruning=None, threshold_evaluation_scaling=0.5, selection_evaluation_pruning=None, selection_evaluation_scaling=0.5, min_history_threshold=20, survival_percentage=1, crossover_probability=0.2, mutate_probability=0.7, mutate_then_crossover_probability=0.05, crossover_then_mutate_probability=0.05, survival_selector=survival_select_NSGA2, parent_selector=tournament_selection_dominated, budget_range=None, budget_scaling=0.5, generations_until_end_budget=1, stepwise_steps=5, n_jobs=1, memory_limit=None, client=None, processes=True, warm_start=False, periodic_checkpoint_folder=None, callback=None, verbose=0, scatter=True, random_state=None)</code>","text":"<p>An sklearn baseestimator that uses genetic programming to optimize a pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>search_space</code> <code>(String, SearchSpace)</code> <ul> <li>String : The default search space to use for the optimization. | String     | Description      | | :---        |    :----:   | | linear  | A linear pipeline with the structure of \"Selector-&gt;(transformers+Passthrough)-&gt;(classifiers/regressors+Passthrough)-&gt;final classifier/regressor.\" For both the transformer and inner estimator layers, TPOT may choose one or more transformers/classifiers, or it may choose none. The inner classifier/regressor layer is optional. | | linear-light | Same search space as linear, but without the inner classifier/regressor layer and with a reduced set of faster running estimators. | | graph | TPOT will optimize a pipeline in the shape of a directed acyclic graph. The nodes of the graph can include selectors, scalers, transformers, or classifiers/regressors (inner classifiers/regressors can optionally be not included). This will return a custom GraphPipeline rather than an sklearn Pipeline. More details in Tutorial 6. | | graph-light | Same as graph search space, but without the inner classifier/regressors and with a reduced set of faster running estimators. | | mdr |TPOT will search over a series of feature selectors and Multifactor Dimensionality Reduction models to find a series of operators that maximize prediction accuracy. The TPOT MDR configuration is specialized for genome-wide association studies (GWAS), and is described in detail online here.</li> </ul> <p>Note that TPOT MDR may be slow to run because the feature selection routines are computationally expensive, especially on large datasets. |</p> <ul> <li>SearchSpace : The search space to use for the optimization. This should be an instance of a SearchSpace.     The search space to use for the optimization. This should be an instance of a SearchSpace.     TPOT has groups of search spaces found in the following folders, tpot.search_spaces.nodes for the nodes in the pipeline and tpot.search_spaces.pipelines for the pipeline structure.</li> </ul> required <code>scorers</code> <code>(list, scorer)</code> <p>A scorer or list of scorers to be used in the cross-validation process. see https://scikit-learn.org/stable/modules/model_evaluation.html</p> required <code>scorers_weights</code> <code>list</code> <p>A list of weights to be applied to the scorers during the optimization process.</p> required <code>classification</code> <code>bool</code> <p>If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem. Used to determine the CV strategy.</p> required <code>cv</code> <code>(int, cross - validator)</code> <ul> <li>(int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.</li> <li>(sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.<ul> <li>max_depth (int): The maximum depth from any node to the root of the pipelines to be generated.</li> </ul> </li> </ul> <code>10</code> <code>other_objective_functions</code> <code>list</code> <p>A list of other objective functions to apply to the pipeline. The function takes a single parameter for the graphpipeline estimator and returns either a single score or a list of scores.</p> <code>[]</code> <code>other_objective_functions_weights</code> <code>list</code> <p>A list of weights to be applied to the other objective functions.</p> <code>[]</code> <code>objective_function_names</code> <code>list</code> <p>A list of names to be applied to the objective functions. If None, will use the names of the objective functions.</p> <code>None</code> <code>bigger_is_better</code> <code>bool</code> <p>If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.</p> <code>True</code> <code>memory</code> <p>If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. This feature is used to avoid computing the fit transformers within a pipeline if the parameters and input data are identical with another fitted pipeline during optimization process. - String 'auto':     TPOT uses memory caching with a temporary directory and cleans it up upon shutdown. - String path of a caching directory     TPOT uses memory caching with the provided directory and TPOT does NOT clean     the caching directory up upon shutdown. If the directory does not exist, TPOT will     create it. - Memory object:     TPOT uses the instance of joblib.Memory for memory caching,     and TPOT does NOT clean the caching directory up upon shutdown. - None:     TPOT does not use memory caching.</p> <code>None</code> <code>categorical_features</code> <p>Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False. - None : If None, TPOT will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing. - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices</p> <code>None</code> <code>preprocessing</code> <code>(bool or BaseEstimator / Pipeline)</code> <p>EXPERIMENTAL - will be changed in future versions A pipeline that will be used to preprocess the data before CV. Note that the parameters for these steps are not optimized. Add them to the search space to be optimized. - bool : If True, will use a default preprocessing pipeline which includes imputation followed by one hot encoding. - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.</p> <code>False</code> <code>population_size</code> <code>int</code> <p>Size of the population</p> <code>50</code> <code>initial_population_size</code> <code>int</code> <p>Size of the initial population. If None, population_size will be used.</p> <code>None</code> <code>population_scaling</code> <code>int</code> <p>Scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.</p> <code>0.5</code> <code>generations_until_end_population</code> <code>int</code> <p>Number of generations until the population size reaches population_size</p> <code>1</code> <code>generations</code> <code>int</code> <p>Number of generations to run</p> <code>50</code> <code>max_time_mins</code> <code>float</code> <p>Maximum time to run the optimization. If none or inf, will run until the end of the generations.</p> <code>float(\"inf\")</code> <code>max_eval_time_mins</code> <code>float</code> <p>Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.</p> <code>5</code> <code>validation_strategy</code> <code>str</code> <p>EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT may overfit the cross validation score. A second validation set can be used to select the final pipeline. - 'auto' : Automatically determine the validation strategy based on the dataset shape. - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets. - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets. - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.</p> <code>'none'</code> <code>validation_fraction</code> <code>float</code> <p>EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.</p> <code>0.2</code> <code>disable_label_encoder</code> <code>bool</code> <p>If True, TPOT will check if the target needs to be relabeled to be sequential ints from 0 to N. This is necessary for XGBoost compatibility. If the labels need to be encoded, TPOT will use sklearn.preprocessing.LabelEncoder to encode the labels. The encoder can be accessed via the self.label_encoder_ attribute. If False, no additional label encoders will be used.</p> <code>False</code> <code>early_stop</code> <code>int</code> <p>Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered. In general a value of around 5-20 is good.</p> <code>None</code> <code>scorers_early_stop_tol</code> <p>-list of floats     list of tolerances for each scorer. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged     If an index of the list is None, that item will not be used for early stopping -int     If an int is given, it will be used as the tolerance for all objectives</p> <code>0.001</code> <code>other_objectives_early_stop_tol</code> <p>-list of floats     list of tolerances for each of the other objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged     If an index of the list is None, that item will not be used for early stopping -int     If an int is given, it will be used as the tolerance for all objectives</p> <code>None</code> <code>threshold_evaluation_pruning</code> <code>list[start, end]</code> <p>starting and ending percentile to use as a threshold for the evaluation early stopping. Values between 0 and 100.</p> <code>None</code> <code>threshold_evaluation_scaling</code> <code>float [0,inf)</code> <p>A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile. Must be greater than zero. Higher numbers will move the threshold to the end faster.</p> <code>0.5</code> <code>selection_evaluation_pruning</code> <code>list</code> <p>A lower and upper percent of the population size to select each round of CV. Values between 0 and 1.</p> <code>None</code> <code>selection_evaluation_scaling</code> <code>float</code> <p>A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile. Must be greater than zero. Higher numbers will move the threshold to the end faster.</p> <code>0.5</code> <code>min_history_threshold</code> <code>int</code> <p>The minimum number of previous scores needed before using threshold early stopping.</p> <code>0</code> <code>survival_percentage</code> <code>float</code> <p>Percentage of the population size to utilize for mutation and crossover at the beginning of the generation. The rest are discarded. Individuals are selected with the selector passed into survival_selector. The value of this parameter must be between 0 and 1, inclusive. For example, if the population size is 100 and the survival percentage is .5, 50 individuals will be selected with NSGA2 from the existing population. These will be used for mutation and crossover to generate the next 100 individuals for the next generation. The remainder are discarded from the live population. In the next generation, there will now be the 50 parents + the 100 individuals for a total of 150. Surivival percentage is based of the population size parameter and not the existing population size (current population size when using successive halving). Therefore, in the next generation we will still select 50 individuals from the currently existing 150.</p> <code>1</code> <code>crossover_probability</code> <code>float</code> <p>Probability of generating a new individual by crossover between two individuals.</p> <code>.2</code> <code>mutate_probability</code> <code>float</code> <p>Probability of generating a new individual by crossover between one individuals.</p> <code>.7</code> <code>mutate_then_crossover_probability</code> <code>float</code> <p>Probability of generating a new individual by mutating two individuals followed by crossover.</p> <code>.05</code> <code>crossover_then_mutate_probability</code> <code>float</code> <p>Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.</p> <code>.05</code> <code>survival_selector</code> <code>function</code> <p>Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes. Used to selected population_size * survival_percentage individuals at the start of each generation to use for mutation and crossover.</p> <code>survival_select_NSGA2</code> <code>parent_selector</code> <code>function</code> <p>Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.</p> <code>parent_select_NSGA2</code> <code>budget_range</code> <code>list[start, end]</code> <p>A starting and ending budget to use for the budget scaling.</p> <code>None</code> <code>budget_scaling</code> <p>A scaling factor to use when determining how fast we move the budget from the start to end budget.</p> <code>0.5</code> <code>generations_until_end_budget</code> <code>int</code> <p>The number of generations to run before reaching the max budget.</p> <code>1</code> <code>stepwise_steps</code> <code>int</code> <p>The number of staircase steps to take when scaling the budget and population size.</p> <code>1</code> <code>n_jobs</code> <code>int</code> <p>Number of processes to run in parallel.</p> <code>1</code> <code>memory_limit</code> <code>str</code> <p>Memory limit for each job. See Dask LocalCluster documentation for more information.</p> <code>None</code> <code>client</code> <code>Client</code> <p>A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.</p> <code>None</code> <code>processes</code> <code>bool</code> <p>If True, will use multiprocessing to parallelize the optimization process. If False, will use threading. True seems to perform better. However, False is required for interactive debugging.</p> <code>True</code> <code>warm_start</code> <code>bool</code> <p>If True, will use the continue the evolutionary algorithm from the last generation of the previous run.</p> <code>False</code> <code>periodic_checkpoint_folder</code> <code>str</code> <p>Folder to save the population to periodically. If None, no periodic saving will be done. If provided, training will resume from this checkpoint.</p> <code>None</code> <code>callback</code> <code>CallBackInterface</code> <p>Callback object. Not implemented</p> <code>None</code> <code>verbose</code> <code>int</code> <p>How much information to print during the optimization process. Higher values include the information from lower values. 0. nothing 1. progress bar</p> <ol> <li>best individual</li> <li>warnings <p>=5. full warnings trace</p> </li> <li>evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_mins in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)</li> </ol> <code>1</code> <code>scatter</code> <code>bool</code> <p>If True, will scatter the data to the dask workers. If False, will not scatter the data. This can be useful for debugging.</p> <code>True</code> <code>random_state</code> <code>(int, None)</code> <p>A seed for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes</p> <ul> <li>int     Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'</li> <li>None     Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS</li> </ul> <code>None</code> <p>Attributes:</p> Name Type Description <code>fitted_pipeline_</code> <code>GraphPipeline</code> <p>A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.</p> <code>evaluated_individuals</code> <code>A pandas data frame containing data for all evaluated individuals in the run.</code> <p>Columns: - objective functions : The first few columns correspond to the passed in scorers and objective functions - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population. - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population. - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator. - Generation : The generation the pipeline first appeared. - Pareto_Front      : The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.                 To save on computational time, the best frontier is updated iteratively each generation.                 The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.                 All other pipelines are set to NaN. - Instance  : The unfitted GraphPipeline BaseEstimator. - validation objective functions : Objective function scores evaluated on the validation set. - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.</p> <code>pareto_front</code> <code>The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.</code> Source code in <code>tpot/tpot_estimator/estimator.py</code> <pre><code>def __init__(self,  \n                    search_space,\n                    scorers,\n                    scorers_weights,\n                    classification,\n                    cv = 10,\n                    other_objective_functions=[],\n                    other_objective_functions_weights = [],\n                    objective_function_names = None,\n                    bigger_is_better = True,\n\n                    export_graphpipeline = False,\n                    memory = None,\n\n                    categorical_features = None,\n                    preprocessing = False,\n                    population_size = 50,\n                    initial_population_size = None,\n                    population_scaling = .5,\n                    generations_until_end_population = 1,\n                    generations = None,\n                    max_time_mins=60,\n                    max_eval_time_mins=10,\n                    validation_strategy = \"none\",\n                    validation_fraction = .2,\n                    disable_label_encoder = False,\n\n                    #early stopping parameters\n                    early_stop = None,\n                    scorers_early_stop_tol = 0.001,\n                    other_objectives_early_stop_tol =None,\n                    threshold_evaluation_pruning = None,\n                    threshold_evaluation_scaling = .5,\n                    selection_evaluation_pruning = None,\n                    selection_evaluation_scaling = .5,\n                    min_history_threshold = 20,\n\n                    #evolver parameters\n                    survival_percentage = 1,\n                    crossover_probability=.2,\n                    mutate_probability=.7,\n                    mutate_then_crossover_probability=.05,\n                    crossover_then_mutate_probability=.05,\n                    survival_selector = survival_select_NSGA2,\n                    parent_selector = tournament_selection_dominated,\n\n                    #budget parameters\n                    budget_range = None,\n                    budget_scaling = .5,\n                    generations_until_end_budget = 1,\n                    stepwise_steps = 5,\n\n                    #dask parameters\n                    n_jobs=1,\n                    memory_limit = None,\n                    client = None,\n                    processes = True,\n\n                    #debugging and logging parameters\n                    warm_start = False,\n                    periodic_checkpoint_folder = None,\n                    callback = None,\n\n                    verbose = 0,\n                    scatter = True,\n\n                     # random seed for random number generator (rng)\n                    random_state = None,\n\n                    ):\n\n    '''\n    An sklearn baseestimator that uses genetic programming to optimize a pipeline.\n\n    Parameters\n    ----------\n    search_space : (String, tpot.search_spaces.SearchSpace)\n        - String : The default search space to use for the optimization.\n        | String     | Description      |\n        | :---        |    :----:   |\n        | linear  | A linear pipeline with the structure of \"Selector-&gt;(transformers+Passthrough)-&gt;(classifiers/regressors+Passthrough)-&gt;final classifier/regressor.\" For both the transformer and inner estimator layers, TPOT may choose one or more transformers/classifiers, or it may choose none. The inner classifier/regressor layer is optional. |\n        | linear-light | Same search space as linear, but without the inner classifier/regressor layer and with a reduced set of faster running estimators. |\n        | graph | TPOT will optimize a pipeline in the shape of a directed acyclic graph. The nodes of the graph can include selectors, scalers, transformers, or classifiers/regressors (inner classifiers/regressors can optionally be not included). This will return a custom GraphPipeline rather than an sklearn Pipeline. More details in Tutorial 6. |\n        | graph-light | Same as graph search space, but without the inner classifier/regressors and with a reduced set of faster running estimators. |\n        | mdr |TPOT will search over a series of feature selectors and Multifactor Dimensionality Reduction models to find a series of operators that maximize prediction accuracy. The TPOT MDR configuration is specialized for genome-wide association studies (GWAS), and is described in detail online here.\n\n        Note that TPOT MDR may be slow to run because the feature selection routines are computationally expensive, especially on large datasets. |\n\n\n        - SearchSpace : The search space to use for the optimization. This should be an instance of a SearchSpace.\n            The search space to use for the optimization. This should be an instance of a SearchSpace.\n            TPOT has groups of search spaces found in the following folders, tpot.search_spaces.nodes for the nodes in the pipeline and tpot.search_spaces.pipelines for the pipeline structure.\n\n    scorers : (list, scorer)\n        A scorer or list of scorers to be used in the cross-validation process.\n        see https://scikit-learn.org/stable/modules/model_evaluation.html\n\n    scorers_weights : list\n        A list of weights to be applied to the scorers during the optimization process.\n\n    classification : bool\n        If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem.\n        Used to determine the CV strategy.\n\n    cv : int, cross-validator\n        - (int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.\n        - (sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.\n            - max_depth (int): The maximum depth from any node to the root of the pipelines to be generated.\n\n    other_objective_functions : list, default=[]\n        A list of other objective functions to apply to the pipeline. The function takes a single parameter for the graphpipeline estimator and returns either a single score or a list of scores.\n\n    other_objective_functions_weights : list, default=[]\n        A list of weights to be applied to the other objective functions.\n\n    objective_function_names : list, default=None\n        A list of names to be applied to the objective functions. If None, will use the names of the objective functions.\n\n    bigger_is_better : bool, default=True\n        If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n\n    memory: Memory object or string, default=None\n        If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. This feature\n        is used to avoid computing the fit transformers within a pipeline if the parameters\n        and input data are identical with another fitted pipeline during optimization process.\n        - String 'auto':\n            TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.\n        - String path of a caching directory\n            TPOT uses memory caching with the provided directory and TPOT does NOT clean\n            the caching directory up upon shutdown. If the directory does not exist, TPOT will\n            create it.\n        - Memory object:\n            TPOT uses the instance of joblib.Memory for memory caching,\n            and TPOT does NOT clean the caching directory up upon shutdown.\n        - None:\n            TPOT does not use memory caching.              \n\n    categorical_features: list or None\n        Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n        - None : If None, TPOT will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing.\n        - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices\n\n    preprocessing : bool or BaseEstimator/Pipeline,\n        EXPERIMENTAL - will be changed in future versions\n        A pipeline that will be used to preprocess the data before CV. Note that the parameters for these steps are not optimized. Add them to the search space to be optimized.\n        - bool : If True, will use a default preprocessing pipeline which includes imputation followed by one hot encoding.\n        - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.\n\n    population_size : int, default=50\n        Size of the population\n\n    initial_population_size : int, default=None\n        Size of the initial population. If None, population_size will be used.\n\n    population_scaling : int, default=0.5\n        Scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n\n    generations_until_end_population : int, default=1\n        Number of generations until the population size reaches population_size\n\n    generations : int, default=50\n        Number of generations to run\n\n    max_time_mins : float, default=float(\"inf\")\n        Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n\n    max_eval_time_mins : float, default=5\n        Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n\n    validation_strategy : str, default='none'\n        EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT may overfit the cross validation score. A second validation set can be used to select the final pipeline.\n        - 'auto' : Automatically determine the validation strategy based on the dataset shape.\n        - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets.\n        - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets.\n        - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.\n\n    validation_fraction : float, default=0.2\n      EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.\n\n    disable_label_encoder : bool, default=False\n        If True, TPOT will check if the target needs to be relabeled to be sequential ints from 0 to N. This is necessary for XGBoost compatibility. If the labels need to be encoded, TPOT will use sklearn.preprocessing.LabelEncoder to encode the labels. The encoder can be accessed via the self.label_encoder_ attribute.\n        If False, no additional label encoders will be used.\n\n    early_stop : int, default=None\n        Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered. In general a value of around 5-20 is good.\n\n    scorers_early_stop_tol :\n        -list of floats\n            list of tolerances for each scorer. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n            If an index of the list is None, that item will not be used for early stopping\n        -int\n            If an int is given, it will be used as the tolerance for all objectives\n\n    other_objectives_early_stop_tol :\n        -list of floats\n            list of tolerances for each of the other objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n            If an index of the list is None, that item will not be used for early stopping\n        -int\n            If an int is given, it will be used as the tolerance for all objectives\n\n    threshold_evaluation_pruning : list [start, end], default=None\n        starting and ending percentile to use as a threshold for the evaluation early stopping.\n        Values between 0 and 100.\n\n    threshold_evaluation_scaling : float [0,inf), default=0.5\n        A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n        Must be greater than zero. Higher numbers will move the threshold to the end faster.\n\n    selection_evaluation_pruning : list, default=None\n        A lower and upper percent of the population size to select each round of CV.\n        Values between 0 and 1.\n\n    selection_evaluation_scaling : float, default=0.5\n        A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n        Must be greater than zero. Higher numbers will move the threshold to the end faster.\n\n    min_history_threshold : int, default=0\n        The minimum number of previous scores needed before using threshold early stopping.\n\n    survival_percentage : float, default=1\n        Percentage of the population size to utilize for mutation and crossover at the beginning of the generation. The rest are discarded. Individuals are selected with the selector passed into survival_selector. The value of this parameter must be between 0 and 1, inclusive.\n        For example, if the population size is 100 and the survival percentage is .5, 50 individuals will be selected with NSGA2 from the existing population. These will be used for mutation and crossover to generate the next 100 individuals for the next generation. The remainder are discarded from the live population. In the next generation, there will now be the 50 parents + the 100 individuals for a total of 150. Surivival percentage is based of the population size parameter and not the existing population size (current population size when using successive halving). Therefore, in the next generation we will still select 50 individuals from the currently existing 150.\n\n    crossover_probability : float, default=.2\n        Probability of generating a new individual by crossover between two individuals.\n\n    mutate_probability : float, default=.7\n        Probability of generating a new individual by crossover between one individuals.\n\n    mutate_then_crossover_probability : float, default=.05\n        Probability of generating a new individual by mutating two individuals followed by crossover.\n\n    crossover_then_mutate_probability : float, default=.05\n        Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.\n\n    survival_selector : function, default=survival_select_NSGA2\n        Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes.\n        Used to selected population_size * survival_percentage individuals at the start of each generation to use for mutation and crossover.\n\n    parent_selector : function, default=parent_select_NSGA2\n        Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.\n\n    budget_range : list [start, end], default=None\n        A starting and ending budget to use for the budget scaling.\n\n    budget_scaling float : [0,1], default=0.5\n        A scaling factor to use when determining how fast we move the budget from the start to end budget.\n\n    generations_until_end_budget : int, default=1\n        The number of generations to run before reaching the max budget.\n\n    stepwise_steps : int, default=1\n        The number of staircase steps to take when scaling the budget and population size.\n\n    n_jobs : int, default=1\n        Number of processes to run in parallel.\n\n    memory_limit : str, default=None\n        Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n\n    client : dask.distributed.Client, default=None\n        A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.\n\n    processes : bool, default=True\n        If True, will use multiprocessing to parallelize the optimization process. If False, will use threading.\n        True seems to perform better. However, False is required for interactive debugging.\n\n    warm_start : bool, default=False\n        If True, will use the continue the evolutionary algorithm from the last generation of the previous run.\n\n    periodic_checkpoint_folder : str, default=None\n        Folder to save the population to periodically. If None, no periodic saving will be done.\n        If provided, training will resume from this checkpoint.\n\n    callback : tpot.CallBackInterface, default=None\n        Callback object. Not implemented\n\n    verbose : int, default=1\n        How much information to print during the optimization process. Higher values include the information from lower values.\n        0. nothing\n        1. progress bar\n\n        3. best individual\n        4. warnings\n        &gt;=5. full warnings trace\n        6. evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_mins in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)\n\n    scatter : bool, default=True\n        If True, will scatter the data to the dask workers. If False, will not scatter the data. This can be useful for debugging.\n\n    random_state : int, None, default=None\n        A seed for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes\n\n        - int\n            Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'\n        - None\n            Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS\n\n    Attributes\n    ----------\n\n    fitted_pipeline_ : GraphPipeline\n        A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.\n\n    evaluated_individuals : A pandas data frame containing data for all evaluated individuals in the run.\n        Columns:\n        - *objective functions : The first few columns correspond to the passed in scorers and objective functions\n        - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population.\n        - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population.\n        - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator.\n        - Generation : The generation the pipeline first appeared.\n        - Pareto_Front\t: The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.\n                        To save on computational time, the best frontier is updated iteratively each generation.\n                        The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.\n                        All other pipelines are set to NaN.\n        - Instance\t: The unfitted GraphPipeline BaseEstimator.\n        - *validation objective functions : Objective function scores evaluated on the validation set.\n        - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.\n\n    pareto_front : The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.\n    '''\n\n    # sklearn BaseEstimator must have a corresponding attribute for each parameter.\n    # These should not be modified once set.\n\n    self.scorers = scorers\n    self.scorers_weights = scorers_weights\n    self.classification = classification\n    self.cv = cv\n    self.other_objective_functions = other_objective_functions\n    self.other_objective_functions_weights = other_objective_functions_weights\n    self.objective_function_names = objective_function_names\n    self.bigger_is_better = bigger_is_better\n\n    self.search_space = search_space\n\n    self.export_graphpipeline = export_graphpipeline\n    self.memory = memory\n\n    self.categorical_features = categorical_features\n\n    self.preprocessing = preprocessing\n    self.validation_strategy = validation_strategy\n    self.validation_fraction = validation_fraction\n    self.disable_label_encoder = disable_label_encoder\n    self.population_size = population_size\n    self.initial_population_size = initial_population_size\n    self.population_scaling = population_scaling\n    self.generations_until_end_population = generations_until_end_population\n    self.generations = generations\n    self.early_stop = early_stop\n    self.scorers_early_stop_tol = scorers_early_stop_tol\n    self.other_objectives_early_stop_tol = other_objectives_early_stop_tol\n    self.max_time_mins = max_time_mins\n    self.max_eval_time_mins = max_eval_time_mins\n    self.n_jobs= n_jobs\n    self.memory_limit = memory_limit\n    self.client = client\n    self.survival_percentage = survival_percentage\n    self.crossover_probability = crossover_probability\n    self.mutate_probability = mutate_probability\n    self.mutate_then_crossover_probability= mutate_then_crossover_probability\n    self.crossover_then_mutate_probability= crossover_then_mutate_probability\n    self.survival_selector=survival_selector\n    self.parent_selector=parent_selector\n    self.budget_range = budget_range\n    self.budget_scaling = budget_scaling\n    self.generations_until_end_budget = generations_until_end_budget\n    self.stepwise_steps = stepwise_steps\n    self.threshold_evaluation_pruning =threshold_evaluation_pruning\n    self.threshold_evaluation_scaling =  threshold_evaluation_scaling\n    self.min_history_threshold = min_history_threshold\n    self.selection_evaluation_pruning = selection_evaluation_pruning\n    self.selection_evaluation_scaling =  selection_evaluation_scaling\n    self.warm_start = warm_start\n    self.verbose = verbose\n    self.periodic_checkpoint_folder = periodic_checkpoint_folder\n    self.callback = callback\n    self.processes = processes\n\n\n    self.scatter = scatter\n\n\n    timer_set = self.max_time_mins != float(\"inf\") and self.max_time_mins is not None\n    if self.generations is not None and timer_set:\n        warnings.warn(\"Both generations and max_time_mins are set. TPOT will terminate when the first condition is met.\")\n\n    # create random number generator based on rngseed\n    self.rng = np.random.default_rng(random_state)\n    # save random state passed to us for other functions that use random_state\n    self.random_state = random_state\n\n    #Initialize other used params\n\n\n    if self.initial_population_size is None:\n        self._initial_population_size = self.population_size\n    else:\n        self._initial_population_size = self.initial_population_size\n\n    if isinstance(self.scorers, str):\n        self._scorers = [self.scorers]\n\n    elif callable(self.scorers):\n        self._scorers = [self.scorers]\n    else:\n        self._scorers = self.scorers\n\n    self._scorers = [sklearn.metrics.get_scorer(scoring) for scoring in self._scorers]\n    self._scorers_early_stop_tol = self.scorers_early_stop_tol\n\n    self._evolver = tpot.evolvers.BaseEvolver\n\n    self.objective_function_weights = [*scorers_weights, *other_objective_functions_weights]\n\n\n    if self.objective_function_names is None:\n        obj_names = [f.__name__ for f in other_objective_functions]\n    else:\n        obj_names = self.objective_function_names\n    self.objective_names = [f._score_func.__name__ if hasattr(f,\"_score_func\") else f.__name__ for f in self._scorers] + obj_names\n\n\n    if not isinstance(self.other_objectives_early_stop_tol, list):\n        self._other_objectives_early_stop_tol = [self.other_objectives_early_stop_tol for _ in range(len(self.other_objective_functions))]\n    else:\n        self._other_objectives_early_stop_tol = self.other_objectives_early_stop_tol\n\n    if not isinstance(self._scorers_early_stop_tol, list):\n        self._scorers_early_stop_tol = [self._scorers_early_stop_tol for _ in range(len(self._scorers))]\n    else:\n        self._scorers_early_stop_tol = self._scorers_early_stop_tol\n\n    self.early_stop_tol = [*self._scorers_early_stop_tol, *self._other_objectives_early_stop_tol]\n\n    self._evolver_instance = None\n    self.evaluated_individuals = None\n\n\n    self.label_encoder_ = None\n\n\n    set_dask_settings()\n</code></pre>"},{"location":"tpot_api/estimator/#tpot.tpot_estimator.estimator.apply_make_pipeline","title":"<code>apply_make_pipeline(ind, preprocessing_pipeline=None, export_graphpipeline=False, **pipeline_kwargs)</code>","text":"<p>Helper function to create a column of sklearn pipelines from the tpot individual class.</p> <p>Parameters:</p> Name Type Description Default <code>ind</code> <p>The individual to convert to a pipeline.</p> required <code>preprocessing_pipeline</code> <p>The preprocessing pipeline to include before the individual's pipeline.</p> <code>None</code> <code>export_graphpipeline</code> <p>Force the pipeline to be exported as a graph pipeline. Flattens all nested pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.</p> <code>False</code> <code>pipeline_kwargs</code> <p>Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>sklearn estimator</code> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def apply_make_pipeline(ind, preprocessing_pipeline=None, export_graphpipeline=False, **pipeline_kwargs):\n    \"\"\"\n    Helper function to create a column of sklearn pipelines from the tpot individual class.\n\n    Parameters\n    ----------\n    ind: tpot.SklearnIndividual\n        The individual to convert to a pipeline.\n    preprocessing_pipeline: sklearn.pipeline.Pipeline, optional\n        The preprocessing pipeline to include before the individual's pipeline.\n    export_graphpipeline: bool, default=False\n        Force the pipeline to be exported as a graph pipeline. Flattens all nested pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.\n    pipeline_kwargs: dict\n        Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.\n\n    Returns\n    -------\n    sklearn estimator\n    \"\"\"\n\n    try:\n\n        if export_graphpipeline:\n            est = ind.export_flattened_graphpipeline(**pipeline_kwargs)\n        else:\n            est = ind.export_pipeline(**pipeline_kwargs)\n\n\n        if preprocessing_pipeline is None:\n            return est\n        else:\n            return sklearn.pipeline.make_pipeline(sklearn.base.clone(preprocessing_pipeline), est)\n    except:\n        return None\n</code></pre>"},{"location":"tpot_api/estimator/#tpot.tpot_estimator.estimator.check_empty_values","title":"<code>check_empty_values(data)</code>","text":"<p>Checks for empty values in a dataset.</p> <p>Args:     data (numpy.ndarray or pandas.DataFrame): The dataset to check.</p> <p>Returns:     bool: True if the dataset contains empty values, False otherwise.</p> Source code in <code>tpot/tpot_estimator/estimator.py</code> <pre><code>def check_empty_values(data):\n    \"\"\"\n    Checks for empty values in a dataset.\n\n    Args:\n        data (numpy.ndarray or pandas.DataFrame): The dataset to check.\n\n    Returns:\n        bool: True if the dataset contains empty values, False otherwise.\n    \"\"\"\n    if isinstance(data, pd.DataFrame):\n        return data.isnull().values.any()\n    elif isinstance(data, np.ndarray):\n        return np.isnan(data).any()\n    else:\n        raise ValueError(\"Unsupported data type\")\n</code></pre>"},{"location":"tpot_api/estimator/#tpot.tpot_estimator.estimator.check_if_y_is_encoded","title":"<code>check_if_y_is_encoded(y)</code>","text":"<p>Checks if the target y is composed of sequential ints from 0 to N. XGBoost requires the target to be encoded in this way.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <p>The target vector.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the target is encoded as sequential ints from 0 to N, False otherwise</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def check_if_y_is_encoded(y):\n    '''\n    Checks if the target y is composed of sequential ints from 0 to N.\n    XGBoost requires the target to be encoded in this way.\n\n    Parameters\n    ----------\n    y: np.ndarray\n        The target vector.\n\n    Returns\n    -------\n    bool\n        True if the target is encoded as sequential ints from 0 to N, False otherwise\n    '''\n    y = sorted(set(y))\n    return all(i == j for i, j in enumerate(y))\n</code></pre>"},{"location":"tpot_api/estimator/#tpot.tpot_estimator.estimator.convert_parents_tuples_to_integers","title":"<code>convert_parents_tuples_to_integers(row, object_to_int)</code>","text":"<p>Helper function to convert the parent rows into integers representing the index of the parent in the population.</p> <p>Original pandas dataframe using a custom index for the parents. This function converts the custom index to an integer index for easier manipulation by end users.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <p>The row to convert.</p> required <code>object_to_int</code> <p>A dictionary mapping the object to an integer index.</p> required Returns <p>tuple     The row with the custom index converted to an integer index.</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def convert_parents_tuples_to_integers(row, object_to_int):\n    \"\"\"\n    Helper function to convert the parent rows into integers representing the index of the parent in the population.\n\n    Original pandas dataframe using a custom index for the parents. This function converts the custom index to an integer index for easier manipulation by end users.\n\n    Parameters\n    ----------\n    row: list, np.ndarray, tuple\n        The row to convert.\n    object_to_int: dict\n        A dictionary mapping the object to an integer index.\n\n    Returns \n    -------\n    tuple\n        The row with the custom index converted to an integer index.\n    \"\"\"\n    if type(row) == list or type(row) == np.ndarray or type(row) == tuple:\n        return tuple(object_to_int[obj] for obj in row)\n    else:\n        return np.nan\n</code></pre>"},{"location":"tpot_api/estimator/#tpot.tpot_estimator.estimator.cross_val_score_objective","title":"<code>cross_val_score_objective(estimator, X, y, scorers, cv, fold=None)</code>","text":"<p>Compute the cross validated scores for a estimator. Only fits the estimator once per fold, and loops over the scorers to evaluate the estimator.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <p>The estimator to fit and score.</p> required <code>X</code> <p>The feature matrix.</p> required <code>y</code> <p>The target vector.</p> required <code>scorers</code> <p>The scorers to use.  If a list, will loop over the scorers and return a list of scorers. If a single scorer, will return a single score.</p> required <code>cv</code> <p>The cross-validator to use. For example, sklearn.model_selection.KFold or sklearn.model_selection.StratifiedKFold.</p> required <code>fold</code> <p>The fold to return the scores for. If None, will return the mean of all the scores (per scorer). Default is None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>scores</code> <code>ndarray or float</code> <p>The scores for the estimator per scorer. If fold is None, will return the mean of all the scores (per scorer). Returns a list if multiple scorers are used, otherwise returns a float for the single scorer.</p> Source code in <code>tpot/tpot_estimator/cross_val_utils.py</code> <pre><code>def cross_val_score_objective(estimator, X, y, scorers, cv, fold=None):\n    \"\"\"\n    Compute the cross validated scores for a estimator. Only fits the estimator once per fold, and loops over the scorers to evaluate the estimator.\n\n    Parameters\n    ----------\n    estimator: sklearn.base.BaseEstimator\n        The estimator to fit and score.\n    X: np.ndarray or pd.DataFrame\n        The feature matrix.\n    y: np.ndarray or pd.Series\n        The target vector.\n    scorers: list or scorer\n        The scorers to use. \n        If a list, will loop over the scorers and return a list of scorers.\n        If a single scorer, will return a single score.\n    cv: sklearn cross-validator\n        The cross-validator to use. For example, sklearn.model_selection.KFold or sklearn.model_selection.StratifiedKFold.\n    fold: int, optional\n        The fold to return the scores for. If None, will return the mean of all the scores (per scorer). Default is None.\n\n    Returns\n    -------\n    scores: np.ndarray or float\n        The scores for the estimator per scorer. If fold is None, will return the mean of all the scores (per scorer).\n        Returns a list if multiple scorers are used, otherwise returns a float for the single scorer.\n\n    \"\"\"\n\n    #check if scores is not iterable\n    if not isinstance(scorers, Iterable): \n        scorers = [scorers]\n    scores = []\n    if fold is None:\n        for train_index, test_index in cv.split(X, y):\n            this_fold_estimator = sklearn.base.clone(estimator)\n            if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n            else:\n                X_train, X_test = X[train_index], X[test_index]\n\n            if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n            else:\n                y_train, y_test = y[train_index], y[test_index]\n\n\n            start = time.time()\n            this_fold_estimator.fit(X_train,y_train)\n            duration = time.time() - start\n\n            this_fold_scores = [sklearn.metrics.get_scorer(scorer)(this_fold_estimator, X_test, y_test) for scorer in scorers] \n            scores.append(this_fold_scores)\n            del this_fold_estimator\n            del X_train\n            del X_test\n            del y_train\n            del y_test\n\n\n        return np.mean(scores,0)\n    else:\n        this_fold_estimator = sklearn.base.clone(estimator)\n        train_index, test_index = list(cv.split(X, y))[fold]\n        if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        else:\n            X_train, X_test = X[train_index], X[test_index]\n\n        if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n        else:\n            y_train, y_test = y[train_index], y[test_index]\n\n        start = time.time()\n        this_fold_estimator.fit(X_train,y_train)\n        duration = time.time() - start\n        this_fold_scores = [sklearn.metrics.get_scorer(scorer)(this_fold_estimator, X_test, y_test) for scorer in scorers] \n        return this_fold_scores\n</code></pre>"},{"location":"tpot_api/estimator/#tpot.tpot_estimator.estimator.objective_function_generator","title":"<code>objective_function_generator(pipeline, x, y, scorers, cv, other_objective_functions, step=None, budget=None, is_classification=True, export_graphpipeline=False, **pipeline_kwargs)</code>","text":"<p>Uses cross validation to evaluate the pipeline using the scorers, and concatenates results with scores from standalone other objective functions.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <p>The individual to evaluate.</p> required <code>x</code> <p>The feature matrix.</p> required <code>y</code> <p>The target vector.</p> required <code>scorers</code> <p>The scorers to use for cross validation.</p> required <code>cv</code> <p>The cross-validator to use. For example, sklearn.model_selection.KFold or sklearn.model_selection.StratifiedKFold. If an int, will use sklearn.model_selection.KFold with n_splits=cv.</p> required <code>other_objective_functions</code> <p>A list of standalone objective functions to evaluate the pipeline. With signature obj(pipeline) -&gt; float. or obj(pipeline) -&gt; np.ndarray These functions take in the unfitted estimator.</p> required <code>step</code> <p>The fold to return the scores for. If None, will return the mean of all the scores (per scorer). Default is None.</p> <code>None</code> <code>budget</code> <p>The budget to subsample the data. If None, will use the full dataset. Default is None. Will subsample budget*len(x) samples.</p> <code>None</code> <code>is_classification</code> <p>If True, will stratify the subsampling. Default is True.</p> <code>True</code> <code>export_graphpipeline</code> <p>Force the pipeline to be exported as a graph pipeline. Flattens all nested sklearn pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.</p> <code>False</code> <code>pipeline_kwargs</code> <p>Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The concatenated scores for the pipeline. The first len(scorers) elements are the cross validation scores, and the remaining elements are the standalone objective functions.</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def objective_function_generator(pipeline, x,y, scorers, cv, other_objective_functions, step=None, budget=None, is_classification=True, export_graphpipeline=False, **pipeline_kwargs):\n    \"\"\"\n    Uses cross validation to evaluate the pipeline using the scorers, and concatenates results with scores from standalone other objective functions.\n\n    Parameters\n    ----------\n    pipeline: tpot.SklearnIndividual\n        The individual to evaluate.\n    x: np.ndarray\n        The feature matrix.\n    y: np.ndarray\n        The target vector.\n    scorers: list\n        The scorers to use for cross validation. \n    cv: int, float, or sklearn cross-validator\n        The cross-validator to use. For example, sklearn.model_selection.KFold or sklearn.model_selection.StratifiedKFold.\n        If an int, will use sklearn.model_selection.KFold with n_splits=cv.\n    other_objective_functions: list\n        A list of standalone objective functions to evaluate the pipeline. With signature obj(pipeline) -&gt; float. or obj(pipeline) -&gt; np.ndarray\n        These functions take in the unfitted estimator.\n    step: int, optional\n        The fold to return the scores for. If None, will return the mean of all the scores (per scorer). Default is None.\n    budget: float, optional\n        The budget to subsample the data. If None, will use the full dataset. Default is None.\n        Will subsample budget*len(x) samples.\n    is_classification: bool, default=True\n        If True, will stratify the subsampling. Default is True.\n    export_graphpipeline: bool, default=False\n        Force the pipeline to be exported as a graph pipeline. Flattens all nested sklearn pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.\n    pipeline_kwargs: dict\n        Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.\n\n    Returns\n    -------\n    np.ndarray\n        The concatenated scores for the pipeline. The first len(scorers) elements are the cross validation scores, and the remaining elements are the standalone objective functions.\n\n    \"\"\"\n\n    if export_graphpipeline:\n        pipeline = pipeline.export_flattened_graphpipeline(**pipeline_kwargs)\n    else:\n        pipeline = pipeline.export_pipeline(**pipeline_kwargs)\n\n    if budget is not None and budget &lt; 1:\n        if is_classification:\n            x,y = sklearn.utils.resample(x,y, stratify=y, n_samples=int(budget*len(x)), replace=False, random_state=1)\n        else:\n            x,y = sklearn.utils.resample(x,y, n_samples=int(budget*len(x)), replace=False, random_state=1)\n\n        if isinstance(cv, int) or isinstance(cv, float):\n            n_splits = cv\n        else:\n            n_splits = cv.n_splits\n\n    if len(scorers) &gt; 0:\n        cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)\n    else:\n        cv_obj_scores = []\n\n    if other_objective_functions is not None and len(other_objective_functions) &gt;0:\n        other_scores = [obj(sklearn.base.clone(pipeline)) for obj in other_objective_functions]\n        #flatten\n        other_scores = np.array(other_scores).flatten().tolist()\n    else:\n        other_scores = []\n\n    return np.concatenate([cv_obj_scores,other_scores])\n</code></pre>"},{"location":"tpot_api/estimator/#tpot.tpot_estimator.estimator.remove_underrepresented_classes","title":"<code>remove_underrepresented_classes(x, y, min_count)</code>","text":"<p>Helper function to remove classes with less than min_count samples from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>The feature matrix.</p> required <code>y</code> <p>The target vector.</p> required <code>min_count</code> <p>The minimum number of samples to keep a class.</p> required <p>Returns:</p> Type Description <code>(ndarray, ndarray)</code> <p>The feature matrix and target vector with rows from classes with less than min_count samples removed.</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def remove_underrepresented_classes(x, y, min_count):\n    \"\"\"\n    Helper function to remove classes with less than min_count samples from the dataset.\n\n    Parameters\n    ----------\n    x: np.ndarray or pd.DataFrame\n        The feature matrix.\n    y: np.ndarray or pd.Series\n        The target vector.\n    min_count: int\n        The minimum number of samples to keep a class.\n\n    Returns\n    -------\n    np.ndarray, np.ndarray\n        The feature matrix and target vector with rows from classes with less than min_count samples removed.\n    \"\"\"\n    if isinstance(y, (np.ndarray, pd.Series)):\n        unique, counts = np.unique(y, return_counts=True)\n        if min(counts) &gt;= min_count:\n            return x, y\n        keep_classes = unique[counts &gt;= min_count]\n        mask = np.isin(y, keep_classes)\n        x = x[mask]\n        y = y[mask]\n    elif isinstance(y, pd.DataFrame):\n        counts = y.apply(pd.Series.value_counts)\n        if min(counts) &gt;= min_count:\n            return x, y\n        keep_classes = counts.index[counts &gt;= min_count].tolist()\n        mask = y.isin(keep_classes).all(axis=1)\n        x = x[mask]\n        y = y[mask]\n    else:\n        raise TypeError(\"y must be a numpy array or a pandas Series/DataFrame\")\n    return x, y\n</code></pre>"},{"location":"tpot_api/estimator/#tpot.tpot_estimator.estimator.val_objective_function_generator","title":"<code>val_objective_function_generator(pipeline, X_train, y_train, X_test, y_test, scorers, other_objective_functions, export_graphpipeline=False, **pipeline_kwargs)</code>","text":"<p>Trains a pipeline on a training set and evaluates it on a test set using the scorers and other objective functions.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <p>The individual to evaluate.</p> required <code>X_train</code> <p>The feature matrix of the training set.</p> required <code>y_train</code> <p>The target vector of the training set.</p> required <code>X_test</code> <p>The feature matrix of the test set.</p> required <code>y_test</code> <p>The target vector of the test set.</p> required <code>scorers</code> <p>The scorers to use for cross validation.</p> required <code>other_objective_functions</code> <p>A list of standalone objective functions to evaluate the pipeline. With signature obj(pipeline) -&gt; float. or obj(pipeline) -&gt; np.ndarray These functions take in the unfitted estimator.</p> required <code>export_graphpipeline</code> <p>Force the pipeline to be exported as a graph pipeline. Flattens all nested sklearn pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.</p> <code>False</code> <code>pipeline_kwargs</code> <p>Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The concatenated scores for the pipeline. The first len(scorers) elements are the cross validation scores, and the remaining elements are the standalone objective functions.</p> Source code in <code>tpot/tpot_estimator/estimator_utils.py</code> <pre><code>def val_objective_function_generator(pipeline, X_train, y_train, X_test, y_test, scorers, other_objective_functions, export_graphpipeline=False, **pipeline_kwargs):\n    \"\"\"\n    Trains a pipeline on a training set and evaluates it on a test set using the scorers and other objective functions.\n\n    Parameters\n    ----------\n\n    pipeline: tpot.SklearnIndividual\n        The individual to evaluate.\n    X_train: np.ndarray\n        The feature matrix of the training set.\n    y_train: np.ndarray\n        The target vector of the training set.\n    X_test: np.ndarray\n        The feature matrix of the test set.\n    y_test: np.ndarray\n        The target vector of the test set.\n    scorers: list\n        The scorers to use for cross validation.\n    other_objective_functions: list\n        A list of standalone objective functions to evaluate the pipeline. With signature obj(pipeline) -&gt; float. or obj(pipeline) -&gt; np.ndarray\n        These functions take in the unfitted estimator.\n    export_graphpipeline: bool, default=False\n        Force the pipeline to be exported as a graph pipeline. Flattens all nested sklearn pipelines, FeatureUnions, and GraphPipelines into a single GraphPipeline.\n    pipeline_kwargs: dict\n        Keyword arguments to pass to the export_pipeline or export_flattened_graphpipeline method.\n\n    Returns\n    -------\n    np.ndarray\n        The concatenated scores for the pipeline. The first len(scorers) elements are the cross validation scores, and the remaining elements are the standalone objective functions.\n\n\n    \"\"\"\n\n    #subsample the data\n    if export_graphpipeline:\n        pipeline = pipeline.export_flattened_graphpipeline(**pipeline_kwargs)\n    else:\n        pipeline = pipeline.export_pipeline(**pipeline_kwargs)\n\n    fitted_pipeline = sklearn.base.clone(pipeline)\n    fitted_pipeline.fit(X_train, y_train)\n\n    if len(scorers) &gt; 0:\n        scores =[sklearn.metrics.get_scorer(scorer)(fitted_pipeline, X_test, y_test) for scorer in scorers]\n\n    other_scores = []\n    if other_objective_functions is not None and len(other_objective_functions) &gt;0:\n        other_scores = [obj(sklearn.base.clone(pipeline)) for obj in other_objective_functions]\n\n    return np.concatenate([scores,other_scores])\n</code></pre>"},{"location":"tpot_api/regressor/","title":"Regressor","text":"<p>               Bases: <code>TPOTEstimator</code></p> Source code in <code>tpot/tpot_estimator/templates/tpottemplates.py</code> <pre><code>class TPOTRegressor(TPOTEstimator):\n    def __init__(       self,\n                        search_space = \"linear\",\n                        scorers=['neg_mean_squared_error'], \n                        scorers_weights=[1],\n                        cv = 10, #remove this and use a value based on dataset size?\n                        other_objective_functions=[], #tpot.objectives.estimator_objective_functions.number_of_nodes_objective],\n                        other_objective_functions_weights = [],\n                        objective_function_names = None,\n                        bigger_is_better = True,\n                        categorical_features = None,\n                        memory = None,\n                        preprocessing = False,\n                        max_time_mins=60, \n                        max_eval_time_mins=10, \n                        n_jobs = 1,\n                        validation_strategy = \"none\",\n                        validation_fraction = .2, \n                        early_stop = None,\n                        warm_start = False,\n                        periodic_checkpoint_folder = None, \n                        verbose = 2,\n                        memory_limit = None,\n                        client = None,\n                        random_state=None,\n                        allow_inner_regressors=None,\n                        **tpotestimator_kwargs,\n        ):\n        '''\n        An sklearn baseestimator that uses genetic programming to optimize a regression pipeline.\n        For more parameters, see the TPOTEstimator class.\n\n        Parameters\n        ----------\n\n        search_space : (String, tpot.search_spaces.SearchSpace)\n                        - String : The default search space to use for the optimization.\n            | String     | Description      |\n            | :---        |    :----:   |\n            | linear  | A linear pipeline with the structure of \"Selector-&gt;(transformers+Passthrough)-&gt;(classifiers/regressors+Passthrough)-&gt;final classifier/regressor.\" For both the transformer and inner estimator layers, TPOT may choose one or more transformers/classifiers, or it may choose none. The inner classifier/regressor layer is optional. |\n            | linear-light | Same search space as linear, but without the inner classifier/regressor layer and with a reduced set of faster running estimators. |\n            | graph | TPOT will optimize a pipeline in the shape of a directed acyclic graph. The nodes of the graph can include selectors, scalers, transformers, or classifiers/regressors (inner classifiers/regressors can optionally be not included). This will return a custom GraphPipeline rather than an sklearn Pipeline. More details in Tutorial 6. |\n            | graph-light | Same as graph search space, but without the inner classifier/regressors and with a reduced set of faster running estimators. |\n            | mdr |TPOT will search over a series of feature selectors and Multifactor Dimensionality Reduction models to find a series of operators that maximize prediction accuracy. The TPOT MDR configuration is specialized for genome-wide association studies (GWAS), and is described in detail online here.\n\n            Note that TPOT MDR may be slow to run because the feature selection routines are computationally expensive, especially on large datasets. |\n            - SearchSpace : The search space to use for the optimization. This should be an instance of a SearchSpace.\n                The search space to use for the optimization. This should be an instance of a SearchSpace.\n                TPOT has groups of search spaces found in the following folders, tpot.search_spaces.nodes for the nodes in the pipeline and tpot.search_spaces.pipelines for the pipeline structure.\n\n        scorers : (list, scorer)\n            A scorer or list of scorers to be used in the cross-validation process.\n            see https://scikit-learn.org/stable/modules/model_evaluation.html\n\n        scorers_weights : list\n            A list of weights to be applied to the scorers during the optimization process.\n\n        classification : bool\n            If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem.\n            Used to determine the CV strategy.\n\n        cv : int, cross-validator\n            - (int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.\n            - (sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.\n                - max_depth (int): The maximum depth from any node to the root of the pipelines to be generated.\n\n        other_objective_functions : list, default=[]\n            A list of other objective functions to apply to the pipeline. The function takes a single parameter for the graphpipeline estimator and returns either a single score or a list of scores.\n\n        other_objective_functions_weights : list, default=[]\n            A list of weights to be applied to the other objective functions.\n\n        objective_function_names : list, default=None\n            A list of names to be applied to the objective functions. If None, will use the names of the objective functions.\n\n        bigger_is_better : bool, default=True\n            If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n\n        categorical_features : list or None\n            Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n\n        categorical_features: list or None\n            Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n            - None : If None, TPOT will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing.\n            - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices\n\n\n        memory: Memory object or string, default=None\n            If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. This feature\n            is used to avoid computing the fit transformers within a pipeline if the parameters\n            and input data are identical with another fitted pipeline during optimization process.\n            - String 'auto':\n                TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.\n            - String path of a caching directory\n                TPOT uses memory caching with the provided directory and TPOT does NOT clean\n                the caching directory up upon shutdown. If the directory does not exist, TPOT will\n                create it.\n            - Memory object:\n                TPOT uses the instance of joblib.Memory for memory caching,\n                and TPOT does NOT clean the caching directory up upon shutdown.\n            - None:\n                TPOT does not use memory caching.\n\n        preprocessing : bool or BaseEstimator/Pipeline,\n            EXPERIMENTAL\n            A pipeline that will be used to preprocess the data before CV. Note that the parameters for these steps are not optimized. Add them to the search space to be optimized.\n            - bool : If True, will use a default preprocessing pipeline which includes imputation followed by one hot encoding.\n            - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.\n\n        max_time_mins : float, default=float(\"inf\")\n            Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n\n        max_eval_time_mins : float, default=60*5\n            Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n\n\n        n_jobs : int, default=1\n            Number of processes to run in parallel.\n\n        validation_strategy : str, default='none'\n            EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT may overfit the cross validation score. A second validation set can be used to select the final pipeline.\n            - 'auto' : Automatically determine the validation strategy based on the dataset shape.\n            - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets.\n            - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets.\n            - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.\n\n        validation_fraction : float, default=0.2\n          EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.\n\n        early_stop : int, default=None\n            Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered. In general a value of around 5-20 is good.\n\n        warm_start : bool, default=False\n            If True, will use the continue the evolutionary algorithm from the last generation of the previous run.\n\n        periodic_checkpoint_folder : str, default=None\n            Folder to save the population to periodically. If None, no periodic saving will be done.\n            If provided, training will resume from this checkpoint.\n\n\n        verbose : int, default=1\n            How much information to print during the optimization process. Higher values include the information from lower values.\n            0. nothing\n            1. progress bar\n\n            3. best individual\n            4. warnings\n            &gt;=5. full warnings trace\n            6. evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_mins in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)\n\n\n        memory_limit : str, default=None\n            Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n\n        client : dask.distributed.Client, default=None\n            A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.\n\n        random_state : int, None, default=None\n            A seed for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes\n\n            - int\n                Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'\n            - None\n                Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS\n\n        allow_inner_regressors : bool, default=True\n            If True, the search space will include ensembled regressors.\n\n        Attributes\n        ----------\n\n        fitted_pipeline_ : GraphPipeline\n            A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.\n\n        evaluated_individuals : A pandas data frame containing data for all evaluated individuals in the run.\n            Columns:\n            - *objective functions : The first few columns correspond to the passed in scorers and objective functions\n            - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population.\n            - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population.\n            - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator.\n            - Generation : The generation the pipeline first appeared.\n            - Pareto_Front\t: The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.\n                            To save on computational time, the best frontier is updated iteratively each generation.\n                            The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.\n                            All other pipelines are set to NaN.\n            - Instance\t: The unfitted GraphPipeline BaseEstimator.\n            - *validation objective functions : Objective function scores evaluated on the validation set.\n            - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.\n\n        pareto_front : The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.\n        '''\n\n        self.search_space = search_space\n        self.scorers = scorers\n        self.scorers_weights = scorers_weights\n        self.cv = cv\n        self.other_objective_functions = other_objective_functions\n        self.other_objective_functions_weights = other_objective_functions_weights\n        self.objective_function_names = objective_function_names\n        self.bigger_is_better = bigger_is_better\n        self.categorical_features = categorical_features\n        self.memory = memory\n        self.preprocessing = preprocessing\n        self.max_time_mins = max_time_mins\n        self.max_eval_time_mins = max_eval_time_mins\n        self.n_jobs = n_jobs\n        self.validation_strategy = validation_strategy\n        self.validation_fraction = validation_fraction\n        self.early_stop = early_stop\n        self.warm_start = warm_start\n        self.periodic_checkpoint_folder = periodic_checkpoint_folder\n        self.verbose = verbose\n        self.memory_limit = memory_limit\n        self.client = client\n        self.random_state = random_state\n        self.allow_inner_regressors = allow_inner_regressors\n        self.tpotestimator_kwargs = tpotestimator_kwargs\n\n        self.initialized = False\n\n\n    def fit(self, X, y):\n\n        if not self.initialized:\n            get_search_space_params = {\"n_classes\": None, \n                                        \"n_samples\":len(y), \n                                        \"n_features\":X.shape[1], \n                                        \"random_state\":self.random_state}\n\n            search_space = get_template_search_spaces(self.search_space, classification=False, inner_predictors=self.allow_inner_regressors, **get_search_space_params)\n\n            super(TPOTRegressor,self).__init__(\n                search_space=search_space,\n                scorers=self.scorers, \n                scorers_weights=self.scorers_weights,\n                cv=self.cv,\n                other_objective_functions=self.other_objective_functions, #tpot.objectives.estimator_objective_functions.number_of_nodes_objective],\n                other_objective_functions_weights = self.other_objective_functions_weights,\n                objective_function_names = self.objective_function_names,\n                bigger_is_better = self.bigger_is_better,\n                categorical_features = self.categorical_features,\n                memory = self.memory,\n                preprocessing = self.preprocessing,\n                max_time_mins=self.max_time_mins, \n                max_eval_time_mins=self.max_eval_time_mins, \n                n_jobs=self.n_jobs,\n                validation_strategy = self.validation_strategy,\n                validation_fraction = self.validation_fraction, \n                early_stop = self.early_stop,\n                warm_start = self.warm_start,\n                periodic_checkpoint_folder = self.periodic_checkpoint_folder, \n                verbose = self.verbose,\n                classification=False,\n                memory_limit = self.memory_limit,\n                client = self.client,\n                random_state=self.random_state,\n                **self.tpotestimator_kwargs)\n            self.initialized = True\n\n        return super().fit(X,y)\n</code></pre>"},{"location":"tpot_api/regressor/#tpot.tpot_estimator.templates.tpottemplates.TPOTRegressor.__init__","title":"<code>__init__(search_space='linear', scorers=['neg_mean_squared_error'], scorers_weights=[1], cv=10, other_objective_functions=[], other_objective_functions_weights=[], objective_function_names=None, bigger_is_better=True, categorical_features=None, memory=None, preprocessing=False, max_time_mins=60, max_eval_time_mins=10, n_jobs=1, validation_strategy='none', validation_fraction=0.2, early_stop=None, warm_start=False, periodic_checkpoint_folder=None, verbose=2, memory_limit=None, client=None, random_state=None, allow_inner_regressors=None, **tpotestimator_kwargs)</code>","text":"<p>An sklearn baseestimator that uses genetic programming to optimize a regression pipeline. For more parameters, see the TPOTEstimator class.</p> <p>Parameters:</p> Name Type Description Default <code>search_space</code> <code>(String, SearchSpace)</code> <pre><code>        - String : The default search space to use for the optimization.\n</code></pre> String Description linear A linear pipeline with the structure of \"Selector-&gt;(transformers+Passthrough)-&gt;(classifiers/regressors+Passthrough)-&gt;final classifier/regressor.\" For both the transformer and inner estimator layers, TPOT may choose one or more transformers/classifiers, or it may choose none. The inner classifier/regressor layer is optional. linear-light Same search space as linear, but without the inner classifier/regressor layer and with a reduced set of faster running estimators. graph TPOT will optimize a pipeline in the shape of a directed acyclic graph. The nodes of the graph can include selectors, scalers, transformers, or classifiers/regressors (inner classifiers/regressors can optionally be not included). This will return a custom GraphPipeline rather than an sklearn Pipeline. More details in Tutorial 6. graph-light Same as graph search space, but without the inner classifier/regressors and with a reduced set of faster running estimators. mdr TPOT will search over a series of feature selectors and Multifactor Dimensionality Reduction models to find a series of operators that maximize prediction accuracy. The TPOT MDR configuration is specialized for genome-wide association studies (GWAS), and is described in detail online here. <p>Note that TPOT MDR may be slow to run because the feature selection routines are computationally expensive, especially on large datasets. | - SearchSpace : The search space to use for the optimization. This should be an instance of a SearchSpace.     The search space to use for the optimization. This should be an instance of a SearchSpace.     TPOT has groups of search spaces found in the following folders, tpot.search_spaces.nodes for the nodes in the pipeline and tpot.search_spaces.pipelines for the pipeline structure.</p> <code>'linear'</code> <code>scorers</code> <code>(list, scorer)</code> <p>A scorer or list of scorers to be used in the cross-validation process. see https://scikit-learn.org/stable/modules/model_evaluation.html</p> <code>['neg_mean_squared_error']</code> <code>scorers_weights</code> <code>list</code> <p>A list of weights to be applied to the scorers during the optimization process.</p> <code>[1]</code> <code>classification</code> <code>bool</code> <p>If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem. Used to determine the CV strategy.</p> required <code>cv</code> <code>(int, cross - validator)</code> <ul> <li>(int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.</li> <li>(sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.<ul> <li>max_depth (int): The maximum depth from any node to the root of the pipelines to be generated.</li> </ul> </li> </ul> <code>10</code> <code>other_objective_functions</code> <code>list</code> <p>A list of other objective functions to apply to the pipeline. The function takes a single parameter for the graphpipeline estimator and returns either a single score or a list of scores.</p> <code>[]</code> <code>other_objective_functions_weights</code> <code>list</code> <p>A list of weights to be applied to the other objective functions.</p> <code>[]</code> <code>objective_function_names</code> <code>list</code> <p>A list of names to be applied to the objective functions. If None, will use the names of the objective functions.</p> <code>None</code> <code>bigger_is_better</code> <code>bool</code> <p>If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.</p> <code>True</code> <code>categorical_features</code> <code>list or None</code> <p>Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.</p> <code>None</code> <code>categorical_features</code> <p>Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False. - None : If None, TPOT will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing. - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices</p> <code>None</code> <code>memory</code> <p>If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. This feature is used to avoid computing the fit transformers within a pipeline if the parameters and input data are identical with another fitted pipeline during optimization process. - String 'auto':     TPOT uses memory caching with a temporary directory and cleans it up upon shutdown. - String path of a caching directory     TPOT uses memory caching with the provided directory and TPOT does NOT clean     the caching directory up upon shutdown. If the directory does not exist, TPOT will     create it. - Memory object:     TPOT uses the instance of joblib.Memory for memory caching,     and TPOT does NOT clean the caching directory up upon shutdown. - None:     TPOT does not use memory caching.</p> <code>None</code> <code>preprocessing</code> <code>(bool or BaseEstimator / Pipeline)</code> <p>EXPERIMENTAL A pipeline that will be used to preprocess the data before CV. Note that the parameters for these steps are not optimized. Add them to the search space to be optimized. - bool : If True, will use a default preprocessing pipeline which includes imputation followed by one hot encoding. - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.</p> <code>False</code> <code>max_time_mins</code> <code>float</code> <p>Maximum time to run the optimization. If none or inf, will run until the end of the generations.</p> <code>float(\"inf\")</code> <code>max_eval_time_mins</code> <code>float</code> <p>Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.</p> <code>60*5</code> <code>n_jobs</code> <code>int</code> <p>Number of processes to run in parallel.</p> <code>1</code> <code>validation_strategy</code> <code>str</code> <p>EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT may overfit the cross validation score. A second validation set can be used to select the final pipeline. - 'auto' : Automatically determine the validation strategy based on the dataset shape. - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets. - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets. - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.</p> <code>'none'</code> <code>validation_fraction</code> <code>float</code> <p>EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.</p> <code>0.2</code> <code>early_stop</code> <code>int</code> <p>Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered. In general a value of around 5-20 is good.</p> <code>None</code> <code>warm_start</code> <code>bool</code> <p>If True, will use the continue the evolutionary algorithm from the last generation of the previous run.</p> <code>False</code> <code>periodic_checkpoint_folder</code> <code>str</code> <p>Folder to save the population to periodically. If None, no periodic saving will be done. If provided, training will resume from this checkpoint.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>How much information to print during the optimization process. Higher values include the information from lower values. 0. nothing 1. progress bar</p> <ol> <li>best individual</li> <li>warnings <p>=5. full warnings trace</p> </li> <li>evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_mins in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)</li> </ol> <code>1</code> <code>memory_limit</code> <code>str</code> <p>Memory limit for each job. See Dask LocalCluster documentation for more information.</p> <code>None</code> <code>client</code> <code>Client</code> <p>A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.</p> <code>None</code> <code>random_state</code> <code>(int, None)</code> <p>A seed for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes</p> <ul> <li>int     Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'</li> <li>None     Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS</li> </ul> <code>None</code> <code>allow_inner_regressors</code> <code>bool</code> <p>If True, the search space will include ensembled regressors.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>fitted_pipeline_</code> <code>GraphPipeline</code> <p>A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.</p> <code>evaluated_individuals</code> <code>A pandas data frame containing data for all evaluated individuals in the run.</code> <p>Columns: - objective functions : The first few columns correspond to the passed in scorers and objective functions - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population. - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population. - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator. - Generation : The generation the pipeline first appeared. - Pareto_Front      : The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.                 To save on computational time, the best frontier is updated iteratively each generation.                 The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.                 All other pipelines are set to NaN. - Instance  : The unfitted GraphPipeline BaseEstimator. - validation objective functions : Objective function scores evaluated on the validation set. - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.</p> <code>pareto_front</code> <code>The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.</code> Source code in <code>tpot/tpot_estimator/templates/tpottemplates.py</code> <pre><code>def __init__(       self,\n                    search_space = \"linear\",\n                    scorers=['neg_mean_squared_error'], \n                    scorers_weights=[1],\n                    cv = 10, #remove this and use a value based on dataset size?\n                    other_objective_functions=[], #tpot.objectives.estimator_objective_functions.number_of_nodes_objective],\n                    other_objective_functions_weights = [],\n                    objective_function_names = None,\n                    bigger_is_better = True,\n                    categorical_features = None,\n                    memory = None,\n                    preprocessing = False,\n                    max_time_mins=60, \n                    max_eval_time_mins=10, \n                    n_jobs = 1,\n                    validation_strategy = \"none\",\n                    validation_fraction = .2, \n                    early_stop = None,\n                    warm_start = False,\n                    periodic_checkpoint_folder = None, \n                    verbose = 2,\n                    memory_limit = None,\n                    client = None,\n                    random_state=None,\n                    allow_inner_regressors=None,\n                    **tpotestimator_kwargs,\n    ):\n    '''\n    An sklearn baseestimator that uses genetic programming to optimize a regression pipeline.\n    For more parameters, see the TPOTEstimator class.\n\n    Parameters\n    ----------\n\n    search_space : (String, tpot.search_spaces.SearchSpace)\n                    - String : The default search space to use for the optimization.\n        | String     | Description      |\n        | :---        |    :----:   |\n        | linear  | A linear pipeline with the structure of \"Selector-&gt;(transformers+Passthrough)-&gt;(classifiers/regressors+Passthrough)-&gt;final classifier/regressor.\" For both the transformer and inner estimator layers, TPOT may choose one or more transformers/classifiers, or it may choose none. The inner classifier/regressor layer is optional. |\n        | linear-light | Same search space as linear, but without the inner classifier/regressor layer and with a reduced set of faster running estimators. |\n        | graph | TPOT will optimize a pipeline in the shape of a directed acyclic graph. The nodes of the graph can include selectors, scalers, transformers, or classifiers/regressors (inner classifiers/regressors can optionally be not included). This will return a custom GraphPipeline rather than an sklearn Pipeline. More details in Tutorial 6. |\n        | graph-light | Same as graph search space, but without the inner classifier/regressors and with a reduced set of faster running estimators. |\n        | mdr |TPOT will search over a series of feature selectors and Multifactor Dimensionality Reduction models to find a series of operators that maximize prediction accuracy. The TPOT MDR configuration is specialized for genome-wide association studies (GWAS), and is described in detail online here.\n\n        Note that TPOT MDR may be slow to run because the feature selection routines are computationally expensive, especially on large datasets. |\n        - SearchSpace : The search space to use for the optimization. This should be an instance of a SearchSpace.\n            The search space to use for the optimization. This should be an instance of a SearchSpace.\n            TPOT has groups of search spaces found in the following folders, tpot.search_spaces.nodes for the nodes in the pipeline and tpot.search_spaces.pipelines for the pipeline structure.\n\n    scorers : (list, scorer)\n        A scorer or list of scorers to be used in the cross-validation process.\n        see https://scikit-learn.org/stable/modules/model_evaluation.html\n\n    scorers_weights : list\n        A list of weights to be applied to the scorers during the optimization process.\n\n    classification : bool\n        If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem.\n        Used to determine the CV strategy.\n\n    cv : int, cross-validator\n        - (int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.\n        - (sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.\n            - max_depth (int): The maximum depth from any node to the root of the pipelines to be generated.\n\n    other_objective_functions : list, default=[]\n        A list of other objective functions to apply to the pipeline. The function takes a single parameter for the graphpipeline estimator and returns either a single score or a list of scores.\n\n    other_objective_functions_weights : list, default=[]\n        A list of weights to be applied to the other objective functions.\n\n    objective_function_names : list, default=None\n        A list of names to be applied to the objective functions. If None, will use the names of the objective functions.\n\n    bigger_is_better : bool, default=True\n        If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n\n    categorical_features : list or None\n        Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n\n    categorical_features: list or None\n        Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n        - None : If None, TPOT will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing.\n        - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices\n\n\n    memory: Memory object or string, default=None\n        If supplied, pipeline will cache each transformer after calling fit with joblib.Memory. This feature\n        is used to avoid computing the fit transformers within a pipeline if the parameters\n        and input data are identical with another fitted pipeline during optimization process.\n        - String 'auto':\n            TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.\n        - String path of a caching directory\n            TPOT uses memory caching with the provided directory and TPOT does NOT clean\n            the caching directory up upon shutdown. If the directory does not exist, TPOT will\n            create it.\n        - Memory object:\n            TPOT uses the instance of joblib.Memory for memory caching,\n            and TPOT does NOT clean the caching directory up upon shutdown.\n        - None:\n            TPOT does not use memory caching.\n\n    preprocessing : bool or BaseEstimator/Pipeline,\n        EXPERIMENTAL\n        A pipeline that will be used to preprocess the data before CV. Note that the parameters for these steps are not optimized. Add them to the search space to be optimized.\n        - bool : If True, will use a default preprocessing pipeline which includes imputation followed by one hot encoding.\n        - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.\n\n    max_time_mins : float, default=float(\"inf\")\n        Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n\n    max_eval_time_mins : float, default=60*5\n        Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n\n\n    n_jobs : int, default=1\n        Number of processes to run in parallel.\n\n    validation_strategy : str, default='none'\n        EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT may overfit the cross validation score. A second validation set can be used to select the final pipeline.\n        - 'auto' : Automatically determine the validation strategy based on the dataset shape.\n        - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets.\n        - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets.\n        - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.\n\n    validation_fraction : float, default=0.2\n      EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.\n\n    early_stop : int, default=None\n        Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered. In general a value of around 5-20 is good.\n\n    warm_start : bool, default=False\n        If True, will use the continue the evolutionary algorithm from the last generation of the previous run.\n\n    periodic_checkpoint_folder : str, default=None\n        Folder to save the population to periodically. If None, no periodic saving will be done.\n        If provided, training will resume from this checkpoint.\n\n\n    verbose : int, default=1\n        How much information to print during the optimization process. Higher values include the information from lower values.\n        0. nothing\n        1. progress bar\n\n        3. best individual\n        4. warnings\n        &gt;=5. full warnings trace\n        6. evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_mins in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)\n\n\n    memory_limit : str, default=None\n        Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n\n    client : dask.distributed.Client, default=None\n        A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.\n\n    random_state : int, None, default=None\n        A seed for reproducability of experiments. This value will be passed to numpy.random.default_rng() to create an instnce of the genrator to pass to other classes\n\n        - int\n            Will be used to create and lock in Generator instance with 'numpy.random.default_rng()'\n        - None\n            Will be used to create Generator for 'numpy.random.default_rng()' where a fresh, unpredictable entropy will be pulled from the OS\n\n    allow_inner_regressors : bool, default=True\n        If True, the search space will include ensembled regressors.\n\n    Attributes\n    ----------\n\n    fitted_pipeline_ : GraphPipeline\n        A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.\n\n    evaluated_individuals : A pandas data frame containing data for all evaluated individuals in the run.\n        Columns:\n        - *objective functions : The first few columns correspond to the passed in scorers and objective functions\n        - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population.\n        - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population.\n        - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator.\n        - Generation : The generation the pipeline first appeared.\n        - Pareto_Front\t: The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.\n                        To save on computational time, the best frontier is updated iteratively each generation.\n                        The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.\n                        All other pipelines are set to NaN.\n        - Instance\t: The unfitted GraphPipeline BaseEstimator.\n        - *validation objective functions : Objective function scores evaluated on the validation set.\n        - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.\n\n    pareto_front : The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.\n    '''\n\n    self.search_space = search_space\n    self.scorers = scorers\n    self.scorers_weights = scorers_weights\n    self.cv = cv\n    self.other_objective_functions = other_objective_functions\n    self.other_objective_functions_weights = other_objective_functions_weights\n    self.objective_function_names = objective_function_names\n    self.bigger_is_better = bigger_is_better\n    self.categorical_features = categorical_features\n    self.memory = memory\n    self.preprocessing = preprocessing\n    self.max_time_mins = max_time_mins\n    self.max_eval_time_mins = max_eval_time_mins\n    self.n_jobs = n_jobs\n    self.validation_strategy = validation_strategy\n    self.validation_fraction = validation_fraction\n    self.early_stop = early_stop\n    self.warm_start = warm_start\n    self.periodic_checkpoint_folder = periodic_checkpoint_folder\n    self.verbose = verbose\n    self.memory_limit = memory_limit\n    self.client = client\n    self.random_state = random_state\n    self.allow_inner_regressors = allow_inner_regressors\n    self.tpotestimator_kwargs = tpotestimator_kwargs\n\n    self.initialized = False\n</code></pre>"}]}